[
  {
    "code": "@@ -31,25 +31,24 @@ def get_api_key(\n         4. api_key setting in config.json\n     \"\"\"\n     # 1\n-    if \"api_key\" in create_args:\n-        return create_args[\"api_key\"]\n+    if f\"{api_name.lower()}_api_key\" in create_args:\n+        return create_args[f\"{api_name.lower()}_api_key\"]\n     # 2\n     connection_args = engine_storage.get_connection_args()\n-    if \"api_key\" in connection_args:\n-        return connection_args[\"api_key\"]\n+    if f\"{api_name.lower()}_api_key\" in connection_args:\n+        return connection_args[f\"{api_name.lower()}_api_key\"]\n     # 3\n-    api_key = os.getenv(f\"{api_name.upper()}_API_KEY\")\n+    api_key = os.getenv(f\"{api_name.lower()}_api_key\")\n     if api_key is not None:\n         return api_key\n     # 4\n     config = Config()\n     api_cfg = config.get(api_name, {})\n-    if \"api_key\" in api_cfg:\n-        return api_cfg[\"api_key\"]\n+    if f\"{api_name.lower()}_api_key\" in api_cfg:\n+        return api_cfg[f\"{api_name.lower()}_api_key\"]\n \n     if strict:\n         raise Exception(\n-            'Missing API key \"api_key\". Either re-create this ML_ENGINE specifying the `api_key` parameter,\\\n-            or re-create this model and pass the API key with `USING` syntax.'\n+            'Missing API key \"api_key\". Either re-create this ML_ENGINE specifying the `api_key` parameter, or re-create this model and pass the API key with `USING` syntax.'",
    "comment": "Nit: the message ideally would be `Missing API key f'\"{api_name.lower()}_api_key\"'...` so that the user knows the exact parameter they are missing.",
    "line_number": 52,
    "enriched": "File: mindsdb/integrations/utilities/handler_utils.py\nCode: @@ -31,25 +31,24 @@ def get_api_key(\n         4. api_key setting in config.json\n     \"\"\"\n     # 1\n-    if \"api_key\" in create_args:\n-        return create_args[\"api_key\"]\n+    if f\"{api_name.lower()}_api_key\" in create_args:\n+        return create_args[f\"{api_name.lower()}_api_key\"]\n     # 2\n     connection_args = engine_storage.get_connection_args()\n-    if \"api_key\" in connection_args:\n-        return connection_args[\"api_key\"]\n+    if f\"{api_name.lower()}_api_key\" in connection_args:\n+        return connection_args[f\"{api_name.lower()}_api_key\"]\n     # 3\n-    api_key = os.getenv(f\"{api_name.upper()}_API_KEY\")\n+    api_key = os.getenv(f\"{api_name.lower()}_api_key\")\n     if api_key is not None:\n         return api_key\n     # 4\n     config = Config()\n     api_cfg = config.get(api_name, {})\n-    if \"api_key\" in api_cfg:\n-        return api_cfg[\"api_key\"]\n+    if f\"{api_name.lower()}_api_key\" in api_cfg:\n+        return api_cfg[f\"{api_name.lower()}_api_key\"]\n \n     if strict:\n         raise Exception(\n-            'Missing API key \"api_key\". Either re-create this ML_ENGINE specifying the `api_key` parameter,\\\n-            or re-create this model and pass the API key with `USING` syntax.'\n+            'Missing API key \"api_key\". Either re-create this ML_ENGINE specifying the `api_key` parameter, or re-create this model and pass the API key with `USING` syntax.'\nComment: Nit: the message ideally would be `Missing API key f'\"{api_name.lower()}_api_key\"'...` so that the user knows the exact parameter they are missing.",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "mindsdb/integrations/utilities/handler_utils.py",
    "pr_number": 8245,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1406981645,
    "comment_created_at": "2023-11-28T01:57:08Z"
  },
  {
    "code": "@@ -2,38 +2,66 @@\n \n > **Please submit your PR in the following format after the underline below `Results` section. Don't forget to add an underline after adding your changes i.e., at the end of your `Results` section.**\n \n-## Testing MySQL Handler with [Dataset Name](URL to the Dataset)\n+## Testing MySQL Handler with [Dry Bean Dataset](https://www.kaggle.com/datasets/muratkokludataset/dry-bean-dataset)\n \n **1. Testing CREATE DATABASE**\n \n-```\n-COMMAND THAT YOU RAN TO CREATE DATABASE.\n+```sql\n+CREATE DATABASE mysql_test                  --- datasource connection name\n+WITH ENGINE = 'mysql',                      --- database engine\n+PARAMETERS = {\n+    \"host\": \"cloud.mindsdb.com\",            --- host name can be an IP address, or URL",
    "comment": "I see you are still using `\"host\": \"cloud.mindsdb.com\"`, and thus, connecting MindsDB to itself.\r\n\r\nThe idea is that you have your own MySQL database (here is how you can install MySQL database: https://dev.mysql.com/doc/mysql-getting-started/en/#mysql-getting-started-installing).\r\n\r\nAnd then you connect this database to MindsDB:\r\n```\r\nCREATE DATABASE mysql_test                  --- datasource connection name\r\nWITH ENGINE = 'mysql',                      --- database engine\r\nPARAMETERS = {\r\n    \"host\": \"localhost\",            --- host name can be an IP address, or URL\r\n    \"port\": 3306,                           --- port used to make TCP/IP connection\r\n    \"database\": \"dry_bean_db\",              --- database name\r\n    \"user\": \"demo-user\",                    --- database user\r\n    \"password\": \"demo-password\"             --- database password\r\n};\r\n```\r\n\r\nIf you use local database, you may need to [setup an ngrok channel](https://docs.mindsdb.com/sql/create/database#making-your-local-database-available-to-mindsdb) and use provided values for host and port.",
    "line_number": 13,
    "enriched": "File: mindsdb/integrations/handlers/mysql_handler/Manual_QA.md\nCode: @@ -2,38 +2,66 @@\n \n > **Please submit your PR in the following format after the underline below `Results` section. Don't forget to add an underline after adding your changes i.e., at the end of your `Results` section.**\n \n-## Testing MySQL Handler with [Dataset Name](URL to the Dataset)\n+## Testing MySQL Handler with [Dry Bean Dataset](https://www.kaggle.com/datasets/muratkokludataset/dry-bean-dataset)\n \n **1. Testing CREATE DATABASE**\n \n-```\n-COMMAND THAT YOU RAN TO CREATE DATABASE.\n+```sql\n+CREATE DATABASE mysql_test                  --- datasource connection name\n+WITH ENGINE = 'mysql',                      --- database engine\n+PARAMETERS = {\n+    \"host\": \"cloud.mindsdb.com\",            --- host name can be an IP address, or URL\nComment: I see you are still using `\"host\": \"cloud.mindsdb.com\"`, and thus, connecting MindsDB to itself.\r\n\r\nThe idea is that you have your own MySQL database (here is how you can install MySQL database: https://dev.mysql.com/doc/mysql-getting-started/en/#mysql-getting-started-installing).\r\n\r\nAnd then you connect this database to MindsDB:\r\n```\r\nCREATE DATABASE mysql_test                  --- datasource connection name\r\nWITH ENGINE = 'mysql',                      --- database engine\r\nPARAMETERS = {\r\n    \"host\": \"localhost\",            --- host name can be an IP address, or URL\r\n    \"port\": 3306,                           --- port used to make TCP/IP connection\r\n    \"database\": \"dry_bean_db\",              --- database name\r\n    \"user\": \"demo-user\",                    --- database user\r\n    \"password\": \"demo-password\"             --- database password\r\n};\r\n```\r\n\r\nIf you use local database, you may need to [setup an ngrok channel](https://docs.mindsdb.com/sql/create/database#making-your-local-database-available-to-mindsdb) and use provided values for host and port.",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "mindsdb/integrations/handlers/mysql_handler/Manual_QA.md",
    "pr_number": 6282,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1210026157,
    "comment_created_at": "2023-05-30T09:49:13Z"
  },
  {
    "code": "@@ -128,8 +128,7 @@ MindsDB works with most of the SQL and NoSQL databases, data lakes, data Streams\n | <a href=\"https://docs.mindsdb.com/data-integrations/mongodb\"><img src=\"https://img.shields.io/badge/MongoDB-4EA94B?style=for-the-badge&logo=mongodb&logoColor=white\" alt=\"Connect MongoDB\"></a> | <a href=\"https://docs.mindsdb.com/data-integrations/d0lt\"><img src=\"https://img.shields.io/badge/DoIt-00B388?logo=DoIt&logoColor=fff&style=for-the-badge\" alt=\"DoIt Badge\"></a> | <a href=\"https://docs.mindsdb.com/data-integrations/amazon-dynamodb\"><img src=\"https://img.shields.io/badge/Amazon%20DynamoDB-4053D6?logo=amazondynamodb&logoColor=fff&style=for-the-badge\" alt=\"Amazon DynamoDB Badge\"></a> |\n | <a href=\"https://docs.mindsdb.com/data-integrations/postgresql\"><img src=\"https://img.shields.io/badge/PostgreSQL-316192?style=for-the-badge&logo=postgresql&logoColor=white\" alt=\"Connect PostgreSQL\"></a> | <a href=\"https://docs.mindsdb.com/data-integrations/ibm-db2\"><img src=\"https://img.shields.io/badge/IBMDB2-008000?logo=IBMDB2&logoColor=fff&style=for-the-badge\" alt=\"IBMDB2 Badge\"></a> | <a href=\"https://docs.mindsdb.com/data-integrations/opengauss\"><img src=\"https://img.shields.io/badge/openGauss-02458D?logo=opengauss&logoColor=fff&style=for-the-badge\" alt=\"openGauss Badge\"></a> |\n | <a href=\"https://docs.mindsdb.com/data-integrations/mysql\"><img src=\"https://img.shields.io/badge/MySQL-00758F?style=for-the-badge&logo=mysql&logoColor=white\" alt=\"Connect MySQL\"></a> | <a href=\" https://docs.mindsdb.com/data-integrations/databricks\"><img src=\"https://img.shields.io/badge/Databricks-FF3621?logo=databricks&logoColor=fff&style=for-the-badge\" alt=\"Databricks Badge\"></a> | <a href=\"https://docs.mindsdb.com/data-integrations/yugabytedb\"><img src=\"https://img.shields.io/badge/YugabyteDB-02458D?logo=yugabytedb&logoColor=fff&style=for-the-badge\" alt=\"YugabyteDB Badge\"></a>\n-| <a href=\"https://docs.mindsdb.com/data-integrations/questdb\"><img src=\"https://img.shields.io/badge/QuestDB-d14671?style=for-the-badge&logo=questdb&logoColor=white\" alt=\"Connect QuestDB\"></a> | <a href=\"https://docs.mindsdb.com/data-integrations/apache-druid\"><img src=\"https://img.shields.io/badge/Apache%20Druid-29F1FB?logo=apachedruid&logoColor=000&style=for-the-badge\" alt=\"Apache Druid Badge\"></a> | <a href=\"https://docs.mindsdb.com/data-integrations/starrocks\"><img src=\"https://img.shields.io/badge/starrocks-000000?style=for-the-badge&logo=starrocks&logoColor=white\" alt=\"StarRocks Badge\"></a> |\n-| <a href=\"https://docs.mindsdb.com/\"><img src=\"https://img.shields.io/badge/redis-%23DD0031.svg?&style=for-the-badge&logo=redis&logoColor=white\" alt=\"Connect Redis\"></a> | <a href=\"https://docs.mindsdb.com/data-integrations/vertica\"><img src=\"https://img.shields.io/badge/Vertica-000?logo=vertica&logoColor=fff&style=for-the-badge\" alt=\"Vertica Badge\"></a> | <a href=\"https://docs.mindsdb.com/data-integrations/duckdb\"><img src=\"https://img.shields.io/badge/duckdb-fff?logo=duckdb&logoColor=000&style=for-the-badge\" alt=\"DuckDB Badge\"></a> |\n+| <a href=\"https://docs.mindsdb.com/data-integrations/questdb\"><img src=\"https://img.shields.io/badge/QuestDB-d14671?style=for-the-badge&logo=questdb&logoColor=white\" alt=\"Connect QuestDB\"></a> | <a href=\"https://docs.mindsdb.com/data-integrations/apache-druid\"><img src=\"https://img.shields.io/badge/Apache%20Druid-29F1FB?logo=apachedruid&logoColor=000&style=for-the-badge\" alt=\"Apache Druid Badge\"></a> | <a href=\"https://docs.mindsdb.com/data-integrations/starrocks\"><img src=\"https://img.shields.io/badge/starrocks-000000?style=for-the-badge&logo=starrocks&logoColor=white\" alt=\"StarRocks Badge\"></a> | <a href=\"https://docs.mindsdb.com/data-integrations/vertica\"><img src=\"https://img.shields.io/badge/Vertica-000?logo=vertica&logoColor=fff&style=for-the-badge\" alt=\"Vertica Badge\"></a> | <a href=\"https://docs.mindsdb.com/data-integrations/duckdb\"><img src=\"https://img.shields.io/badge/duckdb-fff?logo=duckdb&logoColor=000&style=for-the-badge\" alt=\"DuckDB Badge\"></a> |",
    "comment": "You merged lines 131 and 132 such that now one line contains 5 tags.\r\n\r\nPlease correct it to have one line with 2 tags and another line with 3 tags. For example:\r\n\r\nline 131:\r\n```\r\n| <a href=\"https://docs.mindsdb.com/data-integrations/questdb\"><img src=\"https://img.shields.io/badge/QuestDB-d14671?style=for-the-badge&logo=questdb&logoColor=white\" alt=\"Connect QuestDB\"></a> | <a href=\"https://docs.mindsdb.com/data-integrations/apache-druid\"><img src=\"https://img.shields.io/badge/Apache%20Druid-29F1FB?logo=apachedruid&logoColor=000&style=for-the-badge\" alt=\"Apache Druid Badge\"></a> | <a href=\"https://docs.mindsdb.com/data-integrations/starrocks\"><img src=\"https://img.shields.io/badge/starrocks-000000?style=for-the-badge&logo=starrocks&logoColor=white\" alt=\"StarRocks Badge\"></a> |\r\n```\r\n\r\nline 132:\r\n```\r\n| <a href=\"https://docs.mindsdb.com/data-integrations/vertica\"><img src=\"https://img.shields.io/badge/Vertica-000?logo=vertica&logoColor=fff&style=for-the-badge\" alt=\"Vertica Badge\"></a> | <a href=\"https://docs.mindsdb.com/data-integrations/duckdb\"><img src=\"https://img.shields.io/badge/duckdb-fff?logo=duckdb&logoColor=000&style=for-the-badge\" alt=\"DuckDB Badge\"></a> |\r\n```",
    "line_number": 131,
    "enriched": "File: README.md\nCode: @@ -128,8 +128,7 @@ MindsDB works with most of the SQL and NoSQL databases, data lakes, data Streams\n | <a href=\"https://docs.mindsdb.com/data-integrations/mongodb\"><img src=\"https://img.shields.io/badge/MongoDB-4EA94B?style=for-the-badge&logo=mongodb&logoColor=white\" alt=\"Connect MongoDB\"></a> | <a href=\"https://docs.mindsdb.com/data-integrations/d0lt\"><img src=\"https://img.shields.io/badge/DoIt-00B388?logo=DoIt&logoColor=fff&style=for-the-badge\" alt=\"DoIt Badge\"></a> | <a href=\"https://docs.mindsdb.com/data-integrations/amazon-dynamodb\"><img src=\"https://img.shields.io/badge/Amazon%20DynamoDB-4053D6?logo=amazondynamodb&logoColor=fff&style=for-the-badge\" alt=\"Amazon DynamoDB Badge\"></a> |\n | <a href=\"https://docs.mindsdb.com/data-integrations/postgresql\"><img src=\"https://img.shields.io/badge/PostgreSQL-316192?style=for-the-badge&logo=postgresql&logoColor=white\" alt=\"Connect PostgreSQL\"></a> | <a href=\"https://docs.mindsdb.com/data-integrations/ibm-db2\"><img src=\"https://img.shields.io/badge/IBMDB2-008000?logo=IBMDB2&logoColor=fff&style=for-the-badge\" alt=\"IBMDB2 Badge\"></a> | <a href=\"https://docs.mindsdb.com/data-integrations/opengauss\"><img src=\"https://img.shields.io/badge/openGauss-02458D?logo=opengauss&logoColor=fff&style=for-the-badge\" alt=\"openGauss Badge\"></a> |\n | <a href=\"https://docs.mindsdb.com/data-integrations/mysql\"><img src=\"https://img.shields.io/badge/MySQL-00758F?style=for-the-badge&logo=mysql&logoColor=white\" alt=\"Connect MySQL\"></a> | <a href=\" https://docs.mindsdb.com/data-integrations/databricks\"><img src=\"https://img.shields.io/badge/Databricks-FF3621?logo=databricks&logoColor=fff&style=for-the-badge\" alt=\"Databricks Badge\"></a> | <a href=\"https://docs.mindsdb.com/data-integrations/yugabytedb\"><img src=\"https://img.shields.io/badge/YugabyteDB-02458D?logo=yugabytedb&logoColor=fff&style=for-the-badge\" alt=\"YugabyteDB Badge\"></a>\n-| <a href=\"https://docs.mindsdb.com/data-integrations/questdb\"><img src=\"https://img.shields.io/badge/QuestDB-d14671?style=for-the-badge&logo=questdb&logoColor=white\" alt=\"Connect QuestDB\"></a> | <a href=\"https://docs.mindsdb.com/data-integrations/apache-druid\"><img src=\"https://img.shields.io/badge/Apache%20Druid-29F1FB?logo=apachedruid&logoColor=000&style=for-the-badge\" alt=\"Apache Druid Badge\"></a> | <a href=\"https://docs.mindsdb.com/data-integrations/starrocks\"><img src=\"https://img.shields.io/badge/starrocks-000000?style=for-the-badge&logo=starrocks&logoColor=white\" alt=\"StarRocks Badge\"></a> |\n-| <a href=\"https://docs.mindsdb.com/\"><img src=\"https://img.shields.io/badge/redis-%23DD0031.svg?&style=for-the-badge&logo=redis&logoColor=white\" alt=\"Connect Redis\"></a> | <a href=\"https://docs.mindsdb.com/data-integrations/vertica\"><img src=\"https://img.shields.io/badge/Vertica-000?logo=vertica&logoColor=fff&style=for-the-badge\" alt=\"Vertica Badge\"></a> | <a href=\"https://docs.mindsdb.com/data-integrations/duckdb\"><img src=\"https://img.shields.io/badge/duckdb-fff?logo=duckdb&logoColor=000&style=for-the-badge\" alt=\"DuckDB Badge\"></a> |\n+| <a href=\"https://docs.mindsdb.com/data-integrations/questdb\"><img src=\"https://img.shields.io/badge/QuestDB-d14671?style=for-the-badge&logo=questdb&logoColor=white\" alt=\"Connect QuestDB\"></a> | <a href=\"https://docs.mindsdb.com/data-integrations/apache-druid\"><img src=\"https://img.shields.io/badge/Apache%20Druid-29F1FB?logo=apachedruid&logoColor=000&style=for-the-badge\" alt=\"Apache Druid Badge\"></a> | <a href=\"https://docs.mindsdb.com/data-integrations/starrocks\"><img src=\"https://img.shields.io/badge/starrocks-000000?style=for-the-badge&logo=starrocks&logoColor=white\" alt=\"StarRocks Badge\"></a> | <a href=\"https://docs.mindsdb.com/data-integrations/vertica\"><img src=\"https://img.shields.io/badge/Vertica-000?logo=vertica&logoColor=fff&style=for-the-badge\" alt=\"Vertica Badge\"></a> | <a href=\"https://docs.mindsdb.com/data-integrations/duckdb\"><img src=\"https://img.shields.io/badge/duckdb-fff?logo=duckdb&logoColor=000&style=for-the-badge\" alt=\"DuckDB Badge\"></a> |\nComment: You merged lines 131 and 132 such that now one line contains 5 tags.\r\n\r\nPlease correct it to have one line with 2 tags and another line with 3 tags. For example:\r\n\r\nline 131:\r\n```\r\n| <a href=\"https://docs.mindsdb.com/data-integrations/questdb\"><img src=\"https://img.shields.io/badge/QuestDB-d14671?style=for-the-badge&logo=questdb&logoColor=white\" alt=\"Connect QuestDB\"></a> | <a href=\"https://docs.mindsdb.com/data-integrations/apache-druid\"><img src=\"https://img.shields.io/badge/Apache%20Druid-29F1FB?logo=apachedruid&logoColor=000&style=for-the-badge\" alt=\"Apache Druid Badge\"></a> | <a href=\"https://docs.mindsdb.com/data-integrations/starrocks\"><img src=\"https://img.shields.io/badge/starrocks-000000?style=for-the-badge&logo=starrocks&logoColor=white\" alt=\"StarRocks Badge\"></a> |\r\n```\r\n\r\nline 132:\r\n```\r\n| <a href=\"https://docs.mindsdb.com/data-integrations/vertica\"><img src=\"https://img.shields.io/badge/Vertica-000?logo=vertica&logoColor=fff&style=for-the-badge\" alt=\"Vertica Badge\"></a> | <a href=\"https://docs.mindsdb.com/data-integrations/duckdb\"><img src=\"https://img.shields.io/badge/duckdb-fff?logo=duckdb&logoColor=000&style=for-the-badge\" alt=\"DuckDB Badge\"></a> |\r\n```",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "README.md",
    "pr_number": 6214,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1200406075,
    "comment_created_at": "2023-05-22T11:49:56Z"
  },
  {
    "code": "@@ -15,7 +15,7 @@ The required arguments to establish a connection are as follows:\n \n * `user` is the user to authenticate.\n * `password` is the password to authenticate the user.\n-* `secure_connection_bundle` is the path to the `secure_connection_bundle` zip file.\n+* `secure_connection_bundle` is the path to the `secure_connect_bundle` zip file.",
    "comment": "```\r\n`secure_connect_bundle` is the path to the `secure_connect_bundle` zip file.\r\n```\r\nor not?",
    "line_number": 18,
    "enriched": "File: docs/integrations/data-integrations/datastax.mdx\nCode: @@ -15,7 +15,7 @@ The required arguments to establish a connection are as follows:\n \n * `user` is the user to authenticate.\n * `password` is the password to authenticate the user.\n-* `secure_connection_bundle` is the path to the `secure_connection_bundle` zip file.\n+* `secure_connection_bundle` is the path to the `secure_connect_bundle` zip file.\nComment: ```\r\n`secure_connect_bundle` is the path to the `secure_connect_bundle` zip file.\r\n```\r\nor not?",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/integrations/data-integrations/datastax.mdx",
    "pr_number": 7302,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1322649570,
    "comment_created_at": "2023-09-12T08:30:40Z"
  },
  {
    "code": "@@ -71,4 +87,18 @@ WHERE query = 'search query';\n \n Here, the `query` column is the name of the column containing the search queries as specified in the `query_column` parameter of the `CREATE MODEL` statement.\n \n-Note: At the moment, only a single query can be specified in the `WHERE` clause of the query. The `JOIN` clause for making multiple predictions will be added in a future release.\n\\ No newline at end of file\n+Note: At the moment, only a single query can be specified in the `WHERE` clause of the query. The `JOIN` clause for making multiple predictions will be added in a future release.\n+\n+### Summarization\n+```sql\n+SELECT *\n+FROM mindsdb.twelve_labs_summarization\n+WHERE video_id = 'video_1';\n+```\n+\n+Here, the video IDs that were indexed by a model can be found by running a `DESCRIBE` statement on the it. The URL or file path of the video will be available in the `video_reference` column. The following is an example of how to run such a `DESCRIBE` statement,\n+```sql\n+DESCRIBE mindsdb.twelve_labs_summarization.indexed_videos;\n+```\n+",
    "comment": "Let's include a response of the DESCRIBE as an example here",
    "line_number": 103,
    "enriched": "File: mindsdb/integrations/handlers/twelve_labs_handler/README.md\nCode: @@ -71,4 +87,18 @@ WHERE query = 'search query';\n \n Here, the `query` column is the name of the column containing the search queries as specified in the `query_column` parameter of the `CREATE MODEL` statement.\n \n-Note: At the moment, only a single query can be specified in the `WHERE` clause of the query. The `JOIN` clause for making multiple predictions will be added in a future release.\n\\ No newline at end of file\n+Note: At the moment, only a single query can be specified in the `WHERE` clause of the query. The `JOIN` clause for making multiple predictions will be added in a future release.\n+\n+### Summarization\n+```sql\n+SELECT *\n+FROM mindsdb.twelve_labs_summarization\n+WHERE video_id = 'video_1';\n+```\n+\n+Here, the video IDs that were indexed by a model can be found by running a `DESCRIBE` statement on the it. The URL or file path of the video will be available in the `video_reference` column. The following is an example of how to run such a `DESCRIBE` statement,\n+```sql\n+DESCRIBE mindsdb.twelve_labs_summarization.indexed_videos;\n+```\n+\nComment: Let's include a response of the DESCRIBE as an example here",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "mindsdb/integrations/handlers/twelve_labs_handler/README.md",
    "pr_number": 8795,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1500527027,
    "comment_created_at": "2024-02-23T11:20:55Z"
  },
  {
    "code": "@@ -41,9 +41,16 @@ Where:\n -   `docker run` is a native Docker command used to spin up a container.\n -   `--name mindsdb_container` defines a name for the container.\n -   `-p 47334:47334` publishes port 47334 to access MindsDB GUI and HTTP API.\n--   `-p 47335:47335` publishes port 47335 to access MindsDB MySQL API.\n -   `mindsdb/mindsdb` is a Docker image provided by MindsDB. You can choose a different one from the list above.\n \n+<Tip>\n+By default, MindsDB starts only the `http` API. You can define which APIs to start by passing the `MINDSDB_APIS` environment variable with a comma-separated list when running the container as shown below. To expose the ports for the APIs, you need to add the respective ports to the command with the `-p` flag.",
    "comment": "Thank you Minura. This looks good. A couple of small things:\r\n\r\nLet's change\r\n\\`http\\` API\r\nto\r\nHTTP API\r\nlike it is above.\r\n\r\nAlso, please let's add a link to the environment variables doc page.",
    "line_number": 47,
    "enriched": "File: docs/setup/self-hosted/docker.mdx\nCode: @@ -41,9 +41,16 @@ Where:\n -   `docker run` is a native Docker command used to spin up a container.\n -   `--name mindsdb_container` defines a name for the container.\n -   `-p 47334:47334` publishes port 47334 to access MindsDB GUI and HTTP API.\n--   `-p 47335:47335` publishes port 47335 to access MindsDB MySQL API.\n -   `mindsdb/mindsdb` is a Docker image provided by MindsDB. You can choose a different one from the list above.\n \n+<Tip>\n+By default, MindsDB starts only the `http` API. You can define which APIs to start by passing the `MINDSDB_APIS` environment variable with a comma-separated list when running the container as shown below. To expose the ports for the APIs, you need to add the respective ports to the command with the `-p` flag.\nComment: Thank you Minura. This looks good. A couple of small things:\r\n\r\nLet's change\r\n\\`http\\` API\r\nto\r\nHTTP API\r\nlike it is above.\r\n\r\nAlso, please let's add a link to the environment variables doc page.",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/setup/self-hosted/docker.mdx",
    "pr_number": 10360,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1914545172,
    "comment_created_at": "2025-01-14T09:56:53Z"
  },
  {
    "code": "@@ -23,6 +23,8 @@ services:\n       # SENTRY_IO_FORCE_RUN: \"true\" # Uncomment me to force-start sentry on local development. Good for profiling, but may annoy other devs on sentry.io with the \"noise\"\n       # MINDSDB_LOG_LEVEL: \"DEBUG\"\n       # OPENAI_API_KEY: \"...\"\n+      \n+      # If you wish to perform observing/tracing for AI-driven agents/conversations then add your LangFuse details here.  Supports cloud and self-hosted\n       LANGFUSE_HOST: \"OMITTED_FOR_SECURITY\"\n       LANGFUSE_PUBLIC_KEY: \"OMITTED_FOR_SECURITY\"\n       LANGFUSE_SECRET_KEY: \"OMITTED_FOR_SECURITY\"",
    "comment": "The comment says Langfuse is optional, but there’s no default fallback in the code if these vars are missing. If Langfuse is offline, runtime should skip cleanly — make sure the code reflects this, not just the comment.\r\n\r\n",
    "line_number": 30,
    "enriched": "File: docker-compose.yml\nCode: @@ -23,6 +23,8 @@ services:\n       # SENTRY_IO_FORCE_RUN: \"true\" # Uncomment me to force-start sentry on local development. Good for profiling, but may annoy other devs on sentry.io with the \"noise\"\n       # MINDSDB_LOG_LEVEL: \"DEBUG\"\n       # OPENAI_API_KEY: \"...\"\n+      \n+      # If you wish to perform observing/tracing for AI-driven agents/conversations then add your LangFuse details here.  Supports cloud and self-hosted\n       LANGFUSE_HOST: \"OMITTED_FOR_SECURITY\"\n       LANGFUSE_PUBLIC_KEY: \"OMITTED_FOR_SECURITY\"\n       LANGFUSE_SECRET_KEY: \"OMITTED_FOR_SECURITY\"\nComment: The comment says Langfuse is optional, but there’s no default fallback in the code if these vars are missing. If Langfuse is offline, runtime should skip cleanly — make sure the code reflects this, not just the comment.\r\n\r\n",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docker-compose.yml",
    "pr_number": 9800,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2105769414,
    "comment_created_at": "2025-05-24T09:27:01Z"
  },
  {
    "code": "@@ -0,0 +1,295 @@\n+---\n+title: How MindsDB & Hugging Face Can Help Stop Hate-Speech\n+sidebarTitle: Use MindsDB's, Hugging Face and MySQL Datasource to Perform NLP Queries that Identify Hate Speech in Your Dataset\n+---\n+\n+## Introduction\n+\n+In today's world, digital conversation is king. Whether it be text message, email, or social media, we do the majority of our communicating in a virtual setting. With all this information spreading around, negative language is bound to start showing up. \n+\n+Hate speech is defined as, 'abusive or threatening speech or writing that expresses prejudice based on ethnicity, religion, sexual orientation, or similar grounds'. This offensive language comes in many forms, including memes, messages, social posts, gestures, and icons (even emojis can be used the wrong way these days).",
    "comment": "The paragraph explaining hate speech is not necessary. Please keep it like this:\r\n\r\n> negative language, also known as hate speech, is bound to start showing up.",
    "line_number": 10,
    "enriched": "File: docs/tutorials/text-classification-tutorial.mdx\nCode: @@ -0,0 +1,295 @@\n+---\n+title: How MindsDB & Hugging Face Can Help Stop Hate-Speech\n+sidebarTitle: Use MindsDB's, Hugging Face and MySQL Datasource to Perform NLP Queries that Identify Hate Speech in Your Dataset\n+---\n+\n+## Introduction\n+\n+In today's world, digital conversation is king. Whether it be text message, email, or social media, we do the majority of our communicating in a virtual setting. With all this information spreading around, negative language is bound to start showing up. \n+\n+Hate speech is defined as, 'abusive or threatening speech or writing that expresses prejudice based on ethnicity, religion, sexual orientation, or similar grounds'. This offensive language comes in many forms, including memes, messages, social posts, gestures, and icons (even emojis can be used the wrong way these days).\nComment: The paragraph explaining hate speech is not necessary. Please keep it like this:\r\n\r\n> negative language, also known as hate speech, is bound to start showing up.",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/tutorials/text-classification-tutorial.mdx",
    "pr_number": 5301,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1153386149,
    "comment_created_at": "2023-03-30T14:52:05Z"
  },
  {
    "code": "@@ -1,74 +1,106 @@\n-# SAP HANA Handler\n+---\n+title: SAP HANA\n+sidebarTitle: SAP HANA\n+---\n \n-This is the implementation of the SAP HANA handler for MindsDB.\n+This documentation describes the integration of MindsDB with [SAP HANA](https://www.sap.com/products/technology-platform/hana/what-is-sap-hana.html), a multi-model database with a column-oriented in-memory design that stores data in its memory instead of keeping it on a disk.\n+The integration allows MindsDB to access data from SAP HANA and enhance SAP HANA with AI capabilities.\n \n-## SAP HANA\n+## Prerequisites\n \n-SAP HANA (High-performance ANalytic Appliance) is a multi-model database that stores data in its memory instead of keeping it on a disk. The column-oriented in-memory database design used by SAP HANA allows users to run advanced analytics alongside high-speed transactions in a single system. [Read more](https://www.sap.com/products/technology-platform/hana/what-is-sap-hana.html).\n+Before proceeding, ensure the following prerequisites are met:\n \n-## Implementation\n+1. Install MindsDB locally via [Docker](https://docs.mindsdb.com/setup/self-hosted/docker) or [Docker Desktop](https://docs.mindsdb.com/setup/self-hosted/docker-desktop).\n+2. To connect SAP HANA to MindsDB, install the required dependencies following [this instruction](https://docs.mindsdb.com/setup/self-hosted/docker#install-dependencies).\n \n-This handler was implemented using `hdbcli` - the Python driver for SAP HANA.\n+## Connection\n \n-The required arguments to establish a connection are,\n+Establish a connection to SAP HANA from MindsDB by executing the following SQL command and providing its [handler name](https://github.com/mindsdb/mindsdb/tree/main/mindsdb/integrations/handlers/hana_handler) as an engine.\n \n-* `host`: the host name or IP address of the SAP HANA instance\n-* `port`: the port number of the SAP HANA instance\n-* `user`: specifies the user name\n-* `password`: specifies the password for the user\n-* `schema`: sets the current schema, which is used for identifiers without a schema\n+```sql\n+CREATE DATABASE sap_hana_datasource\n+WITH\n+    ENGINE = 'hana',\n+    PARAMETERS = {\n+        \"host\": \"123e4567-e89b-12d3-a456-426614174000.hana.trial-us10.hanacloud.ondemand.com\",\n+        \"port\": \"443\",\n+        \"user\": \"demo_user\",\n+        \"password\": \"demo_password\",\n+        \"encrypt\": true\n+    };\n+```\n \n-## Usage\n-\n-Assuming you created a schema in SAP HANA called `MINDSDB` and you have a table called `TEST` that was created using\n-the following SQL statements:\n-\n-~~~~sql\n-CREATE SCHEMA MINDSDB;\n+Required connection parameters include the following:\n \n-CREATE TABLE MINDSDB.TEST\n-(\n-    ID          INTEGER NOT NULL,\n-    NAME        NVARCHAR(1),\n-    DESCRIPTION NVARCHAR(1)\n-);\n+* `host`: The hostname, IP address, or URL of the SAP HANA database.",
    "comment": "In the code this is `address`, maybe we change it here?",
    "line_number": 35,
    "enriched": "File: mindsdb/integrations/handlers/hana_handler/README.md\nCode: @@ -1,74 +1,106 @@\n-# SAP HANA Handler\n+---\n+title: SAP HANA\n+sidebarTitle: SAP HANA\n+---\n \n-This is the implementation of the SAP HANA handler for MindsDB.\n+This documentation describes the integration of MindsDB with [SAP HANA](https://www.sap.com/products/technology-platform/hana/what-is-sap-hana.html), a multi-model database with a column-oriented in-memory design that stores data in its memory instead of keeping it on a disk.\n+The integration allows MindsDB to access data from SAP HANA and enhance SAP HANA with AI capabilities.\n \n-## SAP HANA\n+## Prerequisites\n \n-SAP HANA (High-performance ANalytic Appliance) is a multi-model database that stores data in its memory instead of keeping it on a disk. The column-oriented in-memory database design used by SAP HANA allows users to run advanced analytics alongside high-speed transactions in a single system. [Read more](https://www.sap.com/products/technology-platform/hana/what-is-sap-hana.html).\n+Before proceeding, ensure the following prerequisites are met:\n \n-## Implementation\n+1. Install MindsDB locally via [Docker](https://docs.mindsdb.com/setup/self-hosted/docker) or [Docker Desktop](https://docs.mindsdb.com/setup/self-hosted/docker-desktop).\n+2. To connect SAP HANA to MindsDB, install the required dependencies following [this instruction](https://docs.mindsdb.com/setup/self-hosted/docker#install-dependencies).\n \n-This handler was implemented using `hdbcli` - the Python driver for SAP HANA.\n+## Connection\n \n-The required arguments to establish a connection are,\n+Establish a connection to SAP HANA from MindsDB by executing the following SQL command and providing its [handler name](https://github.com/mindsdb/mindsdb/tree/main/mindsdb/integrations/handlers/hana_handler) as an engine.\n \n-* `host`: the host name or IP address of the SAP HANA instance\n-* `port`: the port number of the SAP HANA instance\n-* `user`: specifies the user name\n-* `password`: specifies the password for the user\n-* `schema`: sets the current schema, which is used for identifiers without a schema\n+```sql\n+CREATE DATABASE sap_hana_datasource\n+WITH\n+    ENGINE = 'hana',\n+    PARAMETERS = {\n+        \"host\": \"123e4567-e89b-12d3-a456-426614174000.hana.trial-us10.hanacloud.ondemand.com\",\n+        \"port\": \"443\",\n+        \"user\": \"demo_user\",\n+        \"password\": \"demo_password\",\n+        \"encrypt\": true\n+    };\n+```\n \n-## Usage\n-\n-Assuming you created a schema in SAP HANA called `MINDSDB` and you have a table called `TEST` that was created using\n-the following SQL statements:\n-\n-~~~~sql\n-CREATE SCHEMA MINDSDB;\n+Required connection parameters include the following:\n \n-CREATE TABLE MINDSDB.TEST\n-(\n-    ID          INTEGER NOT NULL,\n-    NAME        NVARCHAR(1),\n-    DESCRIPTION NVARCHAR(1)\n-);\n+* `host`: The hostname, IP address, or URL of the SAP HANA database.\nComment: In the code this is `address`, maybe we change it here?",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "mindsdb/integrations/handlers/hana_handler/README.md",
    "pr_number": 9719,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1775170842,
    "comment_created_at": "2024-09-25T12:50:33Z"
  },
  {
    "code": "@@ -3,45 +3,75 @@ title: PostgreSQL\n sidebarTitle: PostgreSQL\n ---\n \n-This is the implementation of the PostgreSQL data handler for MindsDB.\n+This documentation describes the integration of MindsDB with PostgreSQL, a powerful, open-source, object-relational database system. \n+The integration allows for advanced SQL functionalities, extending PostgreSQL's capabilities with MindsDB's features.\n \n-[PostgreSQL](https://www.postgresql.org) is a powerful, open-source, object-relational database system that uses and extends the SQL language combined with many features that safely store and scale the most complicated data workloads.\n+## Getting Started",
    "comment": "Let's call it \"Prerequisites\" (instead of \"Getting Started\").",
    "line_number": 9,
    "enriched": "File: docs/integrations/data-integrations/postgresql.mdx\nCode: @@ -3,45 +3,75 @@ title: PostgreSQL\n sidebarTitle: PostgreSQL\n ---\n \n-This is the implementation of the PostgreSQL data handler for MindsDB.\n+This documentation describes the integration of MindsDB with PostgreSQL, a powerful, open-source, object-relational database system. \n+The integration allows for advanced SQL functionalities, extending PostgreSQL's capabilities with MindsDB's features.\n \n-[PostgreSQL](https://www.postgresql.org) is a powerful, open-source, object-relational database system that uses and extends the SQL language combined with many features that safely store and scale the most complicated data workloads.\n+## Getting Started\nComment: Let's call it \"Prerequisites\" (instead of \"Getting Started\").",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/integrations/data-integrations/postgresql.mdx",
    "pr_number": 8707,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1469872249,
    "comment_created_at": "2024-01-29T16:37:40Z"
  },
  {
    "code": "@@ -11,6 +11,12 @@ This file contains links to applications that integrate MindsDB. Please append l\n   Link: https://ritwickrajmakhal.github.io/ytinsights\n </Card>\n \n+<Card title=\"JIST\" icon=\"link\" href=\"https://github.com/syedzubeen/Jist_MindsDB_Project\">\n+  Author: Syed Zubeen\n+\n+  Link: TBD",
    "comment": "Please include the url or remove this",
    "line_number": 17,
    "enriched": "File: docs/applications_showcase.mdx\nCode: @@ -11,6 +11,12 @@ This file contains links to applications that integrate MindsDB. Please append l\n   Link: https://ritwickrajmakhal.github.io/ytinsights\n </Card>\n \n+<Card title=\"JIST\" icon=\"link\" href=\"https://github.com/syedzubeen/Jist_MindsDB_Project\">\n+  Author: Syed Zubeen\n+\n+  Link: TBD\nComment: Please include the url or remove this",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/applications_showcase.mdx",
    "pr_number": 8196,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1378809270,
    "comment_created_at": "2023-11-01T13:33:11Z"
  },
  {
    "code": "@@ -0,0 +1,146 @@\n+\n+\n+# Sentiment Analysis on French Tweets using MySQL: Connect MindsDB to Sentiment Analysis on French Tweets using MySQL\n+\n+## Introduction\n+\n+[MindsDB](https://mindsdb.com/) is is an open-source AI layer for existing databases that allows you to effortlessly develop, train and deploy state-of-the-art machine learning models using SQL queries. \n+[Hugging Face](https://huggingface.co/) is a community and data science platform that provides tools that enables users to build, train and deploy machine learning models based on open source code and technologies. \n+**NLP**, or Natural Language Processing, is a subprocess of artificial intelligence that helps machines process and understand the human langage so that they can automatically perform repetitive tasks. \n+Some examples of NLP include machine translation, summarization, ticket classification, and spell check (or \"autocorrect\").\n+\n+The following four NLP tasks are currently supported by MindsDB:\n+\n+1. Text Classification\n+2. Zero-Shot Classification\n+3. Translation\n+4. Summarization\n+\n+**In today's tutorial, I'll show you how to connect MindsDB to Hugging Face and use these technologies to perform NLP queries, specifically text-classication, on your dataset.**\n+\n+## Data Setup\n+\n+Make sure you have access to a working MindsDB installation, either locally or via [MindsDB Cloud](https://cloud.mindsdb.com/). \n+\n+*You can create a **free** MindsDB account and take advantage of their built-in SQL editor [here](https://cloud.mindsdb.com/editor).*\n+\n+If you want to learn more about creating a free MindsDB Cloud account, follow\n+[this guide](/setup/cloud/). Another way is to set up MindsDB locally using [Docker](/setup/self-hosted/docker/) or [Python](/setup/self-hosted/pip/source/).\n+\n+Let's get started!\n+\n+##  Create MySQL Server and upload the data:\n+We have many ways to do this. But I recommend this one if you're learning. \n+1. Create [Railway Account](https://railway.app/) and add My SQL Database in it.\n+![Railway App and uploaded the file](images/sentiment-analysis-on-french-tweets/createmysqldb.png)\n+2. Note the details listed in `Available Variables`\n+![Railway details](images/sentiment-analysis-on-french-tweets/railwaydetails.png)\n+3. Install MySQL Workbench in your local, connect your MySQL Server which is hosted in the Railway App.\n+4. Download [French Tweets](https://www.kaggle.com/datasets/hbaflast/french-twitter-sentiment-analysis?resource=download) dataset from Kaggle.\n+5. In Workbench, create schema in name of `french_tweets`  in your database in Workbench. Then upload the downloaded CSV file to it. \n+\n+\n+## Connect MySQL Database to MindsDB Web\n+\n+To establish a database connection we will access MindsDB's GUI. MindsDB has MySQL Database Integration.\n+\n+First we need to connect MindsDB to the database where the French Tweets data are stored:\n+\n+ - Access MindsDB GUI on cloud.\n+ - On the default page, select the button  `Add`  and dropdown appears down, select the `New Datasource` \n+ - Select MySQL Database and fill the data connection details such as:\n+\t   - Database Alias Connection name as mysql\n+\t   - Database connection name (check railway dashboard)\n+\t   - Host name (check railway dashboard)\n+\t   - Port (check railway dashboard)\n+\t   - Username (check railway dashboard)\n+\t   - Password (check railway dashboard)\n+- Once you entered the details, check for test connection.\n+\n+ \n+ ## Check whether MySQL Server had connected or not\n+\n+Once you added the details correctly, you get the `Saved Connection` popup.  In the MindsDB Web, the sidebar display list of databases. You would have created your database in `alias connection` , check for that name in the list and when you click on it, automatically the MindsDB queries the output. \n+\n+```\n+Select * \n+FROM mysql.french_tweets;\n+```\n+Your output would be like :\n+![select query output](images/sentiment-analysis-on-french-tweets/query.png)\n+\n+## Load the Model\n+\n+We are focusing on text classification, we're going to use Hugging Face's [French Sentiment analysis model based on CamemBERT](https://huggingface.co/philschmid/pt-tblard-tf-allocine/tree/main) pytorch supported model. This Hugging Face transformer is a CamemBERT model trained on Allocine dataset.\n+\n+Our Hugging Face integration automatically manages downloading and deploying of pre-trained transformers from Hugging Face's hub. For example, we can download a transformer which has been trained to analyze the sentiment of text:\n+```\n+CREATE  MODEL mindsdb.sentiment_analyzer\n+PREDICT sentiment\n+USING\n+engine  =  'huggingface',\n+task =  'text-classification',\n+model_name =  'philschmid/pt-tblard-tf-allocine',\n+input_column =  'text';\n+```\n+To create a model in MindsDB, we first specify our chosen model name (sentiment_classifier) with **CREATE MODEL mindsdb.sentiment_analyzer.**\n+\n+Next, we choose the name of the column we would like to predict (sentiment) with **PREDICT sentiment**. Hugging Face models are already trained, so we don’t need to select a column from our training data - we can name our column whatever we like.\n+\n+Finally, we specify all the additional variables we need after **USING**. Each ML handler has slightly different parameters (for more details on Hugging Face you can check our [documentation](https://docs.mindsdb.com/custom-model/huggingface?_gl=1*dxyiz5*_ga*MjEyNzE5MjgzMy4xNjY4Nzg5OTkz*_ga_7LGFPGV6XV*MTY3MDkwMjEwOS41Mi4xLjE2NzA5MDI4ODYuNjAuMC4w).) In this case, we specify:  \n+\n+-   **engine = ‘huggingface’**  - the name of the ML framework to be used. Other options include ‘lightwood’ and ‘luwig’\n+-   **model_name = 'philschmid/pt-tblard-tf-allocine'**  - the name of the model from  [Hugging Face](https://huggingface.co/models)\n+-   **input_column = 'text'**  - the name of the input column that has your text.\n+\n+We currently support four of the Hugging Face NLP tasks: ‘text-classification’, ‘zero-shot-classification’, ‘translation’, and ‘summarization’. This model is ‘text-classification’.\n+\n+Once the above query is executed, we can check the status of the creation process:\n+```\n+SELECT  *\n+FROM mindsdb.models\n+WHERE  name='sentiment_analyzer';\n+```\n+After a few seconds you should see the status change from **generating** to **complete.**\n+\n+## Making predictions\n+\n+Once the status is complete, the behavior is the same as with any other AI table you can query it or by directly specifying your inputs, like so:",
    "comment": "Actually, you could include both single prediction and batch predictions as follows:\r\n \r\n`...the behavior is the same as with any other AI table. You can query it directly by specifying your inputs:\r\nSELECT  *  FROM mindsdb.sentiment_analyzer\r\nWHERE  text='Je vais me crier après avoir regardé Marley et moi.';\r\n\r\nOr by joining with the data table for batch predictions:\r\nSELECT  *  FROM mindsdb.sentiment_analyzer as p\r\nJOIN mysql.french_tweets as d;`",
    "line_number": 108,
    "enriched": "File: docs/tutorials/sentiment-analysis-on-french-tweets-using-mysql.mdx\nCode: @@ -0,0 +1,146 @@\n+\n+\n+# Sentiment Analysis on French Tweets using MySQL: Connect MindsDB to Sentiment Analysis on French Tweets using MySQL\n+\n+## Introduction\n+\n+[MindsDB](https://mindsdb.com/) is is an open-source AI layer for existing databases that allows you to effortlessly develop, train and deploy state-of-the-art machine learning models using SQL queries. \n+[Hugging Face](https://huggingface.co/) is a community and data science platform that provides tools that enables users to build, train and deploy machine learning models based on open source code and technologies. \n+**NLP**, or Natural Language Processing, is a subprocess of artificial intelligence that helps machines process and understand the human langage so that they can automatically perform repetitive tasks. \n+Some examples of NLP include machine translation, summarization, ticket classification, and spell check (or \"autocorrect\").\n+\n+The following four NLP tasks are currently supported by MindsDB:\n+\n+1. Text Classification\n+2. Zero-Shot Classification\n+3. Translation\n+4. Summarization\n+\n+**In today's tutorial, I'll show you how to connect MindsDB to Hugging Face and use these technologies to perform NLP queries, specifically text-classication, on your dataset.**\n+\n+## Data Setup\n+\n+Make sure you have access to a working MindsDB installation, either locally or via [MindsDB Cloud](https://cloud.mindsdb.com/). \n+\n+*You can create a **free** MindsDB account and take advantage of their built-in SQL editor [here](https://cloud.mindsdb.com/editor).*\n+\n+If you want to learn more about creating a free MindsDB Cloud account, follow\n+[this guide](/setup/cloud/). Another way is to set up MindsDB locally using [Docker](/setup/self-hosted/docker/) or [Python](/setup/self-hosted/pip/source/).\n+\n+Let's get started!\n+\n+##  Create MySQL Server and upload the data:\n+We have many ways to do this. But I recommend this one if you're learning. \n+1. Create [Railway Account](https://railway.app/) and add My SQL Database in it.\n+![Railway App and uploaded the file](images/sentiment-analysis-on-french-tweets/createmysqldb.png)\n+2. Note the details listed in `Available Variables`\n+![Railway details](images/sentiment-analysis-on-french-tweets/railwaydetails.png)\n+3. Install MySQL Workbench in your local, connect your MySQL Server which is hosted in the Railway App.\n+4. Download [French Tweets](https://www.kaggle.com/datasets/hbaflast/french-twitter-sentiment-analysis?resource=download) dataset from Kaggle.\n+5. In Workbench, create schema in name of `french_tweets`  in your database in Workbench. Then upload the downloaded CSV file to it. \n+\n+\n+## Connect MySQL Database to MindsDB Web\n+\n+To establish a database connection we will access MindsDB's GUI. MindsDB has MySQL Database Integration.\n+\n+First we need to connect MindsDB to the database where the French Tweets data are stored:\n+\n+ - Access MindsDB GUI on cloud.\n+ - On the default page, select the button  `Add`  and dropdown appears down, select the `New Datasource` \n+ - Select MySQL Database and fill the data connection details such as:\n+\t   - Database Alias Connection name as mysql\n+\t   - Database connection name (check railway dashboard)\n+\t   - Host name (check railway dashboard)\n+\t   - Port (check railway dashboard)\n+\t   - Username (check railway dashboard)\n+\t   - Password (check railway dashboard)\n+- Once you entered the details, check for test connection.\n+\n+ \n+ ## Check whether MySQL Server had connected or not\n+\n+Once you added the details correctly, you get the `Saved Connection` popup.  In the MindsDB Web, the sidebar display list of databases. You would have created your database in `alias connection` , check for that name in the list and when you click on it, automatically the MindsDB queries the output. \n+\n+```\n+Select * \n+FROM mysql.french_tweets;\n+```\n+Your output would be like :\n+![select query output](images/sentiment-analysis-on-french-tweets/query.png)\n+\n+## Load the Model\n+\n+We are focusing on text classification, we're going to use Hugging Face's [French Sentiment analysis model based on CamemBERT](https://huggingface.co/philschmid/pt-tblard-tf-allocine/tree/main) pytorch supported model. This Hugging Face transformer is a CamemBERT model trained on Allocine dataset.\n+\n+Our Hugging Face integration automatically manages downloading and deploying of pre-trained transformers from Hugging Face's hub. For example, we can download a transformer which has been trained to analyze the sentiment of text:\n+```\n+CREATE  MODEL mindsdb.sentiment_analyzer\n+PREDICT sentiment\n+USING\n+engine  =  'huggingface',\n+task =  'text-classification',\n+model_name =  'philschmid/pt-tblard-tf-allocine',\n+input_column =  'text';\n+```\n+To create a model in MindsDB, we first specify our chosen model name (sentiment_classifier) with **CREATE MODEL mindsdb.sentiment_analyzer.**\n+\n+Next, we choose the name of the column we would like to predict (sentiment) with **PREDICT sentiment**. Hugging Face models are already trained, so we don’t need to select a column from our training data - we can name our column whatever we like.\n+\n+Finally, we specify all the additional variables we need after **USING**. Each ML handler has slightly different parameters (for more details on Hugging Face you can check our [documentation](https://docs.mindsdb.com/custom-model/huggingface?_gl=1*dxyiz5*_ga*MjEyNzE5MjgzMy4xNjY4Nzg5OTkz*_ga_7LGFPGV6XV*MTY3MDkwMjEwOS41Mi4xLjE2NzA5MDI4ODYuNjAuMC4w).) In this case, we specify:  \n+\n+-   **engine = ‘huggingface’**  - the name of the ML framework to be used. Other options include ‘lightwood’ and ‘luwig’\n+-   **model_name = 'philschmid/pt-tblard-tf-allocine'**  - the name of the model from  [Hugging Face](https://huggingface.co/models)\n+-   **input_column = 'text'**  - the name of the input column that has your text.\n+\n+We currently support four of the Hugging Face NLP tasks: ‘text-classification’, ‘zero-shot-classification’, ‘translation’, and ‘summarization’. This model is ‘text-classification’.\n+\n+Once the above query is executed, we can check the status of the creation process:\n+```\n+SELECT  *\n+FROM mindsdb.models\n+WHERE  name='sentiment_analyzer';\n+```\n+After a few seconds you should see the status change from **generating** to **complete.**\n+\n+## Making predictions\n+\n+Once the status is complete, the behavior is the same as with any other AI table you can query it or by directly specifying your inputs, like so:\nComment: Actually, you could include both single prediction and batch predictions as follows:\r\n \r\n`...the behavior is the same as with any other AI table. You can query it directly by specifying your inputs:\r\nSELECT  *  FROM mindsdb.sentiment_analyzer\r\nWHERE  text='Je vais me crier après avoir regardé Marley et moi.';\r\n\r\nOr by joining with the data table for batch predictions:\r\nSELECT  *  FROM mindsdb.sentiment_analyzer as p\r\nJOIN mysql.french_tweets as d;`",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/tutorials/sentiment-analysis-on-french-tweets-using-mysql.mdx",
    "pr_number": 5106,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1171383568,
    "comment_created_at": "2023-04-19T13:55:19Z"
  },
  {
    "code": "@@ -0,0 +1,281 @@\n+---\n+title: Implementing a Chatbot for Slack using MindsDB and OpenAI's GPT Model\n+sidebarTitle: Slack Chatbot\n+---\n+\n+The objective of this tutorial is to create an AI-powered personalized chatbot by utilizing the MindsDB's Slack connector, and combining it with OpenAI's GPT-4 Model.\n+\n+To illustrate practically, we will be having a Slack bot - **@Whiz_Fizz** - which will reply to the user's queries with proper context and with a unique persona while responding. It is a weird magician 🪄 and a Space Science Expert! Let's see how it responds.\n+\n+\n+<p align=\"center\">\n+    <img src=\"assets/SLBot-Hero-Whizfizz.png\" />\n+</p>\n+\n+Before Jumping more into it. Let's first see how to create a bot and connect it to our Slack Workspace.\n+\n+## To get started:\n+\n+- [Create an account on MindsDB cloud](https://cloud.mindsdb.com) (if you don’t have one yet).\n+- [Create a Slack Account](https://slack.com/get-started#/createnew) (if you don't already know about Slack.)\n+- Go to your [MindsDB SQL Editor](https://cloud.mindsdb.com/editor)\n+\n+_[MindsDB](https://mindsdb.com/) is a famous open-source application used for simplifying the use of Machine Learning in modern-day applications without the need for any external tool by making forecasts on the Data available in your database. It lets you build any Generative AI application with minimal steps required._\n+\n+<Tip>\n+If you already have the API token of a Slack Bot, feel free to skip the following steps.\n+</Tip>\n+\n+\n+If you have a Slack Account, you have to generate the API token, please follow below instructions:\n+\n+<Accordion title=\"How to Generate API Keys\" icon=\"slack\" iconType=\"thin\">\n+\n+    Here are the steps to generate the Slack API token:\n+\n+    1. Follow this [Link](https://api.slack.com/apps) and sign in with your Slack Account\n+    2. Create a new `App` or select an existing `App`.\n+    3. In the app settings, go to the `OAuth & Permissions` section.\n+    4. Under the `Bot Token Scopes` section, add the necessary scopes for your application. You can add more later as well.\n+    5. Install the bot to your workspace.\n+    6. In the `OAuth Tokens & Redirect URLs` Section, copy the the `Bot User OAuth Access Token` (This is the API token which we need).\n+    7. Open your Slack, in order to use the bot which we created, we have to add the bot into the channel where we want to use this.\n+        - Go to the channel where you want to use the bot.\n+        - Right Click on the `channel` and select `View Channel Details`.\n+        - Select `Integrations`.\n+        - Click on `Add an App`.\n+        - You can see the name of the bot under the `In your workspace` Section, Go ahead and add the app to the channel.\n+\n+    Now, we can use the token from step 6 to initialize the Slack Handler in MindsDB.\n+</Accordion>\n+\n+Now, let's establish a connection to our Slack using MindsDB SQL Editor:\n+\n+```sql\n+CREATE DATABASE mindsdb_slack \n+WITH \n+    ENGINE = 'slack', \n+    PARAMETERS = { \n+        \"token\": \"<slack-bot-token>\" \n+    };\n+```\n+\n+<Note>\n+Please change the `slack-bot-token` with the token mentioned in `Bot User OAuth Access Token`.\n+</Note>\n+\n+## Usage\n+\n+This query will create a database called `mindsdb_slack` and `channels` table automatically.\n+\n+Here is how to retrieve the 10 latest messages from the channel:\n+\n+```sql\n+SELECT *\n+FROM mindsdb_slack.channels\n+WHERE channel=\"<channel-name>\"\n+LIMIT 10;\n+```\n+\n+<Note>\n+Please change the `channel-name` in the `WHERE` clause to the channel where, you added the bot in your Slack Workspace.\n+</Note>\n+\n+However, you can also retrieve messages in alphabetical order by using:\n+\n+```sql\n+SELECT *\n+FROM mindsdb_slack.channels\n+WHERE channel=\"<channel-name>\"\n+ORDER BY messages ASC\n+LIMIT 5;\n+```\n+\n+By default, it retrieves by the order the messages were sent, unless specified ascending/descending.\n+\n+Here is how to post messages:\n+\n+```sql\n+INSERT INTO mindsdb_slack.channels (channel, message)\n+VALUES\n+    (\"<channel-name>\", \"Hey MindsDB, Thanks to you! Now I can respond to my Slack messages through SQL Queries. 🚀 \"),\n+    (\"<channel-name>\", \"It's never been that easy to build ML apps using MindsDB!\");\n+```\n+\n+Whoops! Sent it by mistake, No worries, use this to delete a specific message\n+\n+```sql\n+DELETE FROM mindsdb_slack.channels",
    "comment": "The DELETE statement returns an error in the cloud editor:\r\n```\r\n[slack/mindsdb_slack]: 'Delete' object has no attribute 'columns'\r\n```",
    "line_number": 108,
    "enriched": "File: docs/sql/tutorials/slack-chatbot.mdx\nCode: @@ -0,0 +1,281 @@\n+---\n+title: Implementing a Chatbot for Slack using MindsDB and OpenAI's GPT Model\n+sidebarTitle: Slack Chatbot\n+---\n+\n+The objective of this tutorial is to create an AI-powered personalized chatbot by utilizing the MindsDB's Slack connector, and combining it with OpenAI's GPT-4 Model.\n+\n+To illustrate practically, we will be having a Slack bot - **@Whiz_Fizz** - which will reply to the user's queries with proper context and with a unique persona while responding. It is a weird magician 🪄 and a Space Science Expert! Let's see how it responds.\n+\n+\n+<p align=\"center\">\n+    <img src=\"assets/SLBot-Hero-Whizfizz.png\" />\n+</p>\n+\n+Before Jumping more into it. Let's first see how to create a bot and connect it to our Slack Workspace.\n+\n+## To get started:\n+\n+- [Create an account on MindsDB cloud](https://cloud.mindsdb.com) (if you don’t have one yet).\n+- [Create a Slack Account](https://slack.com/get-started#/createnew) (if you don't already know about Slack.)\n+- Go to your [MindsDB SQL Editor](https://cloud.mindsdb.com/editor)\n+\n+_[MindsDB](https://mindsdb.com/) is a famous open-source application used for simplifying the use of Machine Learning in modern-day applications without the need for any external tool by making forecasts on the Data available in your database. It lets you build any Generative AI application with minimal steps required._\n+\n+<Tip>\n+If you already have the API token of a Slack Bot, feel free to skip the following steps.\n+</Tip>\n+\n+\n+If you have a Slack Account, you have to generate the API token, please follow below instructions:\n+\n+<Accordion title=\"How to Generate API Keys\" icon=\"slack\" iconType=\"thin\">\n+\n+    Here are the steps to generate the Slack API token:\n+\n+    1. Follow this [Link](https://api.slack.com/apps) and sign in with your Slack Account\n+    2. Create a new `App` or select an existing `App`.\n+    3. In the app settings, go to the `OAuth & Permissions` section.\n+    4. Under the `Bot Token Scopes` section, add the necessary scopes for your application. You can add more later as well.\n+    5. Install the bot to your workspace.\n+    6. In the `OAuth Tokens & Redirect URLs` Section, copy the the `Bot User OAuth Access Token` (This is the API token which we need).\n+    7. Open your Slack, in order to use the bot which we created, we have to add the bot into the channel where we want to use this.\n+        - Go to the channel where you want to use the bot.\n+        - Right Click on the `channel` and select `View Channel Details`.\n+        - Select `Integrations`.\n+        - Click on `Add an App`.\n+        - You can see the name of the bot under the `In your workspace` Section, Go ahead and add the app to the channel.\n+\n+    Now, we can use the token from step 6 to initialize the Slack Handler in MindsDB.\n+</Accordion>\n+\n+Now, let's establish a connection to our Slack using MindsDB SQL Editor:\n+\n+```sql\n+CREATE DATABASE mindsdb_slack \n+WITH \n+    ENGINE = 'slack', \n+    PARAMETERS = { \n+        \"token\": \"<slack-bot-token>\" \n+    };\n+```\n+\n+<Note>\n+Please change the `slack-bot-token` with the token mentioned in `Bot User OAuth Access Token`.\n+</Note>\n+\n+## Usage\n+\n+This query will create a database called `mindsdb_slack` and `channels` table automatically.\n+\n+Here is how to retrieve the 10 latest messages from the channel:\n+\n+```sql\n+SELECT *\n+FROM mindsdb_slack.channels\n+WHERE channel=\"<channel-name>\"\n+LIMIT 10;\n+```\n+\n+<Note>\n+Please change the `channel-name` in the `WHERE` clause to the channel where, you added the bot in your Slack Workspace.\n+</Note>\n+\n+However, you can also retrieve messages in alphabetical order by using:\n+\n+```sql\n+SELECT *\n+FROM mindsdb_slack.channels\n+WHERE channel=\"<channel-name>\"\n+ORDER BY messages ASC\n+LIMIT 5;\n+```\n+\n+By default, it retrieves by the order the messages were sent, unless specified ascending/descending.\n+\n+Here is how to post messages:\n+\n+```sql\n+INSERT INTO mindsdb_slack.channels (channel, message)\n+VALUES\n+    (\"<channel-name>\", \"Hey MindsDB, Thanks to you! Now I can respond to my Slack messages through SQL Queries. 🚀 \"),\n+    (\"<channel-name>\", \"It's never been that easy to build ML apps using MindsDB!\");\n+```\n+\n+Whoops! Sent it by mistake, No worries, use this to delete a specific message\n+\n+```sql\n+DELETE FROM mindsdb_slack.channels\nComment: The DELETE statement returns an error in the cloud editor:\r\n```\r\n[slack/mindsdb_slack]: 'Delete' object has no attribute 'columns'\r\n```",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/sql/tutorials/slack-chatbot.mdx",
    "pr_number": 6808,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1266742993,
    "comment_created_at": "2023-07-18T13:03:20Z"
  },
  {
    "code": "@@ -118,4 +118,22 @@ This environment variable defines the level of logging generated by MindsDB. You\n     ```bash Shell\n     export MINDSDB_LOG_LEVEL='DEBUG'\n     ```\n+</CodeGroup>\n+\n+## MindsDB Default Project\n+\n+By default, MindsDB creates a project named `mindsdb` where all the models and other objects are stored. You can change the default project name by setting the `MINDSDB_DEFAULT_PROJECT` environment variable.\n+\n+If this environment variable is set or modified after MindsDB has started, the default project will be **renamed** accordingly upon restart. To start using the new default project, a `USE` statement will also need to be executed.",
    "comment": "@MinuraPunchihewa \r\nIf a user sets `MINDSDB_DEFAULT_PROJECT` and then (re)starts MindsDB -- does the user still need to use the `USE` statement?\r\n\r\nAlso, shall we add a doc page that describes `USE`? That would be [somewhere here](https://docs.mindsdb.com/mindsdb_sql/sql/create/project).",
    "line_number": 127,
    "enriched": "File: docs/setup/environment-vars.mdx\nCode: @@ -118,4 +118,22 @@ This environment variable defines the level of logging generated by MindsDB. You\n     ```bash Shell\n     export MINDSDB_LOG_LEVEL='DEBUG'\n     ```\n+</CodeGroup>\n+\n+## MindsDB Default Project\n+\n+By default, MindsDB creates a project named `mindsdb` where all the models and other objects are stored. You can change the default project name by setting the `MINDSDB_DEFAULT_PROJECT` environment variable.\n+\n+If this environment variable is set or modified after MindsDB has started, the default project will be **renamed** accordingly upon restart. To start using the new default project, a `USE` statement will also need to be executed.\nComment: @MinuraPunchihewa \r\nIf a user sets `MINDSDB_DEFAULT_PROJECT` and then (re)starts MindsDB -- does the user still need to use the `USE` statement?\r\n\r\nAlso, shall we add a doc page that describes `USE`? That would be [somewhere here](https://docs.mindsdb.com/mindsdb_sql/sql/create/project).",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/setup/environment-vars.mdx",
    "pr_number": 10517,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1988102868,
    "comment_created_at": "2025-03-10T22:22:24Z"
  },
  {
    "code": "@@ -0,0 +1,140 @@\n+# Milvus Handler\n+\n+This is the implementation of the Milvus for MindsDB.\n+\n+## Milvus\n+\n+Milvus is an open-source and blazing fast vector database built for scalable similarity search.\n+\n+## Implementation\n+\n+This handler uses `pymilvus` python library connect to a Milvus instance.\n+\n+The required arguments to establish a connection are:\n+\n+* `alias`: alias of the Milvus connection to construct\n+* `host`: IP address of the Milvus server\n+* `port`: port of the Milvus server\n+* `user`: username of the Milvus server\n+* `password`: password of the username of the Milvus server\n+\n+The optional arguments to establish a connection are:\n+\n+These are used for `SELECT` queries:\n+* `search_default_limit`: default limit to be passed in select statements (default=100)\n+* `search_metric_type`: metric type used for searches (default=\"L2\")\n+* `search_ignore_growing`: whether to ignore growing segments during similarity searches (default=False)\n+* `search_params`: specific to the `search_metric_type` (default={\"nprobe\": 10})\n+\n+These are used for `CREATE` queries:\n+* `create_auto_id`: whether to auto generate id when inserting records with no ID (default=False)\n+* `create_id_max_len`: maximum length of the id field when creating a table (default=64)\n+* `create_embedding_dim`: embedding dimension for creating table (default=8)\n+* `create_dynamic_field`: whether or not the created tables have dynamic fields or not (default=True)\n+* `create_content_max_len`: max length of the content column (default=200)\n+* `create_content_default_value`: default value of content column (default='')\n+* `create_schema_description`: description of the created schemas (default='')\n+* `create_alias`: alias of the created schemas (default='default')\n+* `create_index_params`: parameters of the index created on embeddings column (default={})\n+* `create_index_metric_type`: metric used to create the index (default='L2')\n+* `create_index_type`: the type of index (default='AUTOINDEX')\n+\n+For more information about how these perameters map to Milvus API, look at Milvus' documentation\n+\n+## Usage\n+",
    "comment": "@aditya-azad we should add link to running milvus via docker locally",
    "line_number": 45,
    "enriched": "File: mindsdb/integrations/handlers/milvus_handler/README.md\nCode: @@ -0,0 +1,140 @@\n+# Milvus Handler\n+\n+This is the implementation of the Milvus for MindsDB.\n+\n+## Milvus\n+\n+Milvus is an open-source and blazing fast vector database built for scalable similarity search.\n+\n+## Implementation\n+\n+This handler uses `pymilvus` python library connect to a Milvus instance.\n+\n+The required arguments to establish a connection are:\n+\n+* `alias`: alias of the Milvus connection to construct\n+* `host`: IP address of the Milvus server\n+* `port`: port of the Milvus server\n+* `user`: username of the Milvus server\n+* `password`: password of the username of the Milvus server\n+\n+The optional arguments to establish a connection are:\n+\n+These are used for `SELECT` queries:\n+* `search_default_limit`: default limit to be passed in select statements (default=100)\n+* `search_metric_type`: metric type used for searches (default=\"L2\")\n+* `search_ignore_growing`: whether to ignore growing segments during similarity searches (default=False)\n+* `search_params`: specific to the `search_metric_type` (default={\"nprobe\": 10})\n+\n+These are used for `CREATE` queries:\n+* `create_auto_id`: whether to auto generate id when inserting records with no ID (default=False)\n+* `create_id_max_len`: maximum length of the id field when creating a table (default=64)\n+* `create_embedding_dim`: embedding dimension for creating table (default=8)\n+* `create_dynamic_field`: whether or not the created tables have dynamic fields or not (default=True)\n+* `create_content_max_len`: max length of the content column (default=200)\n+* `create_content_default_value`: default value of content column (default='')\n+* `create_schema_description`: description of the created schemas (default='')\n+* `create_alias`: alias of the created schemas (default='default')\n+* `create_index_params`: parameters of the index created on embeddings column (default={})\n+* `create_index_metric_type`: metric used to create the index (default='L2')\n+* `create_index_type`: the type of index (default='AUTOINDEX')\n+\n+For more information about how these perameters map to Milvus API, look at Milvus' documentation\n+\n+## Usage\n+\nComment: @aditya-azad we should add link to running milvus via docker locally",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "mindsdb/integrations/handlers/milvus_handler/README.md",
    "pr_number": 7692,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1356720005,
    "comment_created_at": "2023-10-12T12:10:14Z"
  },
  {
    "code": "@@ -0,0 +1,272 @@\n+---\n+title: Gong\n+sidebarTitle: Gong\n+---\n+\n+This documentation describes the integration of MindsDB with [Gong](https://www.gong.io/), a conversation intelligence platform that captures, analyzes, and provides insights from customer conversations.\n+The integration allows MindsDB to access call recordings, transcripts, analytics, and other conversation data from Gong and enhance it with AI capabilities.\n+\n+## Prerequisites\n+\n+Before proceeding, ensure the following prerequisites are met:\n+\n+1. Install MindsDB locally via [Docker](https://docs.mindsdb.com/setup/self-hosted/docker) or [Docker Desktop](https://docs.mindsdb.com/setup/self-hosted/docker-desktop).\n+2. To connect Gong to MindsDB, install the required dependencies following [this instruction](https://docs.mindsdb.com/setup/self-hosted/docker#install-dependencies).\n+3. Obtain a Gong API key from your [Gong API settings page](https://app.gong.io/settings/api-keys).\n+\n+## Connection\n+\n+Establish a connection to Gong from MindsDB by executing the following SQL command and providing its handler name as an engine.\n+\n+### Using Bearer Token (Recommended)\n+\n+```sql\n+CREATE DATABASE gong_datasource\n+WITH\n+    ENGINE = 'gong',\n+    PARAMETERS = {\n+        \"api_key\": \"your_gong_api_key_here\"\n+    };\n+```\n+\n+### Using Basic Authentication\n+\n+```sql\n+CREATE DATABASE gong_datasource\n+WITH\n+    ENGINE = 'gong',\n+    PARAMETERS = {\n+        \"access_key\": \"your_access_key\",\n+        \"secret_key\": \"your_secret_key\"\n+    };\n+```\n+\n+Required connection parameters include the following:\n+\n+**Authentication (choose one method):**\n+\n+* `api_key`: Bearer token for authentication (recommended)\n+* `access_key` + `secret_key`: Basic authentication credentials (alternative method)\n+\n+Optional connection parameters include the following:\n+\n+* `base_url`: Gong API base URL. This parameter defaults to `https://api.gong.io`.\n+* `timeout`: Request timeout in seconds. This parameter defaults to `30`.\n+\n+<Note>\n+If both authentication methods are provided, basic auth (`access_key` + `secret_key`) takes precedence.\n+</Note>\n+\n+## Usage\n+\n+The following usage examples utilize `gong_datasource` as the datasource name, which is defined in the `CREATE DATABASE` command.\n+\n+### Available Tables\n+\n+The Gong handler provides access to the following tables:\n+\n+* `calls` - Access call recordings and metadata\n+* `users` - Get user information and permissions\n+* `analytics` - Access AI-generated conversation insights\n+* `transcripts` - Get full conversation transcripts\n+\n+### Basic Queries\n+\n+Retrieve recent calls with date filters (recommended for best performance):\n+\n+```sql\n+SELECT * \n+FROM gong_datasource.calls \n+WHERE date >= '2024-01-01' AND date < '2024-02-01'\n+ORDER BY date DESC\n+LIMIT 20;\n+```\n+\n+Get all users in your organization:\n+\n+```sql\n+SELECT user_id, name, email, role, status \n+FROM gong_datasource.users \n+LIMIT 100;\n+```\n+\n+Get analytics for calls with high sentiment scores:\n+\n+```sql\n+SELECT call_id, sentiment_score, key_phrases, topics \n+FROM gong_datasource.analytics \n+WHERE sentiment_score > 0.7 \n+  AND date >= '2024-01-01'\n+LIMIT 50;\n+```\n+\n+Get transcripts for a specific call:\n+\n+```sql\n+SELECT speaker, timestamp, text \n+FROM gong_datasource.transcripts \n+WHERE call_id = '12345'\n+ORDER BY timestamp;\n+```\n+\n+### Advanced Queries with JOINs\n+\n+Get calls with their sentiment analysis:\n+\n+```sql\n+SELECT \n+    c.title,\n+    c.date,\n+    c.duration,\n+    a.sentiment_score,\n+    a.key_phrases\n+FROM gong_datasource.calls c\n+JOIN gong_datasource.analytics a ON c.call_id = a.call_id\n+WHERE c.date >= '2024-01-01' AND c.date < '2024-02-01'\n+ORDER BY a.sentiment_score DESC\n+LIMIT 25;\n+```\n+\n+Find calls where specific keywords were mentioned:\n+\n+```sql\n+SELECT \n+    c.title,\n+    c.date,\n+    t.speaker,\n+    t.text\n+FROM gong_datasource.calls c\n+JOIN gong_datasource.transcripts t ON c.call_id = t.call_id\n+WHERE c.date >= '2024-01-01'\n+  AND t.text LIKE '%pricing%'\n+LIMIT 50;\n+```\n+\n+Get user performance with call sentiment:\n+\n+```sql\n+SELECT \n+    u.name,\n+    u.email,\n+    c.call_id,\n+    c.title,\n+    a.sentiment_score\n+FROM gong_datasource.users u\n+JOIN gong_datasource.calls c ON u.user_id = c.user_id\n+JOIN gong_datasource.analytics a ON c.call_id = a.call_id\n+WHERE c.date >= '2024-01-01'\n+  AND a.sentiment_score > 0.8\n+LIMIT 100;\n+```\n+\n+### Querying the Data Catalog",
    "comment": "These should be 'META_' tables, right? Like `META_TABLES`, `META_COLUMNS` and so forth. Is there a reason to even include these, @ZoranPandovski? Since they are mostly for internal use?",
    "line_number": 162,
    "enriched": "File: docs/integrations/app-integrations/gong.mdx\nCode: @@ -0,0 +1,272 @@\n+---\n+title: Gong\n+sidebarTitle: Gong\n+---\n+\n+This documentation describes the integration of MindsDB with [Gong](https://www.gong.io/), a conversation intelligence platform that captures, analyzes, and provides insights from customer conversations.\n+The integration allows MindsDB to access call recordings, transcripts, analytics, and other conversation data from Gong and enhance it with AI capabilities.\n+\n+## Prerequisites\n+\n+Before proceeding, ensure the following prerequisites are met:\n+\n+1. Install MindsDB locally via [Docker](https://docs.mindsdb.com/setup/self-hosted/docker) or [Docker Desktop](https://docs.mindsdb.com/setup/self-hosted/docker-desktop).\n+2. To connect Gong to MindsDB, install the required dependencies following [this instruction](https://docs.mindsdb.com/setup/self-hosted/docker#install-dependencies).\n+3. Obtain a Gong API key from your [Gong API settings page](https://app.gong.io/settings/api-keys).\n+\n+## Connection\n+\n+Establish a connection to Gong from MindsDB by executing the following SQL command and providing its handler name as an engine.\n+\n+### Using Bearer Token (Recommended)\n+\n+```sql\n+CREATE DATABASE gong_datasource\n+WITH\n+    ENGINE = 'gong',\n+    PARAMETERS = {\n+        \"api_key\": \"your_gong_api_key_here\"\n+    };\n+```\n+\n+### Using Basic Authentication\n+\n+```sql\n+CREATE DATABASE gong_datasource\n+WITH\n+    ENGINE = 'gong',\n+    PARAMETERS = {\n+        \"access_key\": \"your_access_key\",\n+        \"secret_key\": \"your_secret_key\"\n+    };\n+```\n+\n+Required connection parameters include the following:\n+\n+**Authentication (choose one method):**\n+\n+* `api_key`: Bearer token for authentication (recommended)\n+* `access_key` + `secret_key`: Basic authentication credentials (alternative method)\n+\n+Optional connection parameters include the following:\n+\n+* `base_url`: Gong API base URL. This parameter defaults to `https://api.gong.io`.\n+* `timeout`: Request timeout in seconds. This parameter defaults to `30`.\n+\n+<Note>\n+If both authentication methods are provided, basic auth (`access_key` + `secret_key`) takes precedence.\n+</Note>\n+\n+## Usage\n+\n+The following usage examples utilize `gong_datasource` as the datasource name, which is defined in the `CREATE DATABASE` command.\n+\n+### Available Tables\n+\n+The Gong handler provides access to the following tables:\n+\n+* `calls` - Access call recordings and metadata\n+* `users` - Get user information and permissions\n+* `analytics` - Access AI-generated conversation insights\n+* `transcripts` - Get full conversation transcripts\n+\n+### Basic Queries\n+\n+Retrieve recent calls with date filters (recommended for best performance):\n+\n+```sql\n+SELECT * \n+FROM gong_datasource.calls \n+WHERE date >= '2024-01-01' AND date < '2024-02-01'\n+ORDER BY date DESC\n+LIMIT 20;\n+```\n+\n+Get all users in your organization:\n+\n+```sql\n+SELECT user_id, name, email, role, status \n+FROM gong_datasource.users \n+LIMIT 100;\n+```\n+\n+Get analytics for calls with high sentiment scores:\n+\n+```sql\n+SELECT call_id, sentiment_score, key_phrases, topics \n+FROM gong_datasource.analytics \n+WHERE sentiment_score > 0.7 \n+  AND date >= '2024-01-01'\n+LIMIT 50;\n+```\n+\n+Get transcripts for a specific call:\n+\n+```sql\n+SELECT speaker, timestamp, text \n+FROM gong_datasource.transcripts \n+WHERE call_id = '12345'\n+ORDER BY timestamp;\n+```\n+\n+### Advanced Queries with JOINs\n+\n+Get calls with their sentiment analysis:\n+\n+```sql\n+SELECT \n+    c.title,\n+    c.date,\n+    c.duration,\n+    a.sentiment_score,\n+    a.key_phrases\n+FROM gong_datasource.calls c\n+JOIN gong_datasource.analytics a ON c.call_id = a.call_id\n+WHERE c.date >= '2024-01-01' AND c.date < '2024-02-01'\n+ORDER BY a.sentiment_score DESC\n+LIMIT 25;\n+```\n+\n+Find calls where specific keywords were mentioned:\n+\n+```sql\n+SELECT \n+    c.title,\n+    c.date,\n+    t.speaker,\n+    t.text\n+FROM gong_datasource.calls c\n+JOIN gong_datasource.transcripts t ON c.call_id = t.call_id\n+WHERE c.date >= '2024-01-01'\n+  AND t.text LIKE '%pricing%'\n+LIMIT 50;\n+```\n+\n+Get user performance with call sentiment:\n+\n+```sql\n+SELECT \n+    u.name,\n+    u.email,\n+    c.call_id,\n+    c.title,\n+    a.sentiment_score\n+FROM gong_datasource.users u\n+JOIN gong_datasource.calls c ON u.user_id = c.user_id\n+JOIN gong_datasource.analytics a ON c.call_id = a.call_id\n+WHERE c.date >= '2024-01-01'\n+  AND a.sentiment_score > 0.8\n+LIMIT 100;\n+```\n+\n+### Querying the Data Catalog\nComment: These should be 'META_' tables, right? Like `META_TABLES`, `META_COLUMNS` and so forth. Is there a reason to even include these, @ZoranPandovski? Since they are mostly for internal use?",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/integrations/app-integrations/gong.mdx",
    "pr_number": 11829,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2480731958,
    "comment_created_at": "2025-10-31T09:36:02Z"
  },
  {
    "code": "@@ -0,0 +1,108 @@\n+# PirateWeather Handler\n+\n+The PirateWeather handler allows you to query historical weather data from [PirateWeather](http://pirateweather.net/).\n+\n+## PirateWeather Handler Setup\n+\n+The PirateWeather handler is initialized with the following parameters:\n+\n+- `api_key`: your PirateWeather API key to use for authentication\n+\n+Read about creating a PirateWeather API key [here](http://pirateweather.net/en/latest/).\n+\n+## Provided Tables\n+\n+- `hourly` - historical hourly weather data for a given location. Columns:\n+    - `localtime`\n+    - `icon`\n+    - `summary`\n+    - `precipAccumulation`\n+    - `precipType`\n+    - `temperature`\n+    - `apparentTemperature`\n+    - `dewPoint`\n+    - `pressure`\n+    - `windSpeed`\n+    - `windBearing`\n+    - `cloudCover`\n+    - `latitude`\n+    - `longitude`\n+    - `timezone`\n+    - `offset`\n+- `daily` - historical daily weather data for a given location. Columns:\n+    - `localtime`\n+    - `icon`\n+    - `summary`\n+    - `precipAccumulation`\n+    - `precipType`\n+    - `temperature`\n+    - `apparentTemperature`\n+    - `dewPoint`\n+    - `pressure`\n+    - `windSpeed`\n+    - `windBearing`\n+    - `cloudCover`\n+    - `latitude`\n+    - `longitude`\n+    - `timezone`\n+    - `offset`\n+\n+See [here](http://pirateweather.net/en/latest/API/#time-machine-request) for more information.\n+\n+Both tables support the following parameters:\n+\n+* `latitude` - latitude of the location. Required.\n+* `longitude` - longitude of the location. Required.\n+* `time` - Date for which to fetch historical data. Optional, defaults to the current date.\n+* `units` - Units to use for temperature and wind speed. Optional, defaults to `us` (Imperial units). Other options are:\n+    * `ca`: SI, with Wind Speed and Wind Gust in kilometres per hour.\n+    * `uk`: SI, with Wind Speed and Wind Gust in miles per hour and visibility are in miles.\n+    * `us`: Imperial units\n+    * `si`: SI units\n+\n+## Example Usage\n+\n+The first step is to create a database with the new `github` engine.",
    "comment": "Change engine to pirateweather",
    "line_number": 65,
    "enriched": "File: mindsdb/integrations/handlers/pirateweather_handler/README.md\nCode: @@ -0,0 +1,108 @@\n+# PirateWeather Handler\n+\n+The PirateWeather handler allows you to query historical weather data from [PirateWeather](http://pirateweather.net/).\n+\n+## PirateWeather Handler Setup\n+\n+The PirateWeather handler is initialized with the following parameters:\n+\n+- `api_key`: your PirateWeather API key to use for authentication\n+\n+Read about creating a PirateWeather API key [here](http://pirateweather.net/en/latest/).\n+\n+## Provided Tables\n+\n+- `hourly` - historical hourly weather data for a given location. Columns:\n+    - `localtime`\n+    - `icon`\n+    - `summary`\n+    - `precipAccumulation`\n+    - `precipType`\n+    - `temperature`\n+    - `apparentTemperature`\n+    - `dewPoint`\n+    - `pressure`\n+    - `windSpeed`\n+    - `windBearing`\n+    - `cloudCover`\n+    - `latitude`\n+    - `longitude`\n+    - `timezone`\n+    - `offset`\n+- `daily` - historical daily weather data for a given location. Columns:\n+    - `localtime`\n+    - `icon`\n+    - `summary`\n+    - `precipAccumulation`\n+    - `precipType`\n+    - `temperature`\n+    - `apparentTemperature`\n+    - `dewPoint`\n+    - `pressure`\n+    - `windSpeed`\n+    - `windBearing`\n+    - `cloudCover`\n+    - `latitude`\n+    - `longitude`\n+    - `timezone`\n+    - `offset`\n+\n+See [here](http://pirateweather.net/en/latest/API/#time-machine-request) for more information.\n+\n+Both tables support the following parameters:\n+\n+* `latitude` - latitude of the location. Required.\n+* `longitude` - longitude of the location. Required.\n+* `time` - Date for which to fetch historical data. Optional, defaults to the current date.\n+* `units` - Units to use for temperature and wind speed. Optional, defaults to `us` (Imperial units). Other options are:\n+    * `ca`: SI, with Wind Speed and Wind Gust in kilometres per hour.\n+    * `uk`: SI, with Wind Speed and Wind Gust in miles per hour and visibility are in miles.\n+    * `us`: Imperial units\n+    * `si`: SI units\n+\n+## Example Usage\n+\n+The first step is to create a database with the new `github` engine.\nComment: Change engine to pirateweather",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "mindsdb/integrations/handlers/pirateweather_handler/README.md",
    "pr_number": 7968,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1374530808,
    "comment_created_at": "2023-10-27T12:49:36Z"
  },
  {
    "code": "@@ -0,0 +1,59 @@\n+---\n+title: Stripe\n+sidebarTitle: Stripe\n+---\n+\n+This is the implementation of the Stripe app handler for MindsDB.\n+\n+[Stripe](https://www.nerdwallet.com/article/small-business/what-is-stripe) is a payment services provider that lets merchants accept credit and debit cards or other payments.\n+\n+## Connection\n+\n+This handler was implemented using [stripe-python](https://github.com/stripe/stripe-python), the Python library for the Stripe API.\n+\n+There is only one parameter required to set up the connection with Stripe:\n+- `api_key`: a Stripe API key.\n+\n+<Tip>\n+You can find your API keys in the Stripe Dashboard. [Read more](https://stripe.com/docs/keys).\n+</Tip>\n+\n+To connect to Stripe using MindsDB, the following CREATE DATABASE statement can be used:\n+\n+```sql\n+CREATE DATABASE stripe_datasource\n+WITH ENGINE = 'stripe',\n+PARAMETERS = {\n+  \"api_key\": \"sk_...\"\n+};\n+```\n+\n+<Tip>\n+If you installed MindsDB locally via pip, you need to install all handler dependencies manually. To do so, go to the handler's folder (mindsdb/integrations/handlers/mediawiki_handler) and run this command: `pip install -r requirements.txt`.",
    "comment": "Please update it to `mindsdb/integrations/handlers/stripe_handler`.",
    "line_number": 32,
    "enriched": "File: docs/integrations/app-integrations/stripe.mdx\nCode: @@ -0,0 +1,59 @@\n+---\n+title: Stripe\n+sidebarTitle: Stripe\n+---\n+\n+This is the implementation of the Stripe app handler for MindsDB.\n+\n+[Stripe](https://www.nerdwallet.com/article/small-business/what-is-stripe) is a payment services provider that lets merchants accept credit and debit cards or other payments.\n+\n+## Connection\n+\n+This handler was implemented using [stripe-python](https://github.com/stripe/stripe-python), the Python library for the Stripe API.\n+\n+There is only one parameter required to set up the connection with Stripe:\n+- `api_key`: a Stripe API key.\n+\n+<Tip>\n+You can find your API keys in the Stripe Dashboard. [Read more](https://stripe.com/docs/keys).\n+</Tip>\n+\n+To connect to Stripe using MindsDB, the following CREATE DATABASE statement can be used:\n+\n+```sql\n+CREATE DATABASE stripe_datasource\n+WITH ENGINE = 'stripe',\n+PARAMETERS = {\n+  \"api_key\": \"sk_...\"\n+};\n+```\n+\n+<Tip>\n+If you installed MindsDB locally via pip, you need to install all handler dependencies manually. To do so, go to the handler's folder (mindsdb/integrations/handlers/mediawiki_handler) and run this command: `pip install -r requirements.txt`.\nComment: Please update it to `mindsdb/integrations/handlers/stripe_handler`.",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/integrations/app-integrations/stripe.mdx",
    "pr_number": 7401,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1334335481,
    "comment_created_at": "2023-09-22T12:47:56Z"
  },
  {
    "code": "@@ -0,0 +1,89 @@\n+---\n+title: Upload Custom Functions\n+sidebarTitle: Custom Functions\n+---\n+\n+Custom functions provide advanced means of manipulating data. Users can upload custom functions written in Python to MindsDB and apply them to data.\n+\n+## How It Works\n+\n+You can upload your custom functions via the MindsDB editor by clicking `Add` and `Upload custom functions`, like this:\n+\n+<p align=\"center\">\n+  <img src=\"/assets/upload_custom_function.png\" />\n+</p>\n+\n+Here is the form that needs to be filled out in order to bring your custom functions to MindsDB:\n+\n+<p align=\"center\">\n+  <img src=\"/assets/upload_custom_function_empty_form.png\" />\n+</p>\n+\n+Let's briefly go over the files that need to be uploaded:\n+\n+* The Python file stores an implementation of your custom functions. Here is the sample format:",
    "comment": "Lets highlight that input and output types annotation for function are important. If they are not set - default type 'str' will be used\r\n",
    "line_number": 24,
    "enriched": "File: docs/mindsdb_sql/functions/custom_functions.mdx\nCode: @@ -0,0 +1,89 @@\n+---\n+title: Upload Custom Functions\n+sidebarTitle: Custom Functions\n+---\n+\n+Custom functions provide advanced means of manipulating data. Users can upload custom functions written in Python to MindsDB and apply them to data.\n+\n+## How It Works\n+\n+You can upload your custom functions via the MindsDB editor by clicking `Add` and `Upload custom functions`, like this:\n+\n+<p align=\"center\">\n+  <img src=\"/assets/upload_custom_function.png\" />\n+</p>\n+\n+Here is the form that needs to be filled out in order to bring your custom functions to MindsDB:\n+\n+<p align=\"center\">\n+  <img src=\"/assets/upload_custom_function_empty_form.png\" />\n+</p>\n+\n+Let's briefly go over the files that need to be uploaded:\n+\n+* The Python file stores an implementation of your custom functions. Here is the sample format:\nComment: Lets highlight that input and output types annotation for function are important. If they are not set - default type 'str' will be used\r\n",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/mindsdb_sql/functions/custom_functions.mdx",
    "pr_number": 9474,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1684465128,
    "comment_created_at": "2024-07-19T14:27:14Z"
  },
  {
    "code": "@@ -0,0 +1,130 @@\n+---\n+title: Forecasting Quarterly House Sales with TimeGPT\n+sidebarTitle: House Sales with TimeGPT\n+---\n+\n+## Introduction\n+\n+In this tutorial, we introduce Nixtla’s TimeGPT integration which offers the first foundational model for time series forecasting.. We’ll go through an example to predict the real estate sales.\n+\n+## Prerequisites\n+\n+### MindsDB Setup\n+\n+One way is to sign up for an account at [MindsDB Cloud](https://cloud.mindsdb.com/). It is a convenient option as it doesn’t require any installation procedures. You can find the details [here](/setup/cloud).\n+\n+Alternatively, visit our docs and follow the instructions to manually set up a local instance of MindsDB via [Docker](/setup/self-hosted/docker) or [pip](/setup/self-hosted/pip/source). You can also set up MindsDB on AWS following [this instruction set](https://aws.amazon.com/marketplace/pp/prodview-2nmhvskydmyrw).\n+\n+### Creating an ML Engine\n+\n+You can check the available engines with this command:\n+\n+```sql\n+SHOW ML_ENGINES;\n+```\n+\n+If you see the TimeGPT engine on the list, you are ready to follow the tutorials.\n+\n+## Tutorial\n+\n+### Connecting the Data\n+\n+In this tutorial, we take our [House Sales tutorial](/sql/tutorials/house-sales-forecasting) and redo it using the StatsForecast engine.\n+\n+We use a table from our MySQL public demo database, so let’s start by connecting MindsDB to it:\n+\n+```sql\n+CREATE DATABASE mysql_demo_db_houses\n+WITH ENGINE = 'mysql',\n+PARAMETERS = {\n+    \"user\": \"user\",\n+    \"password\": \"MindsDBUser123!\",\n+    \"host\": \"db-demo-data.cwoyhfn6bzs0.us-east-1.rds.amazonaws.com\",\n+    \"port\": \"3306\",\n+    \"database\": \"public\"\n+};\n+```\n+\n+Now that we’ve connected our database to MindsDB, let’s query the data to be used in the example:\n+\n+```sql\n+SELECT *\n+FROM mysql_demo_db.house_sales\n+LIMIT 3;\n+```\n+\n+Here is the output:\n+\n+```sql\n++----------+--------------------------+-----+--------+\n+|saledate  |house_price_moving_average|type |bedrooms|\n++----------+--------------------------+-----+--------+\n+|30/09/2007|441854                    |house|2       |\n+|31/12/2007|441854                    |house|2       |\n+|31/03/2008|441854                    |house|2       |\n++----------+--------------------------+-----+--------+\n+```\n+\n+The `house_sales` table stores quarterly house price moving averages per property.\n+\n+### Creating a Model\n+\n+Let's create a model table to predict the house price moving average values:\n+\n+```sql\n+CREATE MODEL nixtla_timegpt_house_sales_predictor\n+FROM mysql_demo_db\n+  (SELECT * FROM house_sales)\n+PREDICT house_price_moving_average\n+ORDER BY saledate\n+GROUP BY bedrooms, type\n+WINDOW 8\n+HORIZON 4\n+USING ENGINE = 'timegpt';\n+```\n+\n+The syntax is the same as in the original tutorial. But here, we add the `USING` clause that specifies the ML engine used to make predictions.",
    "comment": "We may want to add a link here, not sure if possible, but ideally at least mention the tutorial's title for reference in case the user is completely new to MindsDB.",
    "line_number": 86,
    "enriched": "File: docs/sql/tutorials/house-sales-timegpt.mdx\nCode: @@ -0,0 +1,130 @@\n+---\n+title: Forecasting Quarterly House Sales with TimeGPT\n+sidebarTitle: House Sales with TimeGPT\n+---\n+\n+## Introduction\n+\n+In this tutorial, we introduce Nixtla’s TimeGPT integration which offers the first foundational model for time series forecasting.. We’ll go through an example to predict the real estate sales.\n+\n+## Prerequisites\n+\n+### MindsDB Setup\n+\n+One way is to sign up for an account at [MindsDB Cloud](https://cloud.mindsdb.com/). It is a convenient option as it doesn’t require any installation procedures. You can find the details [here](/setup/cloud).\n+\n+Alternatively, visit our docs and follow the instructions to manually set up a local instance of MindsDB via [Docker](/setup/self-hosted/docker) or [pip](/setup/self-hosted/pip/source). You can also set up MindsDB on AWS following [this instruction set](https://aws.amazon.com/marketplace/pp/prodview-2nmhvskydmyrw).\n+\n+### Creating an ML Engine\n+\n+You can check the available engines with this command:\n+\n+```sql\n+SHOW ML_ENGINES;\n+```\n+\n+If you see the TimeGPT engine on the list, you are ready to follow the tutorials.\n+\n+## Tutorial\n+\n+### Connecting the Data\n+\n+In this tutorial, we take our [House Sales tutorial](/sql/tutorials/house-sales-forecasting) and redo it using the StatsForecast engine.\n+\n+We use a table from our MySQL public demo database, so let’s start by connecting MindsDB to it:\n+\n+```sql\n+CREATE DATABASE mysql_demo_db_houses\n+WITH ENGINE = 'mysql',\n+PARAMETERS = {\n+    \"user\": \"user\",\n+    \"password\": \"MindsDBUser123!\",\n+    \"host\": \"db-demo-data.cwoyhfn6bzs0.us-east-1.rds.amazonaws.com\",\n+    \"port\": \"3306\",\n+    \"database\": \"public\"\n+};\n+```\n+\n+Now that we’ve connected our database to MindsDB, let’s query the data to be used in the example:\n+\n+```sql\n+SELECT *\n+FROM mysql_demo_db.house_sales\n+LIMIT 3;\n+```\n+\n+Here is the output:\n+\n+```sql\n++----------+--------------------------+-----+--------+\n+|saledate  |house_price_moving_average|type |bedrooms|\n++----------+--------------------------+-----+--------+\n+|30/09/2007|441854                    |house|2       |\n+|31/12/2007|441854                    |house|2       |\n+|31/03/2008|441854                    |house|2       |\n++----------+--------------------------+-----+--------+\n+```\n+\n+The `house_sales` table stores quarterly house price moving averages per property.\n+\n+### Creating a Model\n+\n+Let's create a model table to predict the house price moving average values:\n+\n+```sql\n+CREATE MODEL nixtla_timegpt_house_sales_predictor\n+FROM mysql_demo_db\n+  (SELECT * FROM house_sales)\n+PREDICT house_price_moving_average\n+ORDER BY saledate\n+GROUP BY bedrooms, type\n+WINDOW 8\n+HORIZON 4\n+USING ENGINE = 'timegpt';\n+```\n+\n+The syntax is the same as in the original tutorial. But here, we add the `USING` clause that specifies the ML engine used to make predictions.\nComment: We may want to add a link here, not sure if possible, but ideally at least mention the tutorial's title for reference in case the user is completely new to MindsDB.",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/sql/tutorials/house-sales-timegpt.mdx",
    "pr_number": 7146,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1299316824,
    "comment_created_at": "2023-08-20T06:59:54Z"
  },
  {
    "code": "@@ -5,45 +5,19 @@ sidebarTitle: Extend the Default MindsDB Configuration\n \n To follow this guide, install MindsDB locally via [Docker](/setup/self-hosted/docker-desktop) or [PyPI](/setup/self-hosted/pip/source).\n \n-## Starting MindsDB with Default Configuration\n-\n-Start MindsDB locally with the default configuration.\n-\n-1. Activate the virtual environment:\n-\n-```bash\n-source mindsdb/bin/activate\n-```\n-\n-2. Start MindsDB:\n-\n-```bash\n-python -m mindsdb\n-```\n-\n-3. Access MindsDB locally at `127.0.0.1:47334`.\n-\n-<Tip>\n-By default, MindsDB starts the `http` and `mysql` APIs. You can define which APIs to start using the `api` flag as below.\n-\n-```bash\n-python -m mindsdb --api http,mysql,postgres\n-```\n-\n-If you want to start MindsDB without the graphical user interface (GUI), use the `--no_studio` flag as below.\n-\n-```bash\n-python -m mindsdb --no_studio\n-```\n-</Tip>\n-\n ## Starting MindsDB with Extended Configuration\n \n Start MindsDB locally with your custom configuration by providing a path to the `config.json` file that stores custom config parameters listed in this section.\n \n-```bash\n-python -m mindsdb --config=/path-to-the-extended-config-file/config.json\n-```\n+<CodeGroup>\n+    ```bash Docker\n+    docker run --name mindsdb_container -e MINDSDB_CONFIG_PATH=/Users/username/path/config.json -e MINDSDB_APIS=http,mysql -p 47334:47334 -p 47335:47335 mindsdb/mindsdb\n+    ```\n+\n+    ```bash Python\n+    python -m mindsdb --config=/path-to-the-extended-config-file/config.json",
    "comment": "may be add `--api=http,mysql` to make it similar to docker command above?",
    "line_number": 18,
    "enriched": "File: docs/setup/custom-config.mdx\nCode: @@ -5,45 +5,19 @@ sidebarTitle: Extend the Default MindsDB Configuration\n \n To follow this guide, install MindsDB locally via [Docker](/setup/self-hosted/docker-desktop) or [PyPI](/setup/self-hosted/pip/source).\n \n-## Starting MindsDB with Default Configuration\n-\n-Start MindsDB locally with the default configuration.\n-\n-1. Activate the virtual environment:\n-\n-```bash\n-source mindsdb/bin/activate\n-```\n-\n-2. Start MindsDB:\n-\n-```bash\n-python -m mindsdb\n-```\n-\n-3. Access MindsDB locally at `127.0.0.1:47334`.\n-\n-<Tip>\n-By default, MindsDB starts the `http` and `mysql` APIs. You can define which APIs to start using the `api` flag as below.\n-\n-```bash\n-python -m mindsdb --api http,mysql,postgres\n-```\n-\n-If you want to start MindsDB without the graphical user interface (GUI), use the `--no_studio` flag as below.\n-\n-```bash\n-python -m mindsdb --no_studio\n-```\n-</Tip>\n-\n ## Starting MindsDB with Extended Configuration\n \n Start MindsDB locally with your custom configuration by providing a path to the `config.json` file that stores custom config parameters listed in this section.\n \n-```bash\n-python -m mindsdb --config=/path-to-the-extended-config-file/config.json\n-```\n+<CodeGroup>\n+    ```bash Docker\n+    docker run --name mindsdb_container -e MINDSDB_CONFIG_PATH=/Users/username/path/config.json -e MINDSDB_APIS=http,mysql -p 47334:47334 -p 47335:47335 mindsdb/mindsdb\n+    ```\n+\n+    ```bash Python\n+    python -m mindsdb --config=/path-to-the-extended-config-file/config.json\nComment: may be add `--api=http,mysql` to make it similar to docker command above?",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/setup/custom-config.mdx",
    "pr_number": 11577,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2352576090,
    "comment_created_at": "2025-09-16T13:46:11Z"
  },
  {
    "code": "@@ -0,0 +1,128 @@\n+---\n+title: MindsDB System Defaults\n+sidebarTitle: System Defaults\n+---\n+\n+System defaults in MindsDB provide a convenient way to set application-wide configurations for commonly used AI models. By defining these defaults once, users can streamline workflows and avoid repeatedly specifying model parameters when creating or using various MindsDB objects and functions.\n+\n+## Usage of System Defaults\n+\n+When system defaults are set, MindsDB can automatically use the configured models across the platform for various components such as:\n+\n+* [Agents](/mindsdb_sql/agents/agent) that can answer questions over the connected data and are powered by a default large language model (LLM).\n+\n+* [Knowledge Bases](/mindsdb_sql/knowledge_bases/overview) that can store and search both structured and unstructured data, and use a default embedding model for embedding the content and a default reranking model for reranking the search results. Additionally, knowledge bases use a default model for evaluating performance with the [EVALUATE KNOWLEDGE_BASE command](/mindsdb_sql/knowledge_bases/evaluate).\n+\n+* Custom functions such as [LLM()](/mindsdb_sql/functions/llm_function) and [TO_MARKDOWN()](/mindsdb_sql/functions/to_markdown_function) that rely on the default LLM for text generation and formatting.\n+\n+Once configured, users can create and use agents, knowledge bases, and custom functions without having to specify model parameters each time. This ensures consistent behavior across the system and simplifies deployment.\n+\n+## Available System Defaults\n+\n+MindsDB supports the following system defaults:\n+\n+| System Default           | Used By                                                      | Description                                                                                       |\n+|--------------------------|--------------------------------------------------------------|---------------------------------------------------------------------------------------------------|\n+| Default LLM              | Agents, EVALUATE, KNOWLEDGE_BASE, LLM(), TO_MARKDOWN()       | Used as an underlying LLM for reasoning, conversation, and text generation and formatting.        |\n+| Default Embedding Model  | Knowledge Bases                                              | Converts inserted content and user questions into embeddings for semantic search.                 |\n+| Default Reranking Model  | Knowledge Bases                                              | Reranks search results to improve retrieval accuracy.                                             |\n+\n+## Supported Model Providers\n+\n+Different components in MindsDB support different sets of model providers.\n+\n+**Knowledge Bases**\n+\n+Supported providers for **embedding models**:\n+\n+* Azure OpenAI\n+* Bedrock\n+* Google\n+* OpenAI (and OpenAI-compatible model providers)\n+* Snowflake Cortex AI\n+\n+Supported providers for **reranking models**:\n+\n+* Azure OpenAI\n+* Bedrock\n+* Google\n+* OpenAI (and OpenAI-compatible model providers)\n+* Snowflake Cortex AI\n+\n+Supported providers for **models used to evaluate knowledge bases**:\n+\n+* Azure OpenAI\n+* Bedrock\n+* Google\n+* OpenAI (and OpenAI-compatible model providers)\n+* Snowflake Cortex AI\n+\n+**Agents**\n+\n+Supported providers for **default models**:\n+\n+* Bedrock\n+* Google\n+* Ollama\n+* OpenAI (and OpenAI-compatible model providers)\n+\n+**LLM()**\n+\n+Supported providers for **default models**:\n+\n+* Ollama\n+* OpenAI (and OpenAI-compatible model providers)\n+\n+**TO_MARKDOWN()**\n+\n+Supported providers for **default models**:\n+\n+* Azure OpenAI\n+* Google\n+* OpenAI (and OpenAI-compatible model providers)\n+\n+## How to Configure System Defaults\n+\n+You can configure system defaults using either the MindsDB UI or the configuration file, depending on your setup preferences.\n+\n+The configuration variables include a provider name, a model name, and – if available – base URL, API key, and API version.\n+\n+**Option 1: Configure via MindsDB UI**\n+\n+1. Open the MindsDB UI.\n+2. Navigate to Settings → Models.\n+3. Define the models for each of the system defaults as follows:\n+a. Under Provider, select the model provider from the dropdown.\n+b. Under Model, define the model name that is available with the selected model provider.\n+c. Under Base URL, define the base URL of the model provider, if available.\n+d. Under API key, provide the API key, if available.\n+e. Under API version, define the API version, if available.\n+4.Click the Test & Save button to validate and save the configuration.\n+\n+After saving, the defaults take immediate effect across your MindsDB instance.\n+\n+**Option 2: Configure via MindsDB Configuration File**\n+\n+You can also define system defaults in the [MindsDB configuration file](/setup/custom-config). This method is recommended for advanced or automated deployments.\n+\n+When MindsDB is started with the custom configuration file, it will automatically load and apply these default models.\n+\n+**Option 3: Environment Variables**\n+\n+For functions like [LLM()](/mindsdb_sql/functions/llm_function) and [TO_MARKDOWN()](/mindsdb_sql/functions/to_markdown_function), system defaults can also be defined using environment variables. This allows for easy configuration in containerized or cloud deployments.\n+\n+Refer to the individual function documentation for details on environment variables.\n+\n+**Option 4: Define Defaults at Object Creation**\n+\n+You can specify default models when creating [agents](/mindsdb_sql/agents/agent_syntax) and [knowledge bases](/mindsdb_sql/knowledge_bases/create). These local defaults override the global system defaults for that specific object.\n+\n+This allows you to tailor model behavior per agent or per knowledge base while keeping system-wide defaults in place.",
    "comment": "For me it sounds like i could, for example, define default model for all new agents during agent creation. But when i create an agent i can define model only for that particular agent, bot 'default model' for any agent. Should we keep this block?",
    "line_number": 120,
    "enriched": "File: docs/setup/system-defaults.mdx\nCode: @@ -0,0 +1,128 @@\n+---\n+title: MindsDB System Defaults\n+sidebarTitle: System Defaults\n+---\n+\n+System defaults in MindsDB provide a convenient way to set application-wide configurations for commonly used AI models. By defining these defaults once, users can streamline workflows and avoid repeatedly specifying model parameters when creating or using various MindsDB objects and functions.\n+\n+## Usage of System Defaults\n+\n+When system defaults are set, MindsDB can automatically use the configured models across the platform for various components such as:\n+\n+* [Agents](/mindsdb_sql/agents/agent) that can answer questions over the connected data and are powered by a default large language model (LLM).\n+\n+* [Knowledge Bases](/mindsdb_sql/knowledge_bases/overview) that can store and search both structured and unstructured data, and use a default embedding model for embedding the content and a default reranking model for reranking the search results. Additionally, knowledge bases use a default model for evaluating performance with the [EVALUATE KNOWLEDGE_BASE command](/mindsdb_sql/knowledge_bases/evaluate).\n+\n+* Custom functions such as [LLM()](/mindsdb_sql/functions/llm_function) and [TO_MARKDOWN()](/mindsdb_sql/functions/to_markdown_function) that rely on the default LLM for text generation and formatting.\n+\n+Once configured, users can create and use agents, knowledge bases, and custom functions without having to specify model parameters each time. This ensures consistent behavior across the system and simplifies deployment.\n+\n+## Available System Defaults\n+\n+MindsDB supports the following system defaults:\n+\n+| System Default           | Used By                                                      | Description                                                                                       |\n+|--------------------------|--------------------------------------------------------------|---------------------------------------------------------------------------------------------------|\n+| Default LLM              | Agents, EVALUATE, KNOWLEDGE_BASE, LLM(), TO_MARKDOWN()       | Used as an underlying LLM for reasoning, conversation, and text generation and formatting.        |\n+| Default Embedding Model  | Knowledge Bases                                              | Converts inserted content and user questions into embeddings for semantic search.                 |\n+| Default Reranking Model  | Knowledge Bases                                              | Reranks search results to improve retrieval accuracy.                                             |\n+\n+## Supported Model Providers\n+\n+Different components in MindsDB support different sets of model providers.\n+\n+**Knowledge Bases**\n+\n+Supported providers for **embedding models**:\n+\n+* Azure OpenAI\n+* Bedrock\n+* Google\n+* OpenAI (and OpenAI-compatible model providers)\n+* Snowflake Cortex AI\n+\n+Supported providers for **reranking models**:\n+\n+* Azure OpenAI\n+* Bedrock\n+* Google\n+* OpenAI (and OpenAI-compatible model providers)\n+* Snowflake Cortex AI\n+\n+Supported providers for **models used to evaluate knowledge bases**:\n+\n+* Azure OpenAI\n+* Bedrock\n+* Google\n+* OpenAI (and OpenAI-compatible model providers)\n+* Snowflake Cortex AI\n+\n+**Agents**\n+\n+Supported providers for **default models**:\n+\n+* Bedrock\n+* Google\n+* Ollama\n+* OpenAI (and OpenAI-compatible model providers)\n+\n+**LLM()**\n+\n+Supported providers for **default models**:\n+\n+* Ollama\n+* OpenAI (and OpenAI-compatible model providers)\n+\n+**TO_MARKDOWN()**\n+\n+Supported providers for **default models**:\n+\n+* Azure OpenAI\n+* Google\n+* OpenAI (and OpenAI-compatible model providers)\n+\n+## How to Configure System Defaults\n+\n+You can configure system defaults using either the MindsDB UI or the configuration file, depending on your setup preferences.\n+\n+The configuration variables include a provider name, a model name, and – if available – base URL, API key, and API version.\n+\n+**Option 1: Configure via MindsDB UI**\n+\n+1. Open the MindsDB UI.\n+2. Navigate to Settings → Models.\n+3. Define the models for each of the system defaults as follows:\n+a. Under Provider, select the model provider from the dropdown.\n+b. Under Model, define the model name that is available with the selected model provider.\n+c. Under Base URL, define the base URL of the model provider, if available.\n+d. Under API key, provide the API key, if available.\n+e. Under API version, define the API version, if available.\n+4.Click the Test & Save button to validate and save the configuration.\n+\n+After saving, the defaults take immediate effect across your MindsDB instance.\n+\n+**Option 2: Configure via MindsDB Configuration File**\n+\n+You can also define system defaults in the [MindsDB configuration file](/setup/custom-config). This method is recommended for advanced or automated deployments.\n+\n+When MindsDB is started with the custom configuration file, it will automatically load and apply these default models.\n+\n+**Option 3: Environment Variables**\n+\n+For functions like [LLM()](/mindsdb_sql/functions/llm_function) and [TO_MARKDOWN()](/mindsdb_sql/functions/to_markdown_function), system defaults can also be defined using environment variables. This allows for easy configuration in containerized or cloud deployments.\n+\n+Refer to the individual function documentation for details on environment variables.\n+\n+**Option 4: Define Defaults at Object Creation**\n+\n+You can specify default models when creating [agents](/mindsdb_sql/agents/agent_syntax) and [knowledge bases](/mindsdb_sql/knowledge_bases/create). These local defaults override the global system defaults for that specific object.\n+\n+This allows you to tailor model behavior per agent or per knowledge base while keeping system-wide defaults in place.\nComment: For me it sounds like i could, for example, define default model for all new agents during agent creation. But when i create an agent i can define model only for that particular agent, bot 'default model' for any agent. Should we keep this block?",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/setup/system-defaults.mdx",
    "pr_number": 11922,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2571736248,
    "comment_created_at": "2025-11-28T13:43:02Z"
  },
  {
    "code": "@@ -64,6 +64,7 @@\n     \"name\": \"Documentation\"\n   },\n   \"redirects\": [\n+    {\"source\": \"/app-integrations/binance\",\"destination\": \"/integrations/app-integrations/binance\"},",
    "comment": "You don't have to add it to redirects, as these are for pages that used to have a different link.\r\n\r\nPlease remove this line.",
    "line_number": 67,
    "enriched": "File: docs/mint.json\nCode: @@ -64,6 +64,7 @@\n     \"name\": \"Documentation\"\n   },\n   \"redirects\": [\n+    {\"source\": \"/app-integrations/binance\",\"destination\": \"/integrations/app-integrations/binance\"},\nComment: You don't have to add it to redirects, as these are for pages that used to have a different link.\r\n\r\nPlease remove this line.",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/mint.json",
    "pr_number": 7088,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1291159487,
    "comment_created_at": "2023-08-11T10:11:30Z"
  },
  {
    "code": "@@ -0,0 +1,55 @@\n+---\n+title: Microsoft Teams\n+sidebarTitle: Microsoft Teams\n+---\n+\n+In this section, we present how to connect Microsoft Teams to MindsDB.\n+\n+[Microsoft Teams](https://support.microsoft.com/en-us/topic/what-is-microsoft-teams-3de4d369-0167-8def-b93b-0eb5286d7a29) is the ultimate messaging app for your organization—a workspace for real-time collaboration and communication, meetings, file and app sharing, and even the occasional emoji! All in one place, all in the open, all accessible to everyone.",
    "comment": "Let's change the link to this one: https://www.microsoft.com/en/microsoft-teams/group-chat-software/",
    "line_number": 8,
    "enriched": "File: docs/integrations/app-integrations/microsoft-teams.mdx\nCode: @@ -0,0 +1,55 @@\n+---\n+title: Microsoft Teams\n+sidebarTitle: Microsoft Teams\n+---\n+\n+In this section, we present how to connect Microsoft Teams to MindsDB.\n+\n+[Microsoft Teams](https://support.microsoft.com/en-us/topic/what-is-microsoft-teams-3de4d369-0167-8def-b93b-0eb5286d7a29) is the ultimate messaging app for your organization—a workspace for real-time collaboration and communication, meetings, file and app sharing, and even the occasional emoji! All in one place, all in the open, all accessible to everyone.\nComment: Let's change the link to this one: https://www.microsoft.com/en/microsoft-teams/group-chat-software/",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/integrations/app-integrations/microsoft-teams.mdx",
    "pr_number": 7422,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1337467007,
    "comment_created_at": "2023-09-26T16:11:59Z"
  },
  {
    "code": "@@ -0,0 +1,133 @@\n+---\n+title: Couchbase\n+sidebarTitle: Couchbase\n+---\n+\n+\n+\n+This is the implementation of the Couchbase Vector store data handler for MindsDB.\n+\n+[Couchbase](https://www.couchbase.com/) is an open-source, distributed multi-model NoSQL document-oriented database software package optimized for interactive applications. These applications may serve many concurrent users by creating, storing, retrieving, aggregating, manipulating, and presenting data.\n+\n+## Prerequisites\n+\n+Before proceeding, ensure the following prerequisites are met:\n+\n+1. Install MindsDB locally via [Docker](/setup/self-hosted/docker) or [Docker Desktop](/setup/self-hosted/docker-desktop).\n+2. To connect Couchbase to MindsDB, install the required dependencies following [this instruction](/setup/self-hosted/docker#install-dependencies).\n+3. Install or ensure access to Couchbase.\n+\n+## Implementation\n+\n+This handler is implemented using the `couchbase` library, the Python driver for Couchbase.\n+\n+The required arguments to establish a connection are as follows:\n+* `connection_string`: the connection string for the endpoint of the Couchbase server\n+* `bucket`: the bucket name to use when connecting with the Couchbase server\n+* `user`: the user to authenticate with the Couchbase server\n+* `password`: the password to authenticate the user with the Couchbase server\n+* `scope`:  scopes are a level of data organization within a bucket. If omitted, will default to `_default`\n+\n+Note: The connection string expects either the couchbases:// or couchbase:// protocol.\n+\n+<Tip>\n+If you are using Couchbase Capella, you can find the connection_string under the Connect tab.\n+It will also be required to whitelist the machine(s) that will be running MindsDB and database credentials will need to be created for the user. These steps can also be taken under the Connect tab.\n+</Tip>\n+\n+In order to make use of this handler and connect to a Couchbase server in MindsDB, the following syntax can be used. Note, that the example uses the default `travel-sample` bucket which can be enabled from the couchbase UI with pre-defined scope and documents. ",
    "comment": "Let's move lines 38-51 to the beginning of the `Implementation` section. So we show the command first, and then we explain the parameters.",
    "line_number": 38,
    "enriched": "File: docs/integrations/vector-db-integrations/couchbase.mdx\nCode: @@ -0,0 +1,133 @@\n+---\n+title: Couchbase\n+sidebarTitle: Couchbase\n+---\n+\n+\n+\n+This is the implementation of the Couchbase Vector store data handler for MindsDB.\n+\n+[Couchbase](https://www.couchbase.com/) is an open-source, distributed multi-model NoSQL document-oriented database software package optimized for interactive applications. These applications may serve many concurrent users by creating, storing, retrieving, aggregating, manipulating, and presenting data.\n+\n+## Prerequisites\n+\n+Before proceeding, ensure the following prerequisites are met:\n+\n+1. Install MindsDB locally via [Docker](/setup/self-hosted/docker) or [Docker Desktop](/setup/self-hosted/docker-desktop).\n+2. To connect Couchbase to MindsDB, install the required dependencies following [this instruction](/setup/self-hosted/docker#install-dependencies).\n+3. Install or ensure access to Couchbase.\n+\n+## Implementation\n+\n+This handler is implemented using the `couchbase` library, the Python driver for Couchbase.\n+\n+The required arguments to establish a connection are as follows:\n+* `connection_string`: the connection string for the endpoint of the Couchbase server\n+* `bucket`: the bucket name to use when connecting with the Couchbase server\n+* `user`: the user to authenticate with the Couchbase server\n+* `password`: the password to authenticate the user with the Couchbase server\n+* `scope`:  scopes are a level of data organization within a bucket. If omitted, will default to `_default`\n+\n+Note: The connection string expects either the couchbases:// or couchbase:// protocol.\n+\n+<Tip>\n+If you are using Couchbase Capella, you can find the connection_string under the Connect tab.\n+It will also be required to whitelist the machine(s) that will be running MindsDB and database credentials will need to be created for the user. These steps can also be taken under the Connect tab.\n+</Tip>\n+\n+In order to make use of this handler and connect to a Couchbase server in MindsDB, the following syntax can be used. Note, that the example uses the default `travel-sample` bucket which can be enabled from the couchbase UI with pre-defined scope and documents. \nComment: Let's move lines 38-51 to the beginning of the `Implementation` section. So we show the command first, and then we explain the parameters.",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/integrations/vector-db-integrations/couchbase.mdx",
    "pr_number": 10039,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1816359600,
    "comment_created_at": "2024-10-25T09:35:33Z"
  },
  {
    "code": "@@ -174,8 +174,8 @@ The combination of MindsDB and your database covers all the phases of the ML lif\n \n \n MindsDB provides easy-to-use predictive models through AI Tables. You can create these predictive models using SQL statements and feeding the input data. Also, you can query them the same way you query a table. The easiest way to get started with Superset is with the free tier for [Preset Cloud](https://preset.io/product/), a hassle-free and fully hosted cloud service for Superset.\n-\n-We encourage you to try some predictions with your own data, so please sign up for a [free MindsDB cloud account](https://cloud.mindsdb.com/signup) and if you need any help with MindsDB, feel free to ask our [Slack](https://join.slack.com/t/mindsdbcommunity/shared_invite/zt-o8mrmx3l-5ai~5H66s6wlxFfBMVI6wQ) and [Github](https://github.com/mindsdb/mindsdb/discussions) communities.\n+[Slack](https://mindsdb.com/joincommunity)\n+We encourage you to try some predictions with your own data, so please sign up for a [free MindsDB cloud account](https://cloud.mindsdb.com/signup) and if you need any help with MindsDB, feel free to ask our  and [Github](https://github.com/mindsdb/mindsdb/discussions) communities.",
    "comment": "You moved the link to the previous line, which is not as in the instructions.\r\n\r\nPlease bring the link back to the sentence. The sentence should be as below:\r\n\r\nWe encourage you to try some predictions with your own data, so please sign up for a \\[free MindsDB cloud account](https://cloud.mindsdb.com/signup) and if you need any help with MindsDB, feel free to ask **our \\[Slack](https://mindsdb.com/joincommunity)** and \\[Github](https://github.com/mindsdb/mindsdb/discussions) communities.\r\n",
    "line_number": 178,
    "enriched": "File: docs/sql/tutorials/mindsdb-superset-snowflake.mdx\nCode: @@ -174,8 +174,8 @@ The combination of MindsDB and your database covers all the phases of the ML lif\n \n \n MindsDB provides easy-to-use predictive models through AI Tables. You can create these predictive models using SQL statements and feeding the input data. Also, you can query them the same way you query a table. The easiest way to get started with Superset is with the free tier for [Preset Cloud](https://preset.io/product/), a hassle-free and fully hosted cloud service for Superset.\n-\n-We encourage you to try some predictions with your own data, so please sign up for a [free MindsDB cloud account](https://cloud.mindsdb.com/signup) and if you need any help with MindsDB, feel free to ask our [Slack](https://join.slack.com/t/mindsdbcommunity/shared_invite/zt-o8mrmx3l-5ai~5H66s6wlxFfBMVI6wQ) and [Github](https://github.com/mindsdb/mindsdb/discussions) communities.\n+[Slack](https://mindsdb.com/joincommunity)\n+We encourage you to try some predictions with your own data, so please sign up for a [free MindsDB cloud account](https://cloud.mindsdb.com/signup) and if you need any help with MindsDB, feel free to ask our  and [Github](https://github.com/mindsdb/mindsdb/discussions) communities.\nComment: You moved the link to the previous line, which is not as in the instructions.\r\n\r\nPlease bring the link back to the sentence. The sentence should be as below:\r\n\r\nWe encourage you to try some predictions with your own data, so please sign up for a \\[free MindsDB cloud account](https://cloud.mindsdb.com/signup) and if you need any help with MindsDB, feel free to ask **our \\[Slack](https://mindsdb.com/joincommunity)** and \\[Github](https://github.com/mindsdb/mindsdb/discussions) communities.\r\n",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/sql/tutorials/mindsdb-superset-snowflake.mdx",
    "pr_number": 6433,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1214625716,
    "comment_created_at": "2023-06-02T17:15:02Z"
  },
  {
    "code": "@@ -0,0 +1,73 @@\n+---\n+title: Instatus\n+sidebarTitle: Instatus\n+---\n+\n+In this section, we present how to connect Instatus to MindsDB.\n+\n+[Instatus](https://instatus.com/) is a cloud-based status page software that enables users to communicate status information using incidents and maintenances. It serves as a SaaS platform for creating status pages for services.\n+\n+The Instatus Handler for MindsDB offers an interface to connect with Instatus via APIs and retrieve status pages.\n+\n+## Connection\n+\n+Initialize the Instatus handler with the following parameter:\n+\n+- `api_key`: Instatus API key for authentication. Obtain it from [Instatus Developer Dashboard](https://dashboard.instatus.com/developer).\n+\n+Start by creating a database with the new instatus engine using the following SQL command:\n+\n+```sql\n+CREATE DATABASE mindsdb_instatus --- Display name for the database.\n+WITH\n+    ENGINE = 'instatus', --- Name of the MindsDB handler.\n+    PARAMETERS = {\n+        \"api_key\": \"<your-instatus-api-key>\" --- Instatus API key to use for authentication.\n+    };\n+```\n+\n+## Usage\n+\n+To get a status page, use the `SELECT` statement:\n+\n+```sql\n+SELECT id, name, status, subdomain\n+FROM mindsdb_instatus.status_pages\n+WHERE id = '<status-page-id>'\n+LIMIT 10;\n+```\n+\n+To create a new status page, use the `INSERT` statement:\n+\n+```sql\n+INSERT INTO mindsdb_instatus.status_pages (email, name, subdomain, components, logoUrl, faviconUrl, websiteUrl, language, useLargeHeader, brandColor, okColor, disruptedColor, degradedColor, downColor, noticeColor, unknownColor, googleAnalytics, subscribeBySms, smsService, twilioSid, twilioToken, twilioSender, nexmoKey, nexmoSecret, nexmoSender, htmlInMeta, htmlAboveHeader, htmlBelowHeader, htmlAboveFooter, htmlBelowFooter, htmlBelowSummary, cssGlobal, launchDate, dateFormat, dateFormatShort, timeFormat)\n+VALUES ('yourname@gmail.com', 'mindsdb', 'mindsdb-instatus', '[\"Website\", \"App\", \"API\"]', 'https://instatus.com/sample.png', 'https://instatus.com/favicon-32x32.png', 'https://instatus.com', 'en', true, '#111', '#33B17E', '#FF8C03', '#ECC94B', '#DC123D', '#70808F', '#DFE0E1', 'UA-00000000-1', true, 'twilio', 'YOUR_TWILIO_SID', 'YOUR_TWILIO_TOKEN', 'YOUR_TWILIO_SENDER', null, null, null, null, null, null, null, null, null, null, 'MMMMMM d, yyyy', 'MMM yyyy', 'p');\n+```\n+\n+<Tip>\n+**Note**:\n+\n+- `email` is a required field (Example: 'yourname@gmail.com')\n+- `name` is a required field (Example: 'mindsdb')\n+- `subdomain` is a required field (Example: 'mindsdb-docs')\n+- `components` is a required field (Example: '[\"Website\", \"App\", \"API\"]')\n+- Other fields are optional.\n+</Tip>",
    "comment": "Let's update this Tip box to the following:\r\n```\r\n<Tip>\r\nThe following fields are required when inserting new status pages:\r\n\r\n- `email` (e.g. 'yourname@gmail.com')\r\n- `name` (e.g 'mindsdb')\r\n- `subdomain` (e.g. 'mindsdb-docs')\r\n- `components` (e.g. '[\"Website\", \"App\", \"API\"]')\r\n\r\nThe other fields are optional.\r\n</Tip>\r\n```",
    "line_number": 55,
    "enriched": "File: docs/integrations/app-integrations/instatus.mdx\nCode: @@ -0,0 +1,73 @@\n+---\n+title: Instatus\n+sidebarTitle: Instatus\n+---\n+\n+In this section, we present how to connect Instatus to MindsDB.\n+\n+[Instatus](https://instatus.com/) is a cloud-based status page software that enables users to communicate status information using incidents and maintenances. It serves as a SaaS platform for creating status pages for services.\n+\n+The Instatus Handler for MindsDB offers an interface to connect with Instatus via APIs and retrieve status pages.\n+\n+## Connection\n+\n+Initialize the Instatus handler with the following parameter:\n+\n+- `api_key`: Instatus API key for authentication. Obtain it from [Instatus Developer Dashboard](https://dashboard.instatus.com/developer).\n+\n+Start by creating a database with the new instatus engine using the following SQL command:\n+\n+```sql\n+CREATE DATABASE mindsdb_instatus --- Display name for the database.\n+WITH\n+    ENGINE = 'instatus', --- Name of the MindsDB handler.\n+    PARAMETERS = {\n+        \"api_key\": \"<your-instatus-api-key>\" --- Instatus API key to use for authentication.\n+    };\n+```\n+\n+## Usage\n+\n+To get a status page, use the `SELECT` statement:\n+\n+```sql\n+SELECT id, name, status, subdomain\n+FROM mindsdb_instatus.status_pages\n+WHERE id = '<status-page-id>'\n+LIMIT 10;\n+```\n+\n+To create a new status page, use the `INSERT` statement:\n+\n+```sql\n+INSERT INTO mindsdb_instatus.status_pages (email, name, subdomain, components, logoUrl, faviconUrl, websiteUrl, language, useLargeHeader, brandColor, okColor, disruptedColor, degradedColor, downColor, noticeColor, unknownColor, googleAnalytics, subscribeBySms, smsService, twilioSid, twilioToken, twilioSender, nexmoKey, nexmoSecret, nexmoSender, htmlInMeta, htmlAboveHeader, htmlBelowHeader, htmlAboveFooter, htmlBelowFooter, htmlBelowSummary, cssGlobal, launchDate, dateFormat, dateFormatShort, timeFormat)\n+VALUES ('yourname@gmail.com', 'mindsdb', 'mindsdb-instatus', '[\"Website\", \"App\", \"API\"]', 'https://instatus.com/sample.png', 'https://instatus.com/favicon-32x32.png', 'https://instatus.com', 'en', true, '#111', '#33B17E', '#FF8C03', '#ECC94B', '#DC123D', '#70808F', '#DFE0E1', 'UA-00000000-1', true, 'twilio', 'YOUR_TWILIO_SID', 'YOUR_TWILIO_TOKEN', 'YOUR_TWILIO_SENDER', null, null, null, null, null, null, null, null, null, null, 'MMMMMM d, yyyy', 'MMM yyyy', 'p');\n+```\n+\n+<Tip>\n+**Note**:\n+\n+- `email` is a required field (Example: 'yourname@gmail.com')\n+- `name` is a required field (Example: 'mindsdb')\n+- `subdomain` is a required field (Example: 'mindsdb-docs')\n+- `components` is a required field (Example: '[\"Website\", \"App\", \"API\"]')\n+- Other fields are optional.\n+</Tip>\nComment: Let's update this Tip box to the following:\r\n```\r\n<Tip>\r\nThe following fields are required when inserting new status pages:\r\n\r\n- `email` (e.g. 'yourname@gmail.com')\r\n- `name` (e.g 'mindsdb')\r\n- `subdomain` (e.g. 'mindsdb-docs')\r\n- `components` (e.g. '[\"Website\", \"App\", \"API\"]')\r\n\r\nThe other fields are optional.\r\n</Tip>\r\n```",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/integrations/app-integrations/instatus.mdx",
    "pr_number": 8271,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1389248898,
    "comment_created_at": "2023-11-10T10:56:20Z"
  },
  {
    "code": "@@ -317,6 +317,15 @@ def update_customers(self, customer_ids: List[int], values_to_update: List[Dict[\n             customer.save()\n             logger.info(f'Customer {customer_id} updated')\n \n+    def __init__(self, handler: ShopifyHandler):",
    "comment": "This `__init__` function is not necessary here. Please see this implementation for an example: https://github.com/mindsdb/mindsdb/blob/staging/mindsdb/integrations/handlers/shopify_handler/shopify_tables.py#L86",
    "line_number": 320,
    "enriched": "File: mindsdb/integrations/handlers/shopify_handler/shopify_tables.py\nCode: @@ -317,6 +317,15 @@ def update_customers(self, customer_ids: List[int], values_to_update: List[Dict[\n             customer.save()\n             logger.info(f'Customer {customer_id} updated')\n \n+    def __init__(self, handler: ShopifyHandler):\nComment: This `__init__` function is not necessary here. Please see this implementation for an example: https://github.com/mindsdb/mindsdb/blob/staging/mindsdb/integrations/handlers/shopify_handler/shopify_tables.py#L86",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "mindsdb/integrations/handlers/shopify_handler/shopify_tables.py",
    "pr_number": 7824,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1361019205,
    "comment_created_at": "2023-10-16T17:24:17Z"
  },
  {
    "code": "@@ -0,0 +1,50 @@\n+---\n+title: Apache Ignite\n+sidebarTitle: Apache Ignite\n+---\n+\n+This is the implementation of the Apache Ignite data handler for MindsDB.\n+\n+[Apache Ignite](https://ignite.apache.org/docs/latest/) is a distributed database for high-performance computing with in-memory speed.\n+\n+## Implementation\n+",
    "comment": "Please add this line here:\r\n```\r\nThis handler is implemented using the `pyignite` library, the Apache Ignite thin (binary protocol) client for Python.\r\n```",
    "line_number": 11,
    "enriched": "File: docs/data-integrations/apache-ignite.mdx\nCode: @@ -0,0 +1,50 @@\n+---\n+title: Apache Ignite\n+sidebarTitle: Apache Ignite\n+---\n+\n+This is the implementation of the Apache Ignite data handler for MindsDB.\n+\n+[Apache Ignite](https://ignite.apache.org/docs/latest/) is a distributed database for high-performance computing with in-memory speed.\n+\n+## Implementation\n+\nComment: Please add this line here:\r\n```\r\nThis handler is implemented using the `pyignite` library, the Apache Ignite thin (binary protocol) client for Python.\r\n```",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/data-integrations/apache-ignite.mdx",
    "pr_number": 6866,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1266798771,
    "comment_created_at": "2023-07-18T13:44:03Z"
  },
  {
    "code": "@@ -0,0 +1,134 @@\n+---\n+title: MindsDB and Grafana\n+sidebarTitle: Grafana\n+---\n+\n+[Grafana](https://grafana.com/) is an open-source analytics and interactive visualization web application\n+that allows users to ingest data from various sources, query this data, and display it on customizable charts for easy analysis. \n+\n+## Data Source Grafana Setup",
    "comment": "Let's call this section `## How to Connect`",
    "line_number": 9,
    "enriched": "File: docs/connect/grafana.mdx\nCode: @@ -0,0 +1,134 @@\n+---\n+title: MindsDB and Grafana\n+sidebarTitle: Grafana\n+---\n+\n+[Grafana](https://grafana.com/) is an open-source analytics and interactive visualization web application\n+that allows users to ingest data from various sources, query this data, and display it on customizable charts for easy analysis. \n+\n+## Data Source Grafana Setup\nComment: Let's call this section `## How to Connect`",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/connect/grafana.mdx",
    "pr_number": 8276,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1391332742,
    "comment_created_at": "2023-11-13T16:11:13Z"
  },
  {
    "code": "@@ -0,0 +1,11 @@\n+---\n+title: List Files\n+openapi: \"GET /api/files\"\n+sidebarTitle: List Files\n+---\n+\n+<Note>\n+\n+The REST API playground can currently only be used with MindsDB running locally at http://127.0.0.1:47334. ",
    "comment": "Maybe add `api` path to the localhost `http://127.0.0.1:47334/api` and remove the `playground`",
    "line_number": 9,
    "enriched": "File: docs/rest/files/list.mdx\nCode: @@ -0,0 +1,11 @@\n+---\n+title: List Files\n+openapi: \"GET /api/files\"\n+sidebarTitle: List Files\n+---\n+\n+<Note>\n+\n+The REST API playground can currently only be used with MindsDB running locally at http://127.0.0.1:47334. \nComment: Maybe add `api` path to the localhost `http://127.0.0.1:47334/api` and remove the `playground`",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/rest/files/list.mdx",
    "pr_number": 9253,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1617034462,
    "comment_created_at": "2024-05-28T11:08:52Z"
  },
  {
    "code": "@@ -0,0 +1,83 @@\n+<div align=\"center\">\n+  <a href=\"https://qdrant.tech/\">\n+    <img height=\"100\" width=\"450\" style=\"display: inline-block;\" src=\"https://github.com/qdrant/qdrant/raw/master/docs/logo.svg\" alt=\"Qdrant\">\n+  </a>\n+  <h1><a href=\"https://qdrant.tech/\">qdrant.tech</a> handler for MindsDB<h1>\n+</div>\n+\n+## About Qdrant 🚀\n+\n+A High-performance, massive-scale vector database for the next generation of AI. Also available in the cloud.\n+\n+## Implementation\n+\n+The handler uses the [qdrant-client](https://github.com/qdrant/qdrant-client) Python library to establish a connection to a Qdrant instance.\n+\n+\n+## Usage\n+To use this handler and get started with Qdrant, the following syntax can be used.\n+```sql\n+CREATE DATABASE qdrant_test\n+WITH ENGINE = \"qdrant\",\n+PARAMETERS = {\n+    \"location\": \":memory:\",\n+    \"collection_config\": {\n+        \"size\": 386,\n+        \"distance\": \"Cosine\"\n+    }\n+}\n+```\n+The available arguments for instantiating Qdrant can be found [here](https://github.com/Anush008/mindsdb/blob/d3a1f861a481de09ba70796f84890fe1ed03d74c/mindsdb/integrations/handlers/qdrant_handler/qdrant_handler.py#L410-L478).\n+\n+## Creating a new table\n+\n+- Qdrant options for creating a collection can be specified as `collection_config` in the `CREATE DATABASE` parameters.\n+- By default, UUIDs are set as collection IDs. You can provide your own IDs under the `ids` column.\n+```sql\n+CREATE TABLE qdrant_test.test_table (\n+   SELECT embeddings,'{\"source\": \"bbc\"}' as metadata FROM mysql_demo_db.test_embeddings\n+);\n+```\n+\n+## Querying the database\n+\n+#### Perform a full retrieval using the following syntax.\n+\n+```sql\n+SELECT * FROM qdrant_test.test_table\n+```\n+By default, the `LIMIT` is set to 10 and the `OFFSET` is set to 0.\n+\n+#### Perform a similarity search using your embeddings, like so\n+```sql\n+SELECT * FROM qdrant_test.test_table\n+WHERE search_vector = (select embeddings from mysql_demo_db.test_embeddings limit 1)\n+```\n+\n+#### Perform a search using filters\n+```sql\n+SELECT * FROM qdrant_test.test_table\n+WHERE `metadata.source` = 'bbc';\n+```\n+\n+#### Delete entries using IDs\n+```sql\n+DELETE FROM qtest.test_table_6\n+WHERE id = 2\n+```\n+\n+#### Delete entries using filters\n+```sql\n+DELETE * FROM qdrant_test.test_table\n+WHERE `metadata.source` = 'bbc';\n+```\n+\n+#### Drop a table\n+```sql\n+ DROP TABLE qdrant_test.test_table;\n+```\n+\n+## NOTICE\n+Qdrant supports payload indexing that vastly improves retrieval efficiency with filters and is highly recommended. Please note that this feature currently cannot be configured via MindsDB and must be set up separately if needed.",
    "comment": "please create an issue so we can not forget about supporting this in future",
    "line_number": 81,
    "enriched": "File: mindsdb/integrations/handlers/qdrant_handler/README.md\nCode: @@ -0,0 +1,83 @@\n+<div align=\"center\">\n+  <a href=\"https://qdrant.tech/\">\n+    <img height=\"100\" width=\"450\" style=\"display: inline-block;\" src=\"https://github.com/qdrant/qdrant/raw/master/docs/logo.svg\" alt=\"Qdrant\">\n+  </a>\n+  <h1><a href=\"https://qdrant.tech/\">qdrant.tech</a> handler for MindsDB<h1>\n+</div>\n+\n+## About Qdrant 🚀\n+\n+A High-performance, massive-scale vector database for the next generation of AI. Also available in the cloud.\n+\n+## Implementation\n+\n+The handler uses the [qdrant-client](https://github.com/qdrant/qdrant-client) Python library to establish a connection to a Qdrant instance.\n+\n+\n+## Usage\n+To use this handler and get started with Qdrant, the following syntax can be used.\n+```sql\n+CREATE DATABASE qdrant_test\n+WITH ENGINE = \"qdrant\",\n+PARAMETERS = {\n+    \"location\": \":memory:\",\n+    \"collection_config\": {\n+        \"size\": 386,\n+        \"distance\": \"Cosine\"\n+    }\n+}\n+```\n+The available arguments for instantiating Qdrant can be found [here](https://github.com/Anush008/mindsdb/blob/d3a1f861a481de09ba70796f84890fe1ed03d74c/mindsdb/integrations/handlers/qdrant_handler/qdrant_handler.py#L410-L478).\n+\n+## Creating a new table\n+\n+- Qdrant options for creating a collection can be specified as `collection_config` in the `CREATE DATABASE` parameters.\n+- By default, UUIDs are set as collection IDs. You can provide your own IDs under the `ids` column.\n+```sql\n+CREATE TABLE qdrant_test.test_table (\n+   SELECT embeddings,'{\"source\": \"bbc\"}' as metadata FROM mysql_demo_db.test_embeddings\n+);\n+```\n+\n+## Querying the database\n+\n+#### Perform a full retrieval using the following syntax.\n+\n+```sql\n+SELECT * FROM qdrant_test.test_table\n+```\n+By default, the `LIMIT` is set to 10 and the `OFFSET` is set to 0.\n+\n+#### Perform a similarity search using your embeddings, like so\n+```sql\n+SELECT * FROM qdrant_test.test_table\n+WHERE search_vector = (select embeddings from mysql_demo_db.test_embeddings limit 1)\n+```\n+\n+#### Perform a search using filters\n+```sql\n+SELECT * FROM qdrant_test.test_table\n+WHERE `metadata.source` = 'bbc';\n+```\n+\n+#### Delete entries using IDs\n+```sql\n+DELETE FROM qtest.test_table_6\n+WHERE id = 2\n+```\n+\n+#### Delete entries using filters\n+```sql\n+DELETE * FROM qdrant_test.test_table\n+WHERE `metadata.source` = 'bbc';\n+```\n+\n+#### Drop a table\n+```sql\n+ DROP TABLE qdrant_test.test_table;\n+```\n+\n+## NOTICE\n+Qdrant supports payload indexing that vastly improves retrieval efficiency with filters and is highly recommended. Please note that this feature currently cannot be configured via MindsDB and must be set up separately if needed.\nComment: please create an issue so we can not forget about supporting this in future",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "mindsdb/integrations/handlers/qdrant_handler/README.md",
    "pr_number": 7833,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1363676879,
    "comment_created_at": "2023-10-18T11:01:10Z"
  },
  {
    "code": "@@ -0,0 +1,96 @@\n+---\n+title: Prometheus\n+sidebarTitle: Prometheus Handler\n+---\n+\n+This documentation describes the intragration of MindsDB with [Prometheus](https://prometheus.io/), an open-source solution that allows you to collect different types of metrics from your host fleet in order to monitor their health, perfomance, usage, etc. \n+\n+## Data Models \n+The data model used by Prometheus to store data is quite different that the relational model of SQL queries. Prometheus uses a timeseries model and exposes the stored data via PromQL. Translating any query from SQL to PromQL would be quite complex and no library was found that could serve that need. For the sake of providing some basic capability, we made some assumptions on how to map Prometheus' timeseries model to a relational model and we support specific types of queries which are simple to translate:\n+\n+1. Each Prometheus metric is mapped to a SQL table.\n+2. Each Prometheus label is mapped to a SQL column. \n+2. We support SELECT queries with WHERE filters and AND conditions. \n+\n+For example, consider the following PromQL query and its SQL mapping: \n+\n+```\n+http_requests_total{environment=\"development\"}\n+```\n+\n+```sql\n+SELECT\n+    *\n+FROM http_requests_total\n+WHERE environment = \"development\"\n+```\n+\n+While quite restrictive, this simplification would make posible a large amount of simple use-cases. \n+\n+## Prerequisites\n+\n+Before proceeding, ensure the following prerequisites are met:\n+\n+1. Install MindsDB locally via [Docker](/setup/self-hosted/docker) or [Docker Desktop](/setup/self-hosted/docker-desktop).\n+2. Install ```prometheus-api-client``` the only requirement of the Prometheus handler: \n+\n+```bash \n+pip install prometheus-api-client\n+```\n+\n+## Connection\n+To setup a connection to Prometheus host, run the following SQL in the MindsDB interface: \n+\n+```sql\n+CREATE DATABASE prometheus_sourxe",
    "comment": "There's a small type here in the name of the data source. It should be `prometheus_source`.",
    "line_number": 45,
    "enriched": "File: mindsdb/integrations/handlers/prometheus_handler/README.md\nCode: @@ -0,0 +1,96 @@\n+---\n+title: Prometheus\n+sidebarTitle: Prometheus Handler\n+---\n+\n+This documentation describes the intragration of MindsDB with [Prometheus](https://prometheus.io/), an open-source solution that allows you to collect different types of metrics from your host fleet in order to monitor their health, perfomance, usage, etc. \n+\n+## Data Models \n+The data model used by Prometheus to store data is quite different that the relational model of SQL queries. Prometheus uses a timeseries model and exposes the stored data via PromQL. Translating any query from SQL to PromQL would be quite complex and no library was found that could serve that need. For the sake of providing some basic capability, we made some assumptions on how to map Prometheus' timeseries model to a relational model and we support specific types of queries which are simple to translate:\n+\n+1. Each Prometheus metric is mapped to a SQL table.\n+2. Each Prometheus label is mapped to a SQL column. \n+2. We support SELECT queries with WHERE filters and AND conditions. \n+\n+For example, consider the following PromQL query and its SQL mapping: \n+\n+```\n+http_requests_total{environment=\"development\"}\n+```\n+\n+```sql\n+SELECT\n+    *\n+FROM http_requests_total\n+WHERE environment = \"development\"\n+```\n+\n+While quite restrictive, this simplification would make posible a large amount of simple use-cases. \n+\n+## Prerequisites\n+\n+Before proceeding, ensure the following prerequisites are met:\n+\n+1. Install MindsDB locally via [Docker](/setup/self-hosted/docker) or [Docker Desktop](/setup/self-hosted/docker-desktop).\n+2. Install ```prometheus-api-client``` the only requirement of the Prometheus handler: \n+\n+```bash \n+pip install prometheus-api-client\n+```\n+\n+## Connection\n+To setup a connection to Prometheus host, run the following SQL in the MindsDB interface: \n+\n+```sql\n+CREATE DATABASE prometheus_sourxe\nComment: There's a small type here in the name of the data source. It should be `prometheus_source`.",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "mindsdb/integrations/handlers/prometheus_handler/README.md",
    "pr_number": 9654,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1785745531,
    "comment_created_at": "2024-10-03T07:19:32Z"
  },
  {
    "code": "@@ -5,66 +5,79 @@ sidebarTitle: YouTube\n \n In this section, we present how to connect YouTube to MindsDB.\n \n-[YouTube](https://www.youtube.com/) is a popular online video-sharing platform and social media website where users can upload, view, share, and interact with videos created by individuals and organizations from around the world.\n+[YouTube](https://www.youtube.com/) is a popular online video-sharing platform and social media website where users\n+can upload, view, share, and interact with videos created by individuals and organizations from around the world.\n \n-Data from YouTube can be utilized within MindsDB to train AI models and chatbots based on user comments, detect sentiment of comments, or use AI models to create replies.\n+Comment Data from YouTube can be utilized within MindsDB to train AI models and chatbots based on user comments,\n+detect sentiment of comments, or use AI models to create replies.\n \n-## Connection\n+## Install the Handler",
    "comment": "Throughout the app integration docs, we use sections \"Connection\" and \"Usage\".  Let's keep it consistent.\r\nSo let's change it back to Connection.",
    "line_number": 14,
    "enriched": "File: docs/integrations/app-integrations/youtube.mdx\nCode: @@ -5,66 +5,79 @@ sidebarTitle: YouTube\n \n In this section, we present how to connect YouTube to MindsDB.\n \n-[YouTube](https://www.youtube.com/) is a popular online video-sharing platform and social media website where users can upload, view, share, and interact with videos created by individuals and organizations from around the world.\n+[YouTube](https://www.youtube.com/) is a popular online video-sharing platform and social media website where users\n+can upload, view, share, and interact with videos created by individuals and organizations from around the world.\n \n-Data from YouTube can be utilized within MindsDB to train AI models and chatbots based on user comments, detect sentiment of comments, or use AI models to create replies.\n+Comment Data from YouTube can be utilized within MindsDB to train AI models and chatbots based on user comments,\n+detect sentiment of comments, or use AI models to create replies.\n \n-## Connection\n+## Install the Handler\nComment: Throughout the app integration docs, we use sections \"Connection\" and \"Usage\".  Let's keep it consistent.\r\nSo let's change it back to Connection.",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/integrations/app-integrations/youtube.mdx",
    "pr_number": 8682,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1466530088,
    "comment_created_at": "2024-01-25T15:19:01Z"
  },
  {
    "code": "@@ -775,6 +775,7 @@\n             \"integrations/app-integrations/dockerhub\",\n             \"integrations/app-integrations/email\",\n             \"integrations/app-integrations/github\",\n+            \"integrations/app-integrations/gitlab\",",
    "comment": "Let's also add a line in this file: https://github.com/mindsdb/mindsdb/blob/staging/docs/integrations/data-sources-overview.mdx#L25\r\n```\r\n<Card title=\"GitLab\" icon=\"link\" href=\"/integrations/app-integrations/gitlab\"></Card>\r\n```",
    "line_number": 778,
    "enriched": "File: docs/mint.json\nCode: @@ -775,6 +775,7 @@\n             \"integrations/app-integrations/dockerhub\",\n             \"integrations/app-integrations/email\",\n             \"integrations/app-integrations/github\",\n+            \"integrations/app-integrations/gitlab\",\nComment: Let's also add a line in this file: https://github.com/mindsdb/mindsdb/blob/staging/docs/integrations/data-sources-overview.mdx#L25\r\n```\r\n<Card title=\"GitLab\" icon=\"link\" href=\"/integrations/app-integrations/gitlab\"></Card>\r\n```",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/mint.json",
    "pr_number": 8251,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1383679056,
    "comment_created_at": "2023-11-06T17:06:51Z"
  },
  {
    "code": "@@ -52,6 +52,24 @@ def get_columns(self) -> List[str]:\n         ]\n \n \n+class SlackUsersTable(APIResource):",
    "comment": "nit: Please add a docstring for the class and the methods",
    "line_number": 55,
    "enriched": "File: mindsdb/integrations/handlers/slack_handler/slack_handler.py\nCode: @@ -52,6 +52,24 @@ def get_columns(self) -> List[str]:\n         ]\n \n \n+class SlackUsersTable(APIResource):\nComment: nit: Please add a docstring for the class and the methods",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "mindsdb/integrations/handlers/slack_handler/slack_handler.py",
    "pr_number": 9402,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1659010873,
    "comment_created_at": "2024-06-28T16:40:20Z"
  },
  {
    "code": "@@ -0,0 +1,83 @@\n+# Google Books API Integration\n+\n+This handler integrates with the [Google Books API](https://developers.google.com/books/docs/overview) to allow you to\n+make book and bookshelf data available to use for model training and predictions.\n+\n+## Example: Automate your book recommendations\n+\n+To see how the Google Calendar handler is used, let's walk through a simple example to create a model to predict",
    "comment": "Please change the Google Clanedar",
    "line_number": 8,
    "enriched": "File: mindsdb/integrations/handlers/google_books_handler/README.md\nCode: @@ -0,0 +1,83 @@\n+# Google Books API Integration\n+\n+This handler integrates with the [Google Books API](https://developers.google.com/books/docs/overview) to allow you to\n+make book and bookshelf data available to use for model training and predictions.\n+\n+## Example: Automate your book recommendations\n+\n+To see how the Google Calendar handler is used, let's walk through a simple example to create a model to predict\nComment: Please change the Google Clanedar",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "mindsdb/integrations/handlers/google_books_handler/README.md",
    "pr_number": 5785,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1176317914,
    "comment_created_at": "2023-04-25T10:25:54Z"
  },
  {
    "code": "@@ -33,6 +36,28 @@ parameters={\n };\n ~~~~\n \n+~~~~sql\n+CREATE DATABASE altibase_datasource\n+WITH\n+engine='Altibase',",
    "comment": "Engine name is case sensitive, so for now, we can keep the `altibase` as set in the handler https://github.com/mindsdb/mindsdb/pull/9159/files#diff-c8ab3928263afa27fed126cbf44f81718ca80b6d440cd190a489c423319bcf0fR28",
    "line_number": 42,
    "enriched": "File: mindsdb/integrations/handlers/altibase_handler/README.md\nCode: @@ -33,6 +36,28 @@ parameters={\n };\n ~~~~\n \n+~~~~sql\n+CREATE DATABASE altibase_datasource\n+WITH\n+engine='Altibase',\nComment: Engine name is case sensitive, so for now, we can keep the `altibase` as set in the handler https://github.com/mindsdb/mindsdb/pull/9159/files#diff-c8ab3928263afa27fed126cbf44f81718ca80b6d440cd190a489c423319bcf0fR28",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "mindsdb/integrations/handlers/altibase_handler/README.md",
    "pr_number": 9159,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1587562972,
    "comment_created_at": "2024-05-02T12:36:03Z"
  },
  {
    "code": "@@ -1,37 +1,35 @@\n # Welcome to the MindsDB Manual QA Testing for Couchbase Handler\n \n-> **Please submit your PR in the following format after the underline below `Results` section. Don't forget to add an underline after adding your changes i.e., at the end of your `Results` section.**\n-\n-## Testing Couchbase Handler with [Dataset Name](URL to the Dataset)\n+## Testing Couchbase Handler with [travel-sample data](https://docs.couchbase.com/server/current/manage/manage-settings/install-sample-buckets.html#install-sample-buckets-with-the-ui)\n \n **1. Testing CREATE DATABASE**\n \n ```\n-COMMAND THAT YOU RAN TO CREATE DATABASE.\n-```\n-\n-![CREATE_DATABASE](Image URL of the screenshot)\n-\n-**2. Testing CREATE PREDICTOR**\n-\n-```\n-COMMAND THAT YOU RAN TO CREATE PREDICTOR.\n+CREATE DATABASE couchbase_datasource\n+WITH\n+    engine = 'couchbase',\n+    parameters = {\n+        \"host\": \"localhost\",\n+        \"bucket\": \"test-bucker\",\n+        \"user\": \"admin\",\n+        \"password\": \"password\"\n+    };\n ```\n \n-![CREATE_PREDICTOR](Image URL of the screenshot)\n+[![couchbase-create-database.png](https://i.postimg.cc/MKwHdRCB/couchbase-create-database.png)](https://postimg.cc/RqPv03VC)\n \n-**3. Testing SELECT FROM PREDICTOR**\n+**2. Testing SELECT FROM DATABASE**\n \n ```\n-COMMAND THAT YOU RAN TO DO A SELECT FROM.\n+SELECT * FROM couchbase_datasource.airport;\n ```\n \n-![SELECT_FROM](Image URL of the screenshot)\n+[![Screenshot-from-2023-09-05-23-26-33.png](https://i.postimg.cc/WpXCHcK7/Screenshot-from-2023-09-05-23-26-33.png)](https://postimg.cc/XXBsrmnZ)\n \n ### Results\n \n Drop a remark based on your observation.\n-- [ ] Works Great 💚 (This means that all the steps were executed successfuly and the expected outputs were returned.)\n-- [ ] There's a Bug 🪲 [Issue Title](URL To the Issue you created) ( This means you encountered a Bug. Please open an issue with all the relevant details with the Bug Issue Template)\n+- [X] Works Great 💚 (This means that all the steps were executed successfuly and the expected outputs were returned.)\n+- [X] There's a Bug 🪲 [Issue Title](URL To the Issue you created) ( This means you encountered a Bug. Please open an issue with all the relevant details with the Bug Issue Template)",
    "comment": "Should this be chechked, if so add link",
    "line_number": 33,
    "enriched": "File: mindsdb/integrations/handlers/couchbase_handler/Manual_QA.md\nCode: @@ -1,37 +1,35 @@\n # Welcome to the MindsDB Manual QA Testing for Couchbase Handler\n \n-> **Please submit your PR in the following format after the underline below `Results` section. Don't forget to add an underline after adding your changes i.e., at the end of your `Results` section.**\n-\n-## Testing Couchbase Handler with [Dataset Name](URL to the Dataset)\n+## Testing Couchbase Handler with [travel-sample data](https://docs.couchbase.com/server/current/manage/manage-settings/install-sample-buckets.html#install-sample-buckets-with-the-ui)\n \n **1. Testing CREATE DATABASE**\n \n ```\n-COMMAND THAT YOU RAN TO CREATE DATABASE.\n-```\n-\n-![CREATE_DATABASE](Image URL of the screenshot)\n-\n-**2. Testing CREATE PREDICTOR**\n-\n-```\n-COMMAND THAT YOU RAN TO CREATE PREDICTOR.\n+CREATE DATABASE couchbase_datasource\n+WITH\n+    engine = 'couchbase',\n+    parameters = {\n+        \"host\": \"localhost\",\n+        \"bucket\": \"test-bucker\",\n+        \"user\": \"admin\",\n+        \"password\": \"password\"\n+    };\n ```\n \n-![CREATE_PREDICTOR](Image URL of the screenshot)\n+[![couchbase-create-database.png](https://i.postimg.cc/MKwHdRCB/couchbase-create-database.png)](https://postimg.cc/RqPv03VC)\n \n-**3. Testing SELECT FROM PREDICTOR**\n+**2. Testing SELECT FROM DATABASE**\n \n ```\n-COMMAND THAT YOU RAN TO DO A SELECT FROM.\n+SELECT * FROM couchbase_datasource.airport;\n ```\n \n-![SELECT_FROM](Image URL of the screenshot)\n+[![Screenshot-from-2023-09-05-23-26-33.png](https://i.postimg.cc/WpXCHcK7/Screenshot-from-2023-09-05-23-26-33.png)](https://postimg.cc/XXBsrmnZ)\n \n ### Results\n \n Drop a remark based on your observation.\n-- [ ] Works Great 💚 (This means that all the steps were executed successfuly and the expected outputs were returned.)\n-- [ ] There's a Bug 🪲 [Issue Title](URL To the Issue you created) ( This means you encountered a Bug. Please open an issue with all the relevant details with the Bug Issue Template)\n+- [X] Works Great 💚 (This means that all the steps were executed successfuly and the expected outputs were returned.)\n+- [X] There's a Bug 🪲 [Issue Title](URL To the Issue you created) ( This means you encountered a Bug. Please open an issue with all the relevant details with the Bug Issue Template)\nComment: Should this be chechked, if so add link",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "mindsdb/integrations/handlers/couchbase_handler/Manual_QA.md",
    "pr_number": 7259,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1317203862,
    "comment_created_at": "2023-09-06T12:27:19Z"
  },
  {
    "code": "@@ -0,0 +1,61 @@\n+---\n+title: Sendinblue\n+sidebarTitle: Sendinblue\n+---\n+\n+In this section, we present how to connect Sendinblue to MindsDB.\n+\n+[Brevo (formerly Sendinblue)](https://www.brevo.com/) is an all-in-one platform to automate your marketing campaigns over Email, SMS, WhatsApp or chat.\n+\n+Data from Sendinblue can be used to understand the impact of email marketing.\n+\n+## Connection\n+\n+This handler is implemented using the [sib-api-v3-sdk](https://github.com/sendinblue/APIv3-python-library) library, a Python library that wraps Sendinblue APIs.\n+\n+The required arguments to establish a connection are as follows:\n+\n+* `api_key`: a required Sendinblue API key to use for authentication\n+\n+<Tip>\n+Check out [this guide](https://developers.brevo.com/docs) on how to create the Sendinblue API key.\n+\n+It is recommended to use the API key to avoid the `API rate limit exceeded` error.\n+</Tip>\n+\n+Here is how to connect the SendinBlue to MindsDB:\n+\n+```sql\n+CREATE DATABASE sib_datasource\n+WITH ENGINE = 'sendinblue',\n+PARAMETERS = {\n+  \"api_key\": \"xkeysib-...\"\n+};\n+```\n+\n+<Tip>\n+If you installed MindsDB locally via pip, you need to install all handler dependencies manually. To do so, go to the handler's folder (mindsdb/integrations/handlers/sendinblue_handler) and run this command: `pip install -r requirements.txt`.\n+</Tip>\n+\n+## Usage\n+\n+Use the established connection to query your database:\n+\n+~~~~sql\n+SELECT * FROM sib_datasource.email_campaigns\n+~~~~",
    "comment": "Please update all code blocks to this:\r\n\r\n> \\```sql\r\n> code goes here\r\n> \\```",
    "line_number": 46,
    "enriched": "File: docs/integrations/app-integrations/sendinblue.mdx\nCode: @@ -0,0 +1,61 @@\n+---\n+title: Sendinblue\n+sidebarTitle: Sendinblue\n+---\n+\n+In this section, we present how to connect Sendinblue to MindsDB.\n+\n+[Brevo (formerly Sendinblue)](https://www.brevo.com/) is an all-in-one platform to automate your marketing campaigns over Email, SMS, WhatsApp or chat.\n+\n+Data from Sendinblue can be used to understand the impact of email marketing.\n+\n+## Connection\n+\n+This handler is implemented using the [sib-api-v3-sdk](https://github.com/sendinblue/APIv3-python-library) library, a Python library that wraps Sendinblue APIs.\n+\n+The required arguments to establish a connection are as follows:\n+\n+* `api_key`: a required Sendinblue API key to use for authentication\n+\n+<Tip>\n+Check out [this guide](https://developers.brevo.com/docs) on how to create the Sendinblue API key.\n+\n+It is recommended to use the API key to avoid the `API rate limit exceeded` error.\n+</Tip>\n+\n+Here is how to connect the SendinBlue to MindsDB:\n+\n+```sql\n+CREATE DATABASE sib_datasource\n+WITH ENGINE = 'sendinblue',\n+PARAMETERS = {\n+  \"api_key\": \"xkeysib-...\"\n+};\n+```\n+\n+<Tip>\n+If you installed MindsDB locally via pip, you need to install all handler dependencies manually. To do so, go to the handler's folder (mindsdb/integrations/handlers/sendinblue_handler) and run this command: `pip install -r requirements.txt`.\n+</Tip>\n+\n+## Usage\n+\n+Use the established connection to query your database:\n+\n+~~~~sql\n+SELECT * FROM sib_datasource.email_campaigns\n+~~~~\nComment: Please update all code blocks to this:\r\n\r\n> \\```sql\r\n> code goes here\r\n> \\```",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/integrations/app-integrations/sendinblue.mdx",
    "pr_number": 7543,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1345904925,
    "comment_created_at": "2023-10-04T14:35:41Z"
  },
  {
    "code": "@@ -39,8 +39,8 @@ These are the optional parameters:\n - `yotpo_app_key`: token needed to access customer reviews via the Yotpo Product Reviews app.\n - `yotpo_access_token`: token needed to access customer reviews via the Yotpo Product Reviews app.\n \n-If you want to query customer reviews, use the [Yotpo Product Reviews](https://apps.shopify.com/yotpo-social-reviews) app available in Shopify. To generate `yotpo_app_key` and `yotpo_access_token` keys, refer to [this](https://support.yotpo.com/docs/finding-your-yotpo-app-key-and-secret-key) and [this](https://apidocs.yotpo.com/reference/yotpo-authentication).\n-\n+If you want to query customer reviews, use the [Yotpo Product Reviews](https://apps.shopify.com/yotpo-social-reviews) app available in Shopify. To generate `yotpo_app_key` and `yotpo_access_token` keys, refer to [this](https://support.yotpo.com/docs/finding-your-yotpo-app-key-and-secret-key) and [this](https://apidocs.yotpo.com/reference/yotpo-authentication)(Note:please don't enter the app secret key instead of yotpo access token both are different).\n+Note:please don't enter the app secret key instead of yotpo access token both are different",
    "comment": "Please replace lines 42 and 43 with the below content:\r\n```\r\nIf you want to query customer reviews, use the [Yotpo Product Reviews](https://apps.shopify.com/yotpo-social-reviews) app available in Shopify. Here are the steps to follow:\r\n1. Install the [Yotpo Product Reviews](https://apps.shopify.com/yotpo-social-reviews) app for your Shopify store.\r\n2. Generate `yotpo_app_key` following [this instruction](https://support.yotpo.com/docs/finding-your-yotpo-app-key-and-secret-key) for retrieving your app key. Learn more about [Yotpo authentication here](https://apidocs.yotpo.com/reference/yotpo-authentication).\r\n3. Generate `yotpo_access_token` following [this instruction](https://develop.yotpo.com/reference/generate-a-token).\r\n```",
    "line_number": 43,
    "enriched": "File: mindsdb/integrations/handlers/shopify_handler/README.md\nCode: @@ -39,8 +39,8 @@ These are the optional parameters:\n - `yotpo_app_key`: token needed to access customer reviews via the Yotpo Product Reviews app.\n - `yotpo_access_token`: token needed to access customer reviews via the Yotpo Product Reviews app.\n \n-If you want to query customer reviews, use the [Yotpo Product Reviews](https://apps.shopify.com/yotpo-social-reviews) app available in Shopify. To generate `yotpo_app_key` and `yotpo_access_token` keys, refer to [this](https://support.yotpo.com/docs/finding-your-yotpo-app-key-and-secret-key) and [this](https://apidocs.yotpo.com/reference/yotpo-authentication).\n-\n+If you want to query customer reviews, use the [Yotpo Product Reviews](https://apps.shopify.com/yotpo-social-reviews) app available in Shopify. To generate `yotpo_app_key` and `yotpo_access_token` keys, refer to [this](https://support.yotpo.com/docs/finding-your-yotpo-app-key-and-secret-key) and [this](https://apidocs.yotpo.com/reference/yotpo-authentication)(Note:please don't enter the app secret key instead of yotpo access token both are different).\n+Note:please don't enter the app secret key instead of yotpo access token both are different\nComment: Please replace lines 42 and 43 with the below content:\r\n```\r\nIf you want to query customer reviews, use the [Yotpo Product Reviews](https://apps.shopify.com/yotpo-social-reviews) app available in Shopify. Here are the steps to follow:\r\n1. Install the [Yotpo Product Reviews](https://apps.shopify.com/yotpo-social-reviews) app for your Shopify store.\r\n2. Generate `yotpo_app_key` following [this instruction](https://support.yotpo.com/docs/finding-your-yotpo-app-key-and-secret-key) for retrieving your app key. Learn more about [Yotpo authentication here](https://apidocs.yotpo.com/reference/yotpo-authentication).\r\n3. Generate `yotpo_access_token` following [this instruction](https://develop.yotpo.com/reference/generate-a-token).\r\n```",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "mindsdb/integrations/handlers/shopify_handler/README.md",
    "pr_number": 8437,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1408275828,
    "comment_created_at": "2023-11-28T19:17:40Z"
  },
  {
    "code": "@@ -21,6 +21,12 @@ Follow the steps to set up MindsDB in Docker Desktop.\n \n ### Install the MindsDB Docker Desktop Extension\n \n+<Tip>\n+If you are a Windows user, ensure that you have enabled Developer Mode under settings before installing the extension.\n+\n+<img src=\"/assets/docker/docker_desktop/enable-win-dev-mode.png\"/>",
    "comment": "Let's add an image with a screenshot where Developer Mode is enabled, like below:\r\n\r\n![image](https://github.com/mindsdb/mindsdb/assets/109554435/ee0c6217-5e22-4c9d-98bc-2c21858ce83e)\r\n",
    "line_number": 27,
    "enriched": "File: docs/setup/self-hosted/docker-desktop.mdx\nCode: @@ -21,6 +21,12 @@ Follow the steps to set up MindsDB in Docker Desktop.\n \n ### Install the MindsDB Docker Desktop Extension\n \n+<Tip>\n+If you are a Windows user, ensure that you have enabled Developer Mode under settings before installing the extension.\n+\n+<img src=\"/assets/docker/docker_desktop/enable-win-dev-mode.png\"/>\nComment: Let's add an image with a screenshot where Developer Mode is enabled, like below:\r\n\r\n![image](https://github.com/mindsdb/mindsdb/assets/109554435/ee0c6217-5e22-4c9d-98bc-2c21858ce83e)\r\n",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/setup/self-hosted/docker-desktop.mdx",
    "pr_number": 9146,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1582945752,
    "comment_created_at": "2024-04-29T11:47:44Z"
  },
  {
    "code": "@@ -21,7 +21,6 @@\n \n <p>\n \t<a href=\"https://github.com/mindsdb/mindsdb/actions\"><img src=\"https://github.com/mindsdb/mindsdb/workflows/MindsDB%20workflow/badge.svg\" alt=\"MindsDB workflow\"></a>\n-\t<a href=\"https://www.python.org/downloads/\" target=\"_blank\"><img src=\"https://img.shields.io/badge/python-3.7.x%20|%203.8.x|%203.9.x-brightgreen.svg\" alt=\"Python supported\"></a>",
    "comment": "Please do not remove this line. We want exclude just 3.7.x version and keep the other versions.\r\n\r\nPlease replace `img src` value with this one: https://img.shields.io/badge/python-3.8.x%7C%203.9.x-brightgreen.svg",
    "line_number": 24,
    "enriched": "File: README.md\nCode: @@ -21,7 +21,6 @@\n \n <p>\n \t<a href=\"https://github.com/mindsdb/mindsdb/actions\"><img src=\"https://github.com/mindsdb/mindsdb/workflows/MindsDB%20workflow/badge.svg\" alt=\"MindsDB workflow\"></a>\n-\t<a href=\"https://www.python.org/downloads/\" target=\"_blank\"><img src=\"https://img.shields.io/badge/python-3.7.x%20|%203.8.x|%203.9.x-brightgreen.svg\" alt=\"Python supported\"></a>\nComment: Please do not remove this line. We want exclude just 3.7.x version and keep the other versions.\r\n\r\nPlease replace `img src` value with this one: https://img.shields.io/badge/python-3.8.x%7C%203.9.x-brightgreen.svg",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "README.md",
    "pr_number": 6526,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1224372092,
    "comment_created_at": "2023-06-09T14:20:48Z"
  },
  {
    "code": "@@ -22,7 +22,7 @@ In order to make use of this handler and connect to a Couchbase server in MindsD\n ```sql\n CREATE DATABASE couchbase_vectorsource\n WITH\n-engine='couchbase',\n+engine='couchbasevector',",
    "comment": "Please note that the `data-integrations/couchbasevector.mdx` file was moved to `vector-db-integrations/couchbase.mdx`.\r\n\r\nThis file `docs/integrations/data-integrations/couchbasevector.mdx` can be removed.",
    "line_number": 25,
    "enriched": "File: docs/integrations/data-integrations/couchbasevector.mdx\nCode: @@ -22,7 +22,7 @@ In order to make use of this handler and connect to a Couchbase server in MindsD\n ```sql\n CREATE DATABASE couchbase_vectorsource\n WITH\n-engine='couchbase',\n+engine='couchbasevector',\nComment: Please note that the `data-integrations/couchbasevector.mdx` file was moved to `vector-db-integrations/couchbase.mdx`.\r\n\r\nThis file `docs/integrations/data-integrations/couchbasevector.mdx` can be removed.",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/integrations/data-integrations/couchbasevector.mdx",
    "pr_number": 10336,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1905401062,
    "comment_created_at": "2025-01-07T12:43:47Z"
  },
  {
    "code": "@@ -0,0 +1,31 @@\n+MindsDB integrations are broadly categorized into two types:\n+\n+1. Datasources\n+\n+Datasources in MindsDB refer to the different data storage and management systems that you can connect with MindsDB. These include traditional databases as well as data accessible through APIs. There are few different types of Datasources:\n+\n+* [Databases](https://docs.mindsdb.com/integrations/data-integrations/all-data-integrations)\n+* [Applications](https://docs.mindsdb.com/integrations/app-integrations/binance)\n+* [Vector Databases](https://docs.mindsdb.com/integrations/vector-db-integrations/chromadb)\n+\n+2. AI-Engines\n+\n+AI-Engines in MindsDB are the core of our AI and ML capabilities. This category encompasses a diverse range of artificial intelligence and machine learning modeling options, including:\n+\n+  * Generative AI: Unlock the potential of generative algorithms for innovative solutions.\n+  *  Automated Machine Learning (Auto-ML): Simplify complex ML processes with automation, making AI more accessible. \n+",
    "comment": "Let's link this overview of available AI engines: https://docs.mindsdb.com/ai-engines/overview",
    "line_number": 17,
    "enriched": "File: mindsdb/integrations/README.md\nCode: @@ -0,0 +1,31 @@\n+MindsDB integrations are broadly categorized into two types:\n+\n+1. Datasources\n+\n+Datasources in MindsDB refer to the different data storage and management systems that you can connect with MindsDB. These include traditional databases as well as data accessible through APIs. There are few different types of Datasources:\n+\n+* [Databases](https://docs.mindsdb.com/integrations/data-integrations/all-data-integrations)\n+* [Applications](https://docs.mindsdb.com/integrations/app-integrations/binance)\n+* [Vector Databases](https://docs.mindsdb.com/integrations/vector-db-integrations/chromadb)\n+\n+2. AI-Engines\n+\n+AI-Engines in MindsDB are the core of our AI and ML capabilities. This category encompasses a diverse range of artificial intelligence and machine learning modeling options, including:\n+\n+  * Generative AI: Unlock the potential of generative algorithms for innovative solutions.\n+  *  Automated Machine Learning (Auto-ML): Simplify complex ML processes with automation, making AI more accessible. \n+\nComment: Let's link this overview of available AI engines: https://docs.mindsdb.com/ai-engines/overview",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "mindsdb/integrations/README.md",
    "pr_number": 8661,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1459044328,
    "comment_created_at": "2024-01-19T13:53:17Z"
  },
  {
    "code": "@@ -62,19 +62,51 @@ WITH\n You can insert data into a new collection like so\n \n ```sql\n-CREATE TABLE pvec.items\n-    (SELECT embedding AS embeddings\n-        FROM (SELECT * FROM mysql_demo_db.demo_fda_context\n-        LIMIT 3) AS d\n-join  openai_emb);\n+--- Public demo database\n+CREATE DATABASE mysql_demo_db\n+WITH ENGINE = \"mysql\",\n+PARAMETERS = {\n+   \"user\": \"user\",\n+   \"password\": \"MindsDBUser123!\",",
    "comment": "Let's change the values of host, user, and password to some placeholders.",
    "line_number": 70,
    "enriched": "File: mindsdb/integrations/handlers/pgvector_handler/README.md\nCode: @@ -62,19 +62,51 @@ WITH\n You can insert data into a new collection like so\n \n ```sql\n-CREATE TABLE pvec.items\n-    (SELECT embedding AS embeddings\n-        FROM (SELECT * FROM mysql_demo_db.demo_fda_context\n-        LIMIT 3) AS d\n-join  openai_emb);\n+--- Public demo database\n+CREATE DATABASE mysql_demo_db\n+WITH ENGINE = \"mysql\",\n+PARAMETERS = {\n+   \"user\": \"user\",\n+   \"password\": \"MindsDBUser123!\",\nComment: Let's change the values of host, user, and password to some placeholders.",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "mindsdb/integrations/handlers/pgvector_handler/README.md",
    "pr_number": 7877,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1392586206,
    "comment_created_at": "2023-11-14T13:30:54Z"
  },
  {
    "code": "@@ -28,85 +28,113 @@\n \n ----------------------------------------\n \n-[MindsDB](https://mindsdb.com?utm_medium=community&utm_source=github&utm_campaign=mindsdb%20repo) is the world’s most widely used platform for building AI that can learn from and answer questions across federated data.\n \n-[![Tweet](https://img.shields.io/twitter/url/http/shields.io.svg?style=social)](https://twitter.com/intent/tweet?text=The%20platform%20for%20building%20AI,%20from%20enterprise%20data&url=https://github.com/mindsdb/mindsdb&via=mindsdb&hashtags=ai,opensource)\n+MindsDB is the world's most effective solution for building AI applications that talk to messy enterprise data sources. Think of it as a librarian Marie Kondo.\n \n-## 📖 About Us\n+<p align=\"center\">\n+  <img src=\"/docs/assets/cloud/main_mdb.png\"/>\n+</p>\n \n-MindsDB is a federated query engine designed for AI agents and applications that need to answer questions from one or multiple [data sources](https://docs.mindsdb.com/integrations/data-overview?utm_medium=community&utm_source=github&utm_campaign=mindsdb%20repo), including both structured and unstructured data.\n+A federated query engine that tidies up your data-sprawl chaos while meticulously answering every single question you throw at it. From structured to unstructured data, whether it's scattered across SaaS applications, databases, or... hibernating in data warehouses like that $100 bill in your tuxedo pocket from prom night, lost, waiting to be discovered.\n \n-## 🚀 Get Started\n+## Install MindsDB Server \n \n-* **[Install MindsDB](https://docs.mindsdb.com/setup/self-hosted/docker-desktop?utm_medium=community&utm_source=github&utm_campaign=mindsdb%20repo)** using [Docker](https://docs.mindsdb.com/setup/self-hosted/docker?utm_medium=community&utm_source=github&utm_campaign=mindsdb%20repo) or [Docker Desktop](https://docs.mindsdb.com/setup/self-hosted/docker-desktop?utm_medium=community&utm_source=github&utm_campaign=mindsdb%20repo).\n-* **[Connect Your Data](https://docs.mindsdb.com/mindsdb_sql/sql/create/database)** — Connect and query hundreds of different [data sources](https://docs.mindsdb.com/integrations/data-overview?utm_medium=community&utm_source=github&utm_campaign=mindsdb%20repo).\n-* **[Prepare Your Data](https://docs.mindsdb.com/use-cases/data_enrichment/overview)** — Prepare, [organize](https://docs.mindsdb.com/mindsdb_sql/sql/create/view), and [automate](https://docs.mindsdb.com/mindsdb_sql/sql/create/jobs) data transformations using AI and ML to fit your needs.\n+MindsDB is an open-source server that can be deployed anywhere - from your laptop to the cloud, and everywhere in between. And yes, you can customize it to your heart's content.\n \n-## 🎯 Use Cases\n \n-After [connecting](https://docs.mindsdb.com/mindsdb_sql/sql/create/database) and [preparing](https://docs.mindsdb.com/use-cases/data_enrichment/overview) your data, you can leverage MindsDB to implement the following use cases:\n+  * [Using Docker Desktop](https://docs.mindsdb.com/setup/self-hosted/docker-desktop). This is the fastest and recommended way to get started and have it all running.\n+  * [Using Docker](https://docs.mindsdb.com/setup/self-hosted/docker). This is also simple, but gives you more flexibility on how to further customize your server.",
    "comment": "Shall we also include the pip option?\r\n\r\nhttps://docs.mindsdb.com/contribute/install",
    "line_number": 46,
    "enriched": "File: README.md\nCode: @@ -28,85 +28,113 @@\n \n ----------------------------------------\n \n-[MindsDB](https://mindsdb.com?utm_medium=community&utm_source=github&utm_campaign=mindsdb%20repo) is the world’s most widely used platform for building AI that can learn from and answer questions across federated data.\n \n-[![Tweet](https://img.shields.io/twitter/url/http/shields.io.svg?style=social)](https://twitter.com/intent/tweet?text=The%20platform%20for%20building%20AI,%20from%20enterprise%20data&url=https://github.com/mindsdb/mindsdb&via=mindsdb&hashtags=ai,opensource)\n+MindsDB is the world's most effective solution for building AI applications that talk to messy enterprise data sources. Think of it as a librarian Marie Kondo.\n \n-## 📖 About Us\n+<p align=\"center\">\n+  <img src=\"/docs/assets/cloud/main_mdb.png\"/>\n+</p>\n \n-MindsDB is a federated query engine designed for AI agents and applications that need to answer questions from one or multiple [data sources](https://docs.mindsdb.com/integrations/data-overview?utm_medium=community&utm_source=github&utm_campaign=mindsdb%20repo), including both structured and unstructured data.\n+A federated query engine that tidies up your data-sprawl chaos while meticulously answering every single question you throw at it. From structured to unstructured data, whether it's scattered across SaaS applications, databases, or... hibernating in data warehouses like that $100 bill in your tuxedo pocket from prom night, lost, waiting to be discovered.\n \n-## 🚀 Get Started\n+## Install MindsDB Server \n \n-* **[Install MindsDB](https://docs.mindsdb.com/setup/self-hosted/docker-desktop?utm_medium=community&utm_source=github&utm_campaign=mindsdb%20repo)** using [Docker](https://docs.mindsdb.com/setup/self-hosted/docker?utm_medium=community&utm_source=github&utm_campaign=mindsdb%20repo) or [Docker Desktop](https://docs.mindsdb.com/setup/self-hosted/docker-desktop?utm_medium=community&utm_source=github&utm_campaign=mindsdb%20repo).\n-* **[Connect Your Data](https://docs.mindsdb.com/mindsdb_sql/sql/create/database)** — Connect and query hundreds of different [data sources](https://docs.mindsdb.com/integrations/data-overview?utm_medium=community&utm_source=github&utm_campaign=mindsdb%20repo).\n-* **[Prepare Your Data](https://docs.mindsdb.com/use-cases/data_enrichment/overview)** — Prepare, [organize](https://docs.mindsdb.com/mindsdb_sql/sql/create/view), and [automate](https://docs.mindsdb.com/mindsdb_sql/sql/create/jobs) data transformations using AI and ML to fit your needs.\n+MindsDB is an open-source server that can be deployed anywhere - from your laptop to the cloud, and everywhere in between. And yes, you can customize it to your heart's content.\n \n-## 🎯 Use Cases\n \n-After [connecting](https://docs.mindsdb.com/mindsdb_sql/sql/create/database) and [preparing](https://docs.mindsdb.com/use-cases/data_enrichment/overview) your data, you can leverage MindsDB to implement the following use cases:\n+  * [Using Docker Desktop](https://docs.mindsdb.com/setup/self-hosted/docker-desktop). This is the fastest and recommended way to get started and have it all running.\n+  * [Using Docker](https://docs.mindsdb.com/setup/self-hosted/docker). This is also simple, but gives you more flexibility on how to further customize your server.\nComment: Shall we also include the pip option?\r\n\r\nhttps://docs.mindsdb.com/contribute/install",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "README.md",
    "pr_number": 10335,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1904306679,
    "comment_created_at": "2025-01-06T15:39:17Z"
  },
  {
    "code": "@@ -0,0 +1,57 @@\n+# Webz Handler\n+\n+This handler integrates with the [Webz API](https://docs.webz.io/reference#1) to make \n+webz data available to use for model training, predictions and automations.\n+\n+\n+\n+## Connect to the Webz API\n+The first step is to create a database with the new `webz` engine \n+by passing in the required `token` parameter:\n+\n+```\n+CREATE DATABASE webz_datasource\n+WITH \n+  ENGINE = 'webz',\n+  PARAMETERS = {\n+    \"token\": \"<webz active API key>\"\n+};\n+```\n+\n+## Querying news articles, blogs entries or open discussions\n+\n+With the previous established connection, you can for instance, \n+query the 5 most relevant news articles, in english that contain\n+the text AI in the title\n+\n+```\n+SELECT *\n+FROM webz_datasource.posts\n+WHERE query=\"language:english title:AI site_type:news\"\n+ORDER BY posts.relevancy DESC\n+LIMIT 5;\n+```\n+\n+The returned results should have ROWs like this:",
    "comment": "Nit: ROws -> rows",
    "line_number": 35,
    "enriched": "File: mindsdb/integrations/handlers/webz_handler/README.md\nCode: @@ -0,0 +1,57 @@\n+# Webz Handler\n+\n+This handler integrates with the [Webz API](https://docs.webz.io/reference#1) to make \n+webz data available to use for model training, predictions and automations.\n+\n+\n+\n+## Connect to the Webz API\n+The first step is to create a database with the new `webz` engine \n+by passing in the required `token` parameter:\n+\n+```\n+CREATE DATABASE webz_datasource\n+WITH \n+  ENGINE = 'webz',\n+  PARAMETERS = {\n+    \"token\": \"<webz active API key>\"\n+};\n+```\n+\n+## Querying news articles, blogs entries or open discussions\n+\n+With the previous established connection, you can for instance, \n+query the 5 most relevant news articles, in english that contain\n+the text AI in the title\n+\n+```\n+SELECT *\n+FROM webz_datasource.posts\n+WHERE query=\"language:english title:AI site_type:news\"\n+ORDER BY posts.relevancy DESC\n+LIMIT 5;\n+```\n+\n+The returned results should have ROWs like this:\nComment: Nit: ROws -> rows",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "mindsdb/integrations/handlers/webz_handler/README.md",
    "pr_number": 6748,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1267209596,
    "comment_created_at": "2023-07-18T19:20:09Z"
  },
  {
    "code": "@@ -0,0 +1,148 @@\n+---\n+title: AI Agents with LLMs and Skills\n+sidebarTitle: AI Agents\n+---\n+\n+With MindsDB, you can create and deploy AI agents that comprise AI models and customizable skills such as knowledge bases and text-to-SQL.\n+\n+<p align=\"center\">\n+  <img src=\"/assets/agent_diagram.png\" />\n+</p>\n+\n+AI agents use a conversational model (like OpenAI) from LangChain utilizing [tools as skills](https://python.langchain.com/docs/modules/agents/tools/) to respond to user input. Users can customize AI agents with their own prompts to fit their use cases.\n+\n+## AI Agents' Components\n+\n+AI agents comprise of skills, such as text2sql and knowledge_base, and a conversational model.\n+\n+### Skills\n+\n+Skills provide data resources to an agent, enabling it to answer questions about available data.\n+\n+Here is how to create and work with skills:\n+\n+```bash\n+# get a skill by name\n+skill = skills.get('my_skill')\n+\n+# list all skills\n+skills = skills.list()\n+\n+# create a new SQL skill\n+text_to_sql_skill = skills.create('text_to_sql', 'sql', { 'tables': ['my_table'], 'database': 'my_database' })\n+\n+# update a skill\n+skill.params = { 'tables': ['new_table'], 'database': 'new_database' }\n+updated_skill = skills.update('my_skill', skill)\n+\n+# delete a skill by name\n+skills.delete('my_skill')\n+```\n+\n+#### Text2SQL\n+\n+To search data efficiently, use the text2sql skills that translate user's input into a query and fetch relevant data from defined datasets.",
    "comment": "It looks like this section is formatted as code. Or everything works through mintlify? Also maybe we can change the ```bash to python",
    "line_number": 44,
    "enriched": "File: docs/sdks/python/agents.mdx\nCode: @@ -0,0 +1,148 @@\n+---\n+title: AI Agents with LLMs and Skills\n+sidebarTitle: AI Agents\n+---\n+\n+With MindsDB, you can create and deploy AI agents that comprise AI models and customizable skills such as knowledge bases and text-to-SQL.\n+\n+<p align=\"center\">\n+  <img src=\"/assets/agent_diagram.png\" />\n+</p>\n+\n+AI agents use a conversational model (like OpenAI) from LangChain utilizing [tools as skills](https://python.langchain.com/docs/modules/agents/tools/) to respond to user input. Users can customize AI agents with their own prompts to fit their use cases.\n+\n+## AI Agents' Components\n+\n+AI agents comprise of skills, such as text2sql and knowledge_base, and a conversational model.\n+\n+### Skills\n+\n+Skills provide data resources to an agent, enabling it to answer questions about available data.\n+\n+Here is how to create and work with skills:\n+\n+```bash\n+# get a skill by name\n+skill = skills.get('my_skill')\n+\n+# list all skills\n+skills = skills.list()\n+\n+# create a new SQL skill\n+text_to_sql_skill = skills.create('text_to_sql', 'sql', { 'tables': ['my_table'], 'database': 'my_database' })\n+\n+# update a skill\n+skill.params = { 'tables': ['new_table'], 'database': 'new_database' }\n+updated_skill = skills.update('my_skill', skill)\n+\n+# delete a skill by name\n+skills.delete('my_skill')\n+```\n+\n+#### Text2SQL\n+\n+To search data efficiently, use the text2sql skills that translate user's input into a query and fetch relevant data from defined datasets.\nComment: It looks like this section is formatted as code. Or everything works through mintlify? Also maybe we can change the ```bash to python",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/sdks/python/agents.mdx",
    "pr_number": 9168,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1587803058,
    "comment_created_at": "2024-05-02T15:03:28Z"
  },
  {
    "code": "@@ -43,49 +43,13 @@ On execution we get:\n +--------------------------------------+--------------------+-----------+---------+--------+---------+----------+----------+--------------+---------------+-----------------+--------+--------------------------------------+----------------------------+--------+\n | tables                               | NAME               | ENGINE    | PROJECT | ACTIVE | VERSION | STATUS   | ACCURACY | PREDICT      | UPDATE_STATUS | MINDSDB_VERSION | ERROR  | SELECT_DATA_QUERY                    | TRAINING_OPTIONS           | TAG    |\n +--------------------------------------+--------------------+-----------+---------+--------+---------+----------+----------+--------------+---------------+-----------------+--------+--------------------------------------+----------------------------+--------+\n-| [\"info\",\"features\",\"model\",\"jsonai\"] | home_rentals_model | lightwood | mindsdb | true   | 1       | complete | 0.999    | rental_price | up_to_date    | 23.4.4.0        | [NULL] | SELECT * FROM demo_data.home_rentals | {'target': 'rental_price'} | [NULL] |\n+| [\"features\",\"model\",\"jsonai\"] | home_rentals_model | lightwood | mindsdb | true   | 1       | complete | 0.999    | rental_price | up_to_date    | 23.4.4.0        | [NULL] | SELECT * FROM demo_data.home_rentals | {'target': 'rental_price'} | [NULL] |",
    "comment": "Could you please add a few spaces after `[\"features\",\"model\",\"jsonai\"]` so that it looks like this:\r\n\r\n![image](https://user-images.githubusercontent.com/109554435/235688031-6def14e3-03b0-4d66-b5c1-6df61ad390d0.png)",
    "line_number": 46,
    "enriched": "File: docs/sql/api/describe.mdx\nCode: @@ -43,49 +43,13 @@ On execution we get:\n +--------------------------------------+--------------------+-----------+---------+--------+---------+----------+----------+--------------+---------------+-----------------+--------+--------------------------------------+----------------------------+--------+\n | tables                               | NAME               | ENGINE    | PROJECT | ACTIVE | VERSION | STATUS   | ACCURACY | PREDICT      | UPDATE_STATUS | MINDSDB_VERSION | ERROR  | SELECT_DATA_QUERY                    | TRAINING_OPTIONS           | TAG    |\n +--------------------------------------+--------------------+-----------+---------+--------+---------+----------+----------+--------------+---------------+-----------------+--------+--------------------------------------+----------------------------+--------+\n-| [\"info\",\"features\",\"model\",\"jsonai\"] | home_rentals_model | lightwood | mindsdb | true   | 1       | complete | 0.999    | rental_price | up_to_date    | 23.4.4.0        | [NULL] | SELECT * FROM demo_data.home_rentals | {'target': 'rental_price'} | [NULL] |\n+| [\"features\",\"model\",\"jsonai\"] | home_rentals_model | lightwood | mindsdb | true   | 1       | complete | 0.999    | rental_price | up_to_date    | 23.4.4.0        | [NULL] | SELECT * FROM demo_data.home_rentals | {'target': 'rental_price'} | [NULL] |\nComment: Could you please add a few spaces after `[\"features\",\"model\",\"jsonai\"]` so that it looks like this:\r\n\r\n![image](https://user-images.githubusercontent.com/109554435/235688031-6def14e3-03b0-4d66-b5c1-6df61ad390d0.png)",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/sql/api/describe.mdx",
    "pr_number": 5943,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1183394131,
    "comment_created_at": "2023-05-03T08:36:50Z"
  },
  {
    "code": "@@ -0,0 +1,48 @@\n+---\n+title: Matrixone\n+sidebarTitle: Matrixone\n+---\n+\n+# Matrixone Handler\n+\n+This is the implementation of the  Matrixone handler for MindsDB.\n+\n+##  Matrixone\n+MatrixOne is a future-oriented hyper-converged cloud and edge native DBMS that supports transactional, analytical, and streaming workloads with a simplified and distributed database engine, across multiple data centers, clouds, edges and other heterogeneous infrastructures.\n+\n+For more Info Click [HERE](https://github.com/matrixorigin/matrixone)\n+\n+## Implementation\n+This handler was implemented using the `PyMySQL`, a Python library that allows you to use Python code to run SQL commands on Matrixone Database.\n+\n+The required arguments to establish a connection are,\n+* `user`: username asscociated with database\n+* `password`: password to authenticate your access\n+* `host`: host to server IP Address or hostname\n+* `port`: port through which TCPIP connection is to be made\n+* `database`: Database name to be connected\n+* `ssl`: If you want to enable SSL Security **(Boolean)**\n+* `ssl_ca`: Path or URL of the Certificate Authority (CA) certificate file\n+* `ssl_cert`: Path name or URL of the server public key certificate file\n+* `ssl_key`: The path name or URL of the server private key file\n+  \n+\n+## Usage\n+In order to make use of this handler and connect to Matrixone in MindsDB, the following syntax can be used,\n+~~~~sql",
    "comment": "The code blocks should be as follows:\r\n\r\n> \\```sql\r\n> SQL GOES HERE\r\n>\\```",
    "line_number": 32,
    "enriched": "File: docs/data-integrations/matrixone.mdx\nCode: @@ -0,0 +1,48 @@\n+---\n+title: Matrixone\n+sidebarTitle: Matrixone\n+---\n+\n+# Matrixone Handler\n+\n+This is the implementation of the  Matrixone handler for MindsDB.\n+\n+##  Matrixone\n+MatrixOne is a future-oriented hyper-converged cloud and edge native DBMS that supports transactional, analytical, and streaming workloads with a simplified and distributed database engine, across multiple data centers, clouds, edges and other heterogeneous infrastructures.\n+\n+For more Info Click [HERE](https://github.com/matrixorigin/matrixone)\n+\n+## Implementation\n+This handler was implemented using the `PyMySQL`, a Python library that allows you to use Python code to run SQL commands on Matrixone Database.\n+\n+The required arguments to establish a connection are,\n+* `user`: username asscociated with database\n+* `password`: password to authenticate your access\n+* `host`: host to server IP Address or hostname\n+* `port`: port through which TCPIP connection is to be made\n+* `database`: Database name to be connected\n+* `ssl`: If you want to enable SSL Security **(Boolean)**\n+* `ssl_ca`: Path or URL of the Certificate Authority (CA) certificate file\n+* `ssl_cert`: Path name or URL of the server public key certificate file\n+* `ssl_key`: The path name or URL of the server private key file\n+  \n+\n+## Usage\n+In order to make use of this handler and connect to Matrixone in MindsDB, the following syntax can be used,\n+~~~~sql\nComment: The code blocks should be as follows:\r\n\r\n> \\```sql\r\n> SQL GOES HERE\r\n>\\```",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/data-integrations/matrixone.mdx",
    "pr_number": 5114,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1143825636,
    "comment_created_at": "2023-03-21T18:19:07Z"
  },
  {
    "code": "@@ -0,0 +1,84 @@\n+from mindsdb_sql import parse_sql\n+\n+from mindsdb.integrations.handlers.lightdash_handler.api import Lightdash\n+from mindsdb.integrations.handlers.lightdash_handler.lightdash_tables import (\n+    UserTable,\n+    UserAbilityTable,\n+    OrgTable,\n+    OrgProjectsTable,\n+    OrgMembersTable,\n+    ProjectTable,\n+    WarehouseConnectionTable,\n+    DBTConnectionTable,\n+    DBTEnvironmentVarsTable,\n+    ChartsTable,\n+    SpacesTable,\n+    AccessTable,\n+    ValidationTable,\n+    DashboardsTable,\n+    QueriesTable,\n+    ChartHistoryTable,\n+    ChartConfigTable,\n+    ChartAdditionalMetricsTable,\n+    ChartTableCalculationsTable,\n+    SchedulerLogsTable,\n+    SchedulerTable,\n+    SchedulerJobsTable,\n+    SchedulerJobStatus,\n+)\n+from mindsdb.integrations.libs.api_handler import APIHandler\n+from mindsdb.integrations.libs.response import HandlerStatusResponse as StatusResponse\n+\n+\n+class LightdashHandler(APIHandler):\n+\n+    def __init__(self, name: str, **kwargs) -> None:\n+        super().__init__(name)\n+        self.connection = None\n+        self.is_connected = False\n+        self.api_key = kwargs.get(\"connection_data\", {}).get(\"api_key\", \"\")\n+        self.base_url = kwargs.get(\"connection_data\", {}).get(\"base_url\", \"\")\n+        _tables = [\n+            UserTable,\n+            UserAbilityTable,\n+            OrgTable,\n+            OrgProjectsTable,\n+            OrgMembersTable,\n+            ProjectTable,\n+            WarehouseConnectionTable,\n+            DBTConnectionTable,\n+            DBTEnvironmentVarsTable,\n+            ChartsTable,\n+            SpacesTable,\n+            AccessTable,\n+            ValidationTable,\n+            DashboardsTable,\n+            QueriesTable,\n+            ChartHistoryTable,\n+            ChartConfigTable,\n+            ChartAdditionalMetricsTable,\n+            ChartTableCalculationsTable,\n+            SchedulerLogsTable,\n+            SchedulerTable,\n+            SchedulerJobsTable,\n+            SchedulerJobStatus,\n+        ]\n+        for Table in _tables:\n+            self._register_table(Table.name, Table(self))\n+        self.connect()\n+\n+    def connect(self) -> Lightdash:\n+        self.connection = Lightdash(self.base_url, self.api_key)\n+        return self.connection\n+\n+    def check_connection(self) -> StatusResponse:\n+        resp = StatusResponse(False)\n+        if self.connection and not self.connection.is_connected():\n+            resp.error = \"Client not connected\"\n+        else:\n+            resp.success = True\n+        return resp\n+\n+    def native_query(self, query: str) -> StatusResponse:\n+        ast = parse_sql(query, dialect=\"mindsdb\")\n+        return self.query(ast)",
    "comment": "Please include the \r\n```\r\nconnection_args = OrderedDict(\r\n    api_key={\r\n        'type': ARG_TYPE.STR,\r\n        'description': 'API Token for accessing ...',\r\n        'required': True,\r\n        'label': '',\r\n    }   \r\n)\r\n\r\nconnection_args_example = OrderedDict(\r\n    api_key ='<your--api-key>',\r\nbase_url ...\r\n)\r\n```",
    "line_number": 84,
    "enriched": "File: mindsdb/integrations/handlers/lightdash_handler/lightdash_handler.py\nCode: @@ -0,0 +1,84 @@\n+from mindsdb_sql import parse_sql\n+\n+from mindsdb.integrations.handlers.lightdash_handler.api import Lightdash\n+from mindsdb.integrations.handlers.lightdash_handler.lightdash_tables import (\n+    UserTable,\n+    UserAbilityTable,\n+    OrgTable,\n+    OrgProjectsTable,\n+    OrgMembersTable,\n+    ProjectTable,\n+    WarehouseConnectionTable,\n+    DBTConnectionTable,\n+    DBTEnvironmentVarsTable,\n+    ChartsTable,\n+    SpacesTable,\n+    AccessTable,\n+    ValidationTable,\n+    DashboardsTable,\n+    QueriesTable,\n+    ChartHistoryTable,\n+    ChartConfigTable,\n+    ChartAdditionalMetricsTable,\n+    ChartTableCalculationsTable,\n+    SchedulerLogsTable,\n+    SchedulerTable,\n+    SchedulerJobsTable,\n+    SchedulerJobStatus,\n+)\n+from mindsdb.integrations.libs.api_handler import APIHandler\n+from mindsdb.integrations.libs.response import HandlerStatusResponse as StatusResponse\n+\n+\n+class LightdashHandler(APIHandler):\n+\n+    def __init__(self, name: str, **kwargs) -> None:\n+        super().__init__(name)\n+        self.connection = None\n+        self.is_connected = False\n+        self.api_key = kwargs.get(\"connection_data\", {}).get(\"api_key\", \"\")\n+        self.base_url = kwargs.get(\"connection_data\", {}).get(\"base_url\", \"\")\n+        _tables = [\n+            UserTable,\n+            UserAbilityTable,\n+            OrgTable,\n+            OrgProjectsTable,\n+            OrgMembersTable,\n+            ProjectTable,\n+            WarehouseConnectionTable,\n+            DBTConnectionTable,\n+            DBTEnvironmentVarsTable,\n+            ChartsTable,\n+            SpacesTable,\n+            AccessTable,\n+            ValidationTable,\n+            DashboardsTable,\n+            QueriesTable,\n+            ChartHistoryTable,\n+            ChartConfigTable,\n+            ChartAdditionalMetricsTable,\n+            ChartTableCalculationsTable,\n+            SchedulerLogsTable,\n+            SchedulerTable,\n+            SchedulerJobsTable,\n+            SchedulerJobStatus,\n+        ]\n+        for Table in _tables:\n+            self._register_table(Table.name, Table(self))\n+        self.connect()\n+\n+    def connect(self) -> Lightdash:\n+        self.connection = Lightdash(self.base_url, self.api_key)\n+        return self.connection\n+\n+    def check_connection(self) -> StatusResponse:\n+        resp = StatusResponse(False)\n+        if self.connection and not self.connection.is_connected():\n+            resp.error = \"Client not connected\"\n+        else:\n+            resp.success = True\n+        return resp\n+\n+    def native_query(self, query: str) -> StatusResponse:\n+        ast = parse_sql(query, dialect=\"mindsdb\")\n+        return self.query(ast)\nComment: Please include the \r\n```\r\nconnection_args = OrderedDict(\r\n    api_key={\r\n        'type': ARG_TYPE.STR,\r\n        'description': 'API Token for accessing ...',\r\n        'required': True,\r\n        'label': '',\r\n    }   \r\n)\r\n\r\nconnection_args_example = OrderedDict(\r\n    api_key ='<your--api-key>',\r\nbase_url ...\r\n)\r\n```",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "mindsdb/integrations/handlers/lightdash_handler/lightdash_handler.py",
    "pr_number": 7963,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1374528648,
    "comment_created_at": "2023-10-27T12:47:37Z"
  },
  {
    "code": "@@ -0,0 +1,125 @@\n+# 🎃 MindsDB Hacktoberfest 2025 \n+\n+## Supercharging AI analytical Apps with Knowledge Bases ⚡\n+\n+This Hacktoberfest, MindsDB challenges you to build AI-powered analytics apps using Knowledge Bases, Hybrid Search, and SQL Algebra.\n+\n+\n+### 🌟 Why Join?\n+MindsDB's Hacktoberfest is your chance to turn code into impact:\n+- Build tools that answer real business questions.\n+- Help teams move beyond rigid dashboards and siloed data.\n+- Level up your open-source contributions with AI-native analytics apps.\n+- Compete for prizes: GitHub sponsorships, swag, and a Prize Draw for a [MacBook Pro 16\" M4 Chip](https://www.apple.com/shop/buy-mac/macbook-pro/16-inch-space-black-standard-display-apple-m4-pro-with-14-core-cpu-and-20-core-gpu-48gb-memory-512gb).\n+- Get your project featured on the MindsDB blog + community.\n+\n+**Your mission:** Create AI apps powered by MindsDB's Knowledge Bases that query enterprise-like data in place—delivering accurate, explainable answers without ETL or data movement.\n+\n+\n+------\n+\n+## 🛠️ Core Task\n+\n+- Build an app powered by MindsDB Knowledge Bases and Agents.\n+- Write a blog post (Medium, Hashnode, dev.to, LinkedIn) explaining how you built it.\n+- Submit a Pull Request to the Hacktoberfest file in the MindsDB repo with your project, blog post and social media post.\n+- Invite people to like/upvote your PR. The PR with the most community likes wins!\n+- Promote your app on LinkedIn and X/Twitter with a post mentioning @mindsdb.\n+\n+-----\n+\n+## 🏆 Prize Categories\n+\n+Stand a chance to win a [MacBook Pro 16\" M4 Chip](https://www.apple.com/shop/buy-mac/macbook-pro/16-inch-space-black-standard-display-apple-m4-pro-with-14-core-cpu-and-20-core-gpu-48gb-memory-512gb) in our Prize Draw!\n+\n+### 🔥 Most Popular Pull Requests\n+- Top 3 Pull Requests with the most likes win GitHub sponsorship prizes.\n+- Every 10 likes = 1 entry into the Apple MacBook Pro prize draw.\n+  \n+**Prizes:**\n+- 🥇 $1500 + MindsDB T-shirt\n+- 🥈 $1000 + MindsDB T-shirt\n+- 🥉 $500 + MindsDB T-shirt\n+\n+\n+(Note: GitHub sponsorship must be available in your country in order to receive the prize, participants to check before they contribute. Automated voting is not allowed—violations will be disqualified.)\n+\n+### 📣 Social Media Awareness\n+Top 3 posts (LinkedIn/X) with the most engagement win:\n+- MindsDB T-shirt\n+- 1 entry into the Apple MacBook Pro prize draw\n+- $100 Github Sponsorship\n+\n+(Github Sponsorship may change depending on the amount of engagement a social media post received).\n+\n+### ✍️ Best Blog Content\n+Top 3 blog posts (as judged by the MindsDB team) win:\n+- MindsDB T-shirt\n+- Blog feature on the official MindsDB website\n+- 1 entry into the Apple MacBook Pro prize draw\n+- $100 Github Sponsorship.\n+\n+----\n+\n+## 🎯 Goals\n+- Showcase zero-ETL, data-in-place AI analytics with MindsDB KBs.\n+- Demonstrate hybrid semantic + SQL logic and use Evaluate KB for quality.\n+- Encourage integrations (Salesforce, BigQuery, Confluence, Gong, Postgres, etc.).\n+- Create repeatable app templates for use cases in accordance to our industries listed on our webpage, i.e [Finance Services](https://mindsdb.com/solutions/industry/ai-data-solution-financial-services), [Energy & Utilities](https://mindsdb.com/solutions/industry/ai-data-solution-energy-utilities), [Retail & E-commerce](https://mindsdb.com/solutions/industry/ai-data-solution-retail-ecommerce), [Enterprise Software Vendors](https://mindsdb.com/solutions/industry/ai-data-solution-b2b-tech), or for another Enterprise industry.\n+\n+## 👩‍💻 Who Should Join?\n+- AI/ML Enthusiasts (especially RAG & semantic search fans)\n+- SQL-savvy developers (data engineers, full-stack devs, data scientists)\n+- Existing MindsDB users & open-source contributors\n+\n+## 🔑 Example Use Cases\n+- Decision BI Re-imagined → NLQ → KPIs/charts (with auditability).\n+- Operations Copilot → Root cause & SOP search across tickets/wikis.\n+- Customer Intelligence → 360° CRM + docs with explainable recs.\n+- Compliance & Controls → Policy/filing QA with citations + risk flags.\n+- Wildcard → Any creative KB-powered analytics app.\n+\n+## 🛤️ Tracks\n+\n+### Track 1: Build an application with MindsDB Knowledge Bases\n+\n+Create a functional application (CLI, Web App, API, Bot Interface etc.) where the primary interaction or feature relies on the semantic query results from the KB. This includes:\n+  - A functional, empty Knowledge Base exists within their MindsDB instance (Cloud or local)\n+  - Participant connects a data source (Salesforce, Gong, Hubspot, Postgres or files) and successfully ingests text data into the KB using INSERT INTO. The KB is populated with text data suitable for semantic querying.\n+  - Demonstrate retrieving meaningful results based on semantic similarity and metadata filtering using [Hybrid Search](https://docs.mindsdb.com/mindsdb_sql/knowledge_bases/hybrid_search). Successfully retrieve relevant data chunks/rows based on semantic queries. \n+  - Provide a public GitHub repo with clear setup instructions and documentation, along with a working application that demonstrates a practical use case for Knowledge Bases, supported by a short, shareable demo video showcasing the app in action.\n+\n+### Track 2: Advanced Capabilities\n+- Jobs Integration: Auto-update KBs with [CREATE JOB](https://docs.mindsdb.com/mindsdb_sql/sql/create/jobs).\n+- [Agent Integration](https://docs.mindsdb.com/mindsdb_sql/agents/agent)\n+- Metadata Filtering: Hybrid search with semantic + structured filters for eg. LIKE and BETWEEN operators.",
    "comment": "Hybrid search could be listed here as well. And EVALUATE KB.",
    "line_number": 95,
    "enriched": "File: mindsdb hacktoberfest/README.md\nCode: @@ -0,0 +1,125 @@\n+# 🎃 MindsDB Hacktoberfest 2025 \n+\n+## Supercharging AI analytical Apps with Knowledge Bases ⚡\n+\n+This Hacktoberfest, MindsDB challenges you to build AI-powered analytics apps using Knowledge Bases, Hybrid Search, and SQL Algebra.\n+\n+\n+### 🌟 Why Join?\n+MindsDB's Hacktoberfest is your chance to turn code into impact:\n+- Build tools that answer real business questions.\n+- Help teams move beyond rigid dashboards and siloed data.\n+- Level up your open-source contributions with AI-native analytics apps.\n+- Compete for prizes: GitHub sponsorships, swag, and a Prize Draw for a [MacBook Pro 16\" M4 Chip](https://www.apple.com/shop/buy-mac/macbook-pro/16-inch-space-black-standard-display-apple-m4-pro-with-14-core-cpu-and-20-core-gpu-48gb-memory-512gb).\n+- Get your project featured on the MindsDB blog + community.\n+\n+**Your mission:** Create AI apps powered by MindsDB's Knowledge Bases that query enterprise-like data in place—delivering accurate, explainable answers without ETL or data movement.\n+\n+\n+------\n+\n+## 🛠️ Core Task\n+\n+- Build an app powered by MindsDB Knowledge Bases and Agents.\n+- Write a blog post (Medium, Hashnode, dev.to, LinkedIn) explaining how you built it.\n+- Submit a Pull Request to the Hacktoberfest file in the MindsDB repo with your project, blog post and social media post.\n+- Invite people to like/upvote your PR. The PR with the most community likes wins!\n+- Promote your app on LinkedIn and X/Twitter with a post mentioning @mindsdb.\n+\n+-----\n+\n+## 🏆 Prize Categories\n+\n+Stand a chance to win a [MacBook Pro 16\" M4 Chip](https://www.apple.com/shop/buy-mac/macbook-pro/16-inch-space-black-standard-display-apple-m4-pro-with-14-core-cpu-and-20-core-gpu-48gb-memory-512gb) in our Prize Draw!\n+\n+### 🔥 Most Popular Pull Requests\n+- Top 3 Pull Requests with the most likes win GitHub sponsorship prizes.\n+- Every 10 likes = 1 entry into the Apple MacBook Pro prize draw.\n+  \n+**Prizes:**\n+- 🥇 $1500 + MindsDB T-shirt\n+- 🥈 $1000 + MindsDB T-shirt\n+- 🥉 $500 + MindsDB T-shirt\n+\n+\n+(Note: GitHub sponsorship must be available in your country in order to receive the prize, participants to check before they contribute. Automated voting is not allowed—violations will be disqualified.)\n+\n+### 📣 Social Media Awareness\n+Top 3 posts (LinkedIn/X) with the most engagement win:\n+- MindsDB T-shirt\n+- 1 entry into the Apple MacBook Pro prize draw\n+- $100 Github Sponsorship\n+\n+(Github Sponsorship may change depending on the amount of engagement a social media post received).\n+\n+### ✍️ Best Blog Content\n+Top 3 blog posts (as judged by the MindsDB team) win:\n+- MindsDB T-shirt\n+- Blog feature on the official MindsDB website\n+- 1 entry into the Apple MacBook Pro prize draw\n+- $100 Github Sponsorship.\n+\n+----\n+\n+## 🎯 Goals\n+- Showcase zero-ETL, data-in-place AI analytics with MindsDB KBs.\n+- Demonstrate hybrid semantic + SQL logic and use Evaluate KB for quality.\n+- Encourage integrations (Salesforce, BigQuery, Confluence, Gong, Postgres, etc.).\n+- Create repeatable app templates for use cases in accordance to our industries listed on our webpage, i.e [Finance Services](https://mindsdb.com/solutions/industry/ai-data-solution-financial-services), [Energy & Utilities](https://mindsdb.com/solutions/industry/ai-data-solution-energy-utilities), [Retail & E-commerce](https://mindsdb.com/solutions/industry/ai-data-solution-retail-ecommerce), [Enterprise Software Vendors](https://mindsdb.com/solutions/industry/ai-data-solution-b2b-tech), or for another Enterprise industry.\n+\n+## 👩‍💻 Who Should Join?\n+- AI/ML Enthusiasts (especially RAG & semantic search fans)\n+- SQL-savvy developers (data engineers, full-stack devs, data scientists)\n+- Existing MindsDB users & open-source contributors\n+\n+## 🔑 Example Use Cases\n+- Decision BI Re-imagined → NLQ → KPIs/charts (with auditability).\n+- Operations Copilot → Root cause & SOP search across tickets/wikis.\n+- Customer Intelligence → 360° CRM + docs with explainable recs.\n+- Compliance & Controls → Policy/filing QA with citations + risk flags.\n+- Wildcard → Any creative KB-powered analytics app.\n+\n+## 🛤️ Tracks\n+\n+### Track 1: Build an application with MindsDB Knowledge Bases\n+\n+Create a functional application (CLI, Web App, API, Bot Interface etc.) where the primary interaction or feature relies on the semantic query results from the KB. This includes:\n+  - A functional, empty Knowledge Base exists within their MindsDB instance (Cloud or local)\n+  - Participant connects a data source (Salesforce, Gong, Hubspot, Postgres or files) and successfully ingests text data into the KB using INSERT INTO. The KB is populated with text data suitable for semantic querying.\n+  - Demonstrate retrieving meaningful results based on semantic similarity and metadata filtering using [Hybrid Search](https://docs.mindsdb.com/mindsdb_sql/knowledge_bases/hybrid_search). Successfully retrieve relevant data chunks/rows based on semantic queries. \n+  - Provide a public GitHub repo with clear setup instructions and documentation, along with a working application that demonstrates a practical use case for Knowledge Bases, supported by a short, shareable demo video showcasing the app in action.\n+\n+### Track 2: Advanced Capabilities\n+- Jobs Integration: Auto-update KBs with [CREATE JOB](https://docs.mindsdb.com/mindsdb_sql/sql/create/jobs).\n+- [Agent Integration](https://docs.mindsdb.com/mindsdb_sql/agents/agent)\n+- Metadata Filtering: Hybrid search with semantic + structured filters for eg. LIKE and BETWEEN operators.\nComment: Hybrid search could be listed here as well. And EVALUATE KB.",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "mindsdb hacktoberfest/README.md",
    "pr_number": 11651,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2392414842,
    "comment_created_at": "2025-09-30T17:59:36Z"
  },
  {
    "code": "@@ -62,6 +71,16 @@ Content-Type: application/json\n }\n ```\n \n+Only the `url` key is required in the Request. All others are optional.\n+ - `url` - this represents the address that will be utilized to send a POST request, containing detailed information about the model's status\n+ - `filter` - this is used to restrict the triggering of callbacks:\n+   - `model_name` - python-style regular expression that aids in filtering the names of models that trigger a callback",
    "comment": "A minor update:\r\nlet's say `python-style regular expression used to filter the names of models that trigger a callback` (like it is for projects.",
    "line_number": 77,
    "enriched": "File: docs/callbacks.mdx\nCode: @@ -62,6 +71,16 @@ Content-Type: application/json\n }\n ```\n \n+Only the `url` key is required in the Request. All others are optional.\n+ - `url` - this represents the address that will be utilized to send a POST request, containing detailed information about the model's status\n+ - `filter` - this is used to restrict the triggering of callbacks:\n+   - `model_name` - python-style regular expression that aids in filtering the names of models that trigger a callback\nComment: A minor update:\r\nlet's say `python-style regular expression used to filter the names of models that trigger a callback` (like it is for projects.",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/callbacks.mdx",
    "pr_number": 8491,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1420698939,
    "comment_created_at": "2023-12-08T15:53:59Z"
  },
  {
    "code": "@@ -0,0 +1,71 @@\n+---\n+title: The TO_MARKDOWN() Function\n+sidebarTitle: The TO_MARKDOWN() Function\n+---\n+\n+MindsDB provides the `TO_MARKDOWN()` function that lets users extract the content of their documents in markdown by simply specifying the document path or URL. This function is especially useful for passing the extracted content of documents through LLMs or for storing them in a Knowledge Base.",
    "comment": "@MinuraPunchihewa \r\nWhen you mention Knowledge Base here, please link the other doc page like this:\r\n```\r\n[Knowledge Base](/mindsdb_sql/agents/knowledge-bases)\r\n```",
    "line_number": 6,
    "enriched": "File: docs/mindsdb_sql/functions/to_markdown_function\nCode: @@ -0,0 +1,71 @@\n+---\n+title: The TO_MARKDOWN() Function\n+sidebarTitle: The TO_MARKDOWN() Function\n+---\n+\n+MindsDB provides the `TO_MARKDOWN()` function that lets users extract the content of their documents in markdown by simply specifying the document path or URL. This function is especially useful for passing the extracted content of documents through LLMs or for storing them in a Knowledge Base.\nComment: @MinuraPunchihewa \r\nWhen you mention Knowledge Base here, please link the other doc page like this:\r\n```\r\n[Knowledge Base](/mindsdb_sql/agents/knowledge-bases)\r\n```",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/mindsdb_sql/functions/to_markdown_function",
    "pr_number": 10650,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2033104515,
    "comment_created_at": "2025-04-08T12:39:59Z"
  },
  {
    "code": "@@ -370,3 +383,80 @@ class Tasks(Base):\n \n     updated_at = Column(DateTime, default=datetime.datetime.now, onupdate=datetime.datetime.now)\n     created_at = Column(DateTime, default=datetime.datetime.now)\n+\n+\n+class KBRetrievalStrategy(str, Enum):\n+    SIMILARITY = \"similarity\"\n+    MMR = \"mmr\"\n+    BM25 = \"bm25\"\n+    HYBRID = \"hybrid\"  # similarity + bm25\n+\n+\n+class KnowledgeBase(Base):\n+    class ParamSpec(pydantic.BaseModel):\n+        \"\"\"\n+        Describe the parameters used to configure a knowledge base.\n+        \"\"\"\n+\n+        retrieval_strategy: KBRetrievalStrategy = KBRetrievalStrategy.SIMILARITY\n+        chunk_size = (\n+            1000  # TODO: not taking effect for now since chunking is not implemented\n+        )\n+        chunk_overlap = 100\n+\n+    __tablename__ = \"knowledge_base\"\n+    id = Column(Integer, primary_key=True)\n+\n+    name = Column(String, nullable=False, comment=\"The name of the knowledge base.\")\n+    company_id = Column(Integer)\n+    user_class = Column(Integer, nullable=True)\n+    project_id = Column(Integer, nullable=False)\n+    _params = Column(JSON, comment=\"Parameters used to create the knowledge base.\")\n+\n+    # reference to the embedding model and the vector database\n+    embedding_model_id = Column(\n+        ForeignKey(\"predictor.id\", name=\"fk_embedding_integration_id\")\n+    )\n+    vector_database_id = Column(\n+        ForeignKey(\"integration.id\", name=\"fk_vectordatabase_integration_id\")\n+    )\n+    # default to the name of the knowledge base + uuid\n+    vector_database_table_name = Column(\n+        String,\n+        nullable=False,\n+        comment=\"The name of the table in the vector database.\",\n+        default=lambda context: f\"{context.current_parameters['name']}_{uuid.uuid4().hex}\",\n+    )\n+\n+    created_at = Column(DateTime, default=datetime.datetime.now)\n+    updated_at = Column(\n+        DateTime, default=datetime.datetime.now, onupdate=datetime.datetime.now\n+    )\n+    deleted_at = Column(DateTime)\n+\n+    __table_args__ = (\n+        UniqueConstraint(\"name\", \"project_id\", name=\"uniq_kb_name_project_id\"),\n+    )\n+\n+    # when vector database table name is not set",
    "comment": "nit: looks like this comment is intended for `vector_database_table_name`, but you're already explaining the default behavior in line 423, so perhaps remove this one",
    "line_number": 441,
    "enriched": "File: mindsdb/interfaces/storage/db.py\nCode: @@ -370,3 +383,80 @@ class Tasks(Base):\n \n     updated_at = Column(DateTime, default=datetime.datetime.now, onupdate=datetime.datetime.now)\n     created_at = Column(DateTime, default=datetime.datetime.now)\n+\n+\n+class KBRetrievalStrategy(str, Enum):\n+    SIMILARITY = \"similarity\"\n+    MMR = \"mmr\"\n+    BM25 = \"bm25\"\n+    HYBRID = \"hybrid\"  # similarity + bm25\n+\n+\n+class KnowledgeBase(Base):\n+    class ParamSpec(pydantic.BaseModel):\n+        \"\"\"\n+        Describe the parameters used to configure a knowledge base.\n+        \"\"\"\n+\n+        retrieval_strategy: KBRetrievalStrategy = KBRetrievalStrategy.SIMILARITY\n+        chunk_size = (\n+            1000  # TODO: not taking effect for now since chunking is not implemented\n+        )\n+        chunk_overlap = 100\n+\n+    __tablename__ = \"knowledge_base\"\n+    id = Column(Integer, primary_key=True)\n+\n+    name = Column(String, nullable=False, comment=\"The name of the knowledge base.\")\n+    company_id = Column(Integer)\n+    user_class = Column(Integer, nullable=True)\n+    project_id = Column(Integer, nullable=False)\n+    _params = Column(JSON, comment=\"Parameters used to create the knowledge base.\")\n+\n+    # reference to the embedding model and the vector database\n+    embedding_model_id = Column(\n+        ForeignKey(\"predictor.id\", name=\"fk_embedding_integration_id\")\n+    )\n+    vector_database_id = Column(\n+        ForeignKey(\"integration.id\", name=\"fk_vectordatabase_integration_id\")\n+    )\n+    # default to the name of the knowledge base + uuid\n+    vector_database_table_name = Column(\n+        String,\n+        nullable=False,\n+        comment=\"The name of the table in the vector database.\",\n+        default=lambda context: f\"{context.current_parameters['name']}_{uuid.uuid4().hex}\",\n+    )\n+\n+    created_at = Column(DateTime, default=datetime.datetime.now)\n+    updated_at = Column(\n+        DateTime, default=datetime.datetime.now, onupdate=datetime.datetime.now\n+    )\n+    deleted_at = Column(DateTime)\n+\n+    __table_args__ = (\n+        UniqueConstraint(\"name\", \"project_id\", name=\"uniq_kb_name_project_id\"),\n+    )\n+\n+    # when vector database table name is not set\nComment: nit: looks like this comment is intended for `vector_database_table_name`, but you're already explaining the default behavior in line 423, so perhaps remove this one",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "mindsdb/interfaces/storage/db.py",
    "pr_number": 7003,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1303691898,
    "comment_created_at": "2023-08-24T01:33:16Z"
  },
  {
    "code": "@@ -0,0 +1,35 @@\n+## Implementation\n+\n+This is the implementation of the Aereospike for MindsDB.\n+\n+The required arguments to establish a connection are as follows:\n+\n+-   `user` is the database user.\n+-   `password` is the database password.\n+-   `host` is the host IP address or URL.\n+-   `port` is the port used to make TCP/IP connection.\n+-   `namespace` is the aerospike namespace.\n+\n+Other optional parameters are not supported as of now.\n+\n+## Usage\n+\n+In order to make use of this handler and connect to the MariaDB database in MindsDB, the following syntax can be used:",
    "comment": "Please replace `MariaDB` with `Aerospike`.",
    "line_number": 17,
    "enriched": "File: mindsdb/integrations/handlers/aerospike_handler/README.md\nCode: @@ -0,0 +1,35 @@\n+## Implementation\n+\n+This is the implementation of the Aereospike for MindsDB.\n+\n+The required arguments to establish a connection are as follows:\n+\n+-   `user` is the database user.\n+-   `password` is the database password.\n+-   `host` is the host IP address or URL.\n+-   `port` is the port used to make TCP/IP connection.\n+-   `namespace` is the aerospike namespace.\n+\n+Other optional parameters are not supported as of now.\n+\n+## Usage\n+\n+In order to make use of this handler and connect to the MariaDB database in MindsDB, the following syntax can be used:\nComment: Please replace `MariaDB` with `Aerospike`.",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "mindsdb/integrations/handlers/aerospike_handler/README.md",
    "pr_number": 7980,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1368363395,
    "comment_created_at": "2023-10-23T09:14:46Z"
  },
  {
    "code": "@@ -1,6 +1,7 @@\n # MindsDB Documentation    <a href=\"https://docs.mindsdb.com?utm_medium=community&utm_source=github&utm_campaign=mindsdb%20repo\"><img src=\"https://img.shields.io/website?url=https%3A%2F%2Fwww.mindsdb.com%2F\" alt=\"MindsDB Docs\"></a>\t\n \n ## Running the docs locally\n+Run the following commands in your local repo, and make sure you are located at 'docs' folder.",
    "comment": "Please remove all the content from the `Running the docs locally` chapter and replace it with the following:\r\n\r\n> \\## Running the Docs Locally\r\n> \r\n> \\<Info>\r\n>     \\*\\*Prerequisite\\*\\*\r\n>     You should have installed Git (version 2.30.1 or higher) and Node.js (version 18.10.0 or higher).\r\n> \\</Info>\r\n> \r\n> Step 1. Clone the MindsDB Git repository:\r\n> \r\n> \\```console\r\n> git clone https://github.com/mindsdb/mindsdb.git\r\n> \\```\r\n> \r\n> Step 2. Install Mintlify on your OS:\r\n> \r\n> \\```console\r\n> npm i mintlify -g\r\n> \\```\r\n> \r\n> Step 3. Go to the \\`docs\\` folder inside the cloned MindsDB Git repository and start Mintlify there:\r\n> \r\n> \\```console\r\n> mintlify dev\r\n> \\```\r\n> \r\n> The documentation website is now available at \\`http://localhost:3000\\`.\r\n> \r\n> \\<Warning>\r\n>   \\*\\*Getting an Error?\\*\\*\r\n>   If you use the Windows operating system, you may get an error saying \\`no such file or directory: C:/Users/Username/.mintlify/mint/client\\`. Here are the steps to troubleshoot it:\r\n>         - Go to the \\`C:/Users/Username/.mintlify/\\` directory.\r\n>         - Remove the \\`mint\\` folder.\r\n>         - Open the Git Bash in this location and run \\`git clone https://github.com/mintlify/mint.git\\`.\r\n>         - Repeat step 3.\r\n> \\</Warning>",
    "line_number": 4,
    "enriched": "File: docs/README.md\nCode: @@ -1,6 +1,7 @@\n # MindsDB Documentation    <a href=\"https://docs.mindsdb.com?utm_medium=community&utm_source=github&utm_campaign=mindsdb%20repo\"><img src=\"https://img.shields.io/website?url=https%3A%2F%2Fwww.mindsdb.com%2F\" alt=\"MindsDB Docs\"></a>\t\n \n ## Running the docs locally\n+Run the following commands in your local repo, and make sure you are located at 'docs' folder.\nComment: Please remove all the content from the `Running the docs locally` chapter and replace it with the following:\r\n\r\n> \\## Running the Docs Locally\r\n> \r\n> \\<Info>\r\n>     \\*\\*Prerequisite\\*\\*\r\n>     You should have installed Git (version 2.30.1 or higher) and Node.js (version 18.10.0 or higher).\r\n> \\</Info>\r\n> \r\n> Step 1. Clone the MindsDB Git repository:\r\n> \r\n> \\```console\r\n> git clone https://github.com/mindsdb/mindsdb.git\r\n> \\```\r\n> \r\n> Step 2. Install Mintlify on your OS:\r\n> \r\n> \\```console\r\n> npm i mintlify -g\r\n> \\```\r\n> \r\n> Step 3. Go to the \\`docs\\` folder inside the cloned MindsDB Git repository and start Mintlify there:\r\n> \r\n> \\```console\r\n> mintlify dev\r\n> \\```\r\n> \r\n> The documentation website is now available at \\`http://localhost:3000\\`.\r\n> \r\n> \\<Warning>\r\n>   \\*\\*Getting an Error?\\*\\*\r\n>   If you use the Windows operating system, you may get an error saying \\`no such file or directory: C:/Users/Username/.mintlify/mint/client\\`. Here are the steps to troubleshoot it:\r\n>         - Go to the \\`C:/Users/Username/.mintlify/\\` directory.\r\n>         - Remove the \\`mint\\` folder.\r\n>         - Open the Git Bash in this location and run \\`git clone https://github.com/mintlify/mint.git\\`.\r\n>         - Repeat step 3.\r\n> \\</Warning>",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/README.md",
    "pr_number": 6542,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1234531988,
    "comment_created_at": "2023-06-19T22:40:56Z"
  },
  {
    "code": "@@ -123,7 +123,7 @@ MindsDB works with most of the SQL and NoSQL databases, data lakes, data Streams\n | <a href=\"https://docs.mindsdb.com/data-integrations/apache-cassandra\"><img src=\"https://img.shields.io/badge/Cassandra-1287B1?style=for-the-badge&logo=apache%20cassandra&logoColor=white\" alt=\"Connect Cassandra\"></a> | <a href=\" https://docs.mindsdb.com/data-integrations/all-data-integrations#datastax\"><img src=\"https://img.shields.io/badge/DataStax-3A3A42?logo=datastax&logoColor=fff&style=for-the-badge\" alt=\"DataStax Badge\"></a> | <a href=\"https://docs.mindsdb.com/data-integrations/all-data-integrations#s3\"><img src=\"https://img.shields.io/badge/Amazon%20S3-569A31?logo=amazons3&logoColor=fff&style=for-the-badge\" alt=\"Amazon S3 Badge\"></a> |\n | <a href=\"https://docs.mindsdb.com/data-integrations/cockroachdb\"><img src=\"https://img.shields.io/badge/CockroachDB-426EDF?style=for-the-badge&logo=cockroach-labs&logoColor=white\" alt=\"Connect CockroachDB\"></a> | <a href=\"https://docs.mindsdb.com/data-integrations/ckan\"><img src=\"https://img.shields.io/badge/ckan-7D929E?logo=ckan&logoColor=fff&style=for-the-badge\" alt=\"ckan Badge\"></a> | <a href=\"https://docs.mindsdb.com/data-integrations/all-data-integrations#supabase\"><img src=\"https://img.shields.io/badge/Supabase-3ECF8E?logo=supabase&logoColor=fff&style=for-the-badge\" alt=\"Supabase Badge\"></a> |\n | <a href=\"https://docs.mindsdb.com/data-integrations/clickhouse\"><img src=\"https://img.shields.io/badge/Clickhouse-e6e600?style=for-the-badge&logo=clickhouse&logoColor=white\" alt=\"Connect Clickhouse\"></a> | <a href=\"https://docs.mindsdb.com/data-integrations/all-data-integrations#google-bigquery\"><img src=\"https://img.shields.io/badge/Google Big Query-0466C8?logo=Big Query&logoColor=fff&style=for-the-badge\" alt=\"Google Big Query Badge\"></a> | <a href=\"https://docs.mindsdb.com/data-integrations/all-data-integrations#sqlite\"><img src=\"https://img.shields.io/badge/SQLite-003B57?logo=sqlite&logoColor=fff&style=for-the-badge\" alt=\"SQLite Badge\"></a> |\n-| <a href=\"https://docs.mindsdb.com/data-integrations/mariadb\"><img src=\"https://img.shields.io/badge/MariaDB-003545?style=for-the-badge&logo=mariadb&logoColor=white\" alt=\"Connect MariaDB\"></a> | <a href=\"https://docs.mindsdb.com/data-integrations/all-data-integrations#couchbase\"><img src=\"https://img.shields.io/badge/Couchbase-EA2328?logo=couchbase&logoColor=fff&style=for-the-badge\" alt=\"Couchbase Badge\"></a> | <a href=\"https://docs.mindsdb.com/data-integrations/TiDB#tidb-handler\"><img src=\"https://img.shields.io/badge/TiDB-DD0031?logo=tidb&logoColor=fff&style=for-the-badge\" alt=\"TiDB Badge\"></a> |\n+| <a href=\"https://docs.mindsdb.com/data-integrations/mariadb\"><img src=\"https://img.shields.io/badge/MariaDB-003545?style=for-the-badge&logo=mariadb&logoColor=white\" alt=\"Connect MariaDB\"></a> | <a href=https://docs.mindsdb.com/data-integrations/couchbase><img src=\"https://img.shields.io/badge/Couchbase-EA2328?logo=couchbase&logoColor=fff&style=for-the-badge\" alt=\"Couchbase Badge\"></a> | <a href=\"https://docs.mindsdb.com/data-integrations/TiDB#tidb-handler\"><img src=\"https://img.shields.io/badge/TiDB-DD0031?logo=tidb&logoColor=fff&style=for-the-badge\" alt=\"TiDB Badge\"></a> |",
    "comment": "The link should be enclosed in double quotation marks, like this:\r\n\r\n> \\<a href=\"https://docs.mindsdb.com/data-integrations/couchbase\">",
    "line_number": 126,
    "enriched": "File: README.md\nCode: @@ -123,7 +123,7 @@ MindsDB works with most of the SQL and NoSQL databases, data lakes, data Streams\n | <a href=\"https://docs.mindsdb.com/data-integrations/apache-cassandra\"><img src=\"https://img.shields.io/badge/Cassandra-1287B1?style=for-the-badge&logo=apache%20cassandra&logoColor=white\" alt=\"Connect Cassandra\"></a> | <a href=\" https://docs.mindsdb.com/data-integrations/all-data-integrations#datastax\"><img src=\"https://img.shields.io/badge/DataStax-3A3A42?logo=datastax&logoColor=fff&style=for-the-badge\" alt=\"DataStax Badge\"></a> | <a href=\"https://docs.mindsdb.com/data-integrations/all-data-integrations#s3\"><img src=\"https://img.shields.io/badge/Amazon%20S3-569A31?logo=amazons3&logoColor=fff&style=for-the-badge\" alt=\"Amazon S3 Badge\"></a> |\n | <a href=\"https://docs.mindsdb.com/data-integrations/cockroachdb\"><img src=\"https://img.shields.io/badge/CockroachDB-426EDF?style=for-the-badge&logo=cockroach-labs&logoColor=white\" alt=\"Connect CockroachDB\"></a> | <a href=\"https://docs.mindsdb.com/data-integrations/ckan\"><img src=\"https://img.shields.io/badge/ckan-7D929E?logo=ckan&logoColor=fff&style=for-the-badge\" alt=\"ckan Badge\"></a> | <a href=\"https://docs.mindsdb.com/data-integrations/all-data-integrations#supabase\"><img src=\"https://img.shields.io/badge/Supabase-3ECF8E?logo=supabase&logoColor=fff&style=for-the-badge\" alt=\"Supabase Badge\"></a> |\n | <a href=\"https://docs.mindsdb.com/data-integrations/clickhouse\"><img src=\"https://img.shields.io/badge/Clickhouse-e6e600?style=for-the-badge&logo=clickhouse&logoColor=white\" alt=\"Connect Clickhouse\"></a> | <a href=\"https://docs.mindsdb.com/data-integrations/all-data-integrations#google-bigquery\"><img src=\"https://img.shields.io/badge/Google Big Query-0466C8?logo=Big Query&logoColor=fff&style=for-the-badge\" alt=\"Google Big Query Badge\"></a> | <a href=\"https://docs.mindsdb.com/data-integrations/all-data-integrations#sqlite\"><img src=\"https://img.shields.io/badge/SQLite-003B57?logo=sqlite&logoColor=fff&style=for-the-badge\" alt=\"SQLite Badge\"></a> |\n-| <a href=\"https://docs.mindsdb.com/data-integrations/mariadb\"><img src=\"https://img.shields.io/badge/MariaDB-003545?style=for-the-badge&logo=mariadb&logoColor=white\" alt=\"Connect MariaDB\"></a> | <a href=\"https://docs.mindsdb.com/data-integrations/all-data-integrations#couchbase\"><img src=\"https://img.shields.io/badge/Couchbase-EA2328?logo=couchbase&logoColor=fff&style=for-the-badge\" alt=\"Couchbase Badge\"></a> | <a href=\"https://docs.mindsdb.com/data-integrations/TiDB#tidb-handler\"><img src=\"https://img.shields.io/badge/TiDB-DD0031?logo=tidb&logoColor=fff&style=for-the-badge\" alt=\"TiDB Badge\"></a> |\n+| <a href=\"https://docs.mindsdb.com/data-integrations/mariadb\"><img src=\"https://img.shields.io/badge/MariaDB-003545?style=for-the-badge&logo=mariadb&logoColor=white\" alt=\"Connect MariaDB\"></a> | <a href=https://docs.mindsdb.com/data-integrations/couchbase><img src=\"https://img.shields.io/badge/Couchbase-EA2328?logo=couchbase&logoColor=fff&style=for-the-badge\" alt=\"Couchbase Badge\"></a> | <a href=\"https://docs.mindsdb.com/data-integrations/TiDB#tidb-handler\"><img src=\"https://img.shields.io/badge/TiDB-DD0031?logo=tidb&logoColor=fff&style=for-the-badge\" alt=\"TiDB Badge\"></a> |\nComment: The link should be enclosed in double quotation marks, like this:\r\n\r\n> \\<a href=\"https://docs.mindsdb.com/data-integrations/couchbase\">",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "README.md",
    "pr_number": 5931,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1182643279,
    "comment_created_at": "2023-05-02T14:35:47Z"
  },
  {
    "code": "@@ -0,0 +1,37 @@\n+---\n+title: Vitess\n+sidebarTitle: Vitess\n+---\n+\n+# Vitess Handler\n+\n+This is the implementation of the Vitess handler for MindsDB.\n+\n+# Vitess\n+\n+Vitess is a database solution for deploying, scaling and managing large clusters of open-source database instances. It currently supports MySQL and Percona Server for MySQL. It's architected to run as effectively in a public or private cloud architecture as it does on dedicated hardware. It combines and extends many important SQL features with the scalability of a NoSQL database.\n+\n+\n+# Usage\n+\n+  In order to make use of this handler and connect to a Vitess database in MindsDB, the following syntax can be used,\n+\n+  ```json",
    "comment": "Please replace it with ```sql",
    "line_number": 19,
    "enriched": "File: docs/data-integrations/Vitess.mdx\nCode: @@ -0,0 +1,37 @@\n+---\n+title: Vitess\n+sidebarTitle: Vitess\n+---\n+\n+# Vitess Handler\n+\n+This is the implementation of the Vitess handler for MindsDB.\n+\n+# Vitess\n+\n+Vitess is a database solution for deploying, scaling and managing large clusters of open-source database instances. It currently supports MySQL and Percona Server for MySQL. It's architected to run as effectively in a public or private cloud architecture as it does on dedicated hardware. It combines and extends many important SQL features with the scalability of a NoSQL database.\n+\n+\n+# Usage\n+\n+  In order to make use of this handler and connect to a Vitess database in MindsDB, the following syntax can be used,\n+\n+  ```json\nComment: Please replace it with ```sql",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/data-integrations/Vitess.mdx",
    "pr_number": 5333,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1154520025,
    "comment_created_at": "2023-03-31T14:04:54Z"
  },
  {
    "code": "@@ -80,6 +80,18 @@ Available models:\n - claude-instant-1.2\n </Accordion>\n \n+<Accordion title=\"Bedrock\">\n+Available models include all models accessible from Bedrock.\n+\n+Note that in order to use Bedrock as a model provider, you should ensure the following packages are installed: `langchain_aws` and `transformers`.\n+\n+The following parameters are specific to this provider:\n+\n+* `aws_region_name`\n+* `aws_access_key_id`\n+* `aws_secret_access_key`\n+</Accordion>",
    "comment": "there is one more parameter available:\r\n- aws_session_token\r\nit depends on how aws premissions are setup, sometimes it required, somethimes - not\r\nfor example was required [here](https://github.com/mindsdb/mindsdb/pull/10855)",
    "line_number": 93,
    "enriched": "File: docs/mindsdb_sql/agents/agent_syntax.mdx\nCode: @@ -80,6 +80,18 @@ Available models:\n - claude-instant-1.2\n </Accordion>\n \n+<Accordion title=\"Bedrock\">\n+Available models include all models accessible from Bedrock.\n+\n+Note that in order to use Bedrock as a model provider, you should ensure the following packages are installed: `langchain_aws` and `transformers`.\n+\n+The following parameters are specific to this provider:\n+\n+* `aws_region_name`\n+* `aws_access_key_id`\n+* `aws_secret_access_key`\n+</Accordion>\nComment: there is one more parameter available:\r\n- aws_session_token\r\nit depends on how aws premissions are setup, sometimes it required, somethimes - not\r\nfor example was required [here](https://github.com/mindsdb/mindsdb/pull/10855)",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/mindsdb_sql/agents/agent_syntax.mdx",
    "pr_number": 11515,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2321812354,
    "comment_created_at": "2025-09-04T11:53:31Z"
  },
  {
    "code": "@@ -54,9 +54,11 @@ services:\n       LANGFUSE_TIMEOUT: \"10\"\n       LANGFUSE_SAMPLE_RATE: \"1.0\"\n       OTEL_EXPORTER_TYPE: \"console\" # or \"console\" # Define the exporter type (console/otlp)\n-      # OTEL_OTLP_ENDPOINT: \"http://jaeger:4317\" # Define the endpoint for the otlp exporter. Uncomment this line if you want to use jaeger.\n+      # OTEL_OTLP_ENDPOINT: \"http://otel-collector:4317\" # Define the endpoint for the otlp exporter.",
    "comment": "Worth mentioning in the comment that this is in fact GRPC? Tripped me up the first time but I suppose the port is clear 👍\n\nJust in terms of GRPC/HTTP support.",
    "line_number": 58,
    "enriched": "File: docker-compose.yml\nCode: @@ -54,9 +54,11 @@ services:\n       LANGFUSE_TIMEOUT: \"10\"\n       LANGFUSE_SAMPLE_RATE: \"1.0\"\n       OTEL_EXPORTER_TYPE: \"console\" # or \"console\" # Define the exporter type (console/otlp)\n-      # OTEL_OTLP_ENDPOINT: \"http://jaeger:4317\" # Define the endpoint for the otlp exporter. Uncomment this line if you want to use jaeger.\n+      # OTEL_OTLP_ENDPOINT: \"http://otel-collector:4317\" # Define the endpoint for the otlp exporter.\nComment: Worth mentioning in the comment that this is in fact GRPC? Tripped me up the first time but I suppose the port is clear 👍\n\nJust in terms of GRPC/HTTP support.",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docker-compose.yml",
    "pr_number": 10318,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1895843979,
    "comment_created_at": "2024-12-23T15:00:10Z"
  },
  {
    "code": "@@ -25,10 +25,22 @@ Go to the Extensions page in Docker Desktop and search for MindsDB.\n \n Install the MindsDB extension.\n \n+<Tip>\n+The first time the extension is installed, it will run the latest version of MindsDB. Moving forward, it's advisable to regularly update the MindsDB image used by the extension to ensure access to the latest features and improvements. \n+\n+As mentioned previously, the extension uses the `mindsdb/mindsdb:latest` Docker image. To update the image, follow these steps:\n+1. Navigate to the 'Images' tab in Docker Desktop.\n+2. Search or locate the mindsdb/mindsdb:latest image.\n+3. Click on the three dots on the right side of the image and click 'Pull'. If the image is already up to date, you will see a message stating so and you can skip the next step.\n+4. Wait for the image to be pulled and restart Docker Desktop.\n+\n+<img src=\"/assets/docker/docker_desktop/pull-latest-image.png\"/>\n+</Tip>\n+\n Access MindsDB inside Docker Desktop.\n \n <p align=\"center\">\n-  <img src=\"/assets/mindsdb_docker_desktop.png\"/>\n+  <img src=\"/assets/docker/docker_desktop/mindsdb_docker_desktop.png\"/>\n </p>\n \n ## Install dependencies",
    "comment": "Please change `##` to `###` for these sections: `Install MindsDB`, `Install dependencies`, and `View logs`.",
    "line_number": 46,
    "enriched": "File: docs/setup/self-hosted/docker-desktop.mdx\nCode: @@ -25,10 +25,22 @@ Go to the Extensions page in Docker Desktop and search for MindsDB.\n \n Install the MindsDB extension.\n \n+<Tip>\n+The first time the extension is installed, it will run the latest version of MindsDB. Moving forward, it's advisable to regularly update the MindsDB image used by the extension to ensure access to the latest features and improvements. \n+\n+As mentioned previously, the extension uses the `mindsdb/mindsdb:latest` Docker image. To update the image, follow these steps:\n+1. Navigate to the 'Images' tab in Docker Desktop.\n+2. Search or locate the mindsdb/mindsdb:latest image.\n+3. Click on the three dots on the right side of the image and click 'Pull'. If the image is already up to date, you will see a message stating so and you can skip the next step.\n+4. Wait for the image to be pulled and restart Docker Desktop.\n+\n+<img src=\"/assets/docker/docker_desktop/pull-latest-image.png\"/>\n+</Tip>\n+\n Access MindsDB inside Docker Desktop.\n \n <p align=\"center\">\n-  <img src=\"/assets/mindsdb_docker_desktop.png\"/>\n+  <img src=\"/assets/docker/docker_desktop/mindsdb_docker_desktop.png\"/>\n </p>\n \n ## Install dependencies\nComment: Please change `##` to `###` for these sections: `Install MindsDB`, `Install dependencies`, and `View logs`.",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/setup/self-hosted/docker-desktop.mdx",
    "pr_number": 9062,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1561048194,
    "comment_created_at": "2024-04-11T13:44:01Z"
  },
  {
    "code": "@@ -26,12 +26,12 @@\n \n ----------------------------------------\n \n-[MindsDB](https://mindsdb.com?utm_medium=community&utm_source=github&utm_campaign=mindsdb%20repo) is the platform for customizing AI from enterprise data. You can create, serve, and fine-tune models in real-time from your database, vector store, and application data.\n- [![Tweet](https://img.shields.io/twitter/url/http/shields.io.svg?style=social)](https://twitter.com/intent/tweet?text=The%platform%20for%customizing%20AI,%20from%20enterprise%20data&url=https://github.com/mindsdb/mindsdb&via=mindsdb&hashtags=ai,opensource)\n+[MindsDB](https://mindsdb.com?utm_medium=community&utm_source=github&utm_campaign=mindsdb%20repo) is the platform for building AI from enterprise data. You can create, serve, and fine-tune models in real-time from your database, vector store, and application data.\n+ [![Tweet](https://img.shields.io/twitter/url/http/shields.io.svg?style=social)](https://twitter.com/intent/tweet?text=The%platform%20for%building%20AI,%20from%20enterprise%20data&url=https://github.com/mindsdb/mindsdb&via=mindsdb&hashtags=ai,opensource)",
    "comment": "Both Twitter links don't work. It goes to `This page isn’t working`\r\nProbably because it is X now.",
    "line_number": 30,
    "enriched": "File: README.md\nCode: @@ -26,12 +26,12 @@\n \n ----------------------------------------\n \n-[MindsDB](https://mindsdb.com?utm_medium=community&utm_source=github&utm_campaign=mindsdb%20repo) is the platform for customizing AI from enterprise data. You can create, serve, and fine-tune models in real-time from your database, vector store, and application data.\n- [![Tweet](https://img.shields.io/twitter/url/http/shields.io.svg?style=social)](https://twitter.com/intent/tweet?text=The%platform%20for%customizing%20AI,%20from%20enterprise%20data&url=https://github.com/mindsdb/mindsdb&via=mindsdb&hashtags=ai,opensource)\n+[MindsDB](https://mindsdb.com?utm_medium=community&utm_source=github&utm_campaign=mindsdb%20repo) is the platform for building AI from enterprise data. You can create, serve, and fine-tune models in real-time from your database, vector store, and application data.\n+ [![Tweet](https://img.shields.io/twitter/url/http/shields.io.svg?style=social)](https://twitter.com/intent/tweet?text=The%platform%20for%building%20AI,%20from%20enterprise%20data&url=https://github.com/mindsdb/mindsdb&via=mindsdb&hashtags=ai,opensource)\nComment: Both Twitter links don't work. It goes to `This page isn’t working`\r\nProbably because it is X now.",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "README.md",
    "pr_number": 9461,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1668474904,
    "comment_created_at": "2024-07-08T11:42:31Z"
  },
  {
    "code": "@@ -33,7 +33,7 @@ If you don't already have a Twitter developer account, follow the steps in the v\n \n   [Begin here to apply for a Twitter developer account](https://developer.twitter.com/apply-for-access)\n \n-  <div style={{position: \"relative\", paddingBottom: \"59.602649006622514%\", height: 0}}><iframe src=\"https://www.youtube.com/embed/qVe7PeC0sUQ\" title=\"YouTube video player\" frameBorder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; fullscreen; gyroscope; picture-in-picture; web-share\" style={{position: \"absolute\", top: 0, left: 0, width: \"100%\", height: \"100%\"}}></iframe></div>\n+  Watch this ]step-by-step video](https://www.youtube.com/watch?v=qVe7PeC0sUQ) explaining the process.",
    "comment": "Is this correctly formated please double check `  Watch this ]step-by-step video](https://www.youtube.com/watch?v=qVe7PeC0sUQ) explaining the process.`",
    "line_number": 36,
    "enriched": "File: docs/integrations/app-integrations/twitter.mdx\nCode: @@ -33,7 +33,7 @@ If you don't already have a Twitter developer account, follow the steps in the v\n \n   [Begin here to apply for a Twitter developer account](https://developer.twitter.com/apply-for-access)\n \n-  <div style={{position: \"relative\", paddingBottom: \"59.602649006622514%\", height: 0}}><iframe src=\"https://www.youtube.com/embed/qVe7PeC0sUQ\" title=\"YouTube video player\" frameBorder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; fullscreen; gyroscope; picture-in-picture; web-share\" style={{position: \"absolute\", top: 0, left: 0, width: \"100%\", height: \"100%\"}}></iframe></div>\n+  Watch this ]step-by-step video](https://www.youtube.com/watch?v=qVe7PeC0sUQ) explaining the process.\nComment: Is this correctly formated please double check `  Watch this ]step-by-step video](https://www.youtube.com/watch?v=qVe7PeC0sUQ) explaining the process.`",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/integrations/app-integrations/twitter.mdx",
    "pr_number": 11482,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2303540773,
    "comment_created_at": "2025-08-27T10:37:10Z"
  },
  {
    "code": "@@ -0,0 +1,86 @@\n+## LlamaIndex\n+\n+LlamaIndex is a data framework for your LLM application that allows you to work with private or domain-specific data and simplifies the interface between your data source and LLM.\n+\n+Developers can use LlamaIndex to build chatbots, applications, private set-ups, and Q&A over documents and web pages. The implemented features for LlamaIndex is [Support Web Page Reader](https://gpt-index.readthedocs.io/en/latest/examples/data_connectors/WebPageDemo.html) and [Support Database Reader](https://gpt-index.readthedocs.io/en/latest/examples/data_connectors/DatabaseReaderDemo.html)\n+\n+For this use case, you will be creating a question-answering model that retrieves answers from a webpage. OpenAI will be used as the LLM and Blackrock's investment webpage will be used as the datasource.\n+\n+### How to bring LlamaIndex to MindsDB\n+\n+The `CREATE ML_ENGINE` syntax is used to create the engine for LlamaIndex.\n+\n+```sql\n+CREATE ML_ENGINE llamaindex\n+FROM llama_index\n+```\n+\n+On execution you get:\n+<img src=\"/assets/tutorials/llamaindex/1.ml_engine.png\" />\n+\n+Use the `CREATE MODEL` syntax to create a model with Llamaindex.\n+\n+```sql\n+CREATE MODEL qa_blackrock\n+FROM files\n+    (SELECT * FROM about_blackrock)\n+PREDICT Answers\n+USING \n+  engine = 'llamaindex', \n+  index_class = 'GPTVectorStoreIndex',\n+  reader = 'DFReader',\n+  source_url_link = 'https://www.blackrock.com/za/individual/about-us',\n+  input_column = 'Questions',\n+  openai_api_key = 'your_api_key';\n+  ```\n+\n+Where:\n+\n+| Expression                             | Description                                |\n+|----------------------------------------|--------------------------------------------| \n+| `qa_blackrock`                         | The name provided to the model.            |",
    "comment": "Please update description to: `The name of the model`",
    "line_number": 41,
    "enriched": "File: docs/integrations/ai-engines/llamaindex.mdx\nCode: @@ -0,0 +1,86 @@\n+## LlamaIndex\n+\n+LlamaIndex is a data framework for your LLM application that allows you to work with private or domain-specific data and simplifies the interface between your data source and LLM.\n+\n+Developers can use LlamaIndex to build chatbots, applications, private set-ups, and Q&A over documents and web pages. The implemented features for LlamaIndex is [Support Web Page Reader](https://gpt-index.readthedocs.io/en/latest/examples/data_connectors/WebPageDemo.html) and [Support Database Reader](https://gpt-index.readthedocs.io/en/latest/examples/data_connectors/DatabaseReaderDemo.html)\n+\n+For this use case, you will be creating a question-answering model that retrieves answers from a webpage. OpenAI will be used as the LLM and Blackrock's investment webpage will be used as the datasource.\n+\n+### How to bring LlamaIndex to MindsDB\n+\n+The `CREATE ML_ENGINE` syntax is used to create the engine for LlamaIndex.\n+\n+```sql\n+CREATE ML_ENGINE llamaindex\n+FROM llama_index\n+```\n+\n+On execution you get:\n+<img src=\"/assets/tutorials/llamaindex/1.ml_engine.png\" />\n+\n+Use the `CREATE MODEL` syntax to create a model with Llamaindex.\n+\n+```sql\n+CREATE MODEL qa_blackrock\n+FROM files\n+    (SELECT * FROM about_blackrock)\n+PREDICT Answers\n+USING \n+  engine = 'llamaindex', \n+  index_class = 'GPTVectorStoreIndex',\n+  reader = 'DFReader',\n+  source_url_link = 'https://www.blackrock.com/za/individual/about-us',\n+  input_column = 'Questions',\n+  openai_api_key = 'your_api_key';\n+  ```\n+\n+Where:\n+\n+| Expression                             | Description                                |\n+|----------------------------------------|--------------------------------------------| \n+| `qa_blackrock`                         | The name provided to the model.            |\nComment: Please update description to: `The name of the model`",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/integrations/ai-engines/llamaindex.mdx",
    "pr_number": 7328,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1325530885,
    "comment_created_at": "2023-09-14T07:55:09Z"
  },
  {
    "code": "@@ -0,0 +1,84 @@\n+---\n+title: LangChain Embedding\n+sidebarTitle: LangChain Embedding\n+---\n+\n+This documentation describes the integration of MindsDB with [LangChain](https://www.langchain.com/), a framework for developing applications powered by language models.\n+The integration allows for the deployment of LangChain models within MindsDB, providing the models with access to data from various data sources.\n+\n+## Prerequisites\n+\n+Before proceeding, ensure the following prerequisites are met:\n+\n+1. Install MindsDB locally via [Docker](https://docs.mindsdb.com/setup/self-hosted/docker) or [Docker Desktop](https://docs.mindsdb.com/setup/self-hosted/docker-desktop).\n+2. To use LangChain within MindsDB, install the required dependencies following [this instruction](https://docs.mindsdb.com/setup/self-hosted/docker#install-dependencies).\n+\n+## Setup\n+\n+Create an AI engine from the [LangChain Embedding handler](https://github.com/mindsdb/mindsdb/tree/main/mindsdb/integrations/handlers/langchain_embedding_handler).\n+\n+```sql\n+CREATE ML_ENGINE embedding\n+FROM langchain_embedding;\n+```",
    "comment": "@martyna-mindsdb \r\nPlease describe what is the application of the ML_ENGINE and where it will be used later.",
    "line_number": 23,
    "enriched": "File: docs/integrations/ai-engines/langchain_embedding.mdx\nCode: @@ -0,0 +1,84 @@\n+---\n+title: LangChain Embedding\n+sidebarTitle: LangChain Embedding\n+---\n+\n+This documentation describes the integration of MindsDB with [LangChain](https://www.langchain.com/), a framework for developing applications powered by language models.\n+The integration allows for the deployment of LangChain models within MindsDB, providing the models with access to data from various data sources.\n+\n+## Prerequisites\n+\n+Before proceeding, ensure the following prerequisites are met:\n+\n+1. Install MindsDB locally via [Docker](https://docs.mindsdb.com/setup/self-hosted/docker) or [Docker Desktop](https://docs.mindsdb.com/setup/self-hosted/docker-desktop).\n+2. To use LangChain within MindsDB, install the required dependencies following [this instruction](https://docs.mindsdb.com/setup/self-hosted/docker#install-dependencies).\n+\n+## Setup\n+\n+Create an AI engine from the [LangChain Embedding handler](https://github.com/mindsdb/mindsdb/tree/main/mindsdb/integrations/handlers/langchain_embedding_handler).\n+\n+```sql\n+CREATE ML_ENGINE embedding\n+FROM langchain_embedding;\n+```\nComment: @martyna-mindsdb \r\nPlease describe what is the application of the ML_ENGINE and where it will be used later.",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/integrations/ai-engines/langchain_embedding.mdx",
    "pr_number": 10649,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2033955164,
    "comment_created_at": "2025-04-08T20:16:04Z"
  },
  {
    "code": "@@ -0,0 +1,65 @@\n+# Autogluon Handler\n+\n+The Autogluon ML handler integrates Autogluon with MindsDB. You can use it to create and train machine learning models with Autogluon for your existing data. \n+\n+## Autogluon\n+\n+Autogluon is an open-source machine learning library designed to make machine learning easy and accessible. It offers automatic machine learning (AutoML) capabilities, allowing you to quickly build and train machine learning models without requiring in-depth expertise in machine learning.\n+\n+The Autogluon Python SDK can be found at [https://github.com/autogluon/autogluon](https://github.com/autogluon/autogluon).\n+\n+## Example Usage\n+\n+\n+\n+CREATE ML_ENGINE autoglucon_engine FROM autogluon;\n+\n+\n+SELECT *\n+FROM mindsdb.models\n+WHERE name='autogluconpredictor_8';\n+\n+SELECT m.quality\n+FROM files.red_wine_quality AS t\n+JOIN mindsdb.autogluconpredictor_7 AS m;\n+",
    "comment": "Please remove the above CREATE ML_ENGINE and two SELECT statements, as these are duplicated.",
    "line_number": 25,
    "enriched": "File: mindsdb/integrations/handlers/autogluon_handler/README.md\nCode: @@ -0,0 +1,65 @@\n+# Autogluon Handler\n+\n+The Autogluon ML handler integrates Autogluon with MindsDB. You can use it to create and train machine learning models with Autogluon for your existing data. \n+\n+## Autogluon\n+\n+Autogluon is an open-source machine learning library designed to make machine learning easy and accessible. It offers automatic machine learning (AutoML) capabilities, allowing you to quickly build and train machine learning models without requiring in-depth expertise in machine learning.\n+\n+The Autogluon Python SDK can be found at [https://github.com/autogluon/autogluon](https://github.com/autogluon/autogluon).\n+\n+## Example Usage\n+\n+\n+\n+CREATE ML_ENGINE autoglucon_engine FROM autogluon;\n+\n+\n+SELECT *\n+FROM mindsdb.models\n+WHERE name='autogluconpredictor_8';\n+\n+SELECT m.quality\n+FROM files.red_wine_quality AS t\n+JOIN mindsdb.autogluconpredictor_7 AS m;\n+\nComment: Please remove the above CREATE ML_ENGINE and two SELECT statements, as these are duplicated.",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "mindsdb/integrations/handlers/autogluon_handler/README.md",
    "pr_number": 7903,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1366762856,
    "comment_created_at": "2023-10-20T10:05:43Z"
  },
  {
    "code": "@@ -0,0 +1,87 @@\n+---\n+title: Cohere\n+sidebarTitle: Cohere\n+---\n+\n+In this section, we present how to bring Cohere models to MindsDB.\n+\n+[Cohere](https://cohere.com/) offers world-class generative models and industry-best retrieval capabilities, which is the key combination to unlocking Generative AI value for the enterprise.\n+\n+Read on to find out how to use Cohere models within MinsdDB. \n+\n+## Setup\n+\n+MindsDB provides the [Cohere handler](https://github.com/mindsdb/mindsdb/tree/staging/mindsdb/integrations/handlers/cohere_handler) that enables you to create Cohere models within MindsDB.\n+\n+### AI Engine\n+\n+Before creating a model, it is required to create an AI engine based on the provided handler.\n+\n+<Tip>\n+If you installed MindsDB locally, make sure to install all Cohere dependencies by running `pip install mindsdb[cohere]` or from the [requirements.txt](https://github.com/mindsdb/mindsdb/blob/staging/mindsdb/integrations/handlers/cohere_handler/requirements.txt) file.\n+</Tip>\n+\n+You can create an Cohere engine using this command:\n+\n+```sql\n+CREATE ML_ENGINE cohere_engine\n+FROM cohere\n+USING\n+    api_key = 'your-cohere-api-key';\n+```\n+\n+<Tip>\n+Please note that you need to provide your Cohere API key. Once you sign up for a Cohere account, an API key can be requested from the Cohere dashboard.\n+</Tip>\n+\n+The name of the engine (here, `cohere_engine`) should be used as a value for the `engine` parameter in the `USING` clause of the `CREATE MODEL` statement.\n+\n+### AI Model\n+\n+The [`CREATE MODEL`](/sql/create/model) statement is used to create, train, and deploy models within MindsDB.\n+\n+```sql\n+CREATE MODEL cohere_model\n+PREDICT language\n+USING\n+    task = 'language-detection',\n+    column = 'text',\n+    engine = 'cohere_engine',\n+    api_key = 'your-cohere-api-key'",
    "comment": "This `api_key` parameter is not required here (it was passed when creating an engine).",
    "line_number": 50,
    "enriched": "File: docs/integrations/ai-engines/cohere.mdx\nCode: @@ -0,0 +1,87 @@\n+---\n+title: Cohere\n+sidebarTitle: Cohere\n+---\n+\n+In this section, we present how to bring Cohere models to MindsDB.\n+\n+[Cohere](https://cohere.com/) offers world-class generative models and industry-best retrieval capabilities, which is the key combination to unlocking Generative AI value for the enterprise.\n+\n+Read on to find out how to use Cohere models within MinsdDB. \n+\n+## Setup\n+\n+MindsDB provides the [Cohere handler](https://github.com/mindsdb/mindsdb/tree/staging/mindsdb/integrations/handlers/cohere_handler) that enables you to create Cohere models within MindsDB.\n+\n+### AI Engine\n+\n+Before creating a model, it is required to create an AI engine based on the provided handler.\n+\n+<Tip>\n+If you installed MindsDB locally, make sure to install all Cohere dependencies by running `pip install mindsdb[cohere]` or from the [requirements.txt](https://github.com/mindsdb/mindsdb/blob/staging/mindsdb/integrations/handlers/cohere_handler/requirements.txt) file.\n+</Tip>\n+\n+You can create an Cohere engine using this command:\n+\n+```sql\n+CREATE ML_ENGINE cohere_engine\n+FROM cohere\n+USING\n+    api_key = 'your-cohere-api-key';\n+```\n+\n+<Tip>\n+Please note that you need to provide your Cohere API key. Once you sign up for a Cohere account, an API key can be requested from the Cohere dashboard.\n+</Tip>\n+\n+The name of the engine (here, `cohere_engine`) should be used as a value for the `engine` parameter in the `USING` clause of the `CREATE MODEL` statement.\n+\n+### AI Model\n+\n+The [`CREATE MODEL`](/sql/create/model) statement is used to create, train, and deploy models within MindsDB.\n+\n+```sql\n+CREATE MODEL cohere_model\n+PREDICT language\n+USING\n+    task = 'language-detection',\n+    column = 'text',\n+    engine = 'cohere_engine',\n+    api_key = 'your-cohere-api-key'\nComment: This `api_key` parameter is not required here (it was passed when creating an engine).",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/integrations/ai-engines/cohere.mdx",
    "pr_number": 8347,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1392594006,
    "comment_created_at": "2023-11-14T13:36:46Z"
  },
  {
    "code": "@@ -0,0 +1,40 @@\n+# This workflow will install Python dependencies, run tests and lint with a variety of Python versions\n+# For more information see: https://docs.github.com/en/actions/automating-builds-and-tests/building-and-testing-python\n+\n+name: Python package\n+\n+on:\n+  push:\n+    branches: [ \"main\" ]\n+  pull_request:\n+    branches: [ \"main\" ]\n+\n+jobs:\n+  build:\n+\n+    runs-on: ubuntu-latest\n+    strategy:\n+      fail-fast: false\n+      matrix:\n+        python-version: [\"3.9\", \"3.10\", \"3.11\"]\n+\n+    steps:\n+    - uses: actions/checkout@v4\n+    - name: Set up Python ${{ matrix.python-version }}\n+      uses: actions/setup-python@v3\n+      with:\n+        python-version: ${{ matrix.python-version }}\n+    - name: Install dependencies\n+      run: |\n+        python -m pip install --upgrade pip\n+        python -m pip install flake8 pytest\n+        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n+    - name: Lint with flake8\n+      run: |\n+        # stop the build if there are Python syntax errors or undefined names\n+        flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n+        # exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\n+        flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n+    - name: Test with pytest\n+      run: |\n+        pytest",
    "comment": "**correctness**: No significant functional bugs or runtime correctness issues are present; the workflow steps are valid and will not cause incorrect results or crashes.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nNo action required. The file .github/workflows/python-package.yml (lines 1-40) contains no significant functional correctness issues. No changes needed.\n```\n</details>\n<!-- ai_prompt_end -->\n\n",
    "line_number": 40,
    "enriched": "File: .github/workflows/python-package.yml\nCode: @@ -0,0 +1,40 @@\n+# This workflow will install Python dependencies, run tests and lint with a variety of Python versions\n+# For more information see: https://docs.github.com/en/actions/automating-builds-and-tests/building-and-testing-python\n+\n+name: Python package\n+\n+on:\n+  push:\n+    branches: [ \"main\" ]\n+  pull_request:\n+    branches: [ \"main\" ]\n+\n+jobs:\n+  build:\n+\n+    runs-on: ubuntu-latest\n+    strategy:\n+      fail-fast: false\n+      matrix:\n+        python-version: [\"3.9\", \"3.10\", \"3.11\"]\n+\n+    steps:\n+    - uses: actions/checkout@v4\n+    - name: Set up Python ${{ matrix.python-version }}\n+      uses: actions/setup-python@v3\n+      with:\n+        python-version: ${{ matrix.python-version }}\n+    - name: Install dependencies\n+      run: |\n+        python -m pip install --upgrade pip\n+        python -m pip install flake8 pytest\n+        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n+    - name: Lint with flake8\n+      run: |\n+        # stop the build if there are Python syntax errors or undefined names\n+        flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n+        # exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\n+        flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n+    - name: Test with pytest\n+      run: |\n+        pytest\nComment: **correctness**: No significant functional bugs or runtime correctness issues are present; the workflow steps are valid and will not cause incorrect results or crashes.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nNo action required. The file .github/workflows/python-package.yml (lines 1-40) contains no significant functional correctness issues. No changes needed.\n```\n</details>\n<!-- ai_prompt_end -->\n\n",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": ".github/workflows/python-package.yml",
    "pr_number": 11690,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2407822150,
    "comment_created_at": "2025-10-06T18:19:35Z"
  },
  {
    "code": "@@ -20,14 +20,16 @@ Before proceeding, ensure the following prerequisites are met:\n This handler is implemented using the `couchbase` library, the Python driver for Couchbase.\n \n The required arguments to establish a connection are as follows:\n-* `host`: the host name or IP address of the Couchbase server\n+* `connection_string`: the Connection string to specify the cluster endpoint\n * `bucket`: the bucket name to use when connecting with the Couchbase server\n * `user`: the user to authenticate with the Couchbase server\n * `password`: the password to authenticate the user with the Couchbase server\n * `scope`:  scopes are a level of data organization within a bucket. If omitted, will default to `_default`\n \n+Note: The connection string expects the couchbases:// or couchbase:// part.",
    "comment": "Let's make this,\r\nNote: The connection string expects either the couchbases:// or couchbase:// protocol.\r\n\r\nPlease make the same change in the README.",
    "line_number": 29,
    "enriched": "File: docs/integrations/data-integrations/couchbase.mdx\nCode: @@ -20,14 +20,16 @@ Before proceeding, ensure the following prerequisites are met:\n This handler is implemented using the `couchbase` library, the Python driver for Couchbase.\n \n The required arguments to establish a connection are as follows:\n-* `host`: the host name or IP address of the Couchbase server\n+* `connection_string`: the Connection string to specify the cluster endpoint\n * `bucket`: the bucket name to use when connecting with the Couchbase server\n * `user`: the user to authenticate with the Couchbase server\n * `password`: the password to authenticate the user with the Couchbase server\n * `scope`:  scopes are a level of data organization within a bucket. If omitted, will default to `_default`\n \n+Note: The connection string expects the couchbases:// or couchbase:// part.\nComment: Let's make this,\r\nNote: The connection string expects either the couchbases:// or couchbase:// protocol.\r\n\r\nPlease make the same change in the README.",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/integrations/data-integrations/couchbase.mdx",
    "pr_number": 9703,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1785634736,
    "comment_created_at": "2024-10-03T05:04:42Z"
  },
  {
    "code": "@@ -0,0 +1,89 @@\n+# Introduction\n+When it comes to AutoML, one of the most remarkable open-source products is MindsDB. MindsDB enhances the existing databases and modifies their tables to function as AI Tables that can train robust Predictor models using the available data, without the need to learn how to code such models in Python, handle their dependencies, or maintain them at a later stage. With MindsDB, you can use simple SQL-like statements and obtain accurate predictions with just a few clicks.\n+\n+In this tutorial, we will learn how to forecast the stock prices of Uber using MindsDB. The required dataset for this tutorial can be downloaded from here.",
    "comment": "Looks like you forgot to add a dataset download link here.",
    "line_number": 4,
    "enriched": "File: docs/tutorials/Forecast Uber Stock Prices using MindsDB.mdx\nCode: @@ -0,0 +1,89 @@\n+# Introduction\n+When it comes to AutoML, one of the most remarkable open-source products is MindsDB. MindsDB enhances the existing databases and modifies their tables to function as AI Tables that can train robust Predictor models using the available data, without the need to learn how to code such models in Python, handle their dependencies, or maintain them at a later stage. With MindsDB, you can use simple SQL-like statements and obtain accurate predictions with just a few clicks.\n+\n+In this tutorial, we will learn how to forecast the stock prices of Uber using MindsDB. The required dataset for this tutorial can be downloaded from here.\nComment: Looks like you forgot to add a dataset download link here.",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/tutorials/Forecast Uber Stock Prices using MindsDB.mdx",
    "pr_number": 5985,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1185034100,
    "comment_created_at": "2023-05-04T13:40:24Z"
  },
  {
    "code": "@@ -0,0 +1,153 @@\n+---\n+title: Fireworks\n+sidebarTitle: Fireworks\n+---\n+\n+This documentation describes the integration of MindsDB with [Fireworks](https://fireworks.ai/), production AI platform\n+built for developers. Fireworks partners with the world's leading generative AI researchers to serve the best models, at the fastest speeds.\n+\n+## Prerequisites\n+\n+Before proceeding, ensure the following prerequisites are met:\n+\n+1. Install MindsDB [locally via Docker](https://docs.mindsdb.com/setup/self-hosted/docker) or use [MindsDB Cloud](https://cloud.mindsdb.com/).",
    "comment": "Remove MindsDB Cloud as an option. For now keep Docker and Docker Desktop",
    "line_number": 13,
    "enriched": "File: mindsdb/integrations/handlers/fireworks_handler/README.md\nCode: @@ -0,0 +1,153 @@\n+---\n+title: Fireworks\n+sidebarTitle: Fireworks\n+---\n+\n+This documentation describes the integration of MindsDB with [Fireworks](https://fireworks.ai/), production AI platform\n+built for developers. Fireworks partners with the world's leading generative AI researchers to serve the best models, at the fastest speeds.\n+\n+## Prerequisites\n+\n+Before proceeding, ensure the following prerequisites are met:\n+\n+1. Install MindsDB [locally via Docker](https://docs.mindsdb.com/setup/self-hosted/docker) or use [MindsDB Cloud](https://cloud.mindsdb.com/).\nComment: Remove MindsDB Cloud as an option. For now keep Docker and Docker Desktop",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "mindsdb/integrations/handlers/fireworks_handler/README.md",
    "pr_number": 9131,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1579723686,
    "comment_created_at": "2024-04-25T15:37:49Z"
  },
  {
    "code": "@@ -61,9 +61,20 @@ def result(self, query, request_env, mindsdb_env, session):\n         db = query['$db']\n         collection = query['aggregate']\n \n-        ast_query = aggregate_to_ast(query, request_env.get('database', 'mindsdb'))\n+        first_step = query['pipeline'][0]\n+        if '$match' in first_step:\n+            ast_query = aggregate_to_ast(query, request_env.get('database', 'mindsdb'))\n \n-        data = run_sql_command(request_env, ast_query)\n+            data = run_sql_command(request_env, ast_query)\n+\n+        elif '$collStats' in first_step:\n+            raise NotImplementedError(\n+                \"To describe model use:\"\n+                \" db.runCommand( { collStats: 'model_name', scale: 'describe_type'})\"",
    "comment": "maybe also add a link to the doc? https://docs.mindsdb.com/sdks/mongo/models/describe",
    "line_number": 73,
    "enriched": "File: mindsdb/api/mongo/responders/aggregate.py\nCode: @@ -61,9 +61,20 @@ def result(self, query, request_env, mindsdb_env, session):\n         db = query['$db']\n         collection = query['aggregate']\n \n-        ast_query = aggregate_to_ast(query, request_env.get('database', 'mindsdb'))\n+        first_step = query['pipeline'][0]\n+        if '$match' in first_step:\n+            ast_query = aggregate_to_ast(query, request_env.get('database', 'mindsdb'))\n \n-        data = run_sql_command(request_env, ast_query)\n+            data = run_sql_command(request_env, ast_query)\n+\n+        elif '$collStats' in first_step:\n+            raise NotImplementedError(\n+                \"To describe model use:\"\n+                \" db.runCommand( { collStats: 'model_name', scale: 'describe_type'})\"\nComment: maybe also add a link to the doc? https://docs.mindsdb.com/sdks/mongo/models/describe",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "mindsdb/api/mongo/responders/aggregate.py",
    "pr_number": 9126,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1586328407,
    "comment_created_at": "2024-05-01T13:58:32Z"
  },
  {
    "code": "@@ -288,15 +288,23 @@ The `type` parameter defines the type of task queue to use.\n     - Using a Redis queue requires additional configuration such as the `host`, `port`, `db`, `username`, and `password` parameters.\n     - To use the Redis queue, start MindsDB with the following command to initiate a queue consumer process: `python3 -m mindsdb --ml_task_queue_consumer`. This process will monitor the queue and fetch tasks for execution only when sufficient resources are available.\n \n-#### `file_upload_domains`\n+#### `url_file_upload`\n \n ```bash\n-    \"file_upload_domains\": [],\n+   \"url_file_upload\": {\n+           \"enabled\": true,\n+           \"allowed_origins\": [\"https://example.com\"],\n+           \"disallowed_origins\": [\"http://example.com\"]\n+    }\n ```\n \n-The `file_upload_domains` parameter restricts file uploads to trusted sources by specifying a list of allowed domains. This ensures that users can only upload files from the defined sources, such as S3 or Google Drive (`\"file_upload_domains\": [\"https://s3.amazonaws.com\", \"https://drive.google.com\"]`).\n+The `url_file_upload` parameter restricts file uploads to trusted sources by specifying a list of allowed domains. This ensures that users can only upload files from the defined sources, such as S3 or Google Drive.\n+\n+The `enabled` flag turns this feature on (`true`) or off (`false`).\n \n-If this parameter is left empty (`[]`), users can upload files from any URL without restriction.\n+The `allowed_origins` parameter lists allowed domains.",
    "comment": "May be add to:\r\nThe `allowed_origins` parameter lists allowed domains. `Empty list means that any domain is allowed.`\r\nThe `disallowed_origins` parameter lists domains that are not allowed. `Empty list means that there are no restrictions.`",
    "line_number": 305,
    "enriched": "File: docs/setup/custom-config.mdx\nCode: @@ -288,15 +288,23 @@ The `type` parameter defines the type of task queue to use.\n     - Using a Redis queue requires additional configuration such as the `host`, `port`, `db`, `username`, and `password` parameters.\n     - To use the Redis queue, start MindsDB with the following command to initiate a queue consumer process: `python3 -m mindsdb --ml_task_queue_consumer`. This process will monitor the queue and fetch tasks for execution only when sufficient resources are available.\n \n-#### `file_upload_domains`\n+#### `url_file_upload`\n \n ```bash\n-    \"file_upload_domains\": [],\n+   \"url_file_upload\": {\n+           \"enabled\": true,\n+           \"allowed_origins\": [\"https://example.com\"],\n+           \"disallowed_origins\": [\"http://example.com\"]\n+    }\n ```\n \n-The `file_upload_domains` parameter restricts file uploads to trusted sources by specifying a list of allowed domains. This ensures that users can only upload files from the defined sources, such as S3 or Google Drive (`\"file_upload_domains\": [\"https://s3.amazonaws.com\", \"https://drive.google.com\"]`).\n+The `url_file_upload` parameter restricts file uploads to trusted sources by specifying a list of allowed domains. This ensures that users can only upload files from the defined sources, such as S3 or Google Drive.\n+\n+The `enabled` flag turns this feature on (`true`) or off (`false`).\n \n-If this parameter is left empty (`[]`), users can upload files from any URL without restriction.\n+The `allowed_origins` parameter lists allowed domains.\nComment: May be add to:\r\nThe `allowed_origins` parameter lists allowed domains. `Empty list means that any domain is allowed.`\r\nThe `disallowed_origins` parameter lists domains that are not allowed. `Empty list means that there are no restrictions.`",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/setup/custom-config.mdx",
    "pr_number": 11244,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2182849332,
    "comment_created_at": "2025-07-03T13:49:17Z"
  },
  {
    "code": "@@ -35,4 +35,5 @@ scikit-learn==1.3.2\n protobuf==3.20.3\n hierarchicalforecast~=0.4.0\n google-auth-oauthlib\n-msal\n\\ No newline at end of file\n+msal\n+langchain==0.1.5",
    "comment": "@hamishfagg could you expand a bit on what skill introduced torch as a dependency? Ref: #8711. \r\n\r\nI'm looking to bump langchain's version on a PR that would immediately follow up this one upon merging, and would like to avoid re-introducing the size increase in our docker images. I think the separation of langchain since `0.0.347` into `langchain, langchain-core, langchain-experimental, langchain-community` may help us here, but I'm not sure what the original culprit was, any info. on that would be appreciated!",
    "line_number": 39,
    "enriched": "File: requirements/requirements.txt\nCode: @@ -35,4 +35,5 @@ scikit-learn==1.3.2\n protobuf==3.20.3\n hierarchicalforecast~=0.4.0\n google-auth-oauthlib\n-msal\n\\ No newline at end of file\n+msal\n+langchain==0.1.5\nComment: @hamishfagg could you expand a bit on what skill introduced torch as a dependency? Ref: #8711. \r\n\r\nI'm looking to bump langchain's version on a PR that would immediately follow up this one upon merging, and would like to avoid re-introducing the size increase in our docker images. I think the separation of langchain since `0.0.347` into `langchain, langchain-core, langchain-experimental, langchain-community` may help us here, but I'm not sure what the original culprit was, any info. on that would be appreciated!",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "requirements/requirements.txt",
    "pr_number": 8759,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1492651081,
    "comment_created_at": "2024-02-16T15:37:38Z"
  },
  {
    "code": "@@ -13,12 +13,12 @@ This handler is implemented using `ibm_db/ibm_db_dbi`, a Python library that all\n \n The required arguments to establish a connection are as follows:\n \n-* `user` is the username associated with the database.\n-* `password` is the password required to authenticate your access to the database.\n-* `host` is the hostname or IP address of the server.\n-* `port` is the port through which TCP/IP connection is to be made.\n-* `database` is the database name to be connected.\n-* `schema` is required to get the tables.\n+- `user` is the username associated with the database.",
    "comment": "You changed `*` into `-`.\r\n\r\nPlease change it back to `*` for all six points.",
    "line_number": 16,
    "enriched": "File: docs/data-integrations/ibm-db2.mdx\nCode: @@ -13,12 +13,12 @@ This handler is implemented using `ibm_db/ibm_db_dbi`, a Python library that all\n \n The required arguments to establish a connection are as follows:\n \n-* `user` is the username associated with the database.\n-* `password` is the password required to authenticate your access to the database.\n-* `host` is the hostname or IP address of the server.\n-* `port` is the port through which TCP/IP connection is to be made.\n-* `database` is the database name to be connected.\n-* `schema` is required to get the tables.\n+- `user` is the username associated with the database.\nComment: You changed `*` into `-`.\r\n\r\nPlease change it back to `*` for all six points.",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/data-integrations/ibm-db2.mdx",
    "pr_number": 5855,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1179420772,
    "comment_created_at": "2023-04-27T16:23:40Z"
  },
  {
    "code": "@@ -0,0 +1,45 @@\n+---\n+title: Yugabyte\n+sidebarTitle: Yugabyte\n+---\n+\n+# YugabyteDB Handler\n+\n+This is the implementation of the  YugabyteDB handler for MindsDB.\n+\n+## YugabyteDB\n+YugabyteDB is a high-performance, cloud-native distributed SQL database that aims to support all PostgreSQL features. It is best to fit for cloud-native OLTP (i.e. real-time, business-critical) applications that need absolute data correctness and require at least one of the following: scalability, high tolerance to failures, or globally-distributed deployments.\n+\n+## Implementation\n+This handler was implemented using the `psycopg2`, a Python library that allows you to use Python code to run SQL commands on YugabyteDB.\n+\n+The required arguments to establish a connection are,\n+* `user`: username asscociated with database\n+* `password`: password to authenticate your access\n+* `host`: host to server IP Address or hostname\n+* `port`: port through which TCPIP connection is to be made\n+* `database`: Database name to be connected\n+\n+\n+## Usage\n+In order to make use of this handler and connect to yugabyte in MindsDB, the following syntax can be used,\n+~~~~sql",
    "comment": "Please correct the code blocks to be as below:\r\n\r\n> \\```sql\r\n> SQL GOES HERE\r\n> \\```",
    "line_number": 26,
    "enriched": "File: docs/data-integrations/yugabyte.mdx\nCode: @@ -0,0 +1,45 @@\n+---\n+title: Yugabyte\n+sidebarTitle: Yugabyte\n+---\n+\n+# YugabyteDB Handler\n+\n+This is the implementation of the  YugabyteDB handler for MindsDB.\n+\n+## YugabyteDB\n+YugabyteDB is a high-performance, cloud-native distributed SQL database that aims to support all PostgreSQL features. It is best to fit for cloud-native OLTP (i.e. real-time, business-critical) applications that need absolute data correctness and require at least one of the following: scalability, high tolerance to failures, or globally-distributed deployments.\n+\n+## Implementation\n+This handler was implemented using the `psycopg2`, a Python library that allows you to use Python code to run SQL commands on YugabyteDB.\n+\n+The required arguments to establish a connection are,\n+* `user`: username asscociated with database\n+* `password`: password to authenticate your access\n+* `host`: host to server IP Address or hostname\n+* `port`: port through which TCPIP connection is to be made\n+* `database`: Database name to be connected\n+\n+\n+## Usage\n+In order to make use of this handler and connect to yugabyte in MindsDB, the following syntax can be used,\n+~~~~sql\nComment: Please correct the code blocks to be as below:\r\n\r\n> \\```sql\r\n> SQL GOES HERE\r\n> \\```",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/data-integrations/yugabyte.mdx",
    "pr_number": 5261,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1151632783,
    "comment_created_at": "2023-03-29T09:15:10Z"
  },
  {
    "code": "@@ -0,0 +1,109 @@\n+---\n+title: MindsDB and MonkeyLearn\n+sidebarTitle: MonkeyLearn\n+---\n+\n+MonkeyLearn is a No-code text analysis tool. MindsDB allows you to use pre-built & custom MonkeyLearn models to use its features like classifying text according to user needs and fields of interest like business, reviews, comments, and customer feedback.\n+\n+## How to bring MonkeyLearn Models to MindsDB\n+\n+Before creating a model, you will need to create the ML_ENGINE for MonkeyLearn using the `CREATE ML_ENGINE` syntax\n+\n+```sql\n+Create ML_ENGINE monkeylearn\n+FROM monkeylearn\n+```\n+\n+<p align=\"center\">\n+  <img src=\"/assets/tutorials/1.create_ml.png\" />",
    "comment": "You didn't add images in this PR. Can you add all images to `/assets/tutorials/monkeylearn` folder and update `src`  everywhere in this doc?",
    "line_number": 18,
    "enriched": "File: docs/integrations/ai-engines/monkeylearn.mdx\nCode: @@ -0,0 +1,109 @@\n+---\n+title: MindsDB and MonkeyLearn\n+sidebarTitle: MonkeyLearn\n+---\n+\n+MonkeyLearn is a No-code text analysis tool. MindsDB allows you to use pre-built & custom MonkeyLearn models to use its features like classifying text according to user needs and fields of interest like business, reviews, comments, and customer feedback.\n+\n+## How to bring MonkeyLearn Models to MindsDB\n+\n+Before creating a model, you will need to create the ML_ENGINE for MonkeyLearn using the `CREATE ML_ENGINE` syntax\n+\n+```sql\n+Create ML_ENGINE monkeylearn\n+FROM monkeylearn\n+```\n+\n+<p align=\"center\">\n+  <img src=\"/assets/tutorials/1.create_ml.png\" />\nComment: You didn't add images in this PR. Can you add all images to `/assets/tutorials/monkeylearn` folder and update `src`  everywhere in this doc?",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/integrations/ai-engines/monkeylearn.mdx",
    "pr_number": 7085,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1291298211,
    "comment_created_at": "2023-08-11T12:46:53Z"
  },
  {
    "code": "@@ -0,0 +1,95 @@\n+# Gmail API Integration\n+\n+This handler integrates with the [Gmail API](https://developers.google.com/gmail/api/guides)\n+to make gmail data available to use for model training and predictions.\n+\n+## Example: Write automated emails\n+\n+To see how the Gmail handler is used, let's walk through a simple example to create a model to predict\n+\n+## Connect to the Gmail API\n+## Prequisites\n+*  You will need to have a Google account and have enabled the Gmail API.\n+*  A project in the [Google Cloud Console](https://console.cloud.google.com/) with Gmail Api enabled.\n+* A credentials file for the project. You can find more information on how to do this [here](https://developers.google.com/workspace/guides/create-credentials).\n+\n+We start by creating a database to connect to the Gmail API. In order to do this,as said before, you will need to obtain credentials:\n+\n+\n+**Optional:**  The credentials file can be stored in the gmail_handler folder in\n+the [mindsdb/integrations/gmail_handler](mindsdb/integrations/handlers/gmail_handler) directory.\n+\n+~~~~sql\n+CREATE\n+DATABASE  gmail_test\n+WITH  ENGINE = 'gmail',\n+parameters = {\n+    \"path_to_credentials_file\": \"/home/marios/PycharmProjects/mindsdb/mindsdb/integrations/handlers/gmail_handler/credentials.json\"",
    "comment": "can we make this that it supports a path to s3 signed url?",
    "line_number": 27,
    "enriched": "File: mindsdb/integrations/handlers/gmail_handler/README.md\nCode: @@ -0,0 +1,95 @@\n+# Gmail API Integration\n+\n+This handler integrates with the [Gmail API](https://developers.google.com/gmail/api/guides)\n+to make gmail data available to use for model training and predictions.\n+\n+## Example: Write automated emails\n+\n+To see how the Gmail handler is used, let's walk through a simple example to create a model to predict\n+\n+## Connect to the Gmail API\n+## Prequisites\n+*  You will need to have a Google account and have enabled the Gmail API.\n+*  A project in the [Google Cloud Console](https://console.cloud.google.com/) with Gmail Api enabled.\n+* A credentials file for the project. You can find more information on how to do this [here](https://developers.google.com/workspace/guides/create-credentials).\n+\n+We start by creating a database to connect to the Gmail API. In order to do this,as said before, you will need to obtain credentials:\n+\n+\n+**Optional:**  The credentials file can be stored in the gmail_handler folder in\n+the [mindsdb/integrations/gmail_handler](mindsdb/integrations/handlers/gmail_handler) directory.\n+\n+~~~~sql\n+CREATE\n+DATABASE  gmail_test\n+WITH  ENGINE = 'gmail',\n+parameters = {\n+    \"path_to_credentials_file\": \"/home/marios/PycharmProjects/mindsdb/mindsdb/integrations/handlers/gmail_handler/credentials.json\"\nComment: can we make this that it supports a path to s3 signed url?",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "mindsdb/integrations/handlers/gmail_handler/README.md",
    "pr_number": 5890,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1186809576,
    "comment_created_at": "2023-05-07T08:42:31Z"
  },
  {
    "code": "@@ -14,6 +14,53 @@\n logger = log.getLogger(__name__)\n \n \n+def split_table_name(table_name: str) -> List[str]:\n+    \"\"\"Split table name from llm to parst\n+\n+    Args:\n+        table_name (str): input table name\n+\n+    Returns:\n+        List[str]: parts of table identifier like ['database', 'schema', 'table']\n+\n+    Example:\n+        Input: 'aaa.bbb', Output: ['aaa', 'bbb']\n+        Input: '`aaa.bbb`', Output: ['aaa', 'bbb']\n+        Input: '`aaa.`bbb``', Output: ['aaa', 'bbb']\n+        Input: 'aaa.bbb.ccc', Output: ['aaa', 'bbb', 'ccc']\n+        Input: '`aaa.bbb.ccc`', Output: ['aaa', 'bbb', 'ccc']\n+        Input: '`aaa.`bbb.ccc``', Output: ['aaa', 'bbb.ccc']\n+        Input: 'aaa.`bbb.ccc`', Output: ['aaa', 'bbb.ccc']\n+        Input: 'aaa.`bbb.ccc`', Output: ['aaa', 'bbb.ccc']\n+        Input: '`` aaa.`bbb.ccc``  \\n`', Output: ['aaa', 'bbb.ccc']\n+    \"\"\"\n+    table_name = table_name.strip(' \"\\'\\n\\r')\n+    while table_name.startswith('`') and table_name.endswith('`'):\n+        table_name = table_name[1:-1]\n+        table_name = table_name.strip(' \"\\'\\n\\r')\n+\n+    # Тогда делим строку на элементы, учитывая кавычки внутри",
    "comment": "lets translate this comment",
    "line_number": 42,
    "enriched": "File: mindsdb/interfaces/skills/sql_agent.py\nCode: @@ -14,6 +14,53 @@\n logger = log.getLogger(__name__)\n \n \n+def split_table_name(table_name: str) -> List[str]:\n+    \"\"\"Split table name from llm to parst\n+\n+    Args:\n+        table_name (str): input table name\n+\n+    Returns:\n+        List[str]: parts of table identifier like ['database', 'schema', 'table']\n+\n+    Example:\n+        Input: 'aaa.bbb', Output: ['aaa', 'bbb']\n+        Input: '`aaa.bbb`', Output: ['aaa', 'bbb']\n+        Input: '`aaa.`bbb``', Output: ['aaa', 'bbb']\n+        Input: 'aaa.bbb.ccc', Output: ['aaa', 'bbb', 'ccc']\n+        Input: '`aaa.bbb.ccc`', Output: ['aaa', 'bbb', 'ccc']\n+        Input: '`aaa.`bbb.ccc``', Output: ['aaa', 'bbb.ccc']\n+        Input: 'aaa.`bbb.ccc`', Output: ['aaa', 'bbb.ccc']\n+        Input: 'aaa.`bbb.ccc`', Output: ['aaa', 'bbb.ccc']\n+        Input: '`` aaa.`bbb.ccc``  \\n`', Output: ['aaa', 'bbb.ccc']\n+    \"\"\"\n+    table_name = table_name.strip(' \"\\'\\n\\r')\n+    while table_name.startswith('`') and table_name.endswith('`'):\n+        table_name = table_name[1:-1]\n+        table_name = table_name.strip(' \"\\'\\n\\r')\n+\n+    # Тогда делим строку на элементы, учитывая кавычки внутри\nComment: lets translate this comment",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "mindsdb/interfaces/skills/sql_agent.py",
    "pr_number": 10394,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1926829648,
    "comment_created_at": "2025-01-23T11:31:30Z"
  },
  {
    "code": "@@ -0,0 +1,62 @@\n+---\n+title: Google Cloud SQL\n+sidebarTitle: Google Cloud SQL\n+---\n+\n+This is the implementation of the Google Cloud SQL data handler for MindsDB.\n+\n+[Cloud SQL](https://cloud.google.com/sql) is a fully-managed database service that makes it easy to set up, maintain, manage, and administer your relational PostgreSQL, MySQL, and SQL Server databases in the cloud.\n+## Implementation\n+This handler was implemented using the existing MindsDB handlers for MySQL, PostgreSQL and SQL Server.\n+\n+The required arguments to establish a connection are,\n+* `host`: the host name or IP address of the Google Cloud SQL instance.\n+* `port`: the TCP/IP port of the Google Cloud SQL instance.\n+* `user`: the username used to authenticate with the Google Cloud SQL instance.\n+* `password`: the password to authenticate the user with the Google Cloud SQL instance.\n+* `database`: the database name to use when connecting with the Google Cloud SQL instance.\n+* `db_engine`: the database engine of the Google Cloud SQL instance. This can take one of three values: 'mysql', 'postgresql' or 'mssql'.\n+\n+\n+<Tip>\n+    If you installed MindsDB locally via pip, you need to install all handler dependencies manually. To do so, go to the\n+    handler's folder (mindsdb/integrations/handlers/postgres_handler) and run this command: `pip install -r\n+    requirements.txt`.\n+</Tip>\n+\n+## Usage\n+\n+In order to make use of this handler and connect to the Google Cloud SQL instance, you need to create a datasource with the following syntax:\n+\n+~~~~sql",
    "comment": "Please update this code block to use ``` instead of ~~~~ (like the following code block).",
    "line_number": 31,
    "enriched": "File: docs/data-integrations/google-cloud-sql.mdx\nCode: @@ -0,0 +1,62 @@\n+---\n+title: Google Cloud SQL\n+sidebarTitle: Google Cloud SQL\n+---\n+\n+This is the implementation of the Google Cloud SQL data handler for MindsDB.\n+\n+[Cloud SQL](https://cloud.google.com/sql) is a fully-managed database service that makes it easy to set up, maintain, manage, and administer your relational PostgreSQL, MySQL, and SQL Server databases in the cloud.\n+## Implementation\n+This handler was implemented using the existing MindsDB handlers for MySQL, PostgreSQL and SQL Server.\n+\n+The required arguments to establish a connection are,\n+* `host`: the host name or IP address of the Google Cloud SQL instance.\n+* `port`: the TCP/IP port of the Google Cloud SQL instance.\n+* `user`: the username used to authenticate with the Google Cloud SQL instance.\n+* `password`: the password to authenticate the user with the Google Cloud SQL instance.\n+* `database`: the database name to use when connecting with the Google Cloud SQL instance.\n+* `db_engine`: the database engine of the Google Cloud SQL instance. This can take one of three values: 'mysql', 'postgresql' or 'mssql'.\n+\n+\n+<Tip>\n+    If you installed MindsDB locally via pip, you need to install all handler dependencies manually. To do so, go to the\n+    handler's folder (mindsdb/integrations/handlers/postgres_handler) and run this command: `pip install -r\n+    requirements.txt`.\n+</Tip>\n+\n+## Usage\n+\n+In order to make use of this handler and connect to the Google Cloud SQL instance, you need to create a datasource with the following syntax:\n+\n+~~~~sql\nComment: Please update this code block to use ``` instead of ~~~~ (like the following code block).",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/data-integrations/google-cloud-sql.mdx",
    "pr_number": 6909,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1272009250,
    "comment_created_at": "2023-07-24T09:43:25Z"
  },
  {
    "code": "@@ -1,33 +1,33 @@\n ---\n-title: MindsDB Inference Endpoints\n-sidebarTitle: MindsDB Inference Endpoints\n+title: MindsDB Cloud\n+sidebarTitle: MindsDB Cloud\n ---\n \n-This documentation describes the integration of MindsDB with [MindsDB Inference Endpoints](https://mindsdb-docs.hashnode.space/), a cloud service that simplifies the way developers interact with cutting-edge LLMs through a universal API.\n+This documentation describes the integration of MindsDB with [MindsDB Cloud Endpoints](https://mindsdb-docs.hashnode.space/), a cloud service that simplifies the way developers interact with cutting-edge LLMs through a universal API.\n \n ## Prerequisites\n \n Before proceeding, ensure the following prerequisites are met:\n \n-1. Install MindsDB locally via [Docker](/setup/self-hosted/docker) or [Docker Desktop](/setup/self-hosted/docker-desktop).\n-2. To use MindsDB Inference Endpoints within MindsDB, install the required dependencies following [this instruction](/setup/self-hosted/docker#install-dependencies).\n-3. Obtain the MindsDB Inference API key required to deploy and use MindsDB Inference Endpoints models within MindsDB. Follow the [instructions for obtaining the API key](https://mindsdb-docs.hashnode.space/docs/authentication).\n+1. Install MindsDB [locally via Docker](https://docs.mindsdb.com/setup/self-hosted/docker) or use [MindsDB Cloud](https://cloud.mindsdb.com/).",
    "comment": "@ZoranPandovski \r\nLet's change back the 1st point to this one (as we don't want to reference old cloud):\r\n```\r\n1. Install MindsDB locally via [Docker](/setup/self-hosted/docker) or [Docker Desktop](/setup/self-hosted/docker-desktop).\r\n```",
    "line_number": 12,
    "enriched": "File: docs/integrations/ai-engines/mindsdb_cloud.mdx\nCode: @@ -1,33 +1,33 @@\n ---\n-title: MindsDB Inference Endpoints\n-sidebarTitle: MindsDB Inference Endpoints\n+title: MindsDB Cloud\n+sidebarTitle: MindsDB Cloud\n ---\n \n-This documentation describes the integration of MindsDB with [MindsDB Inference Endpoints](https://mindsdb-docs.hashnode.space/), a cloud service that simplifies the way developers interact with cutting-edge LLMs through a universal API.\n+This documentation describes the integration of MindsDB with [MindsDB Cloud Endpoints](https://mindsdb-docs.hashnode.space/), a cloud service that simplifies the way developers interact with cutting-edge LLMs through a universal API.\n \n ## Prerequisites\n \n Before proceeding, ensure the following prerequisites are met:\n \n-1. Install MindsDB locally via [Docker](/setup/self-hosted/docker) or [Docker Desktop](/setup/self-hosted/docker-desktop).\n-2. To use MindsDB Inference Endpoints within MindsDB, install the required dependencies following [this instruction](/setup/self-hosted/docker#install-dependencies).\n-3. Obtain the MindsDB Inference API key required to deploy and use MindsDB Inference Endpoints models within MindsDB. Follow the [instructions for obtaining the API key](https://mindsdb-docs.hashnode.space/docs/authentication).\n+1. Install MindsDB [locally via Docker](https://docs.mindsdb.com/setup/self-hosted/docker) or use [MindsDB Cloud](https://cloud.mindsdb.com/).\nComment: @ZoranPandovski \r\nLet's change back the 1st point to this one (as we don't want to reference old cloud):\r\n```\r\n1. Install MindsDB locally via [Docker](/setup/self-hosted/docker) or [Docker Desktop](/setup/self-hosted/docker-desktop).\r\n```",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/integrations/ai-engines/mindsdb_cloud.mdx",
    "pr_number": 9133,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1579629429,
    "comment_created_at": "2024-04-25T14:53:10Z"
  },
  {
    "code": "@@ -0,0 +1,52 @@\n+# Confluence Handler\n+\n+Confluence handler for MindsDB provides interfaces to connect with Confluence via APIs and pull confluence data into MindsDB.\n+\n+## Confluence\n+\n+Confluence is a collaborative documentation tool,it can be used to host Wiki pages. In this handler,python client of api is used and more information about this python client can be found (here)[https://pypi.org/project/atlassian-python-api/]\n+\n+\n+## Confluence Handler Initialization\n+\n+The Confluence handler is initialized with the following parameters:\n+\n+- `url`: Confluence hosted url instance\n+- `confluence_api_token`: Confluence API key to use for authentication",
    "comment": "Can we just add a link to the docs for generating the token please?",
    "line_number": 15,
    "enriched": "File: mindsdb/integrations/handlers/confluence_handler/README.md\nCode: @@ -0,0 +1,52 @@\n+# Confluence Handler\n+\n+Confluence handler for MindsDB provides interfaces to connect with Confluence via APIs and pull confluence data into MindsDB.\n+\n+## Confluence\n+\n+Confluence is a collaborative documentation tool,it can be used to host Wiki pages. In this handler,python client of api is used and more information about this python client can be found (here)[https://pypi.org/project/atlassian-python-api/]\n+\n+\n+## Confluence Handler Initialization\n+\n+The Confluence handler is initialized with the following parameters:\n+\n+- `url`: Confluence hosted url instance\n+- `confluence_api_token`: Confluence API key to use for authentication\nComment: Can we just add a link to the docs for generating the token please?",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "mindsdb/integrations/handlers/confluence_handler/README.md",
    "pr_number": 5718,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1173844669,
    "comment_created_at": "2023-04-21T14:30:50Z"
  },
  {
    "code": "@@ -34,4 +34,60 @@ Drop a remark based on your observation.\n - [ ] Works Great 💚 (This means that all the steps were executed successfuly and the expected outputs were returned.)\n - [ ] There's a Bug 🪲 [Issue Title](URL To the Issue you created) ( This means you encountered a Bug. Please open an issue with all the relevant details with the Bug Issue Template)\n \n----\n\\ No newline at end of file\n+---\n+\n+\n+## Testing MariaDB Handler with [All Nobel Laureates 1901-Present](https://www.kaggle.com/datasets/prithusharma1/all-nobel-laureates-1901-present?resource=download)\n+\n+**1. Testing CREATE DATABASE**\n+\n+```\n+CREATE DATABASE mysql_datasource\n+WITH ENGINE = 'mariadb'\n+PARAMETERS = {\n+  \"user\": \"root\",\n+  \"port\": 11433,\n+  \"password\": \"[password]\",\n+  \"host\": \"0.tcp.ngrok.io\",\n+  \"database\": \"mysql\"\n+};\n+```\n+\n+![CREATE_DATABASE](https://github.com/wunzt/mindsdb/assets/102569472/92a5ce8f-a98d-42dd-8aab-2a35a9859abd)\n+\n+\n+**2. Testing CREATE PREDICTOR**\n+\n+```\n+CREATE MODEL mindsdb.nobel_model2\n+FROM mysql_datasource\n+(SELECT * FROM mysql_datasource.nobel_latest2)\n+PREDICT prize_share;\n+```\n+\n+![CREATE_PREDICTOR](https://github.com/wunzt/mindsdb/assets/102569472/74075bd0-1735-450f-aefd-10f1f92039d6)\n+\n+\n+**3. Testing SELECT FROM PREDICTOR**\n+\n+```\n+SELECT category\n+FROM mindsdb.nobel_model2\n+WHERE category='peace';\n+```\n+\n+(No Image Due To Error During CREATE PREDICTOR)",
    "comment": "Please find out the error message and report it in the `Results` section at the bottom. You can create a bug issue.",
    "line_number": 79,
    "enriched": "File: mindsdb/integrations/handlers/mariadb_handler/Manual_QA.md\nCode: @@ -34,4 +34,60 @@ Drop a remark based on your observation.\n - [ ] Works Great 💚 (This means that all the steps were executed successfuly and the expected outputs were returned.)\n - [ ] There's a Bug 🪲 [Issue Title](URL To the Issue you created) ( This means you encountered a Bug. Please open an issue with all the relevant details with the Bug Issue Template)\n \n----\n\\ No newline at end of file\n+---\n+\n+\n+## Testing MariaDB Handler with [All Nobel Laureates 1901-Present](https://www.kaggle.com/datasets/prithusharma1/all-nobel-laureates-1901-present?resource=download)\n+\n+**1. Testing CREATE DATABASE**\n+\n+```\n+CREATE DATABASE mysql_datasource\n+WITH ENGINE = 'mariadb'\n+PARAMETERS = {\n+  \"user\": \"root\",\n+  \"port\": 11433,\n+  \"password\": \"[password]\",\n+  \"host\": \"0.tcp.ngrok.io\",\n+  \"database\": \"mysql\"\n+};\n+```\n+\n+![CREATE_DATABASE](https://github.com/wunzt/mindsdb/assets/102569472/92a5ce8f-a98d-42dd-8aab-2a35a9859abd)\n+\n+\n+**2. Testing CREATE PREDICTOR**\n+\n+```\n+CREATE MODEL mindsdb.nobel_model2\n+FROM mysql_datasource\n+(SELECT * FROM mysql_datasource.nobel_latest2)\n+PREDICT prize_share;\n+```\n+\n+![CREATE_PREDICTOR](https://github.com/wunzt/mindsdb/assets/102569472/74075bd0-1735-450f-aefd-10f1f92039d6)\n+\n+\n+**3. Testing SELECT FROM PREDICTOR**\n+\n+```\n+SELECT category\n+FROM mindsdb.nobel_model2\n+WHERE category='peace';\n+```\n+\n+(No Image Due To Error During CREATE PREDICTOR)\nComment: Please find out the error message and report it in the `Results` section at the bottom. You can create a bug issue.",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "mindsdb/integrations/handlers/mariadb_handler/Manual_QA.md",
    "pr_number": 6531,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1234536360,
    "comment_created_at": "2023-06-19T22:54:20Z"
  },
  {
    "code": "@@ -53,10 +53,10 @@ Furthermore, you can list all existing agents, get agents by name, update agents\n \n ```python\n # list all agents\n-agents = agents.list()\n+agents = server.agents.list()\n \n # get an agent by name\n-agent = agents.get('my_agent')\n+agent = server.agents.get('my_agent')",
    "comment": "Thanks @ivancastanop \r\n\r\nPlease update also lines 66 and 69 with `server.` in front of `agents.`.\r\n\r\nAlso, please provide a short video showcasing that this syntax works.",
    "line_number": 59,
    "enriched": "File: docs/sdks/python/agents.mdx\nCode: @@ -53,10 +53,10 @@ Furthermore, you can list all existing agents, get agents by name, update agents\n \n ```python\n # list all agents\n-agents = agents.list()\n+agents = server.agents.list()\n \n # get an agent by name\n-agent = agents.get('my_agent')\n+agent = server.agents.get('my_agent')\nComment: Thanks @ivancastanop \r\n\r\nPlease update also lines 66 and 69 with `server.` in front of `agents.`.\r\n\r\nAlso, please provide a short video showcasing that this syntax works.",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/sdks/python/agents.mdx",
    "pr_number": 10379,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1930478844,
    "comment_created_at": "2025-01-27T12:50:58Z"
  },
  {
    "code": "@@ -82,6 +83,36 @@ def _get_tabs_meta(self) -> List[Dict]:\n             del tab[\"content\"]\n         return all_tabs\n \n+    def _load_tab_data(self, tab_id: int, raw_data) -> Dict:\n+        \"\"\"Load tab JSON while handling trailing garbage.\"\"\"\n+        if isinstance(raw_data, bytes):\n+            try:\n+                raw_data_str = raw_data.decode(\"utf-8\")\n+            except UnicodeDecodeError as e:\n+                raise e",
    "comment": "this try/except is useless",
    "line_number": 92,
    "enriched": "File: mindsdb/interfaces/tabs/tabs_controller.py\nCode: @@ -82,6 +83,36 @@ def _get_tabs_meta(self) -> List[Dict]:\n             del tab[\"content\"]\n         return all_tabs\n \n+    def _load_tab_data(self, tab_id: int, raw_data) -> Dict:\n+        \"\"\"Load tab JSON while handling trailing garbage.\"\"\"\n+        if isinstance(raw_data, bytes):\n+            try:\n+                raw_data_str = raw_data.decode(\"utf-8\")\n+            except UnicodeDecodeError as e:\n+                raise e\nComment: this try/except is useless",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "mindsdb/interfaces/tabs/tabs_controller.py",
    "pr_number": 11970,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2597653366,
    "comment_created_at": "2025-12-08T08:54:56Z"
  },
  {
    "code": "@@ -14,7 +14,7 @@ If you have a specific task or model in mind, please let us know in the [MindsDB\n \n ## Hugging Face Examples",
    "comment": "Please undo this change. In the `## Hugging Face Examples` section, we present `tasks supported by MindsDB and Hugging Face`.\r\n\r\nAs mentioned in the instructions, please go to the `## OpenAI Examples` section and update it there.",
    "line_number": 15,
    "enriched": "File: docs/nlp/nlp-extended-examples.mdx\nCode: @@ -14,7 +14,7 @@ If you have a specific task or model in mind, please let us know in the [MindsDB\n \n ## Hugging Face Examples\nComment: Please undo this change. In the `## Hugging Face Examples` section, we present `tasks supported by MindsDB and Hugging Face`.\r\n\r\nAs mentioned in the instructions, please go to the `## OpenAI Examples` section and update it there.",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/nlp/nlp-extended-examples.mdx",
    "pr_number": 5104,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1143148481,
    "comment_created_at": "2023-03-21T10:12:00Z"
  },
  {
    "code": "@@ -0,0 +1,82 @@\n+# Reddit Handler\n+\n+Reddit handler for MindsDB provides interfaces to connect to Reddit via APIs and pull data into MindsDB.\n+\n+---\n+\n+## Table of Contents\n+\n+- [Reddit Handler](#reddit-handler)\n+  - [Table of Contents](#table-of-contents)\n+  - [About Reddit](#about-reddit)\n+  - [Reddit Handler Implementation](#reddit-handler-implementation)\n+  - [Reddit Handler Initialization](#reddit-handler-initialization)\n+  - [Implemented Features](#implemented-features)\n+  - [TODO](#todo)\n+  - [Example Usage](#example-usage)\n+\n+---\n+\n+## About Reddit\n+\n+Reddit is a network of communities based on people's interests. It provides a platform for users to submit links, create content, and have discussions about various topics.\n+\n+## Reddit Handler Implementation\n+\n+This handler was implemented using the [PRAW (Python Reddit API Wrapper)](https://praw.readthedocs.io/en/latest/) library. PRAW is a Python package that provides a simple and easy-to-use interface to access the Reddit API.\n+\n+## Reddit Handler Initialization\n+\n+The Reddit handler is initialized with the following parameters:\n+\n+- `client_id`: a required Reddit API client ID\n+- `client_secret`: a required Reddit API client secret\n+- `user_agent`: a required user agent string to identify your application\n+\n+## How to get your Reddit credentials.\n+\n+1. Visit Reddit App Preferences (https://www.reddit.com/prefs/apps) or [https://old.reddit.com/prefs/apps/](https://old.reddit.com/prefs/apps/)\n+2. Scroll to the bottom and click \"create another app...\"\n+3. Fill out the name, description, and redirect url for your app, then click \"create app\"\n+4. Now you should be able to see the personal use script, secret, and name of your app. Store those as environment variables CLIENT_ID, CLIENT_SECRET, and USER_AGENT respecitvely.\n+\n+## Implemented Features\n+\n+- Fetch submissions from a subreddit based on sorting type and limit.\n+- (Add other implemented features here)\n+\n+## TODO\n+\n+- (List any pending features or improvements here)\n+\n+## Example Usage\n+```\n+CREATE DATABASE my_reddit\n+With \n+    ENGINE = 'reddit',\n+    PARAMETERS = {\n+     \"client_id\":\"YOUR_CLIENT_ID\",\n+     \"client_secret\":\"YOUR_CLIENT_SECRET\",\n+     \"user_agent\":\"YOUR_USER_AGENT\"\n+    };\n+```\n+\n+After setting up the Reddit Handler, you can use SQL queries to fetch data from Reddit:\n+\n+```sql\n+SELECT *\n+FROM my_red.submission",
    "comment": "This is a nitpick, but I think this should be `my_reddit` for clarity's sake?",
    "line_number": 68,
    "enriched": "File: mindsdb/integrations/handlers/reddit_handler/README.md\nCode: @@ -0,0 +1,82 @@\n+# Reddit Handler\n+\n+Reddit handler for MindsDB provides interfaces to connect to Reddit via APIs and pull data into MindsDB.\n+\n+---\n+\n+## Table of Contents\n+\n+- [Reddit Handler](#reddit-handler)\n+  - [Table of Contents](#table-of-contents)\n+  - [About Reddit](#about-reddit)\n+  - [Reddit Handler Implementation](#reddit-handler-implementation)\n+  - [Reddit Handler Initialization](#reddit-handler-initialization)\n+  - [Implemented Features](#implemented-features)\n+  - [TODO](#todo)\n+  - [Example Usage](#example-usage)\n+\n+---\n+\n+## About Reddit\n+\n+Reddit is a network of communities based on people's interests. It provides a platform for users to submit links, create content, and have discussions about various topics.\n+\n+## Reddit Handler Implementation\n+\n+This handler was implemented using the [PRAW (Python Reddit API Wrapper)](https://praw.readthedocs.io/en/latest/) library. PRAW is a Python package that provides a simple and easy-to-use interface to access the Reddit API.\n+\n+## Reddit Handler Initialization\n+\n+The Reddit handler is initialized with the following parameters:\n+\n+- `client_id`: a required Reddit API client ID\n+- `client_secret`: a required Reddit API client secret\n+- `user_agent`: a required user agent string to identify your application\n+\n+## How to get your Reddit credentials.\n+\n+1. Visit Reddit App Preferences (https://www.reddit.com/prefs/apps) or [https://old.reddit.com/prefs/apps/](https://old.reddit.com/prefs/apps/)\n+2. Scroll to the bottom and click \"create another app...\"\n+3. Fill out the name, description, and redirect url for your app, then click \"create app\"\n+4. Now you should be able to see the personal use script, secret, and name of your app. Store those as environment variables CLIENT_ID, CLIENT_SECRET, and USER_AGENT respecitvely.\n+\n+## Implemented Features\n+\n+- Fetch submissions from a subreddit based on sorting type and limit.\n+- (Add other implemented features here)\n+\n+## TODO\n+\n+- (List any pending features or improvements here)\n+\n+## Example Usage\n+```\n+CREATE DATABASE my_reddit\n+With \n+    ENGINE = 'reddit',\n+    PARAMETERS = {\n+     \"client_id\":\"YOUR_CLIENT_ID\",\n+     \"client_secret\":\"YOUR_CLIENT_SECRET\",\n+     \"user_agent\":\"YOUR_USER_AGENT\"\n+    };\n+```\n+\n+After setting up the Reddit Handler, you can use SQL queries to fetch data from Reddit:\n+\n+```sql\n+SELECT *\n+FROM my_red.submission\nComment: This is a nitpick, but I think this should be `my_reddit` for clarity's sake?",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "mindsdb/integrations/handlers/reddit_handler/README.md",
    "pr_number": 5682,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1173682753,
    "comment_created_at": "2023-04-21T11:56:09Z"
  },
  {
    "code": "@@ -0,0 +1,116 @@\n+---\n+title: PayPal\n+sidebarTitle: PayPal\n+---\n+\n+In this section, we present how to connect PayPal to MindsDB.\n+\n+[PayPal](https://www.bankrate.com/finance/credit-cards/guide-to-using-paypal/) is an online payment system that makes paying for things online and sending and receiving money safe and secure.\n+\n+Data from PayPal can be utilized within MindsDB to train models and make predictions about your transactions.\n+\n+## Connection\n+\n+This handler is implemented using [PayPal-Python-SDK](https://github.com/paypal/PayPal-Python-SDK), the Python SDK for PayPal RESTful APIs.\n+\n+The required arguments to establish a connection are as follows:\n+\n+* `mode`: The mode of the PayPal API. Can be `sandbox` or `live`.\n+* `client_id`: The client ID of the PayPal API.\n+* `client_secret`: The client secret of the PayPal API.\n+",
    "comment": "Let's add links to instructions on how to get client_id and client_secret.",
    "line_number": 21,
    "enriched": "File: docs/integrations/app-integrations/paypal.mdx\nCode: @@ -0,0 +1,116 @@\n+---\n+title: PayPal\n+sidebarTitle: PayPal\n+---\n+\n+In this section, we present how to connect PayPal to MindsDB.\n+\n+[PayPal](https://www.bankrate.com/finance/credit-cards/guide-to-using-paypal/) is an online payment system that makes paying for things online and sending and receiving money safe and secure.\n+\n+Data from PayPal can be utilized within MindsDB to train models and make predictions about your transactions.\n+\n+## Connection\n+\n+This handler is implemented using [PayPal-Python-SDK](https://github.com/paypal/PayPal-Python-SDK), the Python SDK for PayPal RESTful APIs.\n+\n+The required arguments to establish a connection are as follows:\n+\n+* `mode`: The mode of the PayPal API. Can be `sandbox` or `live`.\n+* `client_id`: The client ID of the PayPal API.\n+* `client_secret`: The client secret of the PayPal API.\n+\nComment: Let's add links to instructions on how to get client_id and client_secret.",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/integrations/app-integrations/paypal.mdx",
    "pr_number": 8397,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1403377032,
    "comment_created_at": "2023-11-23T13:15:52Z"
  },
  {
    "code": "@@ -35,34 +38,37 @@ def get_season_length(frequency):\n         \"BMS\": 12,\n         \"BQ\": 4,\n         \"BH\": 24,\n-        }\n+    }\n     new_freq = frequency.split(\"-\")[0] if \"-\" in frequency else frequency  # shortens longer frequencies like Q-DEC\n     return season_dict[new_freq] if new_freq in season_dict else 1\n \n \n-def find_best_model(frequency, horizon, df):\n-    \"\"\"Finds the best model with in-sample cross-validation.\"\"\"\n-    season_length = get_season_length(frequency)\n-    models = [model(season_length=season_length) for model in model_dict.values()]\n-    sf = StatsForecast(models, frequency)\n-    sf.cross_validation(horizon, df, fitted=True)\n-    results_df = sf.cross_validation_fitted_values()\n-    return get_best_model_from_results_df(results_df)\n+def get_insample_cv_results(model_args, df):\n+    \"\"\"Gets insample cross validation results\"\"\"\n+    season_length = get_season_length(model_args[\"frequency\"])\n+    if model_args[\"model_name\"] == \"auto\":\n+        models = [model(season_length=season_length) for model in model_dict.values()]\n+    else:\n+        models = [model_dict[model_args[\"model_name\"]](season_length=season_length)]\n \n+    sf = StatsForecast(models, model_args[\"frequency\"])\n+    sf.cross_validation(model_args[\"horizon\"], df, fitted=True)\n+    results_df = sf.cross_validation_fitted_values()\n+    return results_df.rename({\"CES\": \"AutoCES\"}, axis=1)  # Fixes a Nixtla bug",
    "comment": "If it's a reported bug, please add the bug number (e.g. #123) in the comment for tracking",
    "line_number": 57,
    "enriched": "File: mindsdb/integrations/handlers/statsforecast_handler/statsforecast_handler.py\nCode: @@ -35,34 +38,37 @@ def get_season_length(frequency):\n         \"BMS\": 12,\n         \"BQ\": 4,\n         \"BH\": 24,\n-        }\n+    }\n     new_freq = frequency.split(\"-\")[0] if \"-\" in frequency else frequency  # shortens longer frequencies like Q-DEC\n     return season_dict[new_freq] if new_freq in season_dict else 1\n \n \n-def find_best_model(frequency, horizon, df):\n-    \"\"\"Finds the best model with in-sample cross-validation.\"\"\"\n-    season_length = get_season_length(frequency)\n-    models = [model(season_length=season_length) for model in model_dict.values()]\n-    sf = StatsForecast(models, frequency)\n-    sf.cross_validation(horizon, df, fitted=True)\n-    results_df = sf.cross_validation_fitted_values()\n-    return get_best_model_from_results_df(results_df)\n+def get_insample_cv_results(model_args, df):\n+    \"\"\"Gets insample cross validation results\"\"\"\n+    season_length = get_season_length(model_args[\"frequency\"])\n+    if model_args[\"model_name\"] == \"auto\":\n+        models = [model(season_length=season_length) for model in model_dict.values()]\n+    else:\n+        models = [model_dict[model_args[\"model_name\"]](season_length=season_length)]\n \n+    sf = StatsForecast(models, model_args[\"frequency\"])\n+    sf.cross_validation(model_args[\"horizon\"], df, fitted=True)\n+    results_df = sf.cross_validation_fitted_values()\n+    return results_df.rename({\"CES\": \"AutoCES\"}, axis=1)  # Fixes a Nixtla bug\nComment: If it's a reported bug, please add the bug number (e.g. #123) in the comment for tracking",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "mindsdb/integrations/handlers/statsforecast_handler/statsforecast_handler.py",
    "pr_number": 5113,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1146984154,
    "comment_created_at": "2023-03-23T23:51:50Z"
  },
  {
    "code": "@@ -0,0 +1,65 @@\n+# Bard Handler\n+The Bard ML handler integrates the Bard library from Google's Generative AI with MindsDB. You can use it to generate text completions with the Gemini Pro model for your existing text data.\n+\n+## Google Generative AI (Bard)\n+Google Generative AI is a library that provides access to powerful language models for text generation. The Bard library, used in this handler, offers various models for generating text. More information about the Google Generative AI library can be found [here](https://github.com/GoogleCloudPlatform/generative-ai).",
    "comment": "I don't see a Bard library in the link. Furthermore, if the underlying integration is to Gemini models I would request that you please rename this integration to \"google_gemini_handler\" (don't worry about the original issue, we can modify it afterwards).",
    "line_number": 5,
    "enriched": "File: mindsdb/integrations/handlers/bard_handler/README.md\nCode: @@ -0,0 +1,65 @@\n+# Bard Handler\n+The Bard ML handler integrates the Bard library from Google's Generative AI with MindsDB. You can use it to generate text completions with the Gemini Pro model for your existing text data.\n+\n+## Google Generative AI (Bard)\n+Google Generative AI is a library that provides access to powerful language models for text generation. The Bard library, used in this handler, offers various models for generating text. More information about the Google Generative AI library can be found [here](https://github.com/GoogleCloudPlatform/generative-ai).\nComment: I don't see a Bard library in the link. Furthermore, if the underlying integration is to Gemini models I would request that you please rename this integration to \"google_gemini_handler\" (don't worry about the original issue, we can modify it afterwards).",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "mindsdb/integrations/handlers/bard_handler/README.md",
    "pr_number": 8512,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1432550363,
    "comment_created_at": "2023-12-20T10:41:42Z"
  },
  {
    "code": "@@ -0,0 +1,189 @@\n+---\n+title: Sentiment Ananlysis using OpenAI GPT-3\n+sidebarTitle: Analysing the sentiments of Donald Trump's tweets using MindsDB and OpenAI GPT-3\n+---\n+\n+# Introduction\n+\n+Sentiment analysis is the process of determining the emotional tone of a piece of text. In this tutorial, we will be using MindsDB Cloud Editor and OpenAI GPT-3 to perform sentiment analysis on Donald Trump's tweets. We will first set up the data and create an OpenAI GPT-3 model, then we will make sentiment predictions on individual tweets and batches of tweets.\n+\n+# Data Setup\n+\n+There are various ways to connect our data to the MindsDB cloud instance. But in this tutorial, we will use a CSV file to do the same. We can find a CSV file containing the tweets of Donald Trump either from [Kaggle](https://www.kaggle.com/) or from the [Trump Twitter Archive](https://www.thetrumparchive.com/).\n+\n+### **Connecting the data**\n+\n+We are going to use this [Donald Trump Tweets](https://www.kaggle.com/datasets/codebreaker619/donald-trump-tweets-dataset?select=tweets.csv) Dataset from Kaggle. We will follow the steps below to create our table from the above CSV file.\n+\n+* Click on the `Add` button on the MindsDB Cloud Editor and then select `Upload File` from the dropdown that appears.\n+    \n+    ![Add Data](https://cdn.hashnode.com/res/hashnode/image/upload/v1680779540100/e73369fb-6c57-45aa-8d21-676d01bb16da.png)",
    "comment": "Please upload all the images into the `docs/tutorials/` folder and refer to them (instead of links). Thanks.",
    "line_number": 20,
    "enriched": "File: docs/tutorials/Sentiment_Ananlysis_using_OpenAI_GPT-3.mdx\nCode: @@ -0,0 +1,189 @@\n+---\n+title: Sentiment Ananlysis using OpenAI GPT-3\n+sidebarTitle: Analysing the sentiments of Donald Trump's tweets using MindsDB and OpenAI GPT-3\n+---\n+\n+# Introduction\n+\n+Sentiment analysis is the process of determining the emotional tone of a piece of text. In this tutorial, we will be using MindsDB Cloud Editor and OpenAI GPT-3 to perform sentiment analysis on Donald Trump's tweets. We will first set up the data and create an OpenAI GPT-3 model, then we will make sentiment predictions on individual tweets and batches of tweets.\n+\n+# Data Setup\n+\n+There are various ways to connect our data to the MindsDB cloud instance. But in this tutorial, we will use a CSV file to do the same. We can find a CSV file containing the tweets of Donald Trump either from [Kaggle](https://www.kaggle.com/) or from the [Trump Twitter Archive](https://www.thetrumparchive.com/).\n+\n+### **Connecting the data**\n+\n+We are going to use this [Donald Trump Tweets](https://www.kaggle.com/datasets/codebreaker619/donald-trump-tweets-dataset?select=tweets.csv) Dataset from Kaggle. We will follow the steps below to create our table from the above CSV file.\n+\n+* Click on the `Add` button on the MindsDB Cloud Editor and then select `Upload File` from the dropdown that appears.\n+    \n+    ![Add Data](https://cdn.hashnode.com/res/hashnode/image/upload/v1680779540100/e73369fb-6c57-45aa-8d21-676d01bb16da.png)\nComment: Please upload all the images into the `docs/tutorials/` folder and refer to them (instead of links). Thanks.",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/tutorials/Sentiment_Ananlysis_using_OpenAI_GPT-3.mdx",
    "pr_number": 5532,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1168391685,
    "comment_created_at": "2023-04-17T09:01:15Z"
  },
  {
    "code": "@@ -0,0 +1,57 @@\n+# Google Docs API Integration\n+This handler integrates with the Google Docs API to make the docs content available for the use case of NLP model training and predictions.",
    "comment": "Maybe keep this simple as: `This handler integrates with the Google Docs API to make the docs content available.`",
    "line_number": 2,
    "enriched": "File: mindsdb/integrations/handlers/google_docs_handler/README.md\nCode: @@ -0,0 +1,57 @@\n+# Google Docs API Integration\n+This handler integrates with the Google Docs API to make the docs content available for the use case of NLP model training and predictions.\nComment: Maybe keep this simple as: `This handler integrates with the Google Docs API to make the docs content available.`",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "mindsdb/integrations/handlers/google_docs_handler/README.md",
    "pr_number": 7012,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1294722400,
    "comment_created_at": "2023-08-15T15:05:47Z"
  },
  {
    "code": "@@ -0,0 +1,37 @@\n+# FootBall API Handler  \n+  \n+This handler integrates with the [API Football](https://www.api-football.com/) to retrieve player information.  \n+  \n+  \n+## Connect to the Football API  \n+We start by creating a database to connect to the Football API. You'll need an access token which can be accessed from [RAPIDAPI](https://api-football-v1.p.rapidapi.com/v3/) or  [API-SPORTS](https://v3.football.api-sports.io/) and the domain you want to send API requests to.  \n+  \n+Example  \n+```  ",
    "comment": "Would you add the `sql` code-block highlighter?\r\n\r\n```\r\n```sql\r\n...\r\n```",
    "line_number": 10,
    "enriched": "File: mindsdb/integrations/handlers/footballApi_handler/README.md\nCode: @@ -0,0 +1,37 @@\n+# FootBall API Handler  \n+  \n+This handler integrates with the [API Football](https://www.api-football.com/) to retrieve player information.  \n+  \n+  \n+## Connect to the Football API  \n+We start by creating a database to connect to the Football API. You'll need an access token which can be accessed from [RAPIDAPI](https://api-football-v1.p.rapidapi.com/v3/) or  [API-SPORTS](https://v3.football.api-sports.io/) and the domain you want to send API requests to.  \n+  \n+Example  \n+```  \nComment: Would you add the `sql` code-block highlighter?\r\n\r\n```\r\n```sql\r\n...\r\n```",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "mindsdb/integrations/handlers/footballApi_handler/README.md",
    "pr_number": 7293,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1323495708,
    "comment_created_at": "2023-09-12T19:55:34Z"
  },
  {
    "code": "@@ -5,8 +5,10 @@\n ## Testing Bigquery Handler with [Dataset Name](URL to the Dataset)\n \n **1. Testing CREATE DATABASE**\n-\n+![Capture1](https://github.com/mindsdb/mindsdb/assets/70659811/68352266-1de5-4a31-81bc-f206f26fe84f)",
    "comment": "This error is expected. The error message clearly states that you need to provide either `service_account_keys` or `service_account_json`. For details, see here: https://docs.mindsdb.com/data-integrations/google-bigquery",
    "line_number": 8,
    "enriched": "File: mindsdb/integrations/handlers/bigquery_handler/Manual_QA.md\nCode: @@ -5,8 +5,10 @@\n ## Testing Bigquery Handler with [Dataset Name](URL to the Dataset)\n \n **1. Testing CREATE DATABASE**\n-\n+![Capture1](https://github.com/mindsdb/mindsdb/assets/70659811/68352266-1de5-4a31-81bc-f206f26fe84f)\nComment: This error is expected. The error message clearly states that you need to provide either `service_account_keys` or `service_account_json`. For details, see here: https://docs.mindsdb.com/data-integrations/google-bigquery",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "mindsdb/integrations/handlers/bigquery_handler/Manual_QA.md",
    "pr_number": 6985,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1279112472,
    "comment_created_at": "2023-07-31T10:38:31Z"
  },
  {
    "code": "@@ -0,0 +1,39 @@\n+---\n+title: Upload Parquet files to MindsDB\n+sidebarTitle: Parquet\n+---\n+\n+You can upload Parquet files of up to 500MB in size to MindsDB.",
    "comment": "Maybe we should mention that the 500MB is a limit in our SQL Editor? Same for all other file types",
    "line_number": 6,
    "enriched": "File: docs/integrations/files/parquet.mdx\nCode: @@ -0,0 +1,39 @@\n+---\n+title: Upload Parquet files to MindsDB\n+sidebarTitle: Parquet\n+---\n+\n+You can upload Parquet files of up to 500MB in size to MindsDB.\nComment: Maybe we should mention that the 500MB is a limit in our SQL Editor? Same for all other file types",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/integrations/files/parquet.mdx",
    "pr_number": 10256,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1878041764,
    "comment_created_at": "2024-12-10T12:51:45Z"
  },
  {
    "code": "@@ -38,6 +38,45 @@ The handler has a number of default parameters set, the user only needs to pass\n \n The other parameters have default values\n \n+Supported query parameters for USING syntax are as follows:\n+\n+[For the Writer LLM API]\n+\n+- `prompt_template` - this is the template that is used to generate the prompt\n+- `writer_api_key` - this is the API key that is used to authenticate with the Writer LLM API\n+- `writer_org_id` - this is the organization ID that is used to authenticate with the Writer LLM API\n+- `base_url` - this is the base URL that is used to authenticate with the Writer LLM API, optional, if not provided uses org_id and model_id\n+- `model_id` - this is the model ID that is used to authenticate with the Writer LLM API\n+- `max_tokens` - this is the maximum number of tokens that are used to generate the output, the default is 1024",
    "comment": "nit: would be great to clarify if this includes the # of input tokens",
    "line_number": 50,
    "enriched": "File: mindsdb/integrations/handlers/writer_handler/README.md\nCode: @@ -38,6 +38,45 @@ The handler has a number of default parameters set, the user only needs to pass\n \n The other parameters have default values\n \n+Supported query parameters for USING syntax are as follows:\n+\n+[For the Writer LLM API]\n+\n+- `prompt_template` - this is the template that is used to generate the prompt\n+- `writer_api_key` - this is the API key that is used to authenticate with the Writer LLM API\n+- `writer_org_id` - this is the organization ID that is used to authenticate with the Writer LLM API\n+- `base_url` - this is the base URL that is used to authenticate with the Writer LLM API, optional, if not provided uses org_id and model_id\n+- `model_id` - this is the model ID that is used to authenticate with the Writer LLM API\n+- `max_tokens` - this is the maximum number of tokens that are used to generate the output, the default is 1024\nComment: nit: would be great to clarify if this includes the # of input tokens",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "mindsdb/integrations/handlers/writer_handler/README.md",
    "pr_number": 7848,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1362312716,
    "comment_created_at": "2023-10-17T15:21:13Z"
  },
  {
    "code": "@@ -13,10 +13,10 @@ This handler is implemented using the `elasticsearch` library, the Python Elasti\n \n The required arguments to establish a connection are as follows:\n \n-* `hosts` is the host name(s) or IP address(es) of the Elasticsearch server(s). If multiple host name(s) or IP address(es) exist, they should be separated by commas. This parameter is optional, but it should be provided if `cloud_id` is not.\n-* `cloud_id` is the unique ID to your hosted Elasticsearch cluster on Elasticsearch Service. This parameter is optional, but it should be provided if `hosts` is not.\n-* `username` is the username used to authenticate with the Elasticsearch server. This parameter is optional.\n-* `password` is the password used to authenticate the user with the Elasticsearch server. This parameter is optional.\n+- `hosts` is the host name(s) or IP address(es) of the Elasticsearch server(s). If multiple host name(s) or IP address(es) exist, they should be separated by commas. This parameter is optional, but it should be provided if `cloud_id` is not.",
    "comment": "You replaced `*` with `-`.\r\nPlease change it back to `*`. Thanks.",
    "line_number": 16,
    "enriched": "File: docs/data-integrations/elasticsearch.mdx\nCode: @@ -13,10 +13,10 @@ This handler is implemented using the `elasticsearch` library, the Python Elasti\n \n The required arguments to establish a connection are as follows:\n \n-* `hosts` is the host name(s) or IP address(es) of the Elasticsearch server(s). If multiple host name(s) or IP address(es) exist, they should be separated by commas. This parameter is optional, but it should be provided if `cloud_id` is not.\n-* `cloud_id` is the unique ID to your hosted Elasticsearch cluster on Elasticsearch Service. This parameter is optional, but it should be provided if `hosts` is not.\n-* `username` is the username used to authenticate with the Elasticsearch server. This parameter is optional.\n-* `password` is the password used to authenticate the user with the Elasticsearch server. This parameter is optional.\n+- `hosts` is the host name(s) or IP address(es) of the Elasticsearch server(s). If multiple host name(s) or IP address(es) exist, they should be separated by commas. This parameter is optional, but it should be provided if `cloud_id` is not.\nComment: You replaced `*` with `-`.\r\nPlease change it back to `*`. Thanks.",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/data-integrations/elasticsearch.mdx",
    "pr_number": 5828,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1178301229,
    "comment_created_at": "2023-04-26T19:23:14Z"
  },
  {
    "code": "@@ -0,0 +1,335 @@\n+\"\"\"\n+Pinecone Handler",
    "comment": "Please remove this comment",
    "line_number": 2,
    "enriched": "File: mindsdb/integrations/handlers/pinecone_handler/pinecone_handler.py\nCode: @@ -0,0 +1,335 @@\n+\"\"\"\n+Pinecone Handler\nComment: Please remove this comment",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "mindsdb/integrations/handlers/pinecone_handler/pinecone_handler.py",
    "pr_number": 7556,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1347497753,
    "comment_created_at": "2023-10-05T14:15:53Z"
  },
  {
    "code": "@@ -0,0 +1,29 @@\n+# Test the Microsoft Access data integration\n+",
    "comment": "You don't need to add this file to the handler",
    "line_number": 2,
    "enriched": "File: mindsdb/integrations/handlers/access_handler/Microsoft_Access.md\nCode: @@ -0,0 +1,29 @@\n+# Test the Microsoft Access data integration\n+\nComment: You don't need to add this file to the handler",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "mindsdb/integrations/handlers/access_handler/Microsoft_Access.md",
    "pr_number": 7994,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1368435337,
    "comment_created_at": "2023-10-23T10:12:51Z"
  },
  {
    "code": "@@ -3,189 +3,101 @@ title: Microsoft Teams\n sidebarTitle: Microsoft Teams\n ---\n \n-In this section, we present how to connect Microsoft Teams to MindsDB.\n-\n-[Microsoft Teams](https://www.microsoft.com/en/microsoft-teams/group-chat-software/) is the ultimate messaging app for your organization—a workspace for real-time collaboration and communication, meetings, file and app sharing, and even the occasional emoji! All in one place, all in the open, all accessible to everyone.\n-\n-The Microsoft Teams handler allows you to read and send messages to a Microsoft Teams channel or chat. It also allows you to create a chatbot that will respond to messages.\n+This documentation describes the integration of MindsDB with [Microsoft Teams](https://www.microsoft.com/en-us/microsoft-teams/group-chat-software), the ultimate messaging app for your organization.\n+The integration allows MindsDB to create chat-bots enhanced with AI capabilities that can respond to messages in Microsoft Teams.\n \n ## Prerequisites\n \n Before proceeding, ensure the following prerequisites are met:\n \n 1. Install MindsDB locally via [Docker](/setup/self-hosted/docker) or [Docker Desktop](/setup/self-hosted/docker-desktop).\n 2. To connect Microsoft Teams to MindsDB, install the required dependencies following [this instruction](/setup/self-hosted/docker#install-dependencies).\n-3. Install or ensure access to Microsoft Teams.\n \n ## Connection\n \n-This handler was implemented using [msal](https://github.com/AzureAD/microsoft-authentication-library-for-python) for authentication and [Requests](https://github.com/psf/requests) to submit requests to the Microsoft Graph API.\n-\n-The Microsoft Teams handler is initialized with the following parameters:\n-\n-- `client_id`: The client ID of the registered Microsoft Entra ID application.\n-- `client_secret`: The client secret of the registered Microsoft Entra ID application.\n-- `tenant_id`: The tenant ID of the registered Microsoft Entra ID application.\n-\n-<Note>\n-Microsoft Entra ID was previously known as Azure Active Directory (Azure AD).\n-</Note>\n-\n-The parameters given above can be obtained by registering an application in Entra ID by following these steps,\n-1. Go to the [Azure Portal](https://portal.azure.com/#home) and sign in with your Microsoft account.\n-2. Locate the **Microsoft Entra ID** service and click on it.\n-3. Click on **App registrations** and then click on **New registration**.\n-4. Enter a name for your application and select the **Accounts in this organizational directory only** option for the **Supported account types** field.\n-5. Keep the **Redirect URI** field empty and click on **Register**.\n-6. Click on **API permissions** and then click on **Add a permission**.\n-7. Select **Microsoft Graph** and then click on **Delegated permissions**.\n-8. Select the following permissions based on the data you want to access and the operations you want to perform:\n-    - Chats:\n-        - Chat.ReadBasic\n-        - Chat.Read\n-        - Chat.ReadWrite\n-\n-    - Chat messages:\n-        - Chat.Read\n-        -\tChat.ReadWrite\n-        - Group.ReadWrite.All\n-  \n-    - Channels:\n-        - ChannelSettings.Read.All\n-        - ChannelSettings.ReadWrite.All\n-        - Directory.Read.All\n-        - Directory.ReadWrite.All\n-        - Group.Read.All\n-        - Group.ReadWrite.All\n-\n-    - Channel messages:\n-        - ChannelMessage.Read.All\n-        - Group.Read.All\n-        - Group.ReadWrite.All\n-9. Click on **Add permissions**.\n-10. Copy the **Application (client) ID** and record it as the `client_id` parameter, and copy the **Directory (tenant) ID** and record it as the `tenant_id` parameter.\n-11. Click on **Certificates & secrets** and then click on **New client secret**.\n-12. Enter a description for your client secret and select an expiration period.\n-13. Click on **Add** and copy the generated client secret and record it as the `client_secret` parameter.\n-14. Click on **Authentication** and then click on **Add a platform**.\n-15. Select **Web** and enter URL where MindsDB has been deployed followed by `/verify-auth` in the **Redirect URIs** field. For example, if you are running MindsDB locally (on `http://localhost:47334`), enter `http://localhost:47334/verify-auth` in the **Redirect URIs** field.\n-\n-<Tip>\n-You can find more information about creating app registrations [here](https://docs.microsoft.com/en-us/graph/auth-register-app-v2).\n-</Tip>\n-\n-To connect to Microsoft Teams using MindsDB, the following CREATE DATABASE statement can be used:\n+Establish a connection to Microsoft Teams from MindsDB by executing the following SQL command and providing its [handler name](https://github.com/mindsdb/mindsdb/tree/main/mindsdb/integrations/handlers/ms_teams_handler) as an engine.\n \n ```sql\n-CREATE DATABASE teams_datasource\n-WITH ENGINE = 'teams',\n+CREATE DATABASE teams_conn\n+WITH ENGINE = 'teams', \n PARAMETERS = {\n-  \"client_id\": \"your-client-id\",\n-  \"client_secret\": \"your-client-secret\",\n-  \"tenant_id\": \"your-tenant-id\"\n+  \"client_id\": \"12345678-90ab-cdef-1234-567890abcdef\",\n+  \"client_secret\": \"a1b2c3d4e5f6g7h8i9j0k1l2m3n4o5p6\"\n };\n ```\n \n-When the above is statement is executed with the given parameters, the handler will open a browser window and prompt you to sign in with your Microsoft account. \n-\n-The handler will then act (via the app registration) as the signed in user and will submit requests to the Microsoft Graph API. This is done using the concept of [delegated permissions](https://docs.microsoft.com/en-us/graph/auth/auth-concepts#delegated-permissions).\n-\n-## Usage\n-\n-Now, you can post a message to a chat:\n-\n-```sql\n-INSERT INTO teams_datasource.chat_messages (chatId, body_content)\n-VALUES\n-('your-chat-id', 'Hello from MindsDB!');\n-```\n-\n-You can also do the same for channels:\n-\n-```sql\n-SELECT * FROM teams_datasource.channels\n-```\n-\n-```sql\n-INSERT INTO teams_datasource.channel_messages (channelIdentity_teamId, channelIdentity_channelId, body_content)\n-VALUES\n-('your-team-id', 'your-channel-id', 'Hello from MindsDB!');\n-```\n-\n-## Supported Tables\n-\n-The following tables are supported by the Microsoft Teams handler:\n-- `chats`: chats that the signed in user is a member of.\n-- `chat_messages`: messages sent to the signed in user's chats.\n-- `channels`: channels that the signed in user is a member of.\n-- `channel_messages`: messages sent to the signed in user's channels.\n-\n-## Chatbot\n-\n-While the Microsoft Teams handler allows you to read/send messages to a chat or channel as shown above, it is also possible to create a chat bot that will listen to messages sent to a chat or channel and respond to them.\n-\n-### Step 1: Create a Microsoft Teams Data Source\n+Required connection parameters include the following:\n \n-As shown above, create a database with the new `teams` engine by passing in the required parameters:\n-\n-```sql\n-CREATE DATABASE teams_datasource\n-WITH ENGINE = 'teams',\n-PARAMETERS = {\n-  \"client_id\": \"your-client-id\",\n-  \"client_secret\": \"your-client-secret\",\n-  \"tenant_id\": \"your-tenant-id\"\n-};\n-```\n+* `client_id`: The client ID of the registered Microsoft Entra ID application.\n+* `client_secret`: The client secret of the registered Microsoft Entra ID application.\n \n <Note>\n-The chat bot will assume the identity of the user who signed in with their Microsoft account when the database was created.\n+Microsoft Entra ID was previously known as Azure Active Directory (Azure AD).\n </Note>\n \n-### Step 2: Create an [Agent](https://docs.mindsdb.com/agents/agent)\n-\n-An agent is created by combining a conversational model, like [LlamaIndex](https://docs.mindsdb.com/integrations/ai-engines/llamaindex), with a set of Skills.\n-\n-Here, we will create an agent without any skills, however, it is possilbe to create agents with skills such as the ability to answer questions from a knowledge base.\n-\n-#### Step 2.1: Create a Conversational Model\n-\n-Create a conversational model using either the [`LangChain`](https://docs.mindsdb.com/integrations/ai-engines/langchain) or [`LlamaIndex`](https://docs.mindsdb.com/integrations/ai-engines/llamaindex) integrations. Given below is an example of a conversational model created using the `LlamaIndex` integration:\n-\n-```sql\n-CREATE ML_ENGINE llama_index_engine\n-FROM llama_index\n-USING openai_api_key='your-openai-api-key';\n-```\n-\n-```sql\n-CREATE MODEL llama_index_convo_model\n-PREDICT answer\n-USING\n-  engine = 'llama_index_engine',\n-  mode = 'conversational',\n-  prompt = 'answer users questions as a helpful assistant',\n-  user_column = 'question',\n-  assistant_column = 'answer';\n-```\n-\n-#### Step 2.2: Create an Agent using the Conversational Model\n-\n-Let's create an agent using the conversational model created above:\n-\n-```sql\n-CREATE AGENT convo_agent\n-USING\n-    model = 'llama_index_convo_model',\n-```\n-\n-### Step 3: Create a [Chatbot](https://docs.mindsdb.com/agents/chatbot)\n-\n-Finally, create a chatbot using the agent and the Microsoft Teams data source created above:\n-\n-```sql\n-CREATE CHATBOT teams_chatbot\n-USING\n-    database = 'teams_datasource',\n-    agent = 'convo_agent';\n-```\n+Follow the instructions given below to set up the Microsoft Teams app that will act as the chatbot:\n+\n+  1. Follow [this link](https://dev.botframework.com/bots/new) to the Microsoft Bot Framework portal and sign in with your Microsoft account.\n+  2. Fill out the *Display name*, *Bot handle*, and, optionally, the *Long description*, but leave the *Messaging endpoint* field empty for now.\n+  3. Set the *App type* to be 'Multi Tenant' and click on *Create Microsoft App ID and password*. This will open a new tab with the Azure portal.\n+  4. Click on *New registration* and fill out the *Name* and select the 'Accounts in any organizational directory (Any Azure AD directory - Multitenant)' option under *Supported account types*, and click on *Register*. Record the *Application (client) ID* for later use.\n+  5. Click on *Certificates & secrets* under *Manage*.\n+  6. Click on *New client secret* and fill out the *Description* and select an appropriate *Expires* period, and click on *Add*.\n+  7. Copy the client secret and save it in a secure location.\n+  <Tip>\n+  If you already have an existing app registration, you can use it instead of creating a new one and skip steps 4-6.\n+  </Tip>\n+\n+  8. Open the MindsDB Editor and create a connection to Microsoft Teams using the client ID and client secret obtained in the previous steps using the SQL command provided above.\n+  9. Using this connection, create a chatbot using the [`CREATE CHATBOT`](/agents/chatbot) syntax.\n+  10. Run the `SHOW CHATBOTS` command and record the `webhook_token` of the chatbot you created.\n+  11. Navigate back to the Microsoft Bot Framework portal and fill out the messaging endpoint in the following format: `<mindsdb_url>/api/webhooks/chatbots/<webhook_token>`.\n+  <Tip>\n+  The `<mindsdb_url>` is the URL of where MindsDB is running.\n+  Please note that if you are running MindsDB locally, it will need to be exposed to the internet using a service like [ngrok](https://ngrok.com/).\n+  </Tip>\n+\n+  12. Fill out the *Microsoft App ID* using the client ID obtained in the previous steps, agree to the terms, and click on *Register*.\n+  13. Under *Add a featured channel*, click on *Microsoft Teams*, select your Microsoft Teams solution, click on *Save* and agree to the terms when prompted.\n+  14. Navigate to Microsoft Teams and then to the *Apps* tab.\n+  15. Search for the *Developer Portal* app and add it to your workspace.\n+  16. Open the *Developer Portal*, click on *Apps* and then on *New app*.\n+  17. Fill out the *Name* for the app and click on *Add*.\n+  18. Navigate to *Basic information* and fill out the required fields: *Short description*, *Long description*, *Developer or company name*, *Website*, *Privacy policy*, *Terms of use*, and *Application (client) ID*. You may also provide any additional information you wish.\n+  <Tip>\n+  Please note that the above fields are required to be filled out for the bot to be usable in Microsoft Teams. The URLs provided in the *Website*, *Privacy policy*, and *Terms of use* fields must be valid URLs.\n+  </Tip>\n+\n+  19. Navigate to *App features* and select *Bot*.\n+  20. Choose the *Select an existing bot* option and select the bot you created earlier in the Microsoft Bot Framework portal.\n+  21. Select all of the scopes where bot is required to be used and click on *Save*.\n+  22. Finally, on the navigation pane, click on *Publish* and then on *Publish to your org*.\n+\n+For the bot to be made available to the users in your organization, you will need to ask your IT administrator to approve the submission at this [link](https://admin.teams.microsoft.com/policies/manage-apps). \n+\n+Once it is approved, users can find it under the *Built for your org* section in the *Apps* tab. They can either chat with the bot directly or add it to a channel. To chat with the bot in a channel, type `@<bot_name>` followed by your message.\n+\n+While waiting for approval, you can test the chatbot by clicking on *Preview in Teams* in the Developer Portal.\n+",
    "comment": "Let's also add the `Usage` section to show how to query tables.",
    "line_number": 81,
    "enriched": "File: docs/integrations/app-integrations/microsoft-teams.mdx\nCode: @@ -3,189 +3,101 @@ title: Microsoft Teams\n sidebarTitle: Microsoft Teams\n ---\n \n-In this section, we present how to connect Microsoft Teams to MindsDB.\n-\n-[Microsoft Teams](https://www.microsoft.com/en/microsoft-teams/group-chat-software/) is the ultimate messaging app for your organization—a workspace for real-time collaboration and communication, meetings, file and app sharing, and even the occasional emoji! All in one place, all in the open, all accessible to everyone.\n-\n-The Microsoft Teams handler allows you to read and send messages to a Microsoft Teams channel or chat. It also allows you to create a chatbot that will respond to messages.\n+This documentation describes the integration of MindsDB with [Microsoft Teams](https://www.microsoft.com/en-us/microsoft-teams/group-chat-software), the ultimate messaging app for your organization.\n+The integration allows MindsDB to create chat-bots enhanced with AI capabilities that can respond to messages in Microsoft Teams.\n \n ## Prerequisites\n \n Before proceeding, ensure the following prerequisites are met:\n \n 1. Install MindsDB locally via [Docker](/setup/self-hosted/docker) or [Docker Desktop](/setup/self-hosted/docker-desktop).\n 2. To connect Microsoft Teams to MindsDB, install the required dependencies following [this instruction](/setup/self-hosted/docker#install-dependencies).\n-3. Install or ensure access to Microsoft Teams.\n \n ## Connection\n \n-This handler was implemented using [msal](https://github.com/AzureAD/microsoft-authentication-library-for-python) for authentication and [Requests](https://github.com/psf/requests) to submit requests to the Microsoft Graph API.\n-\n-The Microsoft Teams handler is initialized with the following parameters:\n-\n-- `client_id`: The client ID of the registered Microsoft Entra ID application.\n-- `client_secret`: The client secret of the registered Microsoft Entra ID application.\n-- `tenant_id`: The tenant ID of the registered Microsoft Entra ID application.\n-\n-<Note>\n-Microsoft Entra ID was previously known as Azure Active Directory (Azure AD).\n-</Note>\n-\n-The parameters given above can be obtained by registering an application in Entra ID by following these steps,\n-1. Go to the [Azure Portal](https://portal.azure.com/#home) and sign in with your Microsoft account.\n-2. Locate the **Microsoft Entra ID** service and click on it.\n-3. Click on **App registrations** and then click on **New registration**.\n-4. Enter a name for your application and select the **Accounts in this organizational directory only** option for the **Supported account types** field.\n-5. Keep the **Redirect URI** field empty and click on **Register**.\n-6. Click on **API permissions** and then click on **Add a permission**.\n-7. Select **Microsoft Graph** and then click on **Delegated permissions**.\n-8. Select the following permissions based on the data you want to access and the operations you want to perform:\n-    - Chats:\n-        - Chat.ReadBasic\n-        - Chat.Read\n-        - Chat.ReadWrite\n-\n-    - Chat messages:\n-        - Chat.Read\n-        -\tChat.ReadWrite\n-        - Group.ReadWrite.All\n-  \n-    - Channels:\n-        - ChannelSettings.Read.All\n-        - ChannelSettings.ReadWrite.All\n-        - Directory.Read.All\n-        - Directory.ReadWrite.All\n-        - Group.Read.All\n-        - Group.ReadWrite.All\n-\n-    - Channel messages:\n-        - ChannelMessage.Read.All\n-        - Group.Read.All\n-        - Group.ReadWrite.All\n-9. Click on **Add permissions**.\n-10. Copy the **Application (client) ID** and record it as the `client_id` parameter, and copy the **Directory (tenant) ID** and record it as the `tenant_id` parameter.\n-11. Click on **Certificates & secrets** and then click on **New client secret**.\n-12. Enter a description for your client secret and select an expiration period.\n-13. Click on **Add** and copy the generated client secret and record it as the `client_secret` parameter.\n-14. Click on **Authentication** and then click on **Add a platform**.\n-15. Select **Web** and enter URL where MindsDB has been deployed followed by `/verify-auth` in the **Redirect URIs** field. For example, if you are running MindsDB locally (on `http://localhost:47334`), enter `http://localhost:47334/verify-auth` in the **Redirect URIs** field.\n-\n-<Tip>\n-You can find more information about creating app registrations [here](https://docs.microsoft.com/en-us/graph/auth-register-app-v2).\n-</Tip>\n-\n-To connect to Microsoft Teams using MindsDB, the following CREATE DATABASE statement can be used:\n+Establish a connection to Microsoft Teams from MindsDB by executing the following SQL command and providing its [handler name](https://github.com/mindsdb/mindsdb/tree/main/mindsdb/integrations/handlers/ms_teams_handler) as an engine.\n \n ```sql\n-CREATE DATABASE teams_datasource\n-WITH ENGINE = 'teams',\n+CREATE DATABASE teams_conn\n+WITH ENGINE = 'teams', \n PARAMETERS = {\n-  \"client_id\": \"your-client-id\",\n-  \"client_secret\": \"your-client-secret\",\n-  \"tenant_id\": \"your-tenant-id\"\n+  \"client_id\": \"12345678-90ab-cdef-1234-567890abcdef\",\n+  \"client_secret\": \"a1b2c3d4e5f6g7h8i9j0k1l2m3n4o5p6\"\n };\n ```\n \n-When the above is statement is executed with the given parameters, the handler will open a browser window and prompt you to sign in with your Microsoft account. \n-\n-The handler will then act (via the app registration) as the signed in user and will submit requests to the Microsoft Graph API. This is done using the concept of [delegated permissions](https://docs.microsoft.com/en-us/graph/auth/auth-concepts#delegated-permissions).\n-\n-## Usage\n-\n-Now, you can post a message to a chat:\n-\n-```sql\n-INSERT INTO teams_datasource.chat_messages (chatId, body_content)\n-VALUES\n-('your-chat-id', 'Hello from MindsDB!');\n-```\n-\n-You can also do the same for channels:\n-\n-```sql\n-SELECT * FROM teams_datasource.channels\n-```\n-\n-```sql\n-INSERT INTO teams_datasource.channel_messages (channelIdentity_teamId, channelIdentity_channelId, body_content)\n-VALUES\n-('your-team-id', 'your-channel-id', 'Hello from MindsDB!');\n-```\n-\n-## Supported Tables\n-\n-The following tables are supported by the Microsoft Teams handler:\n-- `chats`: chats that the signed in user is a member of.\n-- `chat_messages`: messages sent to the signed in user's chats.\n-- `channels`: channels that the signed in user is a member of.\n-- `channel_messages`: messages sent to the signed in user's channels.\n-\n-## Chatbot\n-\n-While the Microsoft Teams handler allows you to read/send messages to a chat or channel as shown above, it is also possible to create a chat bot that will listen to messages sent to a chat or channel and respond to them.\n-\n-### Step 1: Create a Microsoft Teams Data Source\n+Required connection parameters include the following:\n \n-As shown above, create a database with the new `teams` engine by passing in the required parameters:\n-\n-```sql\n-CREATE DATABASE teams_datasource\n-WITH ENGINE = 'teams',\n-PARAMETERS = {\n-  \"client_id\": \"your-client-id\",\n-  \"client_secret\": \"your-client-secret\",\n-  \"tenant_id\": \"your-tenant-id\"\n-};\n-```\n+* `client_id`: The client ID of the registered Microsoft Entra ID application.\n+* `client_secret`: The client secret of the registered Microsoft Entra ID application.\n \n <Note>\n-The chat bot will assume the identity of the user who signed in with their Microsoft account when the database was created.\n+Microsoft Entra ID was previously known as Azure Active Directory (Azure AD).\n </Note>\n \n-### Step 2: Create an [Agent](https://docs.mindsdb.com/agents/agent)\n-\n-An agent is created by combining a conversational model, like [LlamaIndex](https://docs.mindsdb.com/integrations/ai-engines/llamaindex), with a set of Skills.\n-\n-Here, we will create an agent without any skills, however, it is possilbe to create agents with skills such as the ability to answer questions from a knowledge base.\n-\n-#### Step 2.1: Create a Conversational Model\n-\n-Create a conversational model using either the [`LangChain`](https://docs.mindsdb.com/integrations/ai-engines/langchain) or [`LlamaIndex`](https://docs.mindsdb.com/integrations/ai-engines/llamaindex) integrations. Given below is an example of a conversational model created using the `LlamaIndex` integration:\n-\n-```sql\n-CREATE ML_ENGINE llama_index_engine\n-FROM llama_index\n-USING openai_api_key='your-openai-api-key';\n-```\n-\n-```sql\n-CREATE MODEL llama_index_convo_model\n-PREDICT answer\n-USING\n-  engine = 'llama_index_engine',\n-  mode = 'conversational',\n-  prompt = 'answer users questions as a helpful assistant',\n-  user_column = 'question',\n-  assistant_column = 'answer';\n-```\n-\n-#### Step 2.2: Create an Agent using the Conversational Model\n-\n-Let's create an agent using the conversational model created above:\n-\n-```sql\n-CREATE AGENT convo_agent\n-USING\n-    model = 'llama_index_convo_model',\n-```\n-\n-### Step 3: Create a [Chatbot](https://docs.mindsdb.com/agents/chatbot)\n-\n-Finally, create a chatbot using the agent and the Microsoft Teams data source created above:\n-\n-```sql\n-CREATE CHATBOT teams_chatbot\n-USING\n-    database = 'teams_datasource',\n-    agent = 'convo_agent';\n-```\n+Follow the instructions given below to set up the Microsoft Teams app that will act as the chatbot:\n+\n+  1. Follow [this link](https://dev.botframework.com/bots/new) to the Microsoft Bot Framework portal and sign in with your Microsoft account.\n+  2. Fill out the *Display name*, *Bot handle*, and, optionally, the *Long description*, but leave the *Messaging endpoint* field empty for now.\n+  3. Set the *App type* to be 'Multi Tenant' and click on *Create Microsoft App ID and password*. This will open a new tab with the Azure portal.\n+  4. Click on *New registration* and fill out the *Name* and select the 'Accounts in any organizational directory (Any Azure AD directory - Multitenant)' option under *Supported account types*, and click on *Register*. Record the *Application (client) ID* for later use.\n+  5. Click on *Certificates & secrets* under *Manage*.\n+  6. Click on *New client secret* and fill out the *Description* and select an appropriate *Expires* period, and click on *Add*.\n+  7. Copy the client secret and save it in a secure location.\n+  <Tip>\n+  If you already have an existing app registration, you can use it instead of creating a new one and skip steps 4-6.\n+  </Tip>\n+\n+  8. Open the MindsDB Editor and create a connection to Microsoft Teams using the client ID and client secret obtained in the previous steps using the SQL command provided above.\n+  9. Using this connection, create a chatbot using the [`CREATE CHATBOT`](/agents/chatbot) syntax.\n+  10. Run the `SHOW CHATBOTS` command and record the `webhook_token` of the chatbot you created.\n+  11. Navigate back to the Microsoft Bot Framework portal and fill out the messaging endpoint in the following format: `<mindsdb_url>/api/webhooks/chatbots/<webhook_token>`.\n+  <Tip>\n+  The `<mindsdb_url>` is the URL of where MindsDB is running.\n+  Please note that if you are running MindsDB locally, it will need to be exposed to the internet using a service like [ngrok](https://ngrok.com/).\n+  </Tip>\n+\n+  12. Fill out the *Microsoft App ID* using the client ID obtained in the previous steps, agree to the terms, and click on *Register*.\n+  13. Under *Add a featured channel*, click on *Microsoft Teams*, select your Microsoft Teams solution, click on *Save* and agree to the terms when prompted.\n+  14. Navigate to Microsoft Teams and then to the *Apps* tab.\n+  15. Search for the *Developer Portal* app and add it to your workspace.\n+  16. Open the *Developer Portal*, click on *Apps* and then on *New app*.\n+  17. Fill out the *Name* for the app and click on *Add*.\n+  18. Navigate to *Basic information* and fill out the required fields: *Short description*, *Long description*, *Developer or company name*, *Website*, *Privacy policy*, *Terms of use*, and *Application (client) ID*. You may also provide any additional information you wish.\n+  <Tip>\n+  Please note that the above fields are required to be filled out for the bot to be usable in Microsoft Teams. The URLs provided in the *Website*, *Privacy policy*, and *Terms of use* fields must be valid URLs.\n+  </Tip>\n+\n+  19. Navigate to *App features* and select *Bot*.\n+  20. Choose the *Select an existing bot* option and select the bot you created earlier in the Microsoft Bot Framework portal.\n+  21. Select all of the scopes where bot is required to be used and click on *Save*.\n+  22. Finally, on the navigation pane, click on *Publish* and then on *Publish to your org*.\n+\n+For the bot to be made available to the users in your organization, you will need to ask your IT administrator to approve the submission at this [link](https://admin.teams.microsoft.com/policies/manage-apps). \n+\n+Once it is approved, users can find it under the *Built for your org* section in the *Apps* tab. They can either chat with the bot directly or add it to a channel. To chat with the bot in a channel, type `@<bot_name>` followed by your message.\n+\n+While waiting for approval, you can test the chatbot by clicking on *Preview in Teams* in the Developer Portal.\n+\nComment: Let's also add the `Usage` section to show how to query tables.",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/integrations/app-integrations/microsoft-teams.mdx",
    "pr_number": 9864,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1818858314,
    "comment_created_at": "2024-10-28T11:06:57Z"
  },
  {
    "code": "@@ -1,4 +1,4 @@\n----\n+    ---",
    "comment": "Please remove these spaces in front of `---`. Not sure but this may affect the rendering of the doc page.",
    "line_number": 1,
    "enriched": "File: docs/setup/self-hosted/docker.mdx\nCode: @@ -1,4 +1,4 @@\n----\n+    ---\nComment: Please remove these spaces in front of `---`. Not sure but this may affect the rendering of the doc page.",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/setup/self-hosted/docker.mdx",
    "pr_number": 10029,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1814845582,
    "comment_created_at": "2024-10-24T12:04:38Z"
  },
  {
    "code": "@@ -0,0 +1,363 @@\n+---\n+title: Multilingual Spam Data Translation with MindsDB and HuggingFace\n+sidebarTitle: Learn how to connect MindsDB to Hugging Face models to Perform Multilingual Spam Data Text Translation\n+---\n+\n+## Introduction\n+\n+The internet has completely revolutionized the way information is shared across the globe. Nowadays, AI language translation tools use Natural language processing (NLP) to translate written text or spoken words from one source language into a different target language. In this tutorial, you will learn how to use MindsDB and Hugging Face to perform **translation** tasks on a multilingual dataset.\n+\n+## Data Setup\n+\n+If you want to follow along, download this [dataset](https://www.kaggle.com/datasets/rajnathpatel/multilingual-spam-data?resource=download).\n+\n+Make sure you have a working [MindsDB Cloud Acccount](https://cloud.mindsdb.com/) or a local installation of MindsDB running so you can follow along with this tutorial:\n+\n+- **Video:** [Setting Up Docker for MindsDB](https://www.youtube.com/watch?v=dadY-cUpUm0&t=39s)\n+- **Docs:** [Setup for Docker](https://docs.mindsdb.com/setup/self-hosted/docker)\n+\n+If you want to learn how to set up your account at MindsDB Cloud, follow\n+[this guide](/setup/cloud/). Another way is to set up\n+MindsDB locally using\n+[Docker](/setup/self-hosted/docker/) or\n+[Python](/setup/self-hosted/pip/source/).\n+\n+### Understanding the Data\n+\n+The [dataset](https://www.kaggle.com/datasets/rajnathpatel/multilingual-spam-data?resource=download) you are working with contains **multilingual spam data** for English, Hindi, German, and French. It is primarily used to test the zero-shot transfer for text classification using pre-trained language models. The original data was in English and was Machine Translated into Hindi, German, and French. Today, you will be using this dataset to learn how to use MindsDB to perform text translation tasks.\n+\n+You don't need to worry about the labels column (\"ham\"/\"spam\") for this tutorial, as we will be performing **translation** tasks on this dataset.\n+\n+### Adding Data in MindsDB Cloud Editor\n+\n+The instructions for importing data using MindsDB's Cloud Editor go as follows:\n+\n+1. Visit [MindsDB Cloud Editor](https://cloud.mindsdb.com/)\n+2. Click 'Login' or 'Create Account' and authenticate\n+3. Once logged in, click **Add Data**\n+4. Select **\"Files\"**\n+5. Click **\"Import File\"**\n+6. Choose the file to import\n+7. Name your dataset accordingly (no special characters or cases in db name)\n+8. Use SQL `SELECT`  statement to preview the dataset:\n+\n+**input:**\n+```sql\n+SELECT * FROM nlp_spam.mindsdb_1",
    "comment": "After uploading a file to MindsDB Cloud, it is kept in the `files` database. So if you want to query the file uploaded to MindsDB Cloud, it'd be like `SELECT * FROM files.file_name;`.",
    "line_number": 46,
    "enriched": "File: docs/tutorials/multilingual_spam_tutorial.mdx\nCode: @@ -0,0 +1,363 @@\n+---\n+title: Multilingual Spam Data Translation with MindsDB and HuggingFace\n+sidebarTitle: Learn how to connect MindsDB to Hugging Face models to Perform Multilingual Spam Data Text Translation\n+---\n+\n+## Introduction\n+\n+The internet has completely revolutionized the way information is shared across the globe. Nowadays, AI language translation tools use Natural language processing (NLP) to translate written text or spoken words from one source language into a different target language. In this tutorial, you will learn how to use MindsDB and Hugging Face to perform **translation** tasks on a multilingual dataset.\n+\n+## Data Setup\n+\n+If you want to follow along, download this [dataset](https://www.kaggle.com/datasets/rajnathpatel/multilingual-spam-data?resource=download).\n+\n+Make sure you have a working [MindsDB Cloud Acccount](https://cloud.mindsdb.com/) or a local installation of MindsDB running so you can follow along with this tutorial:\n+\n+- **Video:** [Setting Up Docker for MindsDB](https://www.youtube.com/watch?v=dadY-cUpUm0&t=39s)\n+- **Docs:** [Setup for Docker](https://docs.mindsdb.com/setup/self-hosted/docker)\n+\n+If you want to learn how to set up your account at MindsDB Cloud, follow\n+[this guide](/setup/cloud/). Another way is to set up\n+MindsDB locally using\n+[Docker](/setup/self-hosted/docker/) or\n+[Python](/setup/self-hosted/pip/source/).\n+\n+### Understanding the Data\n+\n+The [dataset](https://www.kaggle.com/datasets/rajnathpatel/multilingual-spam-data?resource=download) you are working with contains **multilingual spam data** for English, Hindi, German, and French. It is primarily used to test the zero-shot transfer for text classification using pre-trained language models. The original data was in English and was Machine Translated into Hindi, German, and French. Today, you will be using this dataset to learn how to use MindsDB to perform text translation tasks.\n+\n+You don't need to worry about the labels column (\"ham\"/\"spam\") for this tutorial, as we will be performing **translation** tasks on this dataset.\n+\n+### Adding Data in MindsDB Cloud Editor\n+\n+The instructions for importing data using MindsDB's Cloud Editor go as follows:\n+\n+1. Visit [MindsDB Cloud Editor](https://cloud.mindsdb.com/)\n+2. Click 'Login' or 'Create Account' and authenticate\n+3. Once logged in, click **Add Data**\n+4. Select **\"Files\"**\n+5. Click **\"Import File\"**\n+6. Choose the file to import\n+7. Name your dataset accordingly (no special characters or cases in db name)\n+8. Use SQL `SELECT`  statement to preview the dataset:\n+\n+**input:**\n+```sql\n+SELECT * FROM nlp_spam.mindsdb_1\nComment: After uploading a file to MindsDB Cloud, it is kept in the `files` database. So if you want to query the file uploaded to MindsDB Cloud, it'd be like `SELECT * FROM files.file_name;`.",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/tutorials/multilingual_spam_tutorial.mdx",
    "pr_number": 5393,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1155744429,
    "comment_created_at": "2023-04-03T09:57:43Z"
  },
  {
    "code": "@@ -0,0 +1,42 @@\n+---\n+title: Crate DB",
    "comment": "This file looks like it doesn't belong in this PR. Could you instead complete a readme.md for the AutoGluon ML engine by answering [these](https://docs.google.com/document/d/1yOpRkfjhtL6KmueC0dIvn6rZ-PD3b9XdIa7Ywnyp6Jk/edit) questions?",
    "line_number": 2,
    "enriched": "File: docs/data-integrations/crate-io.mdx\nCode: @@ -0,0 +1,42 @@\n+---\n+title: Crate DB\nComment: This file looks like it doesn't belong in this PR. Could you instead complete a readme.md for the AutoGluon ML engine by answering [these](https://docs.google.com/document/d/1yOpRkfjhtL6KmueC0dIvn6rZ-PD3b9XdIa7Ywnyp6Jk/edit) questions?",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/data-integrations/crate-io.mdx",
    "pr_number": 5067,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1149723285,
    "comment_created_at": "2023-03-27T19:49:15Z"
  },
  {
    "code": "@@ -0,0 +1,53 @@\n+<!DOCTYPE html>\n+<html lang=\"en\">\n+<head>\n+  <meta charset=\"UTF-8\" />\n+  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n+  <title>MindsDB Theme Demo</title>\n+  <style>\n+    :root[data-theme='light'] {\n+      --bg: #ffffff;\n+      --text: #000000;\n+    }\n+    :root[data-theme='dark'] {\n+      --bg: #0d1117;\n+      --text: #e6edf3;\n+    }\n+    body {\n+      background: var(--bg);\n+      color: var(--text);\n+      font-family: Arial, sans-serif;\n+      display: flex;\n+      flex-direction: column;\n+      align-items: center;\n+      justify-content: center;\n+      height: 100vh;\n+      transition: all 0.4s ease;\n+    }\n+    button {\n+      margin-top: 20px;\n+      padding: 10px 20px;\n+      border: none;\n+      border-radius: 10px;\n+      cursor: pointer;\n+    }\n+  </style>\n+</head>\n+<body>\n+  <h1>🧠 MindsDB Custom Theme Demo</h1>\n+  <button onclick=\"toggleTheme()\">Toggle Theme</button>\n+\n+  <script>\n+    const toggleTheme = () => {\n+      const current = document.documentElement.getAttribute(\"data-theme\");\n+      const next = current === \"dark\" ? \"light\" : \"dark\";\n+      document.documentElement.setAttribute(\"data-theme\", next);\n+      localStorage.setItem(\"theme\", next);\n+    };\n+    document.documentElement.setAttribute(\n+      \"data-theme\",\n+      localStorage.getItem(\"theme\") || \"light\"\n+    );\n+  </script>",
    "comment": "**performance**: `toggleTheme` and theme initialization logic are inlined in `<script>`; as the UI grows, this can lead to duplicated logic and maintainability issues, but current scale is too small for measurable performance impact.\n\n",
    "line_number": 51,
    "enriched": "File: mindsdb/feature-demo/theme-demo.html\nCode: @@ -0,0 +1,53 @@\n+<!DOCTYPE html>\n+<html lang=\"en\">\n+<head>\n+  <meta charset=\"UTF-8\" />\n+  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n+  <title>MindsDB Theme Demo</title>\n+  <style>\n+    :root[data-theme='light'] {\n+      --bg: #ffffff;\n+      --text: #000000;\n+    }\n+    :root[data-theme='dark'] {\n+      --bg: #0d1117;\n+      --text: #e6edf3;\n+    }\n+    body {\n+      background: var(--bg);\n+      color: var(--text);\n+      font-family: Arial, sans-serif;\n+      display: flex;\n+      flex-direction: column;\n+      align-items: center;\n+      justify-content: center;\n+      height: 100vh;\n+      transition: all 0.4s ease;\n+    }\n+    button {\n+      margin-top: 20px;\n+      padding: 10px 20px;\n+      border: none;\n+      border-radius: 10px;\n+      cursor: pointer;\n+    }\n+  </style>\n+</head>\n+<body>\n+  <h1>🧠 MindsDB Custom Theme Demo</h1>\n+  <button onclick=\"toggleTheme()\">Toggle Theme</button>\n+\n+  <script>\n+    const toggleTheme = () => {\n+      const current = document.documentElement.getAttribute(\"data-theme\");\n+      const next = current === \"dark\" ? \"light\" : \"dark\";\n+      document.documentElement.setAttribute(\"data-theme\", next);\n+      localStorage.setItem(\"theme\", next);\n+    };\n+    document.documentElement.setAttribute(\n+      \"data-theme\",\n+      localStorage.getItem(\"theme\") || \"light\"\n+    );\n+  </script>\nComment: **performance**: `toggleTheme` and theme initialization logic are inlined in `<script>`; as the UI grows, this can lead to duplicated logic and maintainability issues, but current scale is too small for measurable performance impact.\n\n",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "mindsdb/feature-demo/theme-demo.html",
    "pr_number": 11751,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2434779495,
    "comment_created_at": "2025-10-16T06:55:44Z"
  },
  {
    "code": "@@ -7,53 +7,162 @@ In this section, we present how to connect Microsoft Teams to MindsDB.\n \n [Microsoft Teams](https://www.microsoft.com/en/microsoft-teams/group-chat-software/) is the ultimate messaging app for your organization—a workspace for real-time collaboration and communication, meetings, file and app sharing, and even the occasional emoji! All in one place, all in the open, all accessible to everyone.\n \n-The Microsoft Temas handler allows you to send messages to a channel in Microsoft Teams via a webhook.\n+The Microsoft Temas handler allows you to read and send messages to a Microsoft Teams channel or chat. It also allows you to create a chatbot that will respond to messages.\n \n ## Connection\n \n-This handler was implemented using the [pymsteams](https://pypi.org/project/pymsteams/), which is the Python Wrapper Library to send requests to Microsoft Teams Webhooks.\n+This handler was implemented using [msal](https://github.com/AzureAD/microsoft-authentication-library-for-python) for authentication and [Requests](https://github.com/psf/requests) to submit requests to the Microsoft Graph API.\n \n-The Microsoft Temas handler is initialized with a single parameter:\n+The Microsoft Teams handler is initialized with the following parameters:\n \n-* `webhook_url` is a webhook url for the Microsoft Teams channel to send messages to.\n+- `client_id`: The client ID of the registered Microsoft Entra ID application.\n+- `client_secret`: The client secret of the registered Microsoft Entra ID application.\n+- `tenant_id`: The tenant ID of the registered Microsoft Entra ID application.\n \n <Note>\n-Read about creating an Incoming Webhook in Microsoft Teams [here](https://learn.microsoft.com/en-us/microsoftteams/platform/webhooks-and-connectors/how-to/add-incoming-webhook?tabs=dotnet).\n+Microsoft Entra ID was previously known as Azure Active Directory (Azure AD).\n </Note>\n \n+The parameters given above can be obtained by registering an application in Entra ID by following these steps,\n+1. Go to the [Azure Portal](https://portal.azure.com/#home) and sign in with your Microsoft account.\n+2. Locate the **Microsoft Entra ID** service and click on it.\n+3. Click on **App registrations** and then click on **New registration**.\n+4. Enter a name for your application and select the **Accounts in this organizational directory only** option for the **Supported account types** field.\n+5. Keep the **Redirect URI** field empty and click on **Register**.\n+6. Copy the **Application (client) ID** and paste it as the `client_id` parameter, and copy the **Directory (tenant) ID** and paste it as the `tenant_id` parameter.\n+7. Click on **Certificates & secrets** and then click on **New client secret**.\n+8. Enter a description for your client secret and select an expiration period.\n+9. Click on **Add** and copy the generated client secret and paste it as the `client_secret` parameter.\n+10. Click on **Authentication** and then click on **Add a platform**.\n+11. Select **Web** and enter the following URLs in the **Redirect URIs** field:\n+    - `https://cloud.mindsdb.com/verify-auth`\n+    - `http://localhost:47334/verify-auth` (for local development)\n+\n+<Tip>\n+You can find more information about creating app registrations [here](https://docs.microsoft.com/en-us/graph/auth-register-app-v2).\n+</Tip>\n+\n+When the above is statement is executed with the given parameters, the handler will open a browser window and prompt you to sign in with your Microsoft account. \n+\n+The handler will then act (via the app registration) as the signed in user and will submit requests to the Microsoft Graph API. This is done using the concept of [delegated permissions](https://docs.microsoft.com/en-us/graph/auth/auth-concepts#delegated-permissions).\n+\n To connect to Microsoft Teams using MindsDB, the following CREATE DATABASE statement can be used:\n \n ```sql\n CREATE DATABASE teams_datasource\n WITH ENGINE = 'teams',\n PARAMETERS = {\n-  \"webhook_url\": \"https://...\"\n+  \"client_id\": \"your-client-id\",\n+  \"client_secret\": \"your-client-secret\",\n+  \"tenant_id\": \"your-tenant-id\"\n };\n ```\n \n <Tip>\n-If you installed MindsDB locally via pip, you need to install all handler dependencies manually. To do so, go to the handler's folder (mindsdb/integrations/handlers/ms_teams_handler) and run this command: `pip install -r requirements.txt`.\n+If you installed MindsDB locally, make sure to install all Microsoft Teams dependencies by running `pip install .[teams]` or from the [requirements.txt](indsdb/integrations/handlers/ms_teams_handler/requirements.txt) file.\n </Tip>\n \n ## Usage\n \n-Now, you can send messages to the configured Microsoft Teams channel:\n+Now, you can post a message to a chat:\n \n ```sql\n-INSERT INTO teams_datasource.messages(title, text)\n-VALUES \n-('Hello from MindsDB!', 'This is a test message from MindsDB.');\n+INSERT INTO teams_datasource.chat_messages (chatId, body_content)\n+VALUES\n+('your-chat-id', 'Hello from MindsDB!');\n ```\n \n-You can also send multiple messages at once:\n+You can also do the same for channels:\n \n ```sql\n-INSERT INTO teams_datasource.messages(title, text)\n-VALUES \n-('Hello from MindsDB!', 'This is a test message from MindsDB.'),\n-('Hello again!', 'This is another test message from MindsDB.');\n+SELECT * FROM teams_datasource.channels\n ```\n \n-<Tip>\n-you can use this handler to send outputs from models created in MinsDB as notifications to Microsoft Teams.\n-</Tip>\n\\ No newline at end of file\n+```sql\n+INSERT INTO teams_datasource.channel_messages (channelIdentity_teamId, channelIdentity_channelId, body_content)\n+VALUES\n+('your-team-id', 'your-channel-id', 'Hello from MindsDB!');\n+```\n+\n+## Supported Tables\n+\n+The following tables are supported by the Microsoft Teams handler:\n+- `chats`: chats that the signed in user is a member of.\n+- `chat_messages`: messages sent to the signed in user's chats.\n+- `channels`: channels that the signed in user is a member of.\n+- `channel_messages`: messages sent to the signed in user's channels.\n+\n+## Chatbot\n+\n+While the Microsoft Teams handler allows you to read/send messages to a chat or channel as shown above, it is also possible to create a chat bot that will listen to messages sent to a chat or channel and respond to them.\n+\n+### Step 1: Create a Microsoft Teams Data Source\n+\n+As shown above, create a database with the new `teams` engine by passing in the required parameters:\n+\n+```sql\n+CREATE DATABASE teams_datasource\n+WITH ENGINE = 'teams',\n+PARAMETERS = {\n+  \"client_id\": \"your-client-id\",\n+  \"client_secret\": \"your-client-secret\",\n+  \"tenant_id\": \"your-tenant-id\"\n+};\n+```\n+\n+<Note>\n+The chat bot will assume the identity of the user who signed in with their Microsoft account when the database was created.\n+</Note>\n+\n+### Step 2: Create an [Agent](https://docs.mindsdb.com/agents/agent)\n+\n+An agent is created by combining a [Model](https://docs.mindsdb.com/model-types) (conversational) with optional Skills.",
    "comment": "This link doesn't link to any conversation model. Let's say the following:\r\n```\r\nAn agent is created by combining a conversational model, like [LangChain](/integrations/ai-engines/langchain), with a set of skills.\r\n```",
    "line_number": 119,
    "enriched": "File: docs/integrations/app-integrations/microsoft-teams.mdx\nCode: @@ -7,53 +7,162 @@ In this section, we present how to connect Microsoft Teams to MindsDB.\n \n [Microsoft Teams](https://www.microsoft.com/en/microsoft-teams/group-chat-software/) is the ultimate messaging app for your organization—a workspace for real-time collaboration and communication, meetings, file and app sharing, and even the occasional emoji! All in one place, all in the open, all accessible to everyone.\n \n-The Microsoft Temas handler allows you to send messages to a channel in Microsoft Teams via a webhook.\n+The Microsoft Temas handler allows you to read and send messages to a Microsoft Teams channel or chat. It also allows you to create a chatbot that will respond to messages.\n \n ## Connection\n \n-This handler was implemented using the [pymsteams](https://pypi.org/project/pymsteams/), which is the Python Wrapper Library to send requests to Microsoft Teams Webhooks.\n+This handler was implemented using [msal](https://github.com/AzureAD/microsoft-authentication-library-for-python) for authentication and [Requests](https://github.com/psf/requests) to submit requests to the Microsoft Graph API.\n \n-The Microsoft Temas handler is initialized with a single parameter:\n+The Microsoft Teams handler is initialized with the following parameters:\n \n-* `webhook_url` is a webhook url for the Microsoft Teams channel to send messages to.\n+- `client_id`: The client ID of the registered Microsoft Entra ID application.\n+- `client_secret`: The client secret of the registered Microsoft Entra ID application.\n+- `tenant_id`: The tenant ID of the registered Microsoft Entra ID application.\n \n <Note>\n-Read about creating an Incoming Webhook in Microsoft Teams [here](https://learn.microsoft.com/en-us/microsoftteams/platform/webhooks-and-connectors/how-to/add-incoming-webhook?tabs=dotnet).\n+Microsoft Entra ID was previously known as Azure Active Directory (Azure AD).\n </Note>\n \n+The parameters given above can be obtained by registering an application in Entra ID by following these steps,\n+1. Go to the [Azure Portal](https://portal.azure.com/#home) and sign in with your Microsoft account.\n+2. Locate the **Microsoft Entra ID** service and click on it.\n+3. Click on **App registrations** and then click on **New registration**.\n+4. Enter a name for your application and select the **Accounts in this organizational directory only** option for the **Supported account types** field.\n+5. Keep the **Redirect URI** field empty and click on **Register**.\n+6. Copy the **Application (client) ID** and paste it as the `client_id` parameter, and copy the **Directory (tenant) ID** and paste it as the `tenant_id` parameter.\n+7. Click on **Certificates & secrets** and then click on **New client secret**.\n+8. Enter a description for your client secret and select an expiration period.\n+9. Click on **Add** and copy the generated client secret and paste it as the `client_secret` parameter.\n+10. Click on **Authentication** and then click on **Add a platform**.\n+11. Select **Web** and enter the following URLs in the **Redirect URIs** field:\n+    - `https://cloud.mindsdb.com/verify-auth`\n+    - `http://localhost:47334/verify-auth` (for local development)\n+\n+<Tip>\n+You can find more information about creating app registrations [here](https://docs.microsoft.com/en-us/graph/auth-register-app-v2).\n+</Tip>\n+\n+When the above is statement is executed with the given parameters, the handler will open a browser window and prompt you to sign in with your Microsoft account. \n+\n+The handler will then act (via the app registration) as the signed in user and will submit requests to the Microsoft Graph API. This is done using the concept of [delegated permissions](https://docs.microsoft.com/en-us/graph/auth/auth-concepts#delegated-permissions).\n+\n To connect to Microsoft Teams using MindsDB, the following CREATE DATABASE statement can be used:\n \n ```sql\n CREATE DATABASE teams_datasource\n WITH ENGINE = 'teams',\n PARAMETERS = {\n-  \"webhook_url\": \"https://...\"\n+  \"client_id\": \"your-client-id\",\n+  \"client_secret\": \"your-client-secret\",\n+  \"tenant_id\": \"your-tenant-id\"\n };\n ```\n \n <Tip>\n-If you installed MindsDB locally via pip, you need to install all handler dependencies manually. To do so, go to the handler's folder (mindsdb/integrations/handlers/ms_teams_handler) and run this command: `pip install -r requirements.txt`.\n+If you installed MindsDB locally, make sure to install all Microsoft Teams dependencies by running `pip install .[teams]` or from the [requirements.txt](indsdb/integrations/handlers/ms_teams_handler/requirements.txt) file.\n </Tip>\n \n ## Usage\n \n-Now, you can send messages to the configured Microsoft Teams channel:\n+Now, you can post a message to a chat:\n \n ```sql\n-INSERT INTO teams_datasource.messages(title, text)\n-VALUES \n-('Hello from MindsDB!', 'This is a test message from MindsDB.');\n+INSERT INTO teams_datasource.chat_messages (chatId, body_content)\n+VALUES\n+('your-chat-id', 'Hello from MindsDB!');\n ```\n \n-You can also send multiple messages at once:\n+You can also do the same for channels:\n \n ```sql\n-INSERT INTO teams_datasource.messages(title, text)\n-VALUES \n-('Hello from MindsDB!', 'This is a test message from MindsDB.'),\n-('Hello again!', 'This is another test message from MindsDB.');\n+SELECT * FROM teams_datasource.channels\n ```\n \n-<Tip>\n-you can use this handler to send outputs from models created in MinsDB as notifications to Microsoft Teams.\n-</Tip>\n\\ No newline at end of file\n+```sql\n+INSERT INTO teams_datasource.channel_messages (channelIdentity_teamId, channelIdentity_channelId, body_content)\n+VALUES\n+('your-team-id', 'your-channel-id', 'Hello from MindsDB!');\n+```\n+\n+## Supported Tables\n+\n+The following tables are supported by the Microsoft Teams handler:\n+- `chats`: chats that the signed in user is a member of.\n+- `chat_messages`: messages sent to the signed in user's chats.\n+- `channels`: channels that the signed in user is a member of.\n+- `channel_messages`: messages sent to the signed in user's channels.\n+\n+## Chatbot\n+\n+While the Microsoft Teams handler allows you to read/send messages to a chat or channel as shown above, it is also possible to create a chat bot that will listen to messages sent to a chat or channel and respond to them.\n+\n+### Step 1: Create a Microsoft Teams Data Source\n+\n+As shown above, create a database with the new `teams` engine by passing in the required parameters:\n+\n+```sql\n+CREATE DATABASE teams_datasource\n+WITH ENGINE = 'teams',\n+PARAMETERS = {\n+  \"client_id\": \"your-client-id\",\n+  \"client_secret\": \"your-client-secret\",\n+  \"tenant_id\": \"your-tenant-id\"\n+};\n+```\n+\n+<Note>\n+The chat bot will assume the identity of the user who signed in with their Microsoft account when the database was created.\n+</Note>\n+\n+### Step 2: Create an [Agent](https://docs.mindsdb.com/agents/agent)\n+\n+An agent is created by combining a [Model](https://docs.mindsdb.com/model-types) (conversational) with optional Skills.\nComment: This link doesn't link to any conversation model. Let's say the following:\r\n```\r\nAn agent is created by combining a conversational model, like [LangChain](/integrations/ai-engines/langchain), with a set of skills.\r\n```",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/integrations/app-integrations/microsoft-teams.mdx",
    "pr_number": 8505,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1423913111,
    "comment_created_at": "2023-12-12T12:20:10Z"
  },
  {
    "code": "@@ -0,0 +1,56 @@\n+# Notion Handler\n+\n+This is the implementation of the Notion handler for MindsDB.\n+\n+## Notion\n+Notion is a note-taking and productivity freemium cloud platform.\n+In short, notion has all-in-one workspace tool that integrates kanban boards, tasks, wikis, and database.\n+In this handler. python client of api is used and more information about this python client can be found (here)[https://pypi.org/project/notion-client/]\n+\n+\n+## Implementation\n+This handler was implemented as per the MindsDB API Handler documentation.\n+\n+\n+The required arguments to establish a connection are,\n+* `notion_api_token`: API key for acessing the Notion instance.\n+\n+\n+## Usage\n+In order to make use of this handler and connect to an Notion in MindsDB, the following syntax can be used,\n+~~~~sql\n+CREATE DATABASE notion_source\n+WITH\n+engine='notion',\n+parameters={\n+    \"notion_api_token\": \"<notion-api-token>\",\n+};\n+~~~~\n+\n+## Implemented Features",
    "comment": "Hey @Mr-Destructive,\r\nThank you for this PR. Could you please add how to retrieve the IDs for each of these components (databases, pages and blocks)? I am assuming it is from the URL, but it might be good to include it for users who are not aware.\r\nI think it might also be a good idea to explain what each of these are in maybe a single sentence. For instance, a database is a collection of pages. I actually do not know what a block is.",
    "line_number": 30,
    "enriched": "File: mindsdb/integrations/handlers/notion_handler/README.md\nCode: @@ -0,0 +1,56 @@\n+# Notion Handler\n+\n+This is the implementation of the Notion handler for MindsDB.\n+\n+## Notion\n+Notion is a note-taking and productivity freemium cloud platform.\n+In short, notion has all-in-one workspace tool that integrates kanban boards, tasks, wikis, and database.\n+In this handler. python client of api is used and more information about this python client can be found (here)[https://pypi.org/project/notion-client/]\n+\n+\n+## Implementation\n+This handler was implemented as per the MindsDB API Handler documentation.\n+\n+\n+The required arguments to establish a connection are,\n+* `notion_api_token`: API key for acessing the Notion instance.\n+\n+\n+## Usage\n+In order to make use of this handler and connect to an Notion in MindsDB, the following syntax can be used,\n+~~~~sql\n+CREATE DATABASE notion_source\n+WITH\n+engine='notion',\n+parameters={\n+    \"notion_api_token\": \"<notion-api-token>\",\n+};\n+~~~~\n+\n+## Implemented Features\nComment: Hey @Mr-Destructive,\r\nThank you for this PR. Could you please add how to retrieve the IDs for each of these components (databases, pages and blocks)? I am assuming it is from the URL, but it might be good to include it for users who are not aware.\r\nI think it might also be a good idea to explain what each of these are in maybe a single sentence. For instance, a database is a collection of pages. I actually do not know what a block is.",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "mindsdb/integrations/handlers/notion_handler/README.md",
    "pr_number": 7664,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1352581160,
    "comment_created_at": "2023-10-10T14:02:35Z"
  },
  {
    "code": "@@ -154,6 +154,34 @@ def _get_show_where(\n     return None\n \n \n+def match_one_part_name(identifier: Identifier, ensure_lower_case: bool = False) -> str:",
    "comment": "mabe to name these functions differently according to the purpose of usage? for example\r\n- get_object_name\r\n- get_db_object_name\r\n\r\nor add info about purpose in docstring",
    "line_number": 157,
    "enriched": "File: mindsdb/api/executor/command_executor.py\nCode: @@ -154,6 +154,34 @@ def _get_show_where(\n     return None\n \n \n+def match_one_part_name(identifier: Identifier, ensure_lower_case: bool = False) -> str:\nComment: mabe to name these functions differently according to the purpose of usage? for example\r\n- get_object_name\r\n- get_db_object_name\r\n\r\nor add info about purpose in docstring",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "mindsdb/api/executor/command_executor.py",
    "pr_number": 11252,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2209988552,
    "comment_created_at": "2025-07-16T10:53:17Z"
  },
  {
    "code": "@@ -0,0 +1,105 @@\n+# spaCy Handler\n+\n+This is an integration with spaCy which is a free, open-source library for advanced Natural Language Processing (NLP) in Python. The documentation is available here: https://spacy.io/usage\n+\n+# Initialization\n+\n+When initializing the model, it is important to define **\"linguistic_feature\"** and **\"target_column\"**. With lingustic feature, you define the type of NLP process you want to perform on specific column (\"target_column\").\n+\n+In case of errors, for English model or any model you have to install it in the following way: **python -m spacy download en_core_web_sm**\n+\n+```sql\n+CREATE MODEL spacy__morphology__model\n+PREDICT recognition\n+USING\n+engine = 'spacy',\n+linguistic_feature = 'morphology',\n+target_column = 'review';\n+```\n+\n+After creating the model and defining the specifications, you are ready to perform NLP on columns.\n+\n+## Perform NLP on single data\n+\n+```sql\n+SELECT review, recognition\n+FROM spacy__morphology__model\n+WHERE review = '\"Apple is looking at buying U.K. startup for $1 billion\"';\n+```",
    "comment": "Could you please add the expected output in each example command? Would be great for new users to understand quickly what these SpaCy models should return.",
    "line_number": 28,
    "enriched": "File: mindsdb/integrations/handlers/spacy_handler/README.md\nCode: @@ -0,0 +1,105 @@\n+# spaCy Handler\n+\n+This is an integration with spaCy which is a free, open-source library for advanced Natural Language Processing (NLP) in Python. The documentation is available here: https://spacy.io/usage\n+\n+# Initialization\n+\n+When initializing the model, it is important to define **\"linguistic_feature\"** and **\"target_column\"**. With lingustic feature, you define the type of NLP process you want to perform on specific column (\"target_column\").\n+\n+In case of errors, for English model or any model you have to install it in the following way: **python -m spacy download en_core_web_sm**\n+\n+```sql\n+CREATE MODEL spacy__morphology__model\n+PREDICT recognition\n+USING\n+engine = 'spacy',\n+linguistic_feature = 'morphology',\n+target_column = 'review';\n+```\n+\n+After creating the model and defining the specifications, you are ready to perform NLP on columns.\n+\n+## Perform NLP on single data\n+\n+```sql\n+SELECT review, recognition\n+FROM spacy__morphology__model\n+WHERE review = '\"Apple is looking at buying U.K. startup for $1 billion\"';\n+```\nComment: Could you please add the expected output in each example command? Would be great for new users to understand quickly what these SpaCy models should return.",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "mindsdb/integrations/handlers/spacy_handler/README.md",
    "pr_number": 7879,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1378038581,
    "comment_created_at": "2023-10-31T18:49:46Z"
  },
  {
    "code": "@@ -113,6 +113,7 @@ FROM openai\n USING\n   api_key = 'YOUR_OPENAI_API_KEY';\n ```\n+Please note that it is required to provide an OpenAI API key when using MindsDB Pro version.\n ",
    "comment": "\r\nI have read the CLA Document and I hereby sign the CLA\r\n\r\n",
    "line_number": 117,
    "enriched": "File: docs/nlp/text-summarization-inside-mongodb-with-openai.mdx\nCode: @@ -113,6 +113,7 @@ FROM openai\n USING\n   api_key = 'YOUR_OPENAI_API_KEY';\n ```\n+Please note that it is required to provide an OpenAI API key when using MindsDB Pro version.\n \nComment: \r\nI have read the CLA Document and I hereby sign the CLA\r\n\r\n",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/nlp/text-summarization-inside-mongodb-with-openai.mdx",
    "pr_number": 5675,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1170265382,
    "comment_created_at": "2023-04-18T16:03:59Z"
  },
  {
    "code": "@@ -0,0 +1,48 @@\n+---\n+title: TimescaleDB\n+sidebarTitle: TimescaleDB\n+---\n+\n+#TimescaleDB Handler\n+\n+This is the implementation of the TimescaleDB handler for MindsDB.\n+\n+##TimescaleDB\n+TimescaleDB is an open-source relational database that is optimized for time-series data. It is designed to handle large volumes of data and enables you to query and analyze data in real-time. TimescaleDB can be used for a wide range of applications, including IoT, finance, and monitoring.\n+https://docs.timescale.com\n+\n+##Implementation\n+This handler was implemented using the psycopg2 library, which is a PostgreSQL adapter for the Python programming language. TimescaleDB is built on top of PostgreSQL and therefore can be accessed using the same client libraries and APIs.\n+\n+The required arguments to establish a connection are,\n+\n+*`host`: the host name or IP address of the TimescaleDB server\n+*`port`: the port to use when connecting with the TimescaleDB server\n+*`database`: the database name to use when connecting with the TimescaleDB server\n+*`user`: the user to authenticate the user with the TimescaleDB server\n+*`password`: the password to authenticate the user with the TimescaleDB server\n+\n+##Usage\n+Before attempting to connect to a TimescaleDB server using MindsDB, ensure that it accepts incoming connections. The following can be used as a guideline to accomplish this,\n+https://docs.timescale.com/latest/getting-started/setup/remote-connections/\n+\n+In order to make use of this handler and connect to a TimescaleDB server in MindsDB, the following syntax can be used,\n+\n+```sql\n+CREATE DATABASE timescaledb_datasource\n+WITH\n+engine='postgres',",
    "comment": "It is not a `postgres` database. Please correct it here and throughout this doc page.\r\n\r\nUse the content from this file: https://github.com/mindsdb/mindsdb/blob/staging/mindsdb/integrations/handlers/timescaledb_handler/README.md",
    "line_number": 34,
    "enriched": "File: docs/data-integrations/Timescaledb.mdx\nCode: @@ -0,0 +1,48 @@\n+---\n+title: TimescaleDB\n+sidebarTitle: TimescaleDB\n+---\n+\n+#TimescaleDB Handler\n+\n+This is the implementation of the TimescaleDB handler for MindsDB.\n+\n+##TimescaleDB\n+TimescaleDB is an open-source relational database that is optimized for time-series data. It is designed to handle large volumes of data and enables you to query and analyze data in real-time. TimescaleDB can be used for a wide range of applications, including IoT, finance, and monitoring.\n+https://docs.timescale.com\n+\n+##Implementation\n+This handler was implemented using the psycopg2 library, which is a PostgreSQL adapter for the Python programming language. TimescaleDB is built on top of PostgreSQL and therefore can be accessed using the same client libraries and APIs.\n+\n+The required arguments to establish a connection are,\n+\n+*`host`: the host name or IP address of the TimescaleDB server\n+*`port`: the port to use when connecting with the TimescaleDB server\n+*`database`: the database name to use when connecting with the TimescaleDB server\n+*`user`: the user to authenticate the user with the TimescaleDB server\n+*`password`: the password to authenticate the user with the TimescaleDB server\n+\n+##Usage\n+Before attempting to connect to a TimescaleDB server using MindsDB, ensure that it accepts incoming connections. The following can be used as a guideline to accomplish this,\n+https://docs.timescale.com/latest/getting-started/setup/remote-connections/\n+\n+In order to make use of this handler and connect to a TimescaleDB server in MindsDB, the following syntax can be used,\n+\n+```sql\n+CREATE DATABASE timescaledb_datasource\n+WITH\n+engine='postgres',\nComment: It is not a `postgres` database. Please correct it here and throughout this doc page.\r\n\r\nUse the content from this file: https://github.com/mindsdb/mindsdb/blob/staging/mindsdb/integrations/handlers/timescaledb_handler/README.md",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/data-integrations/Timescaledb.mdx",
    "pr_number": 5237,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1150335934,
    "comment_created_at": "2023-03-28T09:57:04Z"
  },
  {
    "code": "@@ -0,0 +1,124 @@\n+---\n+title: Forecasting Monthly Expenditures with TimeGPT\n+sidebarTitle: House Sales with TimeGPT\n+---\n+\n+## Introduction\n+\n+In this tutorial, we introduce Nixtla’s TimeGPT integration which offers the first foundational model for time series forecasting. Follow along to see how it works.\n+\n+## Prerequisites\n+\n+### MindsDB Setup\n+\n+One way is to sign up for an account at [MindsDB Cloud](https://cloud.mindsdb.com/). It is a convenient option as it doesn’t require any installation procedures. You can find the details [here](/setup/cloud).\n+\n+Alternatively, visit our docs and follow the instructions to manually set up a local instance of MindsDB via [Docker](/setup/self-hosted/docker) or [pip](/setup/self-hosted/pip/source). You can also set up MindsDB on AWS following [this instruction set](https://aws.amazon.com/marketplace/pp/prodview-2nmhvskydmyrw).\n+\n+### Creating an ML Engine\n+\n+You can check the available engines with this command:\n+\n+```sql\n+SHOW ML_ENGINES;\n+```\n+\n+If you see the TimeGPT engine on the list, you are ready to follow the tutorials.\n+",
    "comment": "We should probably add this, or something similar:\r\n\r\n> If you do not see TimeGPT on the list, you will have to create an instance of the engine first with this command:\r\n\r\n> `CREATE ML_ENGINE timegpt FROM timegpt USING api_key = '...'`\r\n\r\n> Notice that the USING clause is optional, but you will need to pass an API key eventually (either at model creation, engine creation, model usage, or in the mindsdb configuration file). The only exception to this is when using cloud.mindsdb.com.",
    "line_number": 27,
    "enriched": "File: docs/sql/tutorials/monthly-expediture-timegpt.mdx\nCode: @@ -0,0 +1,124 @@\n+---\n+title: Forecasting Monthly Expenditures with TimeGPT\n+sidebarTitle: House Sales with TimeGPT\n+---\n+\n+## Introduction\n+\n+In this tutorial, we introduce Nixtla’s TimeGPT integration which offers the first foundational model for time series forecasting. Follow along to see how it works.\n+\n+## Prerequisites\n+\n+### MindsDB Setup\n+\n+One way is to sign up for an account at [MindsDB Cloud](https://cloud.mindsdb.com/). It is a convenient option as it doesn’t require any installation procedures. You can find the details [here](/setup/cloud).\n+\n+Alternatively, visit our docs and follow the instructions to manually set up a local instance of MindsDB via [Docker](/setup/self-hosted/docker) or [pip](/setup/self-hosted/pip/source). You can also set up MindsDB on AWS following [this instruction set](https://aws.amazon.com/marketplace/pp/prodview-2nmhvskydmyrw).\n+\n+### Creating an ML Engine\n+\n+You can check the available engines with this command:\n+\n+```sql\n+SHOW ML_ENGINES;\n+```\n+\n+If you see the TimeGPT engine on the list, you are ready to follow the tutorials.\n+\nComment: We should probably add this, or something similar:\r\n\r\n> If you do not see TimeGPT on the list, you will have to create an instance of the engine first with this command:\r\n\r\n> `CREATE ML_ENGINE timegpt FROM timegpt USING api_key = '...'`\r\n\r\n> Notice that the USING clause is optional, but you will need to pass an API key eventually (either at model creation, engine creation, model usage, or in the mindsdb configuration file). The only exception to this is when using cloud.mindsdb.com.",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/sql/tutorials/monthly-expediture-timegpt.mdx",
    "pr_number": 7145,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1299317355,
    "comment_created_at": "2023-08-20T07:03:51Z"
  },
  {
    "code": "@@ -1047,6 +1031,140 @@ Here is the output:\n +-----+----------------------+------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------+-------------------+\n ```\n \n+## `EVALUATE KNOWLEDGE_BASE` Syntax\n+\n+With the `EVALUATE KNOWLEDGE_BASE` command, users can evaluate the relevancy and accuracy of the documents and data returned by the knowledge base.\n+\n+Below is the complete syntax that includes both required and optional parameters.\n+\n+```sql\n+EVALUATE KNOWLEDGE_BASE my_kb\n+USING\n+    test_table = my_datasource.my_test_table,\n+    version = 'doc_id',\n+    generate_data = {\n+        'from_sql': 'SELECT content FROM my_datasource.my_table',\n+        'count': 100\n+    }, \n+    evaluate = false,\n+    llm = {\n+        'provider': 'openai',\n+        'api_key':'sk-xxx',\n+        'model':'gpt-4'\n+    },\n+    save_to = my_datasource.my_result_table; \n+```\n+\n+### `test_table`\n+\n+This is a required parameter that stores the name of the table from one of the data sources connected to MindsDB. For example, `test_table = my_datasource.my_test_table` defines a table named `my_test_table` from the data source named `my_datasource`.\n+\n+This test table stores test data commonly in form of questions and answers. Its content depends on the `version` parameter defined below.\n+\n+Users can provide their own test data or have the test data generated by the `EVALUATE KNOWLEDGE_BASE` command, which is performed when setting the `generate_data` parameter defined below.\n+\n+### `version`\n+\n+This parameter defines the version of the evaluator. It is an optional parameter with default value of `doc_id`.\n+\n+* `version = 'doc_id'`\n+    The evaluator checks whether the document ID returned by the knowledge base matched the expected document ID as defined in the test table.\n+\n+* `version = 'llm_relevancy'`\n+    The evaluator uses a language model to rank and evaluate responses from the knowledge base.\n+\n+### `generate_data`\n+\n+This is an optional parameter used to configure the tests data generation, which is saved into the table defined in the `test_table` parameter.\n+\n+Available values are as follows:\n+\n+* A dictionary containing the following values:\n+    * `from_sql` defines the SQL query that fetches the test data. For example, `'from_sql': 'SELECT content FROM my_datasource.my_table'`.\n+    * `count` defines the size of the test dataset. For example, `'count': 100`. Its default value is 20.\n+\n+* A value of `true`, such as `generate_data = true`, which implies that default values for `from_sql` and `count` will be used.\n+\n+### `evaluate`\n+\n+This is an optional parameter that defines whether to evaluate the knowledge base.\n+\n+Users can opt for setting it to false, `evaluate = false`, in order to generate test data into the test table without running the evaluator.\n+\n+### `llm`\n+\n+This is an optional parameter that defines a language model to be used for evaluations, if `version` is set to `llm_relevancy`.\n+\n+By default it uses the [`reranking_model` defined with the knowledge base](/mindsdb_sql/knowledge-bases#reranking-model).\n+\n+Users can define it with the `EVALUATE KNOWLEDGE_BASE` command in the same manner.\n+\n+```sql\n+EVALUATE KNOWLEDGE_BASE my_kb\n+USING\n+    ...\n+    llm = {\n+        \"provider\": \"azure_openai\",\n+        \"model_name\" : \"gpt-4o\",\n+        \"api_key\": \"sk-abc123\",\n+        \"base_url\": \"https://ai-6689.openai.azure.com/\",\n+        \"api_version\": \"2024-02-01\",\n+        \"method\": \"multi-class\"\n+    },\n+    ...\n+```\n+\n+### `save_to`\n+\n+This is an optional parameter that stores the name of the table from one of the data sources connected to MindsDB. For example, `save_to = my_datasource.my_result_table` defines a table named `my_result_table` from the data source named `my_datasource`.\n+\n+This table is used to save the evaluation results.\n+\n+By default, evaluation results are returned after executing the `EVALUATE KNOWLEDGE_BASE` statement.\n+\n+### Evaluation Results\n+\n+When using `version = 'doc_id'`, the following columns are included in the evaluation results:",
    "comment": "I mixed up the names of versions in[ PR description](https://github.com/mindsdb/mindsdb/pull/10914). Corrected now. parameters for `doc_id` should be for `llm_relevancy`",
    "line_number": 1127,
    "enriched": "File: docs/mindsdb_sql/knowledge-bases.mdx\nCode: @@ -1047,6 +1031,140 @@ Here is the output:\n +-----+----------------------+------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------+-------------------+\n ```\n \n+## `EVALUATE KNOWLEDGE_BASE` Syntax\n+\n+With the `EVALUATE KNOWLEDGE_BASE` command, users can evaluate the relevancy and accuracy of the documents and data returned by the knowledge base.\n+\n+Below is the complete syntax that includes both required and optional parameters.\n+\n+```sql\n+EVALUATE KNOWLEDGE_BASE my_kb\n+USING\n+    test_table = my_datasource.my_test_table,\n+    version = 'doc_id',\n+    generate_data = {\n+        'from_sql': 'SELECT content FROM my_datasource.my_table',\n+        'count': 100\n+    }, \n+    evaluate = false,\n+    llm = {\n+        'provider': 'openai',\n+        'api_key':'sk-xxx',\n+        'model':'gpt-4'\n+    },\n+    save_to = my_datasource.my_result_table; \n+```\n+\n+### `test_table`\n+\n+This is a required parameter that stores the name of the table from one of the data sources connected to MindsDB. For example, `test_table = my_datasource.my_test_table` defines a table named `my_test_table` from the data source named `my_datasource`.\n+\n+This test table stores test data commonly in form of questions and answers. Its content depends on the `version` parameter defined below.\n+\n+Users can provide their own test data or have the test data generated by the `EVALUATE KNOWLEDGE_BASE` command, which is performed when setting the `generate_data` parameter defined below.\n+\n+### `version`\n+\n+This parameter defines the version of the evaluator. It is an optional parameter with default value of `doc_id`.\n+\n+* `version = 'doc_id'`\n+    The evaluator checks whether the document ID returned by the knowledge base matched the expected document ID as defined in the test table.\n+\n+* `version = 'llm_relevancy'`\n+    The evaluator uses a language model to rank and evaluate responses from the knowledge base.\n+\n+### `generate_data`\n+\n+This is an optional parameter used to configure the tests data generation, which is saved into the table defined in the `test_table` parameter.\n+\n+Available values are as follows:\n+\n+* A dictionary containing the following values:\n+    * `from_sql` defines the SQL query that fetches the test data. For example, `'from_sql': 'SELECT content FROM my_datasource.my_table'`.\n+    * `count` defines the size of the test dataset. For example, `'count': 100`. Its default value is 20.\n+\n+* A value of `true`, such as `generate_data = true`, which implies that default values for `from_sql` and `count` will be used.\n+\n+### `evaluate`\n+\n+This is an optional parameter that defines whether to evaluate the knowledge base.\n+\n+Users can opt for setting it to false, `evaluate = false`, in order to generate test data into the test table without running the evaluator.\n+\n+### `llm`\n+\n+This is an optional parameter that defines a language model to be used for evaluations, if `version` is set to `llm_relevancy`.\n+\n+By default it uses the [`reranking_model` defined with the knowledge base](/mindsdb_sql/knowledge-bases#reranking-model).\n+\n+Users can define it with the `EVALUATE KNOWLEDGE_BASE` command in the same manner.\n+\n+```sql\n+EVALUATE KNOWLEDGE_BASE my_kb\n+USING\n+    ...\n+    llm = {\n+        \"provider\": \"azure_openai\",\n+        \"model_name\" : \"gpt-4o\",\n+        \"api_key\": \"sk-abc123\",\n+        \"base_url\": \"https://ai-6689.openai.azure.com/\",\n+        \"api_version\": \"2024-02-01\",\n+        \"method\": \"multi-class\"\n+    },\n+    ...\n+```\n+\n+### `save_to`\n+\n+This is an optional parameter that stores the name of the table from one of the data sources connected to MindsDB. For example, `save_to = my_datasource.my_result_table` defines a table named `my_result_table` from the data source named `my_datasource`.\n+\n+This table is used to save the evaluation results.\n+\n+By default, evaluation results are returned after executing the `EVALUATE KNOWLEDGE_BASE` statement.\n+\n+### Evaluation Results\n+\n+When using `version = 'doc_id'`, the following columns are included in the evaluation results:\nComment: I mixed up the names of versions in[ PR description](https://github.com/mindsdb/mindsdb/pull/10914). Corrected now. parameters for `doc_id` should be for `llm_relevancy`",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/mindsdb_sql/knowledge-bases.mdx",
    "pr_number": 11035,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2139367866,
    "comment_created_at": "2025-06-11T07:03:11Z"
  },
  {
    "code": "@@ -0,0 +1,212 @@\n+---\n+title: NLP with MindsDB and Hugging Face \n+sidebarTitle: Movie reviews\n+---\n+## Introduction\n+[MindsDB](https://mindsdb.com/) is is an open-source AI layer for existing databases that allows you to effortlessly develop, train and deploy state-of-the-art machine learning models using SQL queries. \n+[Hugging Face](https://huggingface.co/) is a community and data science platform that provides tools that enables users to build, train and deploy machine learning models based on open source code and technologies. \n+\n+\n+There are currently four NLP tasks currently supported by MindsDB:\n+\n+1. Text Classification\n+2. Zero-Shot Classification\n+3. Translation\n+4. ",
    "comment": "The fourth one is Summarization.",
    "line_number": 15,
    "enriched": "File: docs/tutorials/mindsdb-huggingface-tutorial.mdx\nCode: @@ -0,0 +1,212 @@\n+---\n+title: NLP with MindsDB and Hugging Face \n+sidebarTitle: Movie reviews\n+---\n+## Introduction\n+[MindsDB](https://mindsdb.com/) is is an open-source AI layer for existing databases that allows you to effortlessly develop, train and deploy state-of-the-art machine learning models using SQL queries. \n+[Hugging Face](https://huggingface.co/) is a community and data science platform that provides tools that enables users to build, train and deploy machine learning models based on open source code and technologies. \n+\n+\n+There are currently four NLP tasks currently supported by MindsDB:\n+\n+1. Text Classification\n+2. Zero-Shot Classification\n+3. Translation\n+4. \nComment: The fourth one is Summarization.",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/tutorials/mindsdb-huggingface-tutorial.mdx",
    "pr_number": 5068,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1142397231,
    "comment_created_at": "2023-03-20T16:30:09Z"
  },
  {
    "code": "@@ -3,60 +3,58 @@ title: Confluence\n sidebarTitle: Confluence\n ---\n \n-In this section, we present how to connect Confluence to MindsDB.\n-\n-[Confluence](https://www.atlassian.com/software/confluence) is a popular collaboration and documentation tool developed by Atlassian, a software company known for its suite of productivity and project management software. Confluence is designed to help teams and organizations collaborate, share information, and create and manage various types of content.\n-\n-Data from Confluence can be utilized within MindsDB to train AI models and make predictions.\n+This documentation describes the integration of MindsDB with [Confluence](https://www.atlassian.com/software/confluence), a popular collaboration and documentation tool developed by Atlassian.\n+The integration allows MindsDB to access data from Confluence and enhance it with AI capabilities.\n \n ## Prerequisites\n \n Before proceeding, ensure the following prerequisites are met:\n \n-1. Install MindsDB locally via [Docker](/setup/self-hosted/docker) or [Docker Desktop](/setup/self-hosted/docker-desktop).\n-2. To connect Confluence to MindsDB, install the required dependencies following [this instruction](/setup/self-hosted/docker#install-dependencies).\n-3. Install or ensure access to Confluence.\n+1. Install MindsDB locally via [Docker](https://docs.mindsdb.com/setup/self-hosted/docker) or [Docker Desktop](https://docs.mindsdb.com/setup/self-hosted/docker-desktop).",
    "comment": "As a 2nd prerequisite, you can mention this or similar: \"have access to Confluence instance\"",
    "line_number": 13,
    "enriched": "File: docs/integrations/app-integrations/confluence.mdx\nCode: @@ -3,60 +3,58 @@ title: Confluence\n sidebarTitle: Confluence\n ---\n \n-In this section, we present how to connect Confluence to MindsDB.\n-\n-[Confluence](https://www.atlassian.com/software/confluence) is a popular collaboration and documentation tool developed by Atlassian, a software company known for its suite of productivity and project management software. Confluence is designed to help teams and organizations collaborate, share information, and create and manage various types of content.\n-\n-Data from Confluence can be utilized within MindsDB to train AI models and make predictions.\n+This documentation describes the integration of MindsDB with [Confluence](https://www.atlassian.com/software/confluence), a popular collaboration and documentation tool developed by Atlassian.\n+The integration allows MindsDB to access data from Confluence and enhance it with AI capabilities.\n \n ## Prerequisites\n \n Before proceeding, ensure the following prerequisites are met:\n \n-1. Install MindsDB locally via [Docker](/setup/self-hosted/docker) or [Docker Desktop](/setup/self-hosted/docker-desktop).\n-2. To connect Confluence to MindsDB, install the required dependencies following [this instruction](/setup/self-hosted/docker#install-dependencies).\n-3. Install or ensure access to Confluence.\n+1. Install MindsDB locally via [Docker](https://docs.mindsdb.com/setup/self-hosted/docker) or [Docker Desktop](https://docs.mindsdb.com/setup/self-hosted/docker-desktop).\nComment: As a 2nd prerequisite, you can mention this or similar: \"have access to Confluence instance\"",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/integrations/app-integrations/confluence.mdx",
    "pr_number": 10603,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2012739346,
    "comment_created_at": "2025-03-25T18:42:04Z"
  },
  {
    "code": "@@ -0,0 +1,161 @@\n+---\n+title: Sentiment Analysis of Movie Reviews using OpenAI and MongoDB\n+sidebarTitle: Sentiment Analysis of Movie Reviews\n+---\n+\n+## Introduction\n+\n+In this blog post, we present how to create OpenAI models within MongoDB. This example is a sentiment analysis where we infer emotions behind a text i.e, Movie Reviews.\n+\n+- we'll use datasets from [`Hugging Face`](https://huggingface.co/datasets/rotten_tomatoes)\n+\n+## Prerequisites\n+\n+To follow along, you can sign up for an account at [cloud.mindsdb.com](https://cloud.mindsdb.com/register/nlp). Alternatively, head to [MindsDB documentation](https://docs.mindsdb.com/) and follow the instructions to manually set up a local instance of MindsDB via [Docker](/setup/self-hosted/docker) or [pip](/setup/self-hosted/pip/source).\n+\n+## How to Connect MindsDB to a Database\n+\n+We use a collection from our MongoDB public movie reviews database, so let’s start by connecting MindsDB to it.\n+\n+You can use [Mongo Compass](/connect/mongo-compass) or [Mongo Shell](/connect/mongo-shell) to connect your database, which we downloaded from [`Hugging Face`](https://huggingface.co/datasets/rotten_tomatoes), like this:\n+\n+## Connecting a Database\n+\n+We start by connecting a demo database using the CREATE DATABASE statement:\n+\n+- `host` : connection string for the mongodb\n+\n+```bash",
    "comment": "Please note that it is a tutorial using MongoDB. Thus, you should use MQL (and not SQL).\r\n\r\nPlease see an example here: https://docs.mindsdb.com/nlp/sentiment-analysis-inside-mongodb-with-openai",
    "line_number": 28,
    "enriched": "File: docs/tutorials/sentiment-analysis-of-movie-reviews.mdx\nCode: @@ -0,0 +1,161 @@\n+---\n+title: Sentiment Analysis of Movie Reviews using OpenAI and MongoDB\n+sidebarTitle: Sentiment Analysis of Movie Reviews\n+---\n+\n+## Introduction\n+\n+In this blog post, we present how to create OpenAI models within MongoDB. This example is a sentiment analysis where we infer emotions behind a text i.e, Movie Reviews.\n+\n+- we'll use datasets from [`Hugging Face`](https://huggingface.co/datasets/rotten_tomatoes)\n+\n+## Prerequisites\n+\n+To follow along, you can sign up for an account at [cloud.mindsdb.com](https://cloud.mindsdb.com/register/nlp). Alternatively, head to [MindsDB documentation](https://docs.mindsdb.com/) and follow the instructions to manually set up a local instance of MindsDB via [Docker](/setup/self-hosted/docker) or [pip](/setup/self-hosted/pip/source).\n+\n+## How to Connect MindsDB to a Database\n+\n+We use a collection from our MongoDB public movie reviews database, so let’s start by connecting MindsDB to it.\n+\n+You can use [Mongo Compass](/connect/mongo-compass) or [Mongo Shell](/connect/mongo-shell) to connect your database, which we downloaded from [`Hugging Face`](https://huggingface.co/datasets/rotten_tomatoes), like this:\n+\n+## Connecting a Database\n+\n+We start by connecting a demo database using the CREATE DATABASE statement:\n+\n+- `host` : connection string for the mongodb\n+\n+```bash\nComment: Please note that it is a tutorial using MongoDB. Thus, you should use MQL (and not SQL).\r\n\r\nPlease see an example here: https://docs.mindsdb.com/nlp/sentiment-analysis-inside-mongodb-with-openai",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/tutorials/sentiment-analysis-of-movie-reviews.mdx",
    "pr_number": 5038,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1139270796,
    "comment_created_at": "2023-03-16T19:32:21Z"
  },
  {
    "code": "@@ -47,3 +47,4 @@ langfuse==2.35.0\n lark\n prometheus-client==0.20.0\n transformers >= 4.42.4\n+openai",
    "comment": "this is causing some of the CI/CD to fail, we should remove it\r\nhttps://github.com/mindsdb/mindsdb/actions/runs/11035163652/job/30650481390?pr=9754",
    "line_number": 50,
    "enriched": "File: requirements/requirements.txt\nCode: @@ -47,3 +47,4 @@ langfuse==2.35.0\n lark\n prometheus-client==0.20.0\n transformers >= 4.42.4\n+openai\nComment: this is causing some of the CI/CD to fail, we should remove it\r\nhttps://github.com/mindsdb/mindsdb/actions/runs/11035163652/job/30650481390?pr=9754",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "requirements/requirements.txt",
    "pr_number": 9754,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1775405086,
    "comment_created_at": "2024-09-25T14:57:29Z"
  },
  {
    "code": "@@ -0,0 +1,84 @@\n+---\n+title: Symbl\n+sidebarTitle: Symbl\n+---\n+\n+This documentation describes the integration of MindsDB with [Symbl](https://symbl.ai/), a platform with with state-of-the-art and task specific LLMs, enables businesses to analyze multi-party conversations at scale.",
    "comment": "There are some typos in this line. Let's replace it with this:\r\n```\r\nThis documentation describes the integration of MindsDB with [Symbl](https://symbl.ai/), a platform with state-of-the-art and task-specific LLMs that enables businesses to analyze multi-party conversations at scale.\r\n```",
    "line_number": 6,
    "enriched": "File: docs/integrations/app-integrations/symbl.mdx\nCode: @@ -0,0 +1,84 @@\n+---\n+title: Symbl\n+sidebarTitle: Symbl\n+---\n+\n+This documentation describes the integration of MindsDB with [Symbl](https://symbl.ai/), a platform with with state-of-the-art and task specific LLMs, enables businesses to analyze multi-party conversations at scale.\nComment: There are some typos in this line. Let's replace it with this:\r\n```\r\nThis documentation describes the integration of MindsDB with [Symbl](https://symbl.ai/), a platform with state-of-the-art and task-specific LLMs that enables businesses to analyze multi-party conversations at scale.\r\n```",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/integrations/app-integrations/symbl.mdx",
    "pr_number": 9629,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1718203841,
    "comment_created_at": "2024-08-15T09:50:39Z"
  },
  {
    "code": "@@ -0,0 +1,63 @@\n+---\n+title: EdgelessDB\n+sidebarTitle: EdgelessDB\n+---\n+\n+This is the implementation of the EdgelessDB data handler for MindsDB.\n+\n+[Edgeless](https://edgeless.systems/) is a full SQL database, tailor-made for confidential computing. It seamlessly integrates with your existing tools and workflows to help you unlock the full potential of your data.\n+\n+## Implementation\n+",
    "comment": "Please add this line as a first paragraph here:\r\n```\r\nThis handler is implemented by extending the MySQL connector.\r\n```",
    "line_number": 11,
    "enriched": "File: docs/data-integrations/edgelessdb.mdx\nCode: @@ -0,0 +1,63 @@\n+---\n+title: EdgelessDB\n+sidebarTitle: EdgelessDB\n+---\n+\n+This is the implementation of the EdgelessDB data handler for MindsDB.\n+\n+[Edgeless](https://edgeless.systems/) is a full SQL database, tailor-made for confidential computing. It seamlessly integrates with your existing tools and workflows to help you unlock the full potential of your data.\n+\n+## Implementation\n+\nComment: Please add this line as a first paragraph here:\r\n```\r\nThis handler is implemented by extending the MySQL connector.\r\n```",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/data-integrations/edgelessdb.mdx",
    "pr_number": 6874,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1266785980,
    "comment_created_at": "2023-07-18T13:35:47Z"
  },
  {
    "code": "@@ -4,10 +4,9 @@ permissions:\n   contents: read\n \n on:\n-  pull_request_target:\n-    types: [opened, reopened, synchronize, labeled]\n+  push:\n     branches:\n-      - 'main'\n+      - '**' # This pattern matches all branches",
    "comment": "**Security**: Workflow now triggers on push to all branches ('**') without proper security controls, allowing any branch to trigger builds and deployments\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\nname: Build and Deploy Development\n\npermissions:\n  contents: read\n\non:\n  # SECURITY FIX: Restrict to specific branches instead of all branches\n  push:\n    branches:\n      - 'main'\n      - 'staging'\n      - 'dev'\n```\n</details>\n<!-- suggestion_end -->\n",
    "line_number": 9,
    "enriched": "File: .github/workflows/build_deploy_dev.yml\nCode: @@ -4,10 +4,9 @@ permissions:\n   contents: read\n \n on:\n-  pull_request_target:\n-    types: [opened, reopened, synchronize, labeled]\n+  push:\n     branches:\n-      - 'main'\n+      - '**' # This pattern matches all branches\nComment: **Security**: Workflow now triggers on push to all branches ('**') without proper security controls, allowing any branch to trigger builds and deployments\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\nname: Build and Deploy Development\n\npermissions:\n  contents: read\n\non:\n  # SECURITY FIX: Restrict to specific branches instead of all branches\n  push:\n    branches:\n      - 'main'\n      - 'staging'\n      - 'dev'\n```\n</details>\n<!-- suggestion_end -->\n",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": ".github/workflows/build_deploy_dev.yml",
    "pr_number": 11757,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2437340476,
    "comment_created_at": "2025-10-16T20:09:10Z"
  },
  {
    "code": "@@ -0,0 +1,95 @@\n+---\n+title: Google Cloud Storage\n+sidebarTitle: Google Cloud Storage\n+---\n+\n+This documentation describes the integration of MindsDB with [Google Cloud Storage](https://cloud.google.com/storage), an object storage service that offers industry-leading scalability, data availability, security, and performance.\n+\n+## Prerequisites\n+\n+1. Before proceeding, ensure that MindsDB is installed locally via [Docker](/setup/self-hosted/docker) or [Docker Desktop](/setup/self-hosted/docker-desktop).\n+2. Install the dependencies from the `requirements.txt` file. Run the following command",
    "comment": "Can you please replace this second step with what is used throughout our documentation? You can refer [this](https://docs.mindsdb.com/integrations/data-integrations/google-bigquery).",
    "line_number": 11,
    "enriched": "File: mindsdb/integrations/handlers/gcs_handler/README.md\nCode: @@ -0,0 +1,95 @@\n+---\n+title: Google Cloud Storage\n+sidebarTitle: Google Cloud Storage\n+---\n+\n+This documentation describes the integration of MindsDB with [Google Cloud Storage](https://cloud.google.com/storage), an object storage service that offers industry-leading scalability, data availability, security, and performance.\n+\n+## Prerequisites\n+\n+1. Before proceeding, ensure that MindsDB is installed locally via [Docker](/setup/self-hosted/docker) or [Docker Desktop](/setup/self-hosted/docker-desktop).\n+2. Install the dependencies from the `requirements.txt` file. Run the following command\nComment: Can you please replace this second step with what is used throughout our documentation? You can refer [this](https://docs.mindsdb.com/integrations/data-integrations/google-bigquery).",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "mindsdb/integrations/handlers/gcs_handler/README.md",
    "pr_number": 9886,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1802865313,
    "comment_created_at": "2024-10-16T10:49:01Z"
  },
  {
    "code": "@@ -289,9 +289,8 @@ def select(self, query, disable_reranking=False):\n             FilterOperator.GREATER_THAN.value,\n         ]\n         gt_filtering = False",
    "comment": "**Correctness**: Removal of hybrid_search_enabled_flag changes API compatibility\n\n",
    "line_number": 291,
    "enriched": "File: mindsdb/interfaces/knowledge_base/controller.py\nCode: @@ -289,9 +289,8 @@ def select(self, query, disable_reranking=False):\n             FilterOperator.GREATER_THAN.value,\n         ]\n         gt_filtering = False\nComment: **Correctness**: Removal of hybrid_search_enabled_flag changes API compatibility\n\n",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/interfaces/knowledge_base/controller.py",
    "pr_number": 11759,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2440253429,
    "comment_created_at": "2025-10-17T14:39:15Z"
  },
  {
    "code": "@@ -471,6 +472,8 @@ def drop_table(self, table_name: str, if_exists=True):\n         self.connect()\n         try:\n             self._client.delete_collection(table_name)\n+            if \"persist_directory\" in self._client_config:",
    "comment": "persist_directory is always in self._client_config. is better to check if it is none\r\n\r\nalso, I think we can do only for folders that are in self.handler_storage\r\nbecause you can use local folder and create two collection (tables) there\r\nafter deletion of the first table the second shouln't be deleted",
    "line_number": 475,
    "enriched": "File: mindsdb/integrations/handlers/chromadb_handler/chromadb_handler.py\nCode: @@ -471,6 +472,8 @@ def drop_table(self, table_name: str, if_exists=True):\n         self.connect()\n         try:\n             self._client.delete_collection(table_name)\n+            if \"persist_directory\" in self._client_config:\nComment: persist_directory is always in self._client_config. is better to check if it is none\r\n\r\nalso, I think we can do only for folders that are in self.handler_storage\r\nbecause you can use local folder and create two collection (tables) there\r\nafter deletion of the first table the second shouln't be deleted",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/chromadb_handler/chromadb_handler.py",
    "pr_number": 11923,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2564677452,
    "comment_created_at": "2025-11-26T11:48:35Z"
  },
  {
    "code": "@@ -87,7 +87,9 @@ def _init_client(self):\n             elif self.provider in (\"openai\", \"ollama\"):\n                 if self.provider == \"ollama\":\n                     self.method = \"no-logprobs\"",
    "comment": "since we test for `no-logprobs` - then may be delete it from here?",
    "line_number": 89,
    "enriched": "File: mindsdb/integrations/utilities/rag/rerankers/base_reranker.py\nCode: @@ -87,7 +87,9 @@ def _init_client(self):\n             elif self.provider in (\"openai\", \"ollama\"):\n                 if self.provider == \"ollama\":\n                     self.method = \"no-logprobs\"\nComment: since we test for `no-logprobs` - then may be delete it from here?",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/utilities/rag/rerankers/base_reranker.py",
    "pr_number": 11661,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2409600277,
    "comment_created_at": "2025-10-07T07:01:20Z"
  },
  {
    "code": "@@ -120,8 +111,8 @@ def connect(self):\n             self.is_connected = True\n             return self._client\n         except Exception as e:\n+            log.logger.error(f\"Error connecting to ChromaDB client, {e}!\")",
    "comment": "Just an FYI, this way of logging does not work. This [PR](https://github.com/mindsdb/mindsdb/pull/8371) will fix it.\r\n\r\nYou need to do:\r\n\r\n```python\r\nfrom mindsdb.utilities import log\r\nlogger = log.getLogger(__name__)\r\n...\r\n...\r\nlogger.error(...)\r\n```",
    "line_number": 114,
    "enriched": "File: mindsdb/integrations/handlers/chromadb_handler/chromadb_handler.py\nCode: @@ -120,8 +111,8 @@ def connect(self):\n             self.is_connected = True\n             return self._client\n         except Exception as e:\n+            log.logger.error(f\"Error connecting to ChromaDB client, {e}!\")\nComment: Just an FYI, this way of logging does not work. This [PR](https://github.com/mindsdb/mindsdb/pull/8371) will fix it.\r\n\r\nYou need to do:\r\n\r\n```python\r\nfrom mindsdb.utilities import log\r\nlogger = log.getLogger(__name__)\r\n...\r\n...\r\nlogger.error(...)\r\n```",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/chromadb_handler/chromadb_handler.py",
    "pr_number": 8269,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1397731509,
    "comment_created_at": "2023-11-17T18:40:53Z"
  },
  {
    "code": "@@ -371,42 +374,55 @@ def get_tables(self):\n \n         return data\n \n-    def get_columns(self, table_name: str):\n-        # at the moment it works only for models\n-        predictor_record = db.Predictor.query.filter_by(\n-            company_id=ctx.company_id,\n-            project_id=self.id,\n-            name=table_name\n-        ).first()\n+    def get_columns(self, table_name: str) -> list[str] | None:\n         columns = []\n-        if predictor_record is not None:\n-            if isinstance(predictor_record.dtype_dict, dict):\n-                columns = list(predictor_record.dtype_dict.keys())\n-            elif predictor_record.to_predict is not None:\n-                # no dtype_dict, use target\n-                columns = predictor_record.to_predict\n-                if not isinstance(columns, list):\n-                    columns = [columns]\n+        tables = self.get_tables()\n+        table = tables.get(table_name)",
    "comment": "maybe to add case independency? \r\n`show columns from kb3`\r\nbut with upper-case - not\r\n`show columns from KB3`\r\n",
    "line_number": 380,
    "enriched": "File: mindsdb/interfaces/database/projects.py\nCode: @@ -371,42 +374,55 @@ def get_tables(self):\n \n         return data\n \n-    def get_columns(self, table_name: str):\n-        # at the moment it works only for models\n-        predictor_record = db.Predictor.query.filter_by(\n-            company_id=ctx.company_id,\n-            project_id=self.id,\n-            name=table_name\n-        ).first()\n+    def get_columns(self, table_name: str) -> list[str] | None:\n         columns = []\n-        if predictor_record is not None:\n-            if isinstance(predictor_record.dtype_dict, dict):\n-                columns = list(predictor_record.dtype_dict.keys())\n-            elif predictor_record.to_predict is not None:\n-                # no dtype_dict, use target\n-                columns = predictor_record.to_predict\n-                if not isinstance(columns, list):\n-                    columns = [columns]\n+        tables = self.get_tables()\n+        table = tables.get(table_name)\nComment: maybe to add case independency? \r\n`show columns from kb3`\r\nbut with upper-case - not\r\n`show columns from KB3`\r\n",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/interfaces/database/projects.py",
    "pr_number": 10929,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2115474490,
    "comment_created_at": "2025-05-30T09:11:12Z"
  },
  {
    "code": "@@ -67,8 +67,10 @@ def connect(self) -> StatusResponse:\n \n         connection_kwargs = {}\n \n-        if self.connection_data.get(\"api_key\", None):\n-            connection_kwargs[\"login_or_token\"] = self.connection_data[\"api_key\"]\n+        # Support both 'api_key' and 'token' parameters for authentication\n+        api_key = self.connection_data.get(\"api_key\") or self.connection_data.get(\"token\")\n+        if api_key:\n+            connection_kwargs[\"login_or_token\"] = api_key",
    "comment": "**correctness**: `self.connection_data.get('api_key') or self.connection_data.get('token')` will treat an empty string as falsy, so if 'api_key' is set to '' and 'token' is valid, it will use 'token' (possibly unexpected if user explicitly set 'api_key').\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/integrations/handlers/github_handler/github_handler.py, lines 71-73, the code uses `api_key = self.connection_data.get('api_key') or self.connection_data.get('token')`, which will use 'token' if 'api_key' is set to an empty string, even if the user explicitly set 'api_key'. Change this logic to check for None explicitly: if 'api_key' is present (even if empty), use it; otherwise, use 'token' if present. Update the code to:\n\nif self.connection_data.get(\"api_key\") is not None:\n    connection_kwargs[\"login_or_token\"] = self.connection_data[\"api_key\"]\nelif self.connection_data.get(\"token\") is not None:\n    connection_kwargs[\"login_or_token\"] = self.connection_data[\"token\"]\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n        if self.connection_data.get(\"api_key\") is not None:\n            connection_kwargs[\"login_or_token\"] = self.connection_data[\"api_key\"]\n        elif self.connection_data.get(\"token\") is not None:\n            connection_kwargs[\"login_or_token\"] = self.connection_data[\"token\"]\n```\n</details>\n<!-- suggestion_end -->\n",
    "line_number": 73,
    "enriched": "File: mindsdb/integrations/handlers/github_handler/github_handler.py\nCode: @@ -67,8 +67,10 @@ def connect(self) -> StatusResponse:\n \n         connection_kwargs = {}\n \n-        if self.connection_data.get(\"api_key\", None):\n-            connection_kwargs[\"login_or_token\"] = self.connection_data[\"api_key\"]\n+        # Support both 'api_key' and 'token' parameters for authentication\n+        api_key = self.connection_data.get(\"api_key\") or self.connection_data.get(\"token\")\n+        if api_key:\n+            connection_kwargs[\"login_or_token\"] = api_key\nComment: **correctness**: `self.connection_data.get('api_key') or self.connection_data.get('token')` will treat an empty string as falsy, so if 'api_key' is set to '' and 'token' is valid, it will use 'token' (possibly unexpected if user explicitly set 'api_key').\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/integrations/handlers/github_handler/github_handler.py, lines 71-73, the code uses `api_key = self.connection_data.get('api_key') or self.connection_data.get('token')`, which will use 'token' if 'api_key' is set to an empty string, even if the user explicitly set 'api_key'. Change this logic to check for None explicitly: if 'api_key' is present (even if empty), use it; otherwise, use 'token' if present. Update the code to:\n\nif self.connection_data.get(\"api_key\") is not None:\n    connection_kwargs[\"login_or_token\"] = self.connection_data[\"api_key\"]\nelif self.connection_data.get(\"token\") is not None:\n    connection_kwargs[\"login_or_token\"] = self.connection_data[\"token\"]\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n        if self.connection_data.get(\"api_key\") is not None:\n            connection_kwargs[\"login_or_token\"] = self.connection_data[\"api_key\"]\n        elif self.connection_data.get(\"token\") is not None:\n            connection_kwargs[\"login_or_token\"] = self.connection_data[\"token\"]\n```\n</details>\n<!-- suggestion_end -->\n",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/github_handler/github_handler.py",
    "pr_number": 11525,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2324765825,
    "comment_created_at": "2025-09-05T10:53:52Z"
  },
  {
    "code": "@@ -16,6 +16,11 @@ CREATE ML_ENGINE openai2\n FROM openai\n USING\n   api_key = 'YOUR_OPENAI_API_KEY';\n+  <Tip>",
    "comment": "Please take it out of the code block, so it is like this:\r\n\r\n> FROM openai\r\n> USING\r\n>   api_key = 'YOUR_OPENAI_API_KEY';\r\n> \\```\r\n>   \\<Tip>\r\n> \\*\\*MindsDB Pro**\r\n> Please note that it is required to provide an OpenAI API key when using MindsDB Pro version.\r\n> \\</Tip>\r\n",
    "line_number": 19,
    "enriched": "File: docs/custom-model/openai.mdx\nCode: @@ -16,6 +16,11 @@ CREATE ML_ENGINE openai2\n FROM openai\n USING\n   api_key = 'YOUR_OPENAI_API_KEY';\n+  <Tip>\nComment: Please take it out of the code block, so it is like this:\r\n\r\n> FROM openai\r\n> USING\r\n>   api_key = 'YOUR_OPENAI_API_KEY';\r\n> \\```\r\n>   \\<Tip>\r\n> \\*\\*MindsDB Pro**\r\n> Please note that it is required to provide an OpenAI API key when using MindsDB Pro version.\r\n> \\</Tip>\r\n",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "docs/custom-model/openai.mdx",
    "pr_number": 5779,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1175540447,
    "comment_created_at": "2023-04-24T16:38:05Z"
  },
  {
    "code": "@@ -33,6 +50,113 @@ def clear_filename(filename: str) -> str:\n     return filename\n \n \n+def _encrypt_v1(data: bytes) -> str:\n+    protocol_version = 1\n+\n+    key = ctx.encryption_key_bytes\n+    if key is None:\n+        raise Exception(\"Encryption key not found\")\n+\n+    nonce = secrets.token_bytes(12)\n+    encrypted_bytes = nonce + AESGCM(key).encrypt(nonce, data, None)\n+    encrypted_str = base64.b64encode(encrypted_bytes).decode('utf-8')\n+    return ENCRYPT_PREFIX + chr(protocol_version) + encrypted_str\n+\n+\n+def _decrypt(data) -> bytes:\n+    if not data.startswith(ENCRYPT_PREFIX):\n+        # not encrypted\n+        return data.encode('utf-8')\n+\n+    key = ctx.encryption_key_bytes\n+    if key is None:\n+        raise Exception(\"Encryption key not found\")\n+\n+    offset = len(ENCRYPT_PREFIX)\n+    protocol_version = ord(data[offset: offset + 1])\n+\n+    if protocol_version == 1:\n+\n+        encrypted_message = base64.b64decode(data[offset + 1:].encode('utf-8'))\n+        try:\n+            decrypted_message = AESGCM(key).decrypt(encrypted_message[:12], encrypted_message[12:], None)\n+        except Exception as e:\n+            raise Exception(\"Wrong encryption key\") from e\n+    else:\n+        raise ValueError(f'Encrypted object protocol version is unknown: {protocol_version}')\n+\n+    return decrypted_message\n+\n+\n+def encrypt_object(data: object) -> Union[object]:\n+    \"\"\"Serialize object to encrypted string.\n+    If encryption is not enabled it returns the same object\n+\n+    Args:\n+        data (object): any object that can be pickled\n+\n+    Returns:\n+        str or object: encrypted string or input object\n+    \"\"\"\n+    if not Config().encryption_enabled:\n+        return data\n+\n+    message_bytes = pickle.dumps(data)\n+    return {\n+        \"MDB_ENC\": _encrypt_v1(message_bytes)",
    "comment": "should it be `ENCRYPT_JSON_KEY`?",
    "line_number": 106,
    "enriched": "File: mindsdb/utilities/security.py\nCode: @@ -33,6 +50,113 @@ def clear_filename(filename: str) -> str:\n     return filename\n \n \n+def _encrypt_v1(data: bytes) -> str:\n+    protocol_version = 1\n+\n+    key = ctx.encryption_key_bytes\n+    if key is None:\n+        raise Exception(\"Encryption key not found\")\n+\n+    nonce = secrets.token_bytes(12)\n+    encrypted_bytes = nonce + AESGCM(key).encrypt(nonce, data, None)\n+    encrypted_str = base64.b64encode(encrypted_bytes).decode('utf-8')\n+    return ENCRYPT_PREFIX + chr(protocol_version) + encrypted_str\n+\n+\n+def _decrypt(data) -> bytes:\n+    if not data.startswith(ENCRYPT_PREFIX):\n+        # not encrypted\n+        return data.encode('utf-8')\n+\n+    key = ctx.encryption_key_bytes\n+    if key is None:\n+        raise Exception(\"Encryption key not found\")\n+\n+    offset = len(ENCRYPT_PREFIX)\n+    protocol_version = ord(data[offset: offset + 1])\n+\n+    if protocol_version == 1:\n+\n+        encrypted_message = base64.b64decode(data[offset + 1:].encode('utf-8'))\n+        try:\n+            decrypted_message = AESGCM(key).decrypt(encrypted_message[:12], encrypted_message[12:], None)\n+        except Exception as e:\n+            raise Exception(\"Wrong encryption key\") from e\n+    else:\n+        raise ValueError(f'Encrypted object protocol version is unknown: {protocol_version}')\n+\n+    return decrypted_message\n+\n+\n+def encrypt_object(data: object) -> Union[object]:\n+    \"\"\"Serialize object to encrypted string.\n+    If encryption is not enabled it returns the same object\n+\n+    Args:\n+        data (object): any object that can be pickled\n+\n+    Returns:\n+        str or object: encrypted string or input object\n+    \"\"\"\n+    if not Config().encryption_enabled:\n+        return data\n+\n+    message_bytes = pickle.dumps(data)\n+    return {\n+        \"MDB_ENC\": _encrypt_v1(message_bytes)\nComment: should it be `ENCRYPT_JSON_KEY`?",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/utilities/security.py",
    "pr_number": 9705,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1796736578,
    "comment_created_at": "2024-10-11T10:06:36Z"
  },
  {
    "code": "@@ -1,68 +1,114 @@\n from collections import defaultdict\n-from datetime import datetime\n-from pathlib import Path\n \n-import pandas as pd\n-from langchain.chains import RetrievalQA\n from langchain.llms import Writer\n \n+from mindsdb.integrations.handlers.writer_handler.settings import (\n+    VectorStoreConfig,\n+    VectorStoreIndexConfig,\n+    VectorStoreIndexLoader,\n+    VectorStoreLoader,\n+    WriterHandlerParameters,\n+    load_embeddings_model,\n+)\n from mindsdb.utilities.log import get_log\n \n-from .settings import DEFAULT_EMBEDDINGS_MODEL, ModelParameters, get_retriever\n-\n logger = get_log(logger_name=__name__)\n \n \n class QuestionAnswerer:\n-    def __init__(self, args: dict, model_parameters: ModelParameters):\n+    def __init__(self, args: WriterHandlerParameters):\n+\n         self.output_data = defaultdict(list)\n \n-        self.embeddings_model_name = args.get(\n-            \"embeddings_model_name\", DEFAULT_EMBEDDINGS_MODEL\n-        )\n-        self.persist_directory = args[\"chromadb_storage_path\"]\n+        self.args = args\n+\n+        self.embeddings_model = load_embeddings_model(args.embeddings_model_name)\n \n-        llm = Writer(**model_parameters.dict())\n-        retriever = get_retriever(\n-            embeddings_model_name=self.embeddings_model_name,\n+        self.persist_directory = args.vector_store_storage_path\n+\n+        self.collection_or_index_name = args.collection_or_index_name\n+\n+        vector_store_config = VectorStoreConfig(\n+            vector_store_name=args.vector_store_name,\n+            embeddings_model=self.embeddings_model,\n             persist_directory=self.persist_directory,\n-            collection_name=args.get(\"collection_name\", \"langchain\"),\n+            collection_or_index_name=self.collection_or_index_name,\n+        )\n+\n+        vector_store_loader = VectorStoreLoader(vector_store_config)\n+\n+        self.vector_store = vector_store_loader.load_vector_store()\n+\n+        if args.use_index:\n+\n+            vector_store_index_config = VectorStoreIndexConfig(\n+                vector_store_name=args.vector_store_name,\n+                vector_store=self.vector_store,\n+                embeddings_model=self.embeddings_model,\n+                persist_directory=self.persist_directory,\n+                collection_or_index_name=args.collection_or_index_name,\n+                index_name=args.index_name,\n+            )\n+            vector_store_index_loader = VectorStoreIndexLoader(\n+                vector_store_index_config\n+            )\n+\n+            self.index = vector_store_index_loader.load_vector_store_index()\n+\n+        self.prompt_template = args.prompt_template\n+\n+        self.llm = Writer(**args.llm_params.dict())\n+\n+    def _prepare_prompt(self, vector_store_response, question):\n+\n+        # todo ensure contexts don't exceed max length\n+        # todo maybe use a selector model, summarizer or otherwise truncate contexts\n+        context = [doc.page_content for doc in vector_store_response]\n+\n+        combined_context = \"\\n\\n\".join(context)\n+\n+        return self.prompt_template.format(question=question, context=combined_context)\n+\n+    def _query_index(self, question: str):\n+\n+        return self.index.query(\n+            question,\n         )\n \n-        self.qa = RetrievalQA.from_chain_type(",
    "comment": "remove some of the langchain coupling",
    "line_number": 32,
    "enriched": "File: mindsdb/integrations/handlers/writer_handler/question_answer.py\nCode: @@ -1,68 +1,114 @@\n from collections import defaultdict\n-from datetime import datetime\n-from pathlib import Path\n \n-import pandas as pd\n-from langchain.chains import RetrievalQA\n from langchain.llms import Writer\n \n+from mindsdb.integrations.handlers.writer_handler.settings import (\n+    VectorStoreConfig,\n+    VectorStoreIndexConfig,\n+    VectorStoreIndexLoader,\n+    VectorStoreLoader,\n+    WriterHandlerParameters,\n+    load_embeddings_model,\n+)\n from mindsdb.utilities.log import get_log\n \n-from .settings import DEFAULT_EMBEDDINGS_MODEL, ModelParameters, get_retriever\n-\n logger = get_log(logger_name=__name__)\n \n \n class QuestionAnswerer:\n-    def __init__(self, args: dict, model_parameters: ModelParameters):\n+    def __init__(self, args: WriterHandlerParameters):\n+\n         self.output_data = defaultdict(list)\n \n-        self.embeddings_model_name = args.get(\n-            \"embeddings_model_name\", DEFAULT_EMBEDDINGS_MODEL\n-        )\n-        self.persist_directory = args[\"chromadb_storage_path\"]\n+        self.args = args\n+\n+        self.embeddings_model = load_embeddings_model(args.embeddings_model_name)\n \n-        llm = Writer(**model_parameters.dict())\n-        retriever = get_retriever(\n-            embeddings_model_name=self.embeddings_model_name,\n+        self.persist_directory = args.vector_store_storage_path\n+\n+        self.collection_or_index_name = args.collection_or_index_name\n+\n+        vector_store_config = VectorStoreConfig(\n+            vector_store_name=args.vector_store_name,\n+            embeddings_model=self.embeddings_model,\n             persist_directory=self.persist_directory,\n-            collection_name=args.get(\"collection_name\", \"langchain\"),\n+            collection_or_index_name=self.collection_or_index_name,\n+        )\n+\n+        vector_store_loader = VectorStoreLoader(vector_store_config)\n+\n+        self.vector_store = vector_store_loader.load_vector_store()\n+\n+        if args.use_index:\n+\n+            vector_store_index_config = VectorStoreIndexConfig(\n+                vector_store_name=args.vector_store_name,\n+                vector_store=self.vector_store,\n+                embeddings_model=self.embeddings_model,\n+                persist_directory=self.persist_directory,\n+                collection_or_index_name=args.collection_or_index_name,\n+                index_name=args.index_name,\n+            )\n+            vector_store_index_loader = VectorStoreIndexLoader(\n+                vector_store_index_config\n+            )\n+\n+            self.index = vector_store_index_loader.load_vector_store_index()\n+\n+        self.prompt_template = args.prompt_template\n+\n+        self.llm = Writer(**args.llm_params.dict())\n+\n+    def _prepare_prompt(self, vector_store_response, question):\n+\n+        # todo ensure contexts don't exceed max length\n+        # todo maybe use a selector model, summarizer or otherwise truncate contexts\n+        context = [doc.page_content for doc in vector_store_response]\n+\n+        combined_context = \"\\n\\n\".join(context)\n+\n+        return self.prompt_template.format(question=question, context=combined_context)\n+\n+    def _query_index(self, question: str):\n+\n+        return self.index.query(\n+            question,\n         )\n \n-        self.qa = RetrievalQA.from_chain_type(\nComment: remove some of the langchain coupling",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/writer_handler/question_answer.py",
    "pr_number": 7150,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1300427764,
    "comment_created_at": "2023-08-21T17:19:55Z"
  },
  {
    "code": "@@ -118,203 +87,56 @@ def invoke(self, query, session_id) -> Dict[str, Any]:\n                 \"parts\": [{\"type\": \"text\", \"text\": error_msg}],\n             }\n \n-    def streaming_invoke(self, messages: List[dict], timeout: int = DEFAULT_STREAM_TIMEOUT) -> Iterator[Dict[str, Any]]:\n-        \"\"\"Stream responses from the MindsDB agent using the direct API endpoint.\n-\n-        Args:\n-            messages: List of message dictionaries, each containing 'question' and optionally 'answer'.\n-                Example: [{'question': 'what is the average rental price for a three bedroom?', 'answer': None}]\n-            timeout: Request timeout in seconds (default: 300)\n-\n-        Returns:\n-            Iterator yielding chunks of the streaming response.\n-        \"\"\"\n-        try:\n-            # Construct the URL for the streaming completions endpoint\n-            url = f\"{self.base_url}/api/projects/{self.project_name}/agents/{self.agent_name}/completions/stream\"\n-\n-            # Log request for debugging\n-            logger.info(f\"Sending streaming request to MindsDB agent: {self.agent_name}\")\n-            logger.debug(f\"Request messages: {json.dumps(messages)[:200]}...\")\n-\n-            # Send the request to MindsDB streaming API with timeout\n-            stream = requests.post(url, json={\"messages\": messages}, stream=True, timeout=timeout)\n-            stream.raise_for_status()\n-\n-            # Process the streaming response directly\n-            for line in stream.iter_lines():\n-                if line:\n-                    # Parse each non-empty line\n-                    try:\n-                        line = line.decode(\"utf-8\")\n-                        if line.startswith(\"data: \"):\n-                            # Extract the JSON data from the line that starts with 'data: '\n-                            data = line[6:]  # Remove 'data: ' prefix\n-                            try:\n-                                chunk = json.loads(data)\n-                                # Pass through the chunk with minimal modifications\n-                                yield chunk\n-                            except json.JSONDecodeError as e:\n-                                logger.warning(f\"Failed to parse JSON from line: {data}. Error: {str(e)}\")\n-                                # Yield error information but continue processing\n-                                yield {\n-                                    \"error\": f\"JSON parse error: {str(e)}\",\n-                                    \"data\": data,\n-                                    \"is_task_complete\": False,\n-                                    \"parts\": [\n-                                        {\n-                                            \"type\": \"text\",\n-                                            \"text\": f\"Error parsing response: {str(e)}\",\n-                                        }\n-                                    ],\n-                                    \"metadata\": {},\n-                                }\n-                        else:\n-                            # Log other lines for debugging\n-                            logger.debug(f\"Received non-data line: {line}\")\n-\n-                            # If it looks like a raw text response (not SSE format), wrap it\n-                            if not line.startswith(\"event:\") and not line.startswith(\":\"):\n-                                yield {\"content\": line, \"is_task_complete\": False}\n-                    except UnicodeDecodeError as e:\n-                        logger.warning(f\"Failed to decode line: {str(e)}\")\n-                        # Continue processing despite decode errors\n-\n-        except requests.exceptions.Timeout as e:\n-            error_msg = f\"Request timed out after {timeout} seconds: {str(e)}\"\n-            logger.error(error_msg)\n-            yield {\n-                \"content\": error_msg,\n-                \"parts\": [{\"type\": \"text\", \"text\": error_msg}],\n-                \"is_task_complete\": True,\n-                \"error\": \"timeout\",\n-                \"metadata\": {\"error\": True},\n-            }\n-\n-        except requests.exceptions.ChunkedEncodingError as e:\n-            error_msg = f\"Stream was interrupted: {str(e)}\"\n-            logger.error(error_msg)\n-            yield {\n-                \"content\": error_msg,\n-                \"parts\": [{\"type\": \"text\", \"text\": error_msg}],\n-                \"is_task_complete\": True,\n-                \"error\": \"stream_interrupted\",\n-                \"metadata\": {\"error\": True},\n-            }\n-\n-        except requests.exceptions.ConnectionError as e:\n-            error_msg = f\"Connection error: {str(e)}\"\n-            logger.error(error_msg)\n-            yield {\n-                \"content\": error_msg,\n-                \"parts\": [{\"type\": \"text\", \"text\": error_msg}],\n-                \"is_task_complete\": True,\n-                \"error\": \"connection_error\",\n-                \"metadata\": {\"error\": True},\n-            }\n-\n-        except requests.exceptions.RequestException as e:\n-            error_msg = f\"Error connecting to MindsDB streaming API: {str(e)}\"\n-            logger.error(error_msg)\n-            yield {\n-                \"content\": error_msg,\n-                \"parts\": [{\"type\": \"text\", \"text\": error_msg}],\n-                \"is_task_complete\": True,\n-                \"error\": \"request_error\",\n-                \"metadata\": {\"error\": True},\n-            }\n-\n-        except Exception as e:\n-            error_msg = f\"Error in streaming: {str(e)}\"\n-            logger.error(error_msg)\n-            yield {\n-                \"content\": error_msg,\n-                \"parts\": [{\"type\": \"text\", \"text\": error_msg}],\n-                \"is_task_complete\": True,\n-                \"error\": \"unknown_error\",\n-                \"metadata\": {\"error\": True},\n-            }\n-\n-        # Send a final completion message\n-        yield {\"is_task_complete\": True, \"metadata\": {\"complete\": True}}\n+    async def streaming_invoke(self, messages, timeout=60):\n+        url = f\"{self.base_url}/api/projects/{self.project_name}/agents/{self.agent_name}/completions/stream\"\n+        logger.info(f\"Sending streaming request to MindsDB agent: {self.agent_name}\")\n+        async with httpx.AsyncClient(timeout=timeout) as client:\n+            async with client.stream(\"POST\", url, json={\"messages\": to_serializable(messages)}) as response:\n+                response.raise_for_status()\n+                async for line in response.aiter_lines():\n+                    if line.strip():\n+                        yield line\n \n     async def stream(\n         self,\n         query: str,\n         session_id: str,\n         history: List[dict] | None = None,\n     ) -> AsyncIterable[Dict[str, Any]]:\n-        \"\"\"Stream responses from the MindsDB agent (uses streaming API endpoint).\n-\n-        Args:\n-            query: The current query to send to the agent.\n-            session_id: Unique identifier for the conversation session.\n-            history: Optional list of previous messages in the conversation.\n-\n-        Returns:\n-            AsyncIterable yielding chunks of the streaming response.\n-        \"\"\"\n+        \"\"\"Stream responses from the MindsDB agent (uses streaming API endpoint).\"\"\"\n         try:\n             logger.info(f\"Using streaming API for query: {query[:100]}...\")\n-\n-            # Format history into the expected format\n             formatted_messages = []\n             if history:\n                 for msg in history:\n-                    # Convert Message object to dict if needed\n                     msg_dict = msg.dict() if hasattr(msg, \"dict\") else msg\n                     role = msg_dict.get(\"role\", \"user\")\n-\n-                    # Extract text from parts\n                     text = \"\"\n                     for part in msg_dict.get(\"parts\", []):\n                         if part.get(\"type\") == \"text\":\n                             text = part.get(\"text\", \"\")\n                             break\n-\n                     if text:\n                         if role == \"user\":\n                             formatted_messages.append({\"question\": text, \"answer\": None})\n                         elif role == \"assistant\" and formatted_messages:\n-                            # Add the answer to the last question\n                             formatted_messages[-1][\"answer\"] = text\n-\n-            # Add the current query to the messages\n             formatted_messages.append({\"question\": query, \"answer\": None})\n-\n             logger.debug(f\"Formatted messages for agent: {formatted_messages}\")\n-\n-            # Use the streaming_invoke method to get real streaming responses\n             streaming_response = self.streaming_invoke(formatted_messages)\n-\n-            # Yield all chunks directly from the streaming response\n             for chunk in streaming_response:\n-                # Only add required fields if they don't exist\n-                # This preserves the original structure as much as possible\n-                if \"is_task_complete\" not in chunk:\n-                    chunk[\"is_task_complete\"] = False\n-\n-                if \"metadata\" not in chunk:\n-                    chunk[\"metadata\"] = {}\n-\n-                # Ensure parts exist, but try to preserve original content\n+                chunk = {\"is_task_complete\": False, \"content\": chunk}\n+                chunk[\"metadata\"] = {}\n                 if \"parts\" not in chunk:\n-                    # If content exists, create a part from it\n                     if \"content\" in chunk:\n                         chunk[\"parts\"] = [{\"type\": \"text\", \"text\": chunk[\"content\"]}]\n-                    # If output exists, create a part from it\n                     elif \"output\" in chunk:\n                         chunk[\"parts\"] = [{\"type\": \"text\", \"text\": chunk[\"output\"]}]\n-                    # If actions exist, create empty parts\n                     elif \"actions\" in chunk or \"steps\" in chunk or \"messages\" in chunk:\n-                        # These chunks have their own format, just add empty parts\n                         chunk[\"parts\"] = []\n                     else:\n-                        # Skip chunks with no content\n                         continue\n-\n                 yield chunk",
    "comment": "**correctness**: `streaming_invoke` is now an async generator but is called without `await` or `async for` in `stream`, causing it to return a coroutine object instead of yielding results, breaking streaming functionality.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/api/a2a/agent.py, lines 126-139, the function `stream` calls the async generator `self.streaming_invoke` without using `async for`, resulting in a coroutine object instead of streaming results. Change the loop to use `async for chunk in streaming_response:` instead of `for chunk in streaming_response:` to correctly iterate over the async generator and yield streaming results.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n            streaming_response = self.streaming_invoke(formatted_messages)\n            async for chunk in streaming_response:\n                chunk = {\"is_task_complete\": False, \"content\": chunk}\n                chunk[\"metadata\"] = {}\n                if \"parts\" not in chunk:\n                    if \"content\" in chunk:\n                        chunk[\"parts\"] = [{\"type\": \"text\", \"text\": chunk[\"content\"]}]\n                    elif \"output\" in chunk:\n                        chunk[\"parts\"] = [{\"type\": \"text\", \"text\": chunk[\"output\"]}]\n                    elif \"actions\" in chunk or \"steps\" in chunk or \"messages\" in chunk:\n                        chunk[\"parts\"] = []\n                    else:\n                        continue\n                yield chunk\n\n```\n</details>\n<!-- suggestion_end -->\n",
    "line_number": 139,
    "enriched": "File: mindsdb/api/a2a/agent.py\nCode: @@ -118,203 +87,56 @@ def invoke(self, query, session_id) -> Dict[str, Any]:\n                 \"parts\": [{\"type\": \"text\", \"text\": error_msg}],\n             }\n \n-    def streaming_invoke(self, messages: List[dict], timeout: int = DEFAULT_STREAM_TIMEOUT) -> Iterator[Dict[str, Any]]:\n-        \"\"\"Stream responses from the MindsDB agent using the direct API endpoint.\n-\n-        Args:\n-            messages: List of message dictionaries, each containing 'question' and optionally 'answer'.\n-                Example: [{'question': 'what is the average rental price for a three bedroom?', 'answer': None}]\n-            timeout: Request timeout in seconds (default: 300)\n-\n-        Returns:\n-            Iterator yielding chunks of the streaming response.\n-        \"\"\"\n-        try:\n-            # Construct the URL for the streaming completions endpoint\n-            url = f\"{self.base_url}/api/projects/{self.project_name}/agents/{self.agent_name}/completions/stream\"\n-\n-            # Log request for debugging\n-            logger.info(f\"Sending streaming request to MindsDB agent: {self.agent_name}\")\n-            logger.debug(f\"Request messages: {json.dumps(messages)[:200]}...\")\n-\n-            # Send the request to MindsDB streaming API with timeout\n-            stream = requests.post(url, json={\"messages\": messages}, stream=True, timeout=timeout)\n-            stream.raise_for_status()\n-\n-            # Process the streaming response directly\n-            for line in stream.iter_lines():\n-                if line:\n-                    # Parse each non-empty line\n-                    try:\n-                        line = line.decode(\"utf-8\")\n-                        if line.startswith(\"data: \"):\n-                            # Extract the JSON data from the line that starts with 'data: '\n-                            data = line[6:]  # Remove 'data: ' prefix\n-                            try:\n-                                chunk = json.loads(data)\n-                                # Pass through the chunk with minimal modifications\n-                                yield chunk\n-                            except json.JSONDecodeError as e:\n-                                logger.warning(f\"Failed to parse JSON from line: {data}. Error: {str(e)}\")\n-                                # Yield error information but continue processing\n-                                yield {\n-                                    \"error\": f\"JSON parse error: {str(e)}\",\n-                                    \"data\": data,\n-                                    \"is_task_complete\": False,\n-                                    \"parts\": [\n-                                        {\n-                                            \"type\": \"text\",\n-                                            \"text\": f\"Error parsing response: {str(e)}\",\n-                                        }\n-                                    ],\n-                                    \"metadata\": {},\n-                                }\n-                        else:\n-                            # Log other lines for debugging\n-                            logger.debug(f\"Received non-data line: {line}\")\n-\n-                            # If it looks like a raw text response (not SSE format), wrap it\n-                            if not line.startswith(\"event:\") and not line.startswith(\":\"):\n-                                yield {\"content\": line, \"is_task_complete\": False}\n-                    except UnicodeDecodeError as e:\n-                        logger.warning(f\"Failed to decode line: {str(e)}\")\n-                        # Continue processing despite decode errors\n-\n-        except requests.exceptions.Timeout as e:\n-            error_msg = f\"Request timed out after {timeout} seconds: {str(e)}\"\n-            logger.error(error_msg)\n-            yield {\n-                \"content\": error_msg,\n-                \"parts\": [{\"type\": \"text\", \"text\": error_msg}],\n-                \"is_task_complete\": True,\n-                \"error\": \"timeout\",\n-                \"metadata\": {\"error\": True},\n-            }\n-\n-        except requests.exceptions.ChunkedEncodingError as e:\n-            error_msg = f\"Stream was interrupted: {str(e)}\"\n-            logger.error(error_msg)\n-            yield {\n-                \"content\": error_msg,\n-                \"parts\": [{\"type\": \"text\", \"text\": error_msg}],\n-                \"is_task_complete\": True,\n-                \"error\": \"stream_interrupted\",\n-                \"metadata\": {\"error\": True},\n-            }\n-\n-        except requests.exceptions.ConnectionError as e:\n-            error_msg = f\"Connection error: {str(e)}\"\n-            logger.error(error_msg)\n-            yield {\n-                \"content\": error_msg,\n-                \"parts\": [{\"type\": \"text\", \"text\": error_msg}],\n-                \"is_task_complete\": True,\n-                \"error\": \"connection_error\",\n-                \"metadata\": {\"error\": True},\n-            }\n-\n-        except requests.exceptions.RequestException as e:\n-            error_msg = f\"Error connecting to MindsDB streaming API: {str(e)}\"\n-            logger.error(error_msg)\n-            yield {\n-                \"content\": error_msg,\n-                \"parts\": [{\"type\": \"text\", \"text\": error_msg}],\n-                \"is_task_complete\": True,\n-                \"error\": \"request_error\",\n-                \"metadata\": {\"error\": True},\n-            }\n-\n-        except Exception as e:\n-            error_msg = f\"Error in streaming: {str(e)}\"\n-            logger.error(error_msg)\n-            yield {\n-                \"content\": error_msg,\n-                \"parts\": [{\"type\": \"text\", \"text\": error_msg}],\n-                \"is_task_complete\": True,\n-                \"error\": \"unknown_error\",\n-                \"metadata\": {\"error\": True},\n-            }\n-\n-        # Send a final completion message\n-        yield {\"is_task_complete\": True, \"metadata\": {\"complete\": True}}\n+    async def streaming_invoke(self, messages, timeout=60):\n+        url = f\"{self.base_url}/api/projects/{self.project_name}/agents/{self.agent_name}/completions/stream\"\n+        logger.info(f\"Sending streaming request to MindsDB agent: {self.agent_name}\")\n+        async with httpx.AsyncClient(timeout=timeout) as client:\n+            async with client.stream(\"POST\", url, json={\"messages\": to_serializable(messages)}) as response:\n+                response.raise_for_status()\n+                async for line in response.aiter_lines():\n+                    if line.strip():\n+                        yield line\n \n     async def stream(\n         self,\n         query: str,\n         session_id: str,\n         history: List[dict] | None = None,\n     ) -> AsyncIterable[Dict[str, Any]]:\n-        \"\"\"Stream responses from the MindsDB agent (uses streaming API endpoint).\n-\n-        Args:\n-            query: The current query to send to the agent.\n-            session_id: Unique identifier for the conversation session.\n-            history: Optional list of previous messages in the conversation.\n-\n-        Returns:\n-            AsyncIterable yielding chunks of the streaming response.\n-        \"\"\"\n+        \"\"\"Stream responses from the MindsDB agent (uses streaming API endpoint).\"\"\"\n         try:\n             logger.info(f\"Using streaming API for query: {query[:100]}...\")\n-\n-            # Format history into the expected format\n             formatted_messages = []\n             if history:\n                 for msg in history:\n-                    # Convert Message object to dict if needed\n                     msg_dict = msg.dict() if hasattr(msg, \"dict\") else msg\n                     role = msg_dict.get(\"role\", \"user\")\n-\n-                    # Extract text from parts\n                     text = \"\"\n                     for part in msg_dict.get(\"parts\", []):\n                         if part.get(\"type\") == \"text\":\n                             text = part.get(\"text\", \"\")\n                             break\n-\n                     if text:\n                         if role == \"user\":\n                             formatted_messages.append({\"question\": text, \"answer\": None})\n                         elif role == \"assistant\" and formatted_messages:\n-                            # Add the answer to the last question\n                             formatted_messages[-1][\"answer\"] = text\n-\n-            # Add the current query to the messages\n             formatted_messages.append({\"question\": query, \"answer\": None})\n-\n             logger.debug(f\"Formatted messages for agent: {formatted_messages}\")\n-\n-            # Use the streaming_invoke method to get real streaming responses\n             streaming_response = self.streaming_invoke(formatted_messages)\n-\n-            # Yield all chunks directly from the streaming response\n             for chunk in streaming_response:\n-                # Only add required fields if they don't exist\n-                # This preserves the original structure as much as possible\n-                if \"is_task_complete\" not in chunk:\n-                    chunk[\"is_task_complete\"] = False\n-\n-                if \"metadata\" not in chunk:\n-                    chunk[\"metadata\"] = {}\n-\n-                # Ensure parts exist, but try to preserve original content\n+                chunk = {\"is_task_complete\": False, \"content\": chunk}\n+                chunk[\"metadata\"] = {}\n                 if \"parts\" not in chunk:\n-                    # If content exists, create a part from it\n                     if \"content\" in chunk:\n                         chunk[\"parts\"] = [{\"type\": \"text\", \"text\": chunk[\"content\"]}]\n-                    # If output exists, create a part from it\n                     elif \"output\" in chunk:\n                         chunk[\"parts\"] = [{\"type\": \"text\", \"text\": chunk[\"output\"]}]\n-                    # If actions exist, create empty parts\n                     elif \"actions\" in chunk or \"steps\" in chunk or \"messages\" in chunk:\n-                        # These chunks have their own format, just add empty parts\n                         chunk[\"parts\"] = []\n                     else:\n-                        # Skip chunks with no content\n                         continue\n-\n                 yield chunk\nComment: **correctness**: `streaming_invoke` is now an async generator but is called without `await` or `async for` in `stream`, causing it to return a coroutine object instead of yielding results, breaking streaming functionality.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/api/a2a/agent.py, lines 126-139, the function `stream` calls the async generator `self.streaming_invoke` without using `async for`, resulting in a coroutine object instead of streaming results. Change the loop to use `async for chunk in streaming_response:` instead of `for chunk in streaming_response:` to correctly iterate over the async generator and yield streaming results.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n            streaming_response = self.streaming_invoke(formatted_messages)\n            async for chunk in streaming_response:\n                chunk = {\"is_task_complete\": False, \"content\": chunk}\n                chunk[\"metadata\"] = {}\n                if \"parts\" not in chunk:\n                    if \"content\" in chunk:\n                        chunk[\"parts\"] = [{\"type\": \"text\", \"text\": chunk[\"content\"]}]\n                    elif \"output\" in chunk:\n                        chunk[\"parts\"] = [{\"type\": \"text\", \"text\": chunk[\"output\"]}]\n                    elif \"actions\" in chunk or \"steps\" in chunk or \"messages\" in chunk:\n                        chunk[\"parts\"] = []\n                    else:\n                        continue\n                yield chunk\n\n```\n</details>\n<!-- suggestion_end -->\n",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/api/a2a/agent.py",
    "pr_number": 11247,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2185027759,
    "comment_created_at": "2025-07-04T10:45:17Z"
  },
  {
    "code": "@@ -0,0 +1,115 @@\n+# backend/main.py\n+import os\n+import requests\n+from fastapi import FastAPI, HTTPException\n+from pydantic import BaseModel\n+from typing import Optional, List, Dict\n+from datetime import datetime\n+from fastapi.middleware.cors import CORSMiddleware\n+from dotenv import load_dotenv\n+\n+load_dotenv()\n+\n+MINDSDB_URL = os.getenv(\"MINDSDB_URL\", \"http://localhost:47334\")\n+# REST SQL endpoint\n+SQL_ENDPOINT = f\"{MINDSDB_URL}/api/sql/query\"\n+\n+API_KEY = os.getenv(\"MINDSDB_API_KEY\", None)  # optional if MindsDB configured w/o auth\n+\n+app = FastAPI(title=\"MediQuery Backend\")\n+\n+app.add_middleware(\n+    CORSMiddleware,\n+    allow_origins=[\"*\"],\n+    allow_methods=[\"*\"],\n+    allow_headers=[\"*\"],\n+)\n+\n+class AgentRequest(BaseModel):\n+    agent_name: str\n+    question: str\n+    # optional: override model or knowledge bases\n+    using: Optional[Dict] = None  # e.g. {\"knowledge_bases\": [\"project.kb_name\"]}\n+\n+class KBSearchRequest(BaseModel):\n+    kb_full_name: str            # e.g. \"project.patient_reviews_kb\"\n+    query_text: str\n+    limit: Optional[int] = 10\n+    metadata_filters: Optional[Dict[str, str]] = None  # simple equality filters\n+\n+def run_sql(sql: str) -> Dict:\n+    \"\"\"\n+    Run SQL against MindsDB REST SQL endpoint.\n+    \"\"\"\n+    payload = {\"query\": sql}\n+    headers = {\"Content-Type\": \"application/json\"}\n+    if API_KEY:\n+        headers[\"Authorization\"] = f\"Bearer {API_KEY}\"\n+    try:\n+        resp = requests.post(f\"{SQL_ENDPOINT}\", json=payload, headers=headers, timeout=30)\n+        resp.raise_for_status()\n+        return resp.json()\n+    except requests.RequestException as e:\n+        raise HTTPException(status_code=500, detail=f\"MindsDB request failed: {e}\")",
    "comment": "**correctness**: `run_sql` function does not handle or propagate MindsDB SQL errors returned in the response, so API endpoints may return misleading success even if the SQL failed.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb hacktoberfest/use-cases/Mediquery/backend/main.py, lines 40-53, the run_sql function does not check for SQL errors returned by MindsDB in the response JSON. This can cause endpoints to return success even if the SQL failed. Update run_sql to check for 'error' or 'errors' keys in the response and raise an HTTPException with status 400 and the error message if present.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\ndef run_sql(sql: str) -> Dict:\n    \"\"\"\n    Run SQL against MindsDB REST SQL endpoint.\n    \"\"\"\n    payload = {\"query\": sql}\n    headers = {\"Content-Type\": \"application/json\"}\n    if API_KEY:\n        headers[\"Authorization\"] = f\"Bearer {API_KEY}\"\n    try:\n        resp = requests.post(f\"{SQL_ENDPOINT}\", json=payload, headers=headers, timeout=30)\n        resp.raise_for_status()\n        result = resp.json()\n        # MindsDB errors are often under 'error' or 'errors' keys\n        if (isinstance(result, dict) and (\"error\" in result or \"errors\" in result)):\n            raise HTTPException(status_code=400, detail=result.get(\"error\") or result.get(\"errors\"))\n        return result\n    except requests.RequestException as e:\n        raise HTTPException(status_code=500, detail=f\"MindsDB request failed: {e}\")\n```\n</details>\n<!-- suggestion_end -->\n",
    "line_number": 53,
    "enriched": "File: mindsdb hacktoberfest/use-cases/Mediquery/backend/main.py\nCode: @@ -0,0 +1,115 @@\n+# backend/main.py\n+import os\n+import requests\n+from fastapi import FastAPI, HTTPException\n+from pydantic import BaseModel\n+from typing import Optional, List, Dict\n+from datetime import datetime\n+from fastapi.middleware.cors import CORSMiddleware\n+from dotenv import load_dotenv\n+\n+load_dotenv()\n+\n+MINDSDB_URL = os.getenv(\"MINDSDB_URL\", \"http://localhost:47334\")\n+# REST SQL endpoint\n+SQL_ENDPOINT = f\"{MINDSDB_URL}/api/sql/query\"\n+\n+API_KEY = os.getenv(\"MINDSDB_API_KEY\", None)  # optional if MindsDB configured w/o auth\n+\n+app = FastAPI(title=\"MediQuery Backend\")\n+\n+app.add_middleware(\n+    CORSMiddleware,\n+    allow_origins=[\"*\"],\n+    allow_methods=[\"*\"],\n+    allow_headers=[\"*\"],\n+)\n+\n+class AgentRequest(BaseModel):\n+    agent_name: str\n+    question: str\n+    # optional: override model or knowledge bases\n+    using: Optional[Dict] = None  # e.g. {\"knowledge_bases\": [\"project.kb_name\"]}\n+\n+class KBSearchRequest(BaseModel):\n+    kb_full_name: str            # e.g. \"project.patient_reviews_kb\"\n+    query_text: str\n+    limit: Optional[int] = 10\n+    metadata_filters: Optional[Dict[str, str]] = None  # simple equality filters\n+\n+def run_sql(sql: str) -> Dict:\n+    \"\"\"\n+    Run SQL against MindsDB REST SQL endpoint.\n+    \"\"\"\n+    payload = {\"query\": sql}\n+    headers = {\"Content-Type\": \"application/json\"}\n+    if API_KEY:\n+        headers[\"Authorization\"] = f\"Bearer {API_KEY}\"\n+    try:\n+        resp = requests.post(f\"{SQL_ENDPOINT}\", json=payload, headers=headers, timeout=30)\n+        resp.raise_for_status()\n+        return resp.json()\n+    except requests.RequestException as e:\n+        raise HTTPException(status_code=500, detail=f\"MindsDB request failed: {e}\")\nComment: **correctness**: `run_sql` function does not handle or propagate MindsDB SQL errors returned in the response, so API endpoints may return misleading success even if the SQL failed.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb hacktoberfest/use-cases/Mediquery/backend/main.py, lines 40-53, the run_sql function does not check for SQL errors returned by MindsDB in the response JSON. This can cause endpoints to return success even if the SQL failed. Update run_sql to check for 'error' or 'errors' keys in the response and raise an HTTPException with status 400 and the error message if present.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\ndef run_sql(sql: str) -> Dict:\n    \"\"\"\n    Run SQL against MindsDB REST SQL endpoint.\n    \"\"\"\n    payload = {\"query\": sql}\n    headers = {\"Content-Type\": \"application/json\"}\n    if API_KEY:\n        headers[\"Authorization\"] = f\"Bearer {API_KEY}\"\n    try:\n        resp = requests.post(f\"{SQL_ENDPOINT}\", json=payload, headers=headers, timeout=30)\n        resp.raise_for_status()\n        result = resp.json()\n        # MindsDB errors are often under 'error' or 'errors' keys\n        if (isinstance(result, dict) and (\"error\" in result or \"errors\" in result)):\n            raise HTTPException(status_code=400, detail=result.get(\"error\") or result.get(\"errors\"))\n        return result\n    except requests.RequestException as e:\n        raise HTTPException(status_code=500, detail=f\"MindsDB request failed: {e}\")\n```\n</details>\n<!-- suggestion_end -->\n",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb hacktoberfest/use-cases/Mediquery/backend/main.py",
    "pr_number": 11841,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2483016247,
    "comment_created_at": "2025-11-01T00:50:15Z"
  },
  {
    "code": "@@ -272,6 +275,7 @@ def select(\n                 \"query_embeddings\": vector_filter.value if vector_filter is not None else None,\n                 \"include\": include + [\"distances\"],\n             }\n+            print(query_payload)",
    "comment": "Please use `log` instead if required",
    "line_number": 278,
    "enriched": "File: mindsdb/integrations/handlers/chromadb_handler/chromadb_handler.py\nCode: @@ -272,6 +275,7 @@ def select(\n                 \"query_embeddings\": vector_filter.value if vector_filter is not None else None,\n                 \"include\": include + [\"distances\"],\n             }\n+            print(query_payload)\nComment: Please use `log` instead if required",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/chromadb_handler/chromadb_handler.py",
    "pr_number": 11347,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2225532275,
    "comment_created_at": "2025-07-23T13:00:44Z"
  },
  {
    "code": "@@ -85,6 +85,15 @@ mindsdb: http API: started on 47334\n \n You can access the MindsDB Editor at `localhost:47334`.\n \n+## How to Overcome `ImportError: failed to find libmagic`",
    "comment": "Let's change this to `<Tip>` instead of new Header",
    "line_number": 88,
    "enriched": "File: docs/contribute/install.mdx\nCode: @@ -85,6 +85,15 @@ mindsdb: http API: started on 47334\n \n You can access the MindsDB Editor at `localhost:47334`.\n \n+## How to Overcome `ImportError: failed to find libmagic`\nComment: Let's change this to `<Tip>` instead of new Header",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "docs/contribute/install.mdx",
    "pr_number": 8908,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1521331828,
    "comment_created_at": "2024-03-12T11:54:49Z"
  },
  {
    "code": "@@ -50,6 +52,84 @@ def format(self, record):\n }\n \n \n+class LogSanitizer:\n+    \"\"\"Log Sanitizer\"\"\"\n+\n+    SENSITIVE_KEYS = {\n+        'password', 'passwd', 'pwd',\n+        'token', 'access_token', 'refresh_token', 'bearer_token',\n+        'api_key', 'apikey', 'api-key',\n+        'secret', 'secret_key', 'client_secret',\n+        'credentials', 'auth', 'authorization',\n+        'private_key', 'private-key',\n+        'session_id', 'sessionid',\n+        'credit_card', 'card_number', 'cvv'\n+    }\n+\n+    def __init__(self, mask: str | None = None):\n+        self.mask = mask or \"********\"\n+        self._compile_patterns()\n+    \n+    def _compile_patterns(self):\n+        self.patterns = []\n+        for key in self.SENSITIVE_KEYS:\n+            # Patterns for: key=value, key: value, \"key\": \"value\", 'key': 'value'\n+            patterns = [\n+                re.compile(f'{key}[\"\\s]*[:=][\"\\s]*([^\\s,}}\\\\]\"\\n]+)', re.IGNORECASE),\n+                re.compile(f'\"{key}\"[\"\\s]*:[\"\\s]*\"([^\"]+)\"', re.IGNORECASE),\n+                re.compile(f\"'{key}'['\\s]*:['\\s]*'([^']+)'\", re.IGNORECASE),\n+            ]\n+            self.patterns.extend(patterns)\n+    \n+    def sanitize_text(self, text: str) -> str:\n+        for pattern in self.patterns:\n+            text = pattern.sub(lambda m: m.group(0).replace(m.group(1), self.mask), text)\n+        return text\n+    \n+    def sanitize_dict(self, data: dict) -> dict:\n+        if not isinstance(data, dict):\n+            return data\n+\n+        sanitized = {}\n+        for key, value in data.items():\n+            if any(sensitive in str(key).lower() for sensitive in self.sensitive_keys):",
    "comment": "**correctness**: `LogSanitizer.sanitize_dict` uses `self.sensitive_keys` instead of the defined `SENSITIVE_KEYS`, causing all sensitive key checks to fail and sensitive data to leak in logs.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/utilities/log.py, line 95, the code uses `self.sensitive_keys` which is undefined, instead of the class attribute `SENSITIVE_KEYS`. This causes the sensitive key check to always fail, leaking sensitive data in logs. Replace `self.sensitive_keys` with `self.SENSITIVE_KEYS` to ensure sensitive fields are masked.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n            if any(sensitive in str(key).lower() for sensitive in self.SENSITIVE_KEYS):\n```\n</details>\n<!-- suggestion_end -->\n",
    "line_number": 95,
    "enriched": "File: mindsdb/utilities/log.py\nCode: @@ -50,6 +52,84 @@ def format(self, record):\n }\n \n \n+class LogSanitizer:\n+    \"\"\"Log Sanitizer\"\"\"\n+\n+    SENSITIVE_KEYS = {\n+        'password', 'passwd', 'pwd',\n+        'token', 'access_token', 'refresh_token', 'bearer_token',\n+        'api_key', 'apikey', 'api-key',\n+        'secret', 'secret_key', 'client_secret',\n+        'credentials', 'auth', 'authorization',\n+        'private_key', 'private-key',\n+        'session_id', 'sessionid',\n+        'credit_card', 'card_number', 'cvv'\n+    }\n+\n+    def __init__(self, mask: str | None = None):\n+        self.mask = mask or \"********\"\n+        self._compile_patterns()\n+    \n+    def _compile_patterns(self):\n+        self.patterns = []\n+        for key in self.SENSITIVE_KEYS:\n+            # Patterns for: key=value, key: value, \"key\": \"value\", 'key': 'value'\n+            patterns = [\n+                re.compile(f'{key}[\"\\s]*[:=][\"\\s]*([^\\s,}}\\\\]\"\\n]+)', re.IGNORECASE),\n+                re.compile(f'\"{key}\"[\"\\s]*:[\"\\s]*\"([^\"]+)\"', re.IGNORECASE),\n+                re.compile(f\"'{key}'['\\s]*:['\\s]*'([^']+)'\", re.IGNORECASE),\n+            ]\n+            self.patterns.extend(patterns)\n+    \n+    def sanitize_text(self, text: str) -> str:\n+        for pattern in self.patterns:\n+            text = pattern.sub(lambda m: m.group(0).replace(m.group(1), self.mask), text)\n+        return text\n+    \n+    def sanitize_dict(self, data: dict) -> dict:\n+        if not isinstance(data, dict):\n+            return data\n+\n+        sanitized = {}\n+        for key, value in data.items():\n+            if any(sensitive in str(key).lower() for sensitive in self.sensitive_keys):\nComment: **correctness**: `LogSanitizer.sanitize_dict` uses `self.sensitive_keys` instead of the defined `SENSITIVE_KEYS`, causing all sensitive key checks to fail and sensitive data to leak in logs.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/utilities/log.py, line 95, the code uses `self.sensitive_keys` which is undefined, instead of the class attribute `SENSITIVE_KEYS`. This causes the sensitive key check to always fail, leaking sensitive data in logs. Replace `self.sensitive_keys` with `self.SENSITIVE_KEYS` to ensure sensitive fields are masked.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n            if any(sensitive in str(key).lower() for sensitive in self.SENSITIVE_KEYS):\n```\n</details>\n<!-- suggestion_end -->\n",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/utilities/log.py",
    "pr_number": 11938,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2565344430,
    "comment_created_at": "2025-11-26T14:59:47Z"
  },
  {
    "code": "@@ -27,68 +29,52 @@ def select(self, query: ast.Select) -> pd.DataFrame:\n         ValueError\n             If the query contains an unsupported condition\n         \"\"\"\n-        conditions = extract_comparison_conditions(query.where)\n-\n-       \n-        order_by_conditions = {}\n-\n-        if query.order_by and len(query.order_by) > 0:\n-            order_by_conditions[\"columns\"] = []\n-            order_by_conditions[\"ascending\"] = []\n-\n-            for an_order in query.order_by:\n-                if an_order.field.parts[0] != \"\":\n-                    next    \n-                if an_order.field.parts[1] in self.get_columns():\n-                    order_by_conditions[\"columns\"].append(an_order.field.parts[1])\n-\n-                    if an_order.direction == \"ASC\":\n-                        order_by_conditions[\"ascending\"].append(True)\n-                    else:\n-                        order_by_conditions[\"ascending\"].append(False)\n-                else:\n-                    raise ValueError(\n-                        f\"Order by unknown column {an_order.field.parts[1]}\"\n-                    )\n-\n-        influxdb_tables_df = self.handler.call_influxdb_tables()\n-\n-        selected_columns = []\n-        for target in query.targets:\n-            if isinstance(target, ast.Star):\n-                selected_columns = self.get_columns()\n-                break\n-            elif isinstance(target, ast.Identifier):\n-                selected_columns.append(target.parts[-1])\n-            else:\n-                raise ValueError(f\"Unknown query target {type(target)}\")\n-\n-\n-        if len(influxdb_tables_df) == 0:\n-            influxdb_tables_df = pd.DataFrame([], columns=selected_columns)\n-        else:\n-            influxdb_tables_df.columns = self.get_columns()\n-            for col in set(influxdb_tables_df.columns).difference(set(selected_columns)):\n-                influxdb_tables_df = influxdb_tables_df.drop(col, axis=1)\n-\n-            if len(order_by_conditions.get(\"columns\", [])) > 0:\n-                influxdb_tables_df = influxdb_tables_df.sort_values(\n-                    by=order_by_conditions[\"columns\"],\n-                    ascending=order_by_conditions[\"ascending\"],\n-                )\n-\n-        if query.limit:\n-            influxdb_tables_df = influxdb_tables_df.head(query.limit.value)\n+        \n+        table_name=self.handler.connection_data['influxdb_table_name']\n+        head_influx_table = self.handler.call_influxdb_tables(f\"SELECT * FROM {table_name} LIMIT 1\")\n+        select_statement_parser = SELECTQueryParser(\n+            query,\n+            \"tables\",\n+            self.get_columns(head_influx_table)\n+        )\n+        selected_columns, where_conditions, order_by_conditions, _ = select_statement_parser.parse_query()\n+        \n+        try:\n+            selected_columns.remove(\"name\")\n+            selected_columns.remove(\"tags\")\n+        except Exception as e:\n+            logger.warn(e)\n+\n+        formatted_query=self.get_select_query(table_name,selected_columns,where_conditions,order_by_conditions,query.limit)\n+        influxdb_tables_df  = self.handler.call_influxdb_tables(formatted_query)\n \n         return influxdb_tables_df\n \n-    def get_columns(self) -> List[str]:\n+    def get_columns(self,dataframe) -> List[str]:",
    "comment": "Hey @Tanmai2002,\r\nIf you take a look at the `APITable` interface, `get_columns()` is an abstract method that needs to be implemented. Therefore, let's not change the method signature and keep in the same. Since this table is fixed, I think you can move your logic of getting the columns from the head of the `DataFrame` directly to this method without requiring a `dataframe` parameter. I mean this,\r\n`head_influx_table = self.handler.call_influxdb_tables(f\"SELECT * FROM {table_name} LIMIT 1\")`",
    "line_number": 53,
    "enriched": "File: mindsdb/integrations/handlers/influxdb_handler/influxdb_tables.py\nCode: @@ -27,68 +29,52 @@ def select(self, query: ast.Select) -> pd.DataFrame:\n         ValueError\n             If the query contains an unsupported condition\n         \"\"\"\n-        conditions = extract_comparison_conditions(query.where)\n-\n-       \n-        order_by_conditions = {}\n-\n-        if query.order_by and len(query.order_by) > 0:\n-            order_by_conditions[\"columns\"] = []\n-            order_by_conditions[\"ascending\"] = []\n-\n-            for an_order in query.order_by:\n-                if an_order.field.parts[0] != \"\":\n-                    next    \n-                if an_order.field.parts[1] in self.get_columns():\n-                    order_by_conditions[\"columns\"].append(an_order.field.parts[1])\n-\n-                    if an_order.direction == \"ASC\":\n-                        order_by_conditions[\"ascending\"].append(True)\n-                    else:\n-                        order_by_conditions[\"ascending\"].append(False)\n-                else:\n-                    raise ValueError(\n-                        f\"Order by unknown column {an_order.field.parts[1]}\"\n-                    )\n-\n-        influxdb_tables_df = self.handler.call_influxdb_tables()\n-\n-        selected_columns = []\n-        for target in query.targets:\n-            if isinstance(target, ast.Star):\n-                selected_columns = self.get_columns()\n-                break\n-            elif isinstance(target, ast.Identifier):\n-                selected_columns.append(target.parts[-1])\n-            else:\n-                raise ValueError(f\"Unknown query target {type(target)}\")\n-\n-\n-        if len(influxdb_tables_df) == 0:\n-            influxdb_tables_df = pd.DataFrame([], columns=selected_columns)\n-        else:\n-            influxdb_tables_df.columns = self.get_columns()\n-            for col in set(influxdb_tables_df.columns).difference(set(selected_columns)):\n-                influxdb_tables_df = influxdb_tables_df.drop(col, axis=1)\n-\n-            if len(order_by_conditions.get(\"columns\", [])) > 0:\n-                influxdb_tables_df = influxdb_tables_df.sort_values(\n-                    by=order_by_conditions[\"columns\"],\n-                    ascending=order_by_conditions[\"ascending\"],\n-                )\n-\n-        if query.limit:\n-            influxdb_tables_df = influxdb_tables_df.head(query.limit.value)\n+        \n+        table_name=self.handler.connection_data['influxdb_table_name']\n+        head_influx_table = self.handler.call_influxdb_tables(f\"SELECT * FROM {table_name} LIMIT 1\")\n+        select_statement_parser = SELECTQueryParser(\n+            query,\n+            \"tables\",\n+            self.get_columns(head_influx_table)\n+        )\n+        selected_columns, where_conditions, order_by_conditions, _ = select_statement_parser.parse_query()\n+        \n+        try:\n+            selected_columns.remove(\"name\")\n+            selected_columns.remove(\"tags\")\n+        except Exception as e:\n+            logger.warn(e)\n+\n+        formatted_query=self.get_select_query(table_name,selected_columns,where_conditions,order_by_conditions,query.limit)\n+        influxdb_tables_df  = self.handler.call_influxdb_tables(formatted_query)\n \n         return influxdb_tables_df\n \n-    def get_columns(self) -> List[str]:\n+    def get_columns(self,dataframe) -> List[str]:\nComment: Hey @Tanmai2002,\r\nIf you take a look at the `APITable` interface, `get_columns()` is an abstract method that needs to be implemented. Therefore, let's not change the method signature and keep in the same. Since this table is fixed, I think you can move your logic of getting the columns from the head of the `DataFrame` directly to this method without requiring a `dataframe` parameter. I mean this,\r\n`head_influx_table = self.handler.call_influxdb_tables(f\"SELECT * FROM {table_name} LIMIT 1\")`",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/influxdb_handler/influxdb_tables.py",
    "pr_number": 7855,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1368208173,
    "comment_created_at": "2023-10-23T07:02:53Z"
  },
  {
    "code": "@@ -59,15 +60,21 @@ The following is an explanation of the syntax and parameters:\n       * `base_url` is the root URL used to send API requests. It is required when using `Azure_OpenAI` as the provider. Its default value is `https://api.openai.com/v1/`.\n       * `api_key` stores your OpenAI API key. Alternatively, you can use the `OPENAI_API_KEY` environment variable to store your OpenAI API key.\n       * `api_version` is required when using `Azure_OpenAI` as the provider.\n+      * `method` defines the method used to calculate the relevance of the output rows. The available options include `multi-class` and `binary`. It defaults to `multi-class`.\n \n <Info>\n-Note that providing the reranking model is optional. If provided, it is used an LLM to rerank the output based on relevance, or `relevance_threshold`.\n+**Reranking Method**\n \n-- When the ranking model is provided, the default `relevance_threshold` is 0, unless defined otherwise in the `WHERE` clause.\n+The `multi-class` reranking method classifies each document chunk (that meets any specified metadata filtering conditions) into one of four relevance classes:\n \n-- When the reranking model is not provided and the `relevance_threshold` is not defined in the query, then no relevance filtering is applied and the output includes all rows matched based on the similarity and metadata search.\n+1. Not relevant with score between 0.0 and 0.25 and class probability of 0.25.\n+2. Slightly relevant with score between 0.25 and 0.5 and class probability of 0.5.\n+3. Moderately relevant with score between 0.5 and 0.75 and class probability of 0.75.\n+4. Highly relevant with score between 0.75 and 1.0 and class probability of 1.\n \n-- When the reranking model is not provided but the `relevance_threshold` is defined in the query, then the relevance is calculated based on the `distance` column (`1/(1+ distance)`) and the `relevance_threshold` value is compared with this relevance value to filter the output.\n+The overall `relevance_score` of a document is calculated as the sum of each chunk’s score multiplied by its class probability.",
    "comment": "@martyna-mindsdb \r\nThe overall `relevance_score` of a document is calculated as the sum of each chunk’s class weight multiplied by its class probability (from model logprob output).",
    "line_number": 75,
    "enriched": "File: docs/mindsdb_sql/knowledge-bases.mdx\nCode: @@ -59,15 +60,21 @@ The following is an explanation of the syntax and parameters:\n       * `base_url` is the root URL used to send API requests. It is required when using `Azure_OpenAI` as the provider. Its default value is `https://api.openai.com/v1/`.\n       * `api_key` stores your OpenAI API key. Alternatively, you can use the `OPENAI_API_KEY` environment variable to store your OpenAI API key.\n       * `api_version` is required when using `Azure_OpenAI` as the provider.\n+      * `method` defines the method used to calculate the relevance of the output rows. The available options include `multi-class` and `binary`. It defaults to `multi-class`.\n \n <Info>\n-Note that providing the reranking model is optional. If provided, it is used an LLM to rerank the output based on relevance, or `relevance_threshold`.\n+**Reranking Method**\n \n-- When the ranking model is provided, the default `relevance_threshold` is 0, unless defined otherwise in the `WHERE` clause.\n+The `multi-class` reranking method classifies each document chunk (that meets any specified metadata filtering conditions) into one of four relevance classes:\n \n-- When the reranking model is not provided and the `relevance_threshold` is not defined in the query, then no relevance filtering is applied and the output includes all rows matched based on the similarity and metadata search.\n+1. Not relevant with score between 0.0 and 0.25 and class probability of 0.25.\n+2. Slightly relevant with score between 0.25 and 0.5 and class probability of 0.5.\n+3. Moderately relevant with score between 0.5 and 0.75 and class probability of 0.75.\n+4. Highly relevant with score between 0.75 and 1.0 and class probability of 1.\n \n-- When the reranking model is not provided but the `relevance_threshold` is defined in the query, then the relevance is calculated based on the `distance` column (`1/(1+ distance)`) and the `relevance_threshold` value is compared with this relevance value to filter the output.\n+The overall `relevance_score` of a document is calculated as the sum of each chunk’s score multiplied by its class probability.\nComment: @martyna-mindsdb \r\nThe overall `relevance_score` of a document is calculated as the sum of each chunk’s class weight multiplied by its class probability (from model logprob output).",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "docs/mindsdb_sql/knowledge-bases.mdx",
    "pr_number": 10722,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2045268300,
    "comment_created_at": "2025-04-15T18:57:14Z"
  },
  {
    "code": "@@ -271,13 +186,13 @@ def query(self, query: ASTNode) -> Response:\n             self.connect()\n \n             if isinstance(query, Select):\n-                return self.query_collection(query)\n+                return self.similarity_search(query)",
    "comment": "Can we group the \"routing\" and \"parsing\" logic to the BaseClass of the vector database, so individual implementations just need to fill in the details of actual function implementation?\r\nE.g., \r\nFor the `Query` AST, we can route it to a `select` method, with the parsed arguments, like `table_name`, `search_vector`, `limit`, `metadata_filters`.\r\nThe actual function will take in those arguments, and implement the actual `select` logic from there.\r\n\r\n",
    "line_number": 189,
    "enriched": "File: mindsdb/integrations/handlers/chromadb_handler/chromadb_handler.py\nCode: @@ -271,13 +186,13 @@ def query(self, query: ASTNode) -> Response:\n             self.connect()\n \n             if isinstance(query, Select):\n-                return self.query_collection(query)\n+                return self.similarity_search(query)\nComment: Can we group the \"routing\" and \"parsing\" logic to the BaseClass of the vector database, so individual implementations just need to fill in the details of actual function implementation?\r\nE.g., \r\nFor the `Query` AST, we can route it to a `select` method, with the parsed arguments, like `table_name`, `search_vector`, `limit`, `metadata_filters`.\r\nThe actual function will take in those arguments, and implement the actual `select` logic from there.\r\n\r\n",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/chromadb_handler/chromadb_handler.py",
    "pr_number": 6941,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1293714457,
    "comment_created_at": "2023-08-14T16:37:44Z"
  },
  {
    "code": "@@ -14,7 +14,10 @@ Create ML_ENGINE monkeylearn\n FROM monkeylearn\n ```\n \n-![ML_engine](https://raw.githubusercontent.com/mindsdb/mindsdb/staging/docs/assets/tutorials/1.create_ml.png)\n+<p align=\"center\">\n+  <img src=\"/assets/tutorials/1.create_ml.png\" />",
    "comment": "Let's create a folder to store all images like `/assets/tutorials/monkeylearn/1.create_ml.png`",
    "line_number": 18,
    "enriched": "File: docs/integrations/ai-engines/monkeylearn.mdx\nCode: @@ -14,7 +14,10 @@ Create ML_ENGINE monkeylearn\n FROM monkeylearn\n ```\n \n-![ML_engine](https://raw.githubusercontent.com/mindsdb/mindsdb/staging/docs/assets/tutorials/1.create_ml.png)\n+<p align=\"center\">\n+  <img src=\"/assets/tutorials/1.create_ml.png\" />\nComment: Let's create a folder to store all images like `/assets/tutorials/monkeylearn/1.create_ml.png`",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "docs/integrations/ai-engines/monkeylearn.mdx",
    "pr_number": 7091,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1291194806,
    "comment_created_at": "2023-08-11T10:49:19Z"
  },
  {
    "code": "@@ -69,12 +69,12 @@ def file_get(self, name):\n     def file_set(self, name, content):\n         self.fileStorage.file_set(name, content)\n \n-    def folder_get(self, name, update=True):",
    "comment": "update=False is used for big HF models. It allows to not download if it is already downloaded to instance\r\n",
    "line_number": 72,
    "enriched": "File: mindsdb/interfaces/storage/model_fs.py\nCode: @@ -69,12 +69,12 @@ def file_get(self, name):\n     def file_set(self, name, content):\n         self.fileStorage.file_set(name, content)\n \n-    def folder_get(self, name, update=True):\nComment: update=False is used for big HF models. It allows to not download if it is already downloaded to instance\r\n",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/interfaces/storage/model_fs.py",
    "pr_number": 7366,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1333232678,
    "comment_created_at": "2023-09-21T15:13:41Z"
  },
  {
    "code": "@@ -35,11 +36,11 @@ USING\n Then, create a model using this engine:\n \n ```sql\n-CREATE MODEL myllama2\n+CREATE MODEL mymistral7b\n PREDICT completion\n USING\n     engine = 'anyscale_engine',\n-    model_name = 'meta-llama/Llama-2-7b-chat-hf',\n+    model_name = 'mistralai/Mistral-7B-Instruct-v0.1',\n     prompt_template = 'Return a valid SQL string for the following question about MindsDB in-database machine learning: {{prompt}}';\n ```\n ",
    "comment": "Let's update the model name to `mymistral7b` in the below DESCRIBE command.",
    "line_number": 46,
    "enriched": "File: docs/finetune/anyscale.mdx\nCode: @@ -35,11 +36,11 @@ USING\n Then, create a model using this engine:\n \n ```sql\n-CREATE MODEL myllama2\n+CREATE MODEL mymistral7b\n PREDICT completion\n USING\n     engine = 'anyscale_engine',\n-    model_name = 'meta-llama/Llama-2-7b-chat-hf',\n+    model_name = 'mistralai/Mistral-7B-Instruct-v0.1',\n     prompt_template = 'Return a valid SQL string for the following question about MindsDB in-database machine learning: {{prompt}}';\n ```\n \nComment: Let's update the model name to `mymistral7b` in the below DESCRIBE command.",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "docs/finetune/anyscale.mdx",
    "pr_number": 8581,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1444387277,
    "comment_created_at": "2024-01-08T10:06:34Z"
  },
  {
    "code": "@@ -579,6 +584,18 @@ def cmd_args(self):\n             self.parse_cmd_args()\n         return self._cmd_args\n \n+    def get_cmd_arg(self, arg_name: str) -> Any:",
    "comment": "does this function do  the same as `getattr(self.cmd_args, arg_name, None)`?",
    "line_number": 587,
    "enriched": "File: mindsdb/utilities/config.py\nCode: @@ -579,6 +584,18 @@ def cmd_args(self):\n             self.parse_cmd_args()\n         return self._cmd_args\n \n+    def get_cmd_arg(self, arg_name: str) -> Any:\nComment: does this function do  the same as `getattr(self.cmd_args, arg_name, None)`?",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/utilities/config.py",
    "pr_number": 11502,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2321215136,
    "comment_created_at": "2025-09-04T08:07:49Z"
  },
  {
    "code": "@@ -10,6 +10,7 @@\n import sys",
    "comment": "**Correctness**: The test imports ListwiseLLMReranker from mindsdb.integrations.utilities.rag.rerankers.base_reranker, but this class does not exist in the current codebase.\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\nfrom unittest.mock import AsyncMock, MagicMock, patch\n\n# TODO: Remove this import once ListwiseLLMReranker is implemented\n# from mindsdb.integrations.utilities.rag.rerankers.base_reranker import ListwiseLLMReranker\n\n# Mock implementation for testing purposes only\nclass ListwiseLLMReranker:\n    def __init__(self, api_key, model):\n        self.api_key = api_key\n        self.model = model\n        \n    async def rerank(self, docs, query):\n        # This will be replaced by test mocks\n        pass\n```\n</details>\n<!-- suggestion_end -->\n",
    "line_number": 10,
    "enriched": "File: tests/unit/executor/test_knowledge_base.py\nCode: @@ -10,6 +10,7 @@\n import sys\nComment: **Correctness**: The test imports ListwiseLLMReranker from mindsdb.integrations.utilities.rag.rerankers.base_reranker, but this class does not exist in the current codebase.\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\nfrom unittest.mock import AsyncMock, MagicMock, patch\n\n# TODO: Remove this import once ListwiseLLMReranker is implemented\n# from mindsdb.integrations.utilities.rag.rerankers.base_reranker import ListwiseLLMReranker\n\n# Mock implementation for testing purposes only\nclass ListwiseLLMReranker:\n    def __init__(self, api_key, model):\n        self.api_key = api_key\n        self.model = model\n        \n    async def rerank(self, docs, query):\n        # This will be replaced by test mocks\n        pass\n```\n</details>\n<!-- suggestion_end -->\n",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "tests/unit/executor/test_knowledge_base.py",
    "pr_number": 11714,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2435622859,
    "comment_created_at": "2025-10-16T11:55:33Z"
  },
  {
    "code": "@@ -71,22 +71,33 @@ def _stop_clean(self) -> None:\n     def set(self, handler: DatabaseHandler):\n         \"\"\"add (or replace) handler in cache\n \n+        NOTE: If the handler is not thread-safe, then use a lock when making connection. Otherwise, make connection in \n+        the same thread without using a lock to speed up parallel queries. (They don't need to wait for a connection in\n+        another thread.)\n+\n         Args:\n             handler (DatabaseHandler)\n         \"\"\"\n+        thread_safe = getattr(handler, \"thread_safe\", False)\n         with self._lock:\n             try:\n                 # If the handler is defined to be thread safe, set 0 as the last element of the key, otherwise set the thrad ID.\n                 key = (\n                     handler.name,\n                     ctx.company_id,\n-                    0 if getattr(handler, \"thread_safe\", False) else threading.get_native_id(),\n+                    0 if thread_safe else threading.get_native_id(),\n                 )\n-                handler.connect()\n                 self.handlers[key] = {\"handler\": handler, \"expired_at\": time.time() + self.ttl}\n+                if thread_safe:\n+                    handler.connect()\n             except Exception:\n                 pass\n             self._start_clean()\n+        try:\n+            if not thread_safe:\n+                handler.connect()\n+        except Exception:\n+            pass",
    "comment": "**correctness**: `handler.connect()` for thread-safe handlers is now called inside the lock, but for non-thread-safe handlers it is called outside the lock, which can result in the handler being added to the cache before a failed connection attempt, leaving a non-connected handler in the cache and causing runtime errors on later use.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/interfaces/database/integrations.py, lines 81-100, the current logic adds the handler to the cache before attempting to connect for non-thread-safe handlers. This can result in a non-connected handler being cached if connect() fails, leading to runtime errors. Refactor the set() method so that handler.connect() is always called before adding the handler to the cache, and only add the handler if connect() succeeds. For thread-safe handlers, call connect() inside the lock before caching. For non-thread-safe handlers, call connect() outside the lock, and only cache if connect() succeeds. See the code suggestion for details.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n        thread_safe = getattr(handler, \"thread_safe\", False)\n        if thread_safe:\n            with self._lock:\n                try:\n                    key = (\n                        handler.name,\n                        ctx.company_id,\n                        0,\n                    )\n                    handler.connect()\n                    self.handlers[key] = {\"handler\": handler, \"expired_at\": time.time() + self.ttl}\n                except Exception:\n                    pass\n                self._start_clean()\n        else:\n            try:\n                handler.connect()\n            except Exception:\n                return\n            with self._lock:\n                try:\n                    key = (\n                        handler.name,\n                        ctx.company_id,\n                        threading.get_native_id(),\n                    )\n                    self.handlers[key] = {\"handler\": handler, \"expired_at\": time.time() + self.ttl}\n                except Exception:\n                    pass\n                self._start_clean()\n```\n</details>\n<!-- suggestion_end -->\n",
    "line_number": 100,
    "enriched": "File: mindsdb/interfaces/database/integrations.py\nCode: @@ -71,22 +71,33 @@ def _stop_clean(self) -> None:\n     def set(self, handler: DatabaseHandler):\n         \"\"\"add (or replace) handler in cache\n \n+        NOTE: If the handler is not thread-safe, then use a lock when making connection. Otherwise, make connection in \n+        the same thread without using a lock to speed up parallel queries. (They don't need to wait for a connection in\n+        another thread.)\n+\n         Args:\n             handler (DatabaseHandler)\n         \"\"\"\n+        thread_safe = getattr(handler, \"thread_safe\", False)\n         with self._lock:\n             try:\n                 # If the handler is defined to be thread safe, set 0 as the last element of the key, otherwise set the thrad ID.\n                 key = (\n                     handler.name,\n                     ctx.company_id,\n-                    0 if getattr(handler, \"thread_safe\", False) else threading.get_native_id(),\n+                    0 if thread_safe else threading.get_native_id(),\n                 )\n-                handler.connect()\n                 self.handlers[key] = {\"handler\": handler, \"expired_at\": time.time() + self.ttl}\n+                if thread_safe:\n+                    handler.connect()\n             except Exception:\n                 pass\n             self._start_clean()\n+        try:\n+            if not thread_safe:\n+                handler.connect()\n+        except Exception:\n+            pass\nComment: **correctness**: `handler.connect()` for thread-safe handlers is now called inside the lock, but for non-thread-safe handlers it is called outside the lock, which can result in the handler being added to the cache before a failed connection attempt, leaving a non-connected handler in the cache and causing runtime errors on later use.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/interfaces/database/integrations.py, lines 81-100, the current logic adds the handler to the cache before attempting to connect for non-thread-safe handlers. This can result in a non-connected handler being cached if connect() fails, leading to runtime errors. Refactor the set() method so that handler.connect() is always called before adding the handler to the cache, and only add the handler if connect() succeeds. For thread-safe handlers, call connect() inside the lock before caching. For non-thread-safe handlers, call connect() outside the lock, and only cache if connect() succeeds. See the code suggestion for details.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n        thread_safe = getattr(handler, \"thread_safe\", False)\n        if thread_safe:\n            with self._lock:\n                try:\n                    key = (\n                        handler.name,\n                        ctx.company_id,\n                        0,\n                    )\n                    handler.connect()\n                    self.handlers[key] = {\"handler\": handler, \"expired_at\": time.time() + self.ttl}\n                except Exception:\n                    pass\n                self._start_clean()\n        else:\n            try:\n                handler.connect()\n            except Exception:\n                return\n            with self._lock:\n                try:\n                    key = (\n                        handler.name,\n                        ctx.company_id,\n                        threading.get_native_id(),\n                    )\n                    self.handlers[key] = {\"handler\": handler, \"expired_at\": time.time() + self.ttl}\n                except Exception:\n                    pass\n                self._start_clean()\n```\n</details>\n<!-- suggestion_end -->\n",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/interfaces/database/integrations.py",
    "pr_number": 11593,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2362818188,
    "comment_created_at": "2025-09-19T13:03:28Z"
  },
  {
    "code": "@@ -0,0 +1,24 @@\n+# Anthropic Handler\n+Anthropic ML handler integrates Anthropic LLMs with MindsDB. You can train your existing text data with Anthropic LLM",
    "comment": "> You can train your existing text data with Anthropic LLM\r\n\r\nThis doesn't seem accurate. This handler does not support fine-tuning (i.e. no `update()` method), so I would suggest rewording to something like:\r\n\r\n> The Anthropic ML handler integrates Anthropic LLMs with MindsDB. You can use it to generate text completions with the popular Claude LLM family for your existing text data.",
    "line_number": 2,
    "enriched": "File: mindsdb/integrations/handlers/anthropic_handler/README.md\nCode: @@ -0,0 +1,24 @@\n+# Anthropic Handler\n+Anthropic ML handler integrates Anthropic LLMs with MindsDB. You can train your existing text data with Anthropic LLM\nComment: > You can train your existing text data with Anthropic LLM\r\n\r\nThis doesn't seem accurate. This handler does not support fine-tuning (i.e. no `update()` method), so I would suggest rewording to something like:\r\n\r\n> The Anthropic ML handler integrates Anthropic LLMs with MindsDB. You can use it to generate text completions with the popular Claude LLM family for your existing text data.",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/anthropic_handler/README.md",
    "pr_number": 6870,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1265926835,
    "comment_created_at": "2023-07-17T21:36:06Z"
  },
  {
    "code": "@@ -0,0 +1,101 @@\n+import secrets\n+import time\n+import requests\n+from typing import Dict, Any, Optional\n+\n+from mindsdb.utilities.config import Config\n+from mindsdb.utilities import log\n+\n+logger = log.getLogger(__name__)\n+\n+# In-memory storage for A2A tokens (in production, this should be a database)\n+_a2a_tokens: Dict[str, Dict[str, Any]] = {}\n+\n+\n+def generate_auth_token() -> str:\n+    \"\"\"Generate a secure authentication token\"\"\"\n+    return secrets.token_urlsafe(32)\n+\n+\n+def store_auth_token(token: str, user_id: str, description: str = \"\") -> None:\n+    \"\"\"Store an authentication token with metadata\"\"\"\n+    _a2a_tokens[token] = {\n+        'created_at': time.time(),\n+        'description': description,\n+        'user_id': user_id\n+    }\n+\n+\n+def is_auth_token_valid(token: str) -> bool:\n+    \"\"\"Check if an authentication token is valid and not expired\"\"\"\n+    if token not in _a2a_tokens:\n+        return False\n+    \n+    token_data = _a2a_tokens[token]\n+    # Check if token is expired (24 hours)\n+    if time.time() - token_data['created_at'] > 86400:\n+        del _a2a_tokens[token]\n+        return False\n+    \n+    return True\n+\n+\n+def validate_auth_token_remote(token: str, http_api_url: str = None) -> bool:\n+    \"\"\"Validate authentication token by calling the HTTP API (for separate servers)\"\"\"\n+    if http_api_url is None:\n+        # Get from config\n+        config = Config()\n+        http_host = config.get('api', {}).get('http', {}).get('host', 'localhost')\n+        http_port = config.get('api', {}).get('http', {}).get('port', 47334)\n+        http_api_url = f\"http://{http_host}:{http_port}/api\"\n+    \n+    try:\n+        response = requests.post(\n+            f\"{http_api_url}/auth_tokens/token/validate\",\n+            json={\"token\": token},\n+            timeout=5\n+        )\n+        return response.status_code == 200\n+    except Exception as e:\n+        logger.warning(f\"Failed to validate A2A token remotely: {e}\")\n+        return False",
    "comment": "**security**: `validate_auth_token_remote` does not verify the TLS certificate or enforce HTTPS, allowing token interception or MITM attacks when validating tokens over the network.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/utilities/a2a_auth.py, lines 43-61, the function `validate_auth_token_remote` uses HTTP and does not verify TLS certificates, exposing tokens to interception or MITM attacks. Update the code to use HTTPS by default and set `verify=True` in the requests.post call to enforce certificate validation. Ensure the default URL uses 'https://' and document that only secure endpoints should be used.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\ndef validate_auth_token_remote(token: str, http_api_url: str = None) -> bool:\n    \"\"\"Validate authentication token by calling the HTTP API (for separate servers)\"\"\"\n    if http_api_url is None:\n        # Get from config\n        config = Config()\n        http_host = config.get('api', {}).get('http', {}).get('host', 'localhost')\n        http_port = config.get('api', {}).get('http', {}).get('port', 47334)\n        http_api_url = f\"https://{http_host}:{http_port}/api\"\n    \n    try:\n        response = requests.post(\n            f\"{http_api_url}/auth_tokens/token/validate\",\n            json={\"token\": token},\n            timeout=5,\n            verify=True\n        )\n        return response.status_code == 200\n    except Exception as e:\n        logger.warning(f\"Failed to validate A2A token remotely: {e}\")\n        return False\n```\n</details>\n<!-- suggestion_end -->\n",
    "line_number": 61,
    "enriched": "File: mindsdb/utilities/a2a_auth.py\nCode: @@ -0,0 +1,101 @@\n+import secrets\n+import time\n+import requests\n+from typing import Dict, Any, Optional\n+\n+from mindsdb.utilities.config import Config\n+from mindsdb.utilities import log\n+\n+logger = log.getLogger(__name__)\n+\n+# In-memory storage for A2A tokens (in production, this should be a database)\n+_a2a_tokens: Dict[str, Dict[str, Any]] = {}\n+\n+\n+def generate_auth_token() -> str:\n+    \"\"\"Generate a secure authentication token\"\"\"\n+    return secrets.token_urlsafe(32)\n+\n+\n+def store_auth_token(token: str, user_id: str, description: str = \"\") -> None:\n+    \"\"\"Store an authentication token with metadata\"\"\"\n+    _a2a_tokens[token] = {\n+        'created_at': time.time(),\n+        'description': description,\n+        'user_id': user_id\n+    }\n+\n+\n+def is_auth_token_valid(token: str) -> bool:\n+    \"\"\"Check if an authentication token is valid and not expired\"\"\"\n+    if token not in _a2a_tokens:\n+        return False\n+    \n+    token_data = _a2a_tokens[token]\n+    # Check if token is expired (24 hours)\n+    if time.time() - token_data['created_at'] > 86400:\n+        del _a2a_tokens[token]\n+        return False\n+    \n+    return True\n+\n+\n+def validate_auth_token_remote(token: str, http_api_url: str = None) -> bool:\n+    \"\"\"Validate authentication token by calling the HTTP API (for separate servers)\"\"\"\n+    if http_api_url is None:\n+        # Get from config\n+        config = Config()\n+        http_host = config.get('api', {}).get('http', {}).get('host', 'localhost')\n+        http_port = config.get('api', {}).get('http', {}).get('port', 47334)\n+        http_api_url = f\"http://{http_host}:{http_port}/api\"\n+    \n+    try:\n+        response = requests.post(\n+            f\"{http_api_url}/auth_tokens/token/validate\",\n+            json={\"token\": token},\n+            timeout=5\n+        )\n+        return response.status_code == 200\n+    except Exception as e:\n+        logger.warning(f\"Failed to validate A2A token remotely: {e}\")\n+        return False\nComment: **security**: `validate_auth_token_remote` does not verify the TLS certificate or enforce HTTPS, allowing token interception or MITM attacks when validating tokens over the network.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/utilities/a2a_auth.py, lines 43-61, the function `validate_auth_token_remote` uses HTTP and does not verify TLS certificates, exposing tokens to interception or MITM attacks. Update the code to use HTTPS by default and set `verify=True` in the requests.post call to enforce certificate validation. Ensure the default URL uses 'https://' and document that only secure endpoints should be used.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\ndef validate_auth_token_remote(token: str, http_api_url: str = None) -> bool:\n    \"\"\"Validate authentication token by calling the HTTP API (for separate servers)\"\"\"\n    if http_api_url is None:\n        # Get from config\n        config = Config()\n        http_host = config.get('api', {}).get('http', {}).get('host', 'localhost')\n        http_port = config.get('api', {}).get('http', {}).get('port', 47334)\n        http_api_url = f\"https://{http_host}:{http_port}/api\"\n    \n    try:\n        response = requests.post(\n            f\"{http_api_url}/auth_tokens/token/validate\",\n            json={\"token\": token},\n            timeout=5,\n            verify=True\n        )\n        return response.status_code == 200\n    except Exception as e:\n        logger.warning(f\"Failed to validate A2A token remotely: {e}\")\n        return False\n```\n</details>\n<!-- suggestion_end -->\n",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/utilities/a2a_auth.py",
    "pr_number": 11428,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2268570445,
    "comment_created_at": "2025-08-12T04:52:14Z"
  },
  {
    "code": "@@ -682,10 +682,26 @@ def answer_create_chatbot(self, statement):\n         # Database ID cannot be null\n         database_id = database['id'] if database is not None else -1\n \n+        # To be added by https://github.com/mindsdb/mindsdb_sql/pull/307.\n+        # TODO: change to statement.agent when possible.\n+        agent_node = getattr(statement, 'agent', None)\n+        agent = None\n+        if agent_node is not None and agent_node.get_string() != 'NULL':",
    "comment": "I think we can just left 'if agent_node is not None', without checking for !='NULL'\r\nAnd  model_node too",
    "line_number": 689,
    "enriched": "File: mindsdb/api/mysql/mysql_proxy/executor/executor_commands.py\nCode: @@ -682,10 +682,26 @@ def answer_create_chatbot(self, statement):\n         # Database ID cannot be null\n         database_id = database['id'] if database is not None else -1\n \n+        # To be added by https://github.com/mindsdb/mindsdb_sql/pull/307.\n+        # TODO: change to statement.agent when possible.\n+        agent_node = getattr(statement, 'agent', None)\n+        agent = None\n+        if agent_node is not None and agent_node.get_string() != 'NULL':\nComment: I think we can just left 'if agent_node is not None', without checking for !='NULL'\r\nAnd  model_node too",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/api/mysql/mysql_proxy/executor/executor_commands.py",
    "pr_number": 7386,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1339978223,
    "comment_created_at": "2023-09-28T11:29:15Z"
  },
  {
    "code": "@@ -85,11 +85,11 @@ jobs:\n     secrets: inherit\n   \n   tests_completed:\n-    if: github.event.pull_request.merged == true\n+    if: always() && github.event.pull_request.merged == true\n     name: All Tests Succeeded\n     needs: [run_unit_tests, run_integration_tests]\n     runs-on: mdb-dev\n-    if: always()\n+    if: ",
    "comment": "**correctness**: `if:` on line 92 is empty, which causes a YAML syntax error and prevents the `tests_completed` job from running, breaking the workflow.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn .github/workflows/build_deploy_staging.yml, line 92, the 'if:' field for the 'tests_completed' job is empty, which causes a YAML syntax error and prevents the workflow from running. Replace line 92 with 'if: always()' to ensure the job executes as intended.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n92:    if: always()\n```\n</details>\n<!-- suggestion_end -->\n",
    "line_number": 92,
    "enriched": "File: .github/workflows/build_deploy_staging.yml\nCode: @@ -85,11 +85,11 @@ jobs:\n     secrets: inherit\n   \n   tests_completed:\n-    if: github.event.pull_request.merged == true\n+    if: always() && github.event.pull_request.merged == true\n     name: All Tests Succeeded\n     needs: [run_unit_tests, run_integration_tests]\n     runs-on: mdb-dev\n-    if: always()\n+    if: \nComment: **correctness**: `if:` on line 92 is empty, which causes a YAML syntax error and prevents the `tests_completed` job from running, breaking the workflow.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn .github/workflows/build_deploy_staging.yml, line 92, the 'if:' field for the 'tests_completed' job is empty, which causes a YAML syntax error and prevents the workflow from running. Replace line 92 with 'if: always()' to ensure the job executes as intended.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n92:    if: always()\n```\n</details>\n<!-- suggestion_end -->\n",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": ".github/workflows/build_deploy_staging.yml",
    "pr_number": 11646,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2389664908,
    "comment_created_at": "2025-09-30T02:07:16Z"
  },
  {
    "code": "@@ -81,6 +96,15 @@ def get_tools(self, prefix='') -> List[BaseTool]:\n                - Query only necessary columns, not all.\n                - Use only existing column names from correct tables.\n                - Use database-specific syntax for date operations.\n+               - Always try to use ORDER BY to sort information when possible and relevant\n+               - If you need to FILTER always use CASE instead\n+               - Never use of double quotes for column names use backticks instead.\n+               - Always give aliases in lower-case underscore notation, no quotes or backticks for alias that are in lowe-case underscore notation.",
    "comment": "Instead we can ask LLM to use mysql dialect for queries",
    "line_number": 102,
    "enriched": "File: mindsdb/interfaces/skills/custom/text2sql/mindsdb_sql_toolkit.py\nCode: @@ -81,6 +96,15 @@ def get_tools(self, prefix='') -> List[BaseTool]:\n                - Query only necessary columns, not all.\n                - Use only existing column names from correct tables.\n                - Use database-specific syntax for date operations.\n+               - Always try to use ORDER BY to sort information when possible and relevant\n+               - If you need to FILTER always use CASE instead\n+               - Never use of double quotes for column names use backticks instead.\n+               - Always give aliases in lower-case underscore notation, no quotes or backticks for alias that are in lowe-case underscore notation.\nComment: Instead we can ask LLM to use mysql dialect for queries",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/interfaces/skills/custom/text2sql/mindsdb_sql_toolkit.py",
    "pr_number": 10499,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1967074141,
    "comment_created_at": "2025-02-24T06:02:17Z"
  },
  {
    "code": "@@ -0,0 +1,18 @@\n+from mindsdb.integrations.libs.const import HANDLER_TYPE\n+\n+from .__about__ import __version__ as version, __description__ as description\n+\n+try:\n+    from .vertex_handler import VertexHandler as Handler\n+\n+    import_error = None\n+except Exception as e:\n+    Handler = None\n+    import_error = e\n+\n+title = \"Vertex\"\n+name = \"vertex\"\n+type = HANDLER_TYPE.ML\n+permanent = True",
    "comment": "this handler required credentials, so it can't be permanent. it will be used via ml engine (next comment)",
    "line_number": 16,
    "enriched": "File: mindsdb/integrations/handlers/vertex_handler/__init__.py\nCode: @@ -0,0 +1,18 @@\n+from mindsdb.integrations.libs.const import HANDLER_TYPE\n+\n+from .__about__ import __version__ as version, __description__ as description\n+\n+try:\n+    from .vertex_handler import VertexHandler as Handler\n+\n+    import_error = None\n+except Exception as e:\n+    Handler = None\n+    import_error = e\n+\n+title = \"Vertex\"\n+name = \"vertex\"\n+type = HANDLER_TYPE.ML\n+permanent = True\nComment: this handler required credentials, so it can't be permanent. it will be used via ml engine (next comment)",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/vertex_handler/__init__.py",
    "pr_number": 7809,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1362294003,
    "comment_created_at": "2023-10-17T15:09:33Z"
  },
  {
    "code": "@@ -47,7 +47,7 @@\n logger = log.getLogger(__name__)\n \n KB_TO_VECTORDB_COLUMNS = {\n-    'id': 'original_row_id',\n+    'id': 'original_doc_id',",
    "comment": "it makes `id` not the same as original id (from source table). \r\nafter it we can't join KB with source table\r\nHow it used to be before: \r\n![image](https://github.com/user-attachments/assets/f009ab5a-aa11-43a1-bc3c-5b013a5e33b1)\r\n",
    "line_number": 50,
    "enriched": "File: mindsdb/interfaces/knowledge_base/controller.py\nCode: @@ -47,7 +47,7 @@\n logger = log.getLogger(__name__)\n \n KB_TO_VECTORDB_COLUMNS = {\n-    'id': 'original_row_id',\n+    'id': 'original_doc_id',\nComment: it makes `id` not the same as original id (from source table). \r\nafter it we can't join KB with source table\r\nHow it used to be before: \r\n![image](https://github.com/user-attachments/assets/f009ab5a-aa11-43a1-bc3c-5b013a5e33b1)\r\n",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/interfaces/knowledge_base/controller.py",
    "pr_number": 10808,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2084053073,
    "comment_created_at": "2025-05-12T07:54:31Z"
  },
  {
    "code": "@@ -1239,6 +1226,93 @@ def add(\n         db.session.commit()\n         return kb\n \n+    def update(\n+        self,\n+        name: str,\n+        project_name: str,\n+        params: dict,\n+        preprocessing_config: Optional[dict] = None,\n+    ) -> db.KnowledgeBase:\n+        \"\"\"\n+        Update the knowledge base\n+        :param name: The name of the knowledge base\n+        :param project_name: Current project name\n+        :param params: The parameters to update\n+        :param preprocessing_config: Optional preprocessing configuration to validate and store\n+        \"\"\"\n+\n+        # fill variables\n+        params = variables_controller.fill_parameters(params)\n+\n+        # Validate preprocessing config first if provided\n+        if preprocessing_config is not None:\n+            PreprocessingConfig(**preprocessing_config)  # Validate before storing\n+            params = params or {}\n+            params[\"preprocessing\"] = preprocessing_config\n+\n+        self._check_kb_input_params(params)\n+\n+        # get project id\n+        project = self.session.database_controller.get_project(project_name)\n+        project_id = project.id\n+\n+        # get existed KB\n+        kb = self.get(name.lower(), project_id)\n+        if kb is None:\n+            raise EntityNotExistsError(\"Knowledge base doesn't exists\", name)\n+\n+        if \"embedding_model\" in params:\n+            new_config = params[\"embedding_model\"]\n+            # update embedding\n+            embed_params = kb.params.get(\"embedding_model\", {})\n+            if not embed_params:\n+                # maybe old version of KB\n+                raise ValueError(\"No embedding config to update\")\n+\n+            # some parameters are not allowed to update\n+            for key in (\"provider\", \"model_name\"):\n+                if key in new_config and new_config[key] != embed_params.get(key):\n+                    raise ValueError(f\"You can't update '{key}' setting\")\n+\n+            embed_params.update(new_config)\n+\n+            self._check_embedding_model(\n+                project.name,\n+                params=embed_params,\n+                kb_name=name,\n+            )\n+            kb.params[\"embedding_model\"] = embed_params\n+\n+        if \"reranking_model\" in params:\n+            new_config = params[\"reranking_model\"]\n+            # update embedding\n+            rerank_params = kb.params.get(\"reranking_model\", {})\n+\n+            if new_config is False:\n+                # disable reranking\n+                rerank_params = {}\n+            elif \"provider\" in new_config and new_config[\"provider\"] != rerank_params.get(\"provider\"):\n+                # use new config (and include default config)\n+                rerank_params = get_model_params(new_config, \"default_reranking_model\")\n+            else:\n+                # update current config\n+                rerank_params.update(new_config)\n+\n+            if rerank_params:\n+                self._test_reranking(rerank_params)\n+\n+            kb.params[\"reranking_model\"] = rerank_params\n+",
    "comment": "**security**: `update` method in `KnowledgeBaseController` does not restrict updates to sensitive keys in `embedding_model` and `reranking_model`, allowing privilege escalation or configuration tampering via crafted API/SQL input.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/interfaces/knowledge_base/controller.py, lines 1264-1305, the `update` method allows updates to sensitive keys in `embedding_model` and `reranking_model` (such as `provider` and `model_name`) via API/SQL input, which could allow privilege escalation or configuration tampering. Restrict updates so that these keys cannot be changed via the update API. Raise a PermissionError if an attempt is made to update these keys, and only allow non-sensitive fields to be updated. Apply this restriction to both `embedding_model` and `reranking_model` updates.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n        if \"embedding_model\" in params:\n            new_config = params[\"embedding_model\"]\n            embed_params = kb.params.get(\"embedding_model\", {})\n            if not embed_params:\n                raise ValueError(\"No embedding config to update\")\n            # Prevent updates to sensitive keys\n            for key in (\"provider\", \"model_name\"):\n                if key in new_config and new_config[key] != embed_params.get(key):\n                    raise PermissionError(f\"Updating '{key}' is not allowed via update API\")\n            embed_params.update({k: v for k, v in new_config.items() if k not in (\"provider\", \"model_name\")})\n            self._check_embedding_model(\n                project.name,\n                params=embed_params,\n                kb_name=name,\n            )\n            kb.params[\"embedding_model\"] = embed_params\n\n        if \"reranking_model\" in params:\n            new_config = params[\"reranking_model\"]\n            rerank_params = kb.params.get(\"reranking_model\", {})\n            if new_config is False:\n                rerank_params = {}\n            elif \"provider\" in new_config and new_config[\"provider\"] != rerank_params.get(\"provider\"):\n                # Prevent privilege escalation by restricting provider changes\n                raise PermissionError(\"Updating 'provider' for reranking_model is not allowed via update API\")\n            else:\n                rerank_params.update({k: v for k, v in new_config.items() if k != \"provider\"})\n            if rerank_params:\n                self._test_reranking(rerank_params)\n            kb.params[\"reranking_model\"] = rerank_params\n\n        # update other keys\n        for key in [\"id_column\", \"metadata_columns\", \"content_columns\", \"preprocessing\"]:\n            if key in params:\n                kb.params[key] = params[key]\n```\n</details>\n<!-- suggestion_end -->\n",
    "line_number": 1305,
    "enriched": "File: mindsdb/interfaces/knowledge_base/controller.py\nCode: @@ -1239,6 +1226,93 @@ def add(\n         db.session.commit()\n         return kb\n \n+    def update(\n+        self,\n+        name: str,\n+        project_name: str,\n+        params: dict,\n+        preprocessing_config: Optional[dict] = None,\n+    ) -> db.KnowledgeBase:\n+        \"\"\"\n+        Update the knowledge base\n+        :param name: The name of the knowledge base\n+        :param project_name: Current project name\n+        :param params: The parameters to update\n+        :param preprocessing_config: Optional preprocessing configuration to validate and store\n+        \"\"\"\n+\n+        # fill variables\n+        params = variables_controller.fill_parameters(params)\n+\n+        # Validate preprocessing config first if provided\n+        if preprocessing_config is not None:\n+            PreprocessingConfig(**preprocessing_config)  # Validate before storing\n+            params = params or {}\n+            params[\"preprocessing\"] = preprocessing_config\n+\n+        self._check_kb_input_params(params)\n+\n+        # get project id\n+        project = self.session.database_controller.get_project(project_name)\n+        project_id = project.id\n+\n+        # get existed KB\n+        kb = self.get(name.lower(), project_id)\n+        if kb is None:\n+            raise EntityNotExistsError(\"Knowledge base doesn't exists\", name)\n+\n+        if \"embedding_model\" in params:\n+            new_config = params[\"embedding_model\"]\n+            # update embedding\n+            embed_params = kb.params.get(\"embedding_model\", {})\n+            if not embed_params:\n+                # maybe old version of KB\n+                raise ValueError(\"No embedding config to update\")\n+\n+            # some parameters are not allowed to update\n+            for key in (\"provider\", \"model_name\"):\n+                if key in new_config and new_config[key] != embed_params.get(key):\n+                    raise ValueError(f\"You can't update '{key}' setting\")\n+\n+            embed_params.update(new_config)\n+\n+            self._check_embedding_model(\n+                project.name,\n+                params=embed_params,\n+                kb_name=name,\n+            )\n+            kb.params[\"embedding_model\"] = embed_params\n+\n+        if \"reranking_model\" in params:\n+            new_config = params[\"reranking_model\"]\n+            # update embedding\n+            rerank_params = kb.params.get(\"reranking_model\", {})\n+\n+            if new_config is False:\n+                # disable reranking\n+                rerank_params = {}\n+            elif \"provider\" in new_config and new_config[\"provider\"] != rerank_params.get(\"provider\"):\n+                # use new config (and include default config)\n+                rerank_params = get_model_params(new_config, \"default_reranking_model\")\n+            else:\n+                # update current config\n+                rerank_params.update(new_config)\n+\n+            if rerank_params:\n+                self._test_reranking(rerank_params)\n+\n+            kb.params[\"reranking_model\"] = rerank_params\n+\nComment: **security**: `update` method in `KnowledgeBaseController` does not restrict updates to sensitive keys in `embedding_model` and `reranking_model`, allowing privilege escalation or configuration tampering via crafted API/SQL input.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/interfaces/knowledge_base/controller.py, lines 1264-1305, the `update` method allows updates to sensitive keys in `embedding_model` and `reranking_model` (such as `provider` and `model_name`) via API/SQL input, which could allow privilege escalation or configuration tampering. Restrict updates so that these keys cannot be changed via the update API. Raise a PermissionError if an attempt is made to update these keys, and only allow non-sensitive fields to be updated. Apply this restriction to both `embedding_model` and `reranking_model` updates.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n        if \"embedding_model\" in params:\n            new_config = params[\"embedding_model\"]\n            embed_params = kb.params.get(\"embedding_model\", {})\n            if not embed_params:\n                raise ValueError(\"No embedding config to update\")\n            # Prevent updates to sensitive keys\n            for key in (\"provider\", \"model_name\"):\n                if key in new_config and new_config[key] != embed_params.get(key):\n                    raise PermissionError(f\"Updating '{key}' is not allowed via update API\")\n            embed_params.update({k: v for k, v in new_config.items() if k not in (\"provider\", \"model_name\")})\n            self._check_embedding_model(\n                project.name,\n                params=embed_params,\n                kb_name=name,\n            )\n            kb.params[\"embedding_model\"] = embed_params\n\n        if \"reranking_model\" in params:\n            new_config = params[\"reranking_model\"]\n            rerank_params = kb.params.get(\"reranking_model\", {})\n            if new_config is False:\n                rerank_params = {}\n            elif \"provider\" in new_config and new_config[\"provider\"] != rerank_params.get(\"provider\"):\n                # Prevent privilege escalation by restricting provider changes\n                raise PermissionError(\"Updating 'provider' for reranking_model is not allowed via update API\")\n            else:\n                rerank_params.update({k: v for k, v in new_config.items() if k != \"provider\"})\n            if rerank_params:\n                self._test_reranking(rerank_params)\n            kb.params[\"reranking_model\"] = rerank_params\n\n        # update other keys\n        for key in [\"id_column\", \"metadata_columns\", \"content_columns\", \"preprocessing\"]:\n            if key in params:\n                kb.params[key] = params[key]\n```\n</details>\n<!-- suggestion_end -->\n",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/interfaces/knowledge_base/controller.py",
    "pr_number": 11738,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2428827770,
    "comment_created_at": "2025-10-14T11:35:22Z"
  },
  {
    "code": "@@ -285,9 +292,9 @@ def _adapt_column_names(self, df: pd.DataFrame) -> pd.DataFrame:\n             # wrong name\n             id_column = None\n \n-        if id_column is None and TableField.ID.value in columns:\n-            # default value\n-            id_column = TableField.ID.value\n+        # if id_column is None and TableField.ID.value in columns:\n+        #     # default value\n+        #     id_column = TableField.ID.value",
    "comment": "This removes logic to use default 'id' column if `id_column` is not defined.\r\n\r\nSwapping these two blocks should fix the issue:\r\nFirst sets default value for id, second clears column id if setting is set but column is not exists\r\n```python\r\n        if id_column is None and TableField.ID.value in columns:\r\n            # default value\r\n            id_column = TableField.ID.value\r\n\r\n        if id_column is not None and id_column not in columns:\r\n            # wrong name\r\n            id_column = None\r\n\r\n```",
    "line_number": 297,
    "enriched": "File: mindsdb/interfaces/knowledge_base/controller.py\nCode: @@ -285,9 +292,9 @@ def _adapt_column_names(self, df: pd.DataFrame) -> pd.DataFrame:\n             # wrong name\n             id_column = None\n \n-        if id_column is None and TableField.ID.value in columns:\n-            # default value\n-            id_column = TableField.ID.value\n+        # if id_column is None and TableField.ID.value in columns:\n+        #     # default value\n+        #     id_column = TableField.ID.value\nComment: This removes logic to use default 'id' column if `id_column` is not defined.\r\n\r\nSwapping these two blocks should fix the issue:\r\nFirst sets default value for id, second clears column id if setting is set but column is not exists\r\n```python\r\n        if id_column is None and TableField.ID.value in columns:\r\n            # default value\r\n            id_column = TableField.ID.value\r\n\r\n        if id_column is not None and id_column not in columns:\r\n            # wrong name\r\n            id_column = None\r\n\r\n```",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/interfaces/knowledge_base/controller.py",
    "pr_number": 10184,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1848248143,
    "comment_created_at": "2024-11-19T12:17:17Z"
  },
  {
    "code": "@@ -0,0 +1,127 @@\n+import json\n+import requests\n+from typing import Dict, Optional\n+\n+import pandas as pd\n+\n+from mindsdb.integrations.libs.base import BaseMLEngine\n+from mindsdb.integrations.libs.llm_utils import get_completed_prompts\n+\n+\n+class OllamaHandler(BaseMLEngine):\n+    name = \"ollama\"\n+    SERVE_URL = 'http://localhost:11434'\n+    MODEL_LIST_URL = 'https://registry.ollama.ai/v2/_catalog'\n+\n+    @staticmethod\n+    def create_validation(target, args=None, **kwargs):\n+        if 'using' not in args:\n+            raise Exception(\"Ollama engine requires a USING clause! Refer to its documentation for more details.\")\n+        else:\n+            args = args['using']\n+\n+        # check model version is valid\n+        all_models = requests.get(OllamaHandler.MODEL_LIST_URL).json()['repositories']",
    "comment": "Maybe wrap this in try/except in case of connectivity issues",
    "line_number": 24,
    "enriched": "File: mindsdb/integrations/handlers/ollama_handler/ollama_handler.py\nCode: @@ -0,0 +1,127 @@\n+import json\n+import requests\n+from typing import Dict, Optional\n+\n+import pandas as pd\n+\n+from mindsdb.integrations.libs.base import BaseMLEngine\n+from mindsdb.integrations.libs.llm_utils import get_completed_prompts\n+\n+\n+class OllamaHandler(BaseMLEngine):\n+    name = \"ollama\"\n+    SERVE_URL = 'http://localhost:11434'\n+    MODEL_LIST_URL = 'https://registry.ollama.ai/v2/_catalog'\n+\n+    @staticmethod\n+    def create_validation(target, args=None, **kwargs):\n+        if 'using' not in args:\n+            raise Exception(\"Ollama engine requires a USING clause! Refer to its documentation for more details.\")\n+        else:\n+            args = args['using']\n+\n+        # check model version is valid\n+        all_models = requests.get(OllamaHandler.MODEL_LIST_URL).json()['repositories']\nComment: Maybe wrap this in try/except in case of connectivity issues",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/ollama_handler/ollama_handler.py",
    "pr_number": 7403,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1336926853,
    "comment_created_at": "2023-09-26T09:36:30Z"
  },
  {
    "code": "@@ -108,6 +110,15 @@ def safe_pandas_is_datetime(value: str) -> bool:\n         return False\n \n \n+def to_json(obj):\n+    if obj is None:\n+        return None\n+    try:\n+        return json.dumps(obj)\n+    except TypeError:\n+        return obj",
    "comment": "**Correctness**: The `to_json` function doesn't handle `decimal.Decimal` types properly in the `json.dumps()` call, which will raise a `TypeError` since `decimal.Decimal` is not JSON serializable by default.\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\ndef to_json(obj):\n    if obj is None:\n        return None\n    try:\n        return json.dumps(obj, default=str)\n    except TypeError:\n        return obj\n```\n</details>\n<!-- suggestion_end -->\n",
    "line_number": 119,
    "enriched": "File: mindsdb/interfaces/knowledge_base/controller.py\nCode: @@ -108,6 +110,15 @@ def safe_pandas_is_datetime(value: str) -> bool:\n         return False\n \n \n+def to_json(obj):\n+    if obj is None:\n+        return None\n+    try:\n+        return json.dumps(obj)\n+    except TypeError:\n+        return obj\nComment: **Correctness**: The `to_json` function doesn't handle `decimal.Decimal` types properly in the `json.dumps()` call, which will raise a `TypeError` since `decimal.Decimal` is not JSON serializable by default.\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\ndef to_json(obj):\n    if obj is None:\n        return None\n    try:\n        return json.dumps(obj, default=str)\n    except TypeError:\n        return obj\n```\n</details>\n<!-- suggestion_end -->\n",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/interfaces/knowledge_base/controller.py",
    "pr_number": 10933,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2114101565,
    "comment_created_at": "2025-05-29T14:31:54Z"
  },
  {
    "code": "@@ -0,0 +1,85 @@\n+timestamp,prompt,completion",
    "comment": "Please name this file `data.csv` (not just `data`).\r\n\r\nAlso, please move this file to the `docs/use-cases/automated_finetuning` folder, and please update the link in `use-cases/automated_finetuning/openai.mdx` tutorial.",
    "line_number": 1,
    "enriched": "File: docs/finetune/data\nCode: @@ -0,0 +1,85 @@\n+timestamp,prompt,completion\nComment: Please name this file `data.csv` (not just `data`).\r\n\r\nAlso, please move this file to the `docs/use-cases/automated_finetuning` folder, and please update the link in `use-cases/automated_finetuning/openai.mdx` tutorial.",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "docs/finetune/data",
    "pr_number": 9155,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1583684034,
    "comment_created_at": "2024-04-29T20:10:52Z"
  },
  {
    "code": "@@ -349,6 +349,114 @@ def delete(self, query: ASTNode):\n             \n         except SlackApiError as e:\n             raise Exception(f\"Error deleting message from Slack channel '{params['channel']}' with timestamp '{params['ts']}': {e.response['error']}\")\n+        \n+\n+class SlackThreadsTable(APIResource):\n+\n+    def list(\n+        self,\n+        conditions: List[FilterCondition] = None,\n+        limit: int = None,\n+        **kwargs\n+    ) -> pd.DataFrame:\n+        \"\"\"\n+        Retrieves the messages from a thread in a Slack conversation.\n+\n+        Returns:\n+            pd.DataFrame: The messages in the thread.\n+        \"\"\"\n+        client = self.handler.connect()\n+\n+        params = {}\n+\n+        # Parse the conditions.\n+        for condition in conditions:\n+            value = condition.value\n+            op = condition.op\n+\n+            # Handle the column 'channel_id'.\n+            if condition.column == 'channel_id':\n+                if op != FilterOperator.EQUAL:\n+                    raise ValueError(f\"Unsupported operator '{op}' for column 'channel_id'\")\n+\n+                # Check if the channel exists.\n+                try:\n+                    channel = self.handler.get_channel(value)",
    "comment": "maybe we can speed it put by removing this check. just put channer to params and it be checked by slack in `client.conversations_replies(**params)` \r\n",
    "line_number": 384,
    "enriched": "File: mindsdb/integrations/handlers/slack_handler/slack_handler.py\nCode: @@ -349,6 +349,114 @@ def delete(self, query: ASTNode):\n             \n         except SlackApiError as e:\n             raise Exception(f\"Error deleting message from Slack channel '{params['channel']}' with timestamp '{params['ts']}': {e.response['error']}\")\n+        \n+\n+class SlackThreadsTable(APIResource):\n+\n+    def list(\n+        self,\n+        conditions: List[FilterCondition] = None,\n+        limit: int = None,\n+        **kwargs\n+    ) -> pd.DataFrame:\n+        \"\"\"\n+        Retrieves the messages from a thread in a Slack conversation.\n+\n+        Returns:\n+            pd.DataFrame: The messages in the thread.\n+        \"\"\"\n+        client = self.handler.connect()\n+\n+        params = {}\n+\n+        # Parse the conditions.\n+        for condition in conditions:\n+            value = condition.value\n+            op = condition.op\n+\n+            # Handle the column 'channel_id'.\n+            if condition.column == 'channel_id':\n+                if op != FilterOperator.EQUAL:\n+                    raise ValueError(f\"Unsupported operator '{op}' for column 'channel_id'\")\n+\n+                # Check if the channel exists.\n+                try:\n+                    channel = self.handler.get_channel(value)\nComment: maybe we can speed it put by removing this check. just put channer to params and it be checked by slack in `client.conversations_replies(**params)` \r\n",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/slack_handler/slack_handler.py",
    "pr_number": 10201,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1856660614,
    "comment_created_at": "2024-11-25T13:56:14Z"
  },
  {
    "code": "@@ -789,31 +789,50 @@ def answer_evaluate_metric(self, statement, database_name):\n                 f'Nested query failed to execute with error: \"{e}\", please check and try again.'\n             )\n         result = sqlquery.fetch('dataframe')\n+        print(result)\n         df = result[\"result\"]\n+        print(df)\n         df.columns = [\n             str(t.alias) if hasattr(t, \"alias\") else str(t.parts[-1])\n             for t in statement.data.targets\n         ]\n-\n-        for col in [\"actual\", \"prediction\"]:\n+        is_llm = True",
    "comment": "We have to choose how to communicate this `is_llm` flag to the code below in a general way. \r\n\r\nI'm not sure we can get model details reliably here without some major refactoring, so instead I would propose to add a constant of LLM-only (or perhaps more accurately, RAG-only) metrics. Then, we can check `if metric_name is in RAG_METRICS` to enable the new code you've introduced in the evaluator. \r\n\r\nI have a slight inclination for adding this constant in `mindsdb_evaluator`, then importing and using it here, but I think it could also be added here under `libs/llm/utils.py` if you'd prefer.",
    "line_number": 799,
    "enriched": "File: mindsdb/api/executor/command_executor.py\nCode: @@ -789,31 +789,50 @@ def answer_evaluate_metric(self, statement, database_name):\n                 f'Nested query failed to execute with error: \"{e}\", please check and try again.'\n             )\n         result = sqlquery.fetch('dataframe')\n+        print(result)\n         df = result[\"result\"]\n+        print(df)\n         df.columns = [\n             str(t.alias) if hasattr(t, \"alias\") else str(t.parts[-1])\n             for t in statement.data.targets\n         ]\n-\n-        for col in [\"actual\", \"prediction\"]:\n+        is_llm = True\nComment: We have to choose how to communicate this `is_llm` flag to the code below in a general way. \r\n\r\nI'm not sure we can get model details reliably here without some major refactoring, so instead I would propose to add a constant of LLM-only (or perhaps more accurately, RAG-only) metrics. Then, we can check `if metric_name is in RAG_METRICS` to enable the new code you've introduced in the evaluator. \r\n\r\nI have a slight inclination for adding this constant in `mindsdb_evaluator`, then importing and using it here, but I think it could also be added here under `libs/llm/utils.py` if you'd prefer.",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/api/executor/command_executor.py",
    "pr_number": 9581,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1704473026,
    "comment_created_at": "2024-08-05T18:11:21Z"
  },
  {
    "code": "@@ -56,10 +56,19 @@ def connect(self):\n         user = quote(self.connection_data['user'])\n         password = quote(self.connection_data['password'])\n         database = quote(self.connection_data['database'])\n+        verify = self.connection_data.get('verify', False)\n         url = f'{protocol}://{user}:{password}@{host}:{port}/{database}'\n         # This is not redundunt. Check https://clickhouse-sqlalchemy.readthedocs.io/en/latest/connection.html#http\n         if self.protocol == 'https':\n             url = url + \"?protocol=https\"\n+        \n+        # Add SSL verification control\n+        if verify == False:\n+            url = url + \"&verify=false\"\n+        if verify == True:\n+            url = url + \"&verify=true\"",
    "comment": "**correctness**: `url` construction appends `&verify=...` even if no query string exists, resulting in malformed URLs (e.g., `...?protocol=https&verify=false` when no `?` is present)`\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/integrations/handlers/clickhouse_handler/clickhouse_handler.py, lines 64-69, the code appends '&verify=...' to the URL without checking if a query string already exists, which can result in malformed URLs. Update the logic so that if '?' is not present in the URL, it appends '?verify=...', otherwise it appends '&verify=...'.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n        # Add SSL verification control\n        if verify == False:\n            if \"?\" in url:\n                url = url + \"&verify=false\"\n            else:\n                url = url + \"?verify=false\"\n        if verify == True:\n            if \"?\" in url:\n                url = url + \"&verify=true\"\n            else:\n                url = url + \"?verify=true\"\n```\n</details>\n<!-- suggestion_end -->\n",
    "line_number": 69,
    "enriched": "File: mindsdb/integrations/handlers/clickhouse_handler/clickhouse_handler.py\nCode: @@ -56,10 +56,19 @@ def connect(self):\n         user = quote(self.connection_data['user'])\n         password = quote(self.connection_data['password'])\n         database = quote(self.connection_data['database'])\n+        verify = self.connection_data.get('verify', False)\n         url = f'{protocol}://{user}:{password}@{host}:{port}/{database}'\n         # This is not redundunt. Check https://clickhouse-sqlalchemy.readthedocs.io/en/latest/connection.html#http\n         if self.protocol == 'https':\n             url = url + \"?protocol=https\"\n+        \n+        # Add SSL verification control\n+        if verify == False:\n+            url = url + \"&verify=false\"\n+        if verify == True:\n+            url = url + \"&verify=true\"\nComment: **correctness**: `url` construction appends `&verify=...` even if no query string exists, resulting in malformed URLs (e.g., `...?protocol=https&verify=false` when no `?` is present)`\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/integrations/handlers/clickhouse_handler/clickhouse_handler.py, lines 64-69, the code appends '&verify=...' to the URL without checking if a query string already exists, which can result in malformed URLs. Update the logic so that if '?' is not present in the URL, it appends '?verify=...', otherwise it appends '&verify=...'.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n        # Add SSL verification control\n        if verify == False:\n            if \"?\" in url:\n                url = url + \"&verify=false\"\n            else:\n                url = url + \"?verify=false\"\n        if verify == True:\n            if \"?\" in url:\n                url = url + \"&verify=true\"\n            else:\n                url = url + \"?verify=true\"\n```\n</details>\n<!-- suggestion_end -->\n",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/clickhouse_handler/clickhouse_handler.py",
    "pr_number": 11367,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2233849931,
    "comment_created_at": "2025-07-27T08:07:46Z"
  },
  {
    "code": "@@ -64,10 +55,98 @@ class MapReduceStepCall(BaseStepCall):\n \n     bind = MapReduceStep\n \n-    def call(self, step):\n+    def call(self, step: MultipleSteps):\n         if step.reduce != 'union':\n             raise LogicError(f'Unknown MapReduceStep type: {step.reduce}')\n \n+        partition = getattr(step, 'partition', None)\n+\n+        if partition is not None:\n+            data = self._reduce_partition(step, partition)\n+\n+        else:\n+            data = self._reduce_vars(step)\n+\n+        return data\n+\n+    def _reduce_partition(self, step, partition):\n+        if not isinstance(partition, int):\n+            raise NotImplementedError('Only integers are supported in partition definition.')\n+\n+        input_idx = step.values.step_num\n+        input_data = self.steps_data[input_idx]\n+        input_columns = list(input_data.columns)\n+\n+        substeps = step.step\n+        if not isinstance(substeps, list):\n+            substeps = [substeps]\n+\n+        data = ResultSet()\n+\n+        df = input_data.get_raw_df()\n+\n+        # tasks\n+        def split_data_f(df):\n+            chunk = 0\n+            while chunk * partition < len(df):\n+                # create results with partition\n+                df1 = df[chunk * partition: (chunk + 1) * partition]",
    "comment": "Just in case, we may wanna control for `partition > 0`, too, because if it's negative this won't behave correctly.",
    "line_number": 93,
    "enriched": "File: mindsdb/api/executor/sql_query/steps/map_reduce_step.py\nCode: @@ -64,10 +55,98 @@ class MapReduceStepCall(BaseStepCall):\n \n     bind = MapReduceStep\n \n-    def call(self, step):\n+    def call(self, step: MultipleSteps):\n         if step.reduce != 'union':\n             raise LogicError(f'Unknown MapReduceStep type: {step.reduce}')\n \n+        partition = getattr(step, 'partition', None)\n+\n+        if partition is not None:\n+            data = self._reduce_partition(step, partition)\n+\n+        else:\n+            data = self._reduce_vars(step)\n+\n+        return data\n+\n+    def _reduce_partition(self, step, partition):\n+        if not isinstance(partition, int):\n+            raise NotImplementedError('Only integers are supported in partition definition.')\n+\n+        input_idx = step.values.step_num\n+        input_data = self.steps_data[input_idx]\n+        input_columns = list(input_data.columns)\n+\n+        substeps = step.step\n+        if not isinstance(substeps, list):\n+            substeps = [substeps]\n+\n+        data = ResultSet()\n+\n+        df = input_data.get_raw_df()\n+\n+        # tasks\n+        def split_data_f(df):\n+            chunk = 0\n+            while chunk * partition < len(df):\n+                # create results with partition\n+                df1 = df[chunk * partition: (chunk + 1) * partition]\nComment: Just in case, we may wanna control for `partition > 0`, too, because if it's negative this won't behave correctly.",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/api/executor/sql_query/steps/map_reduce_step.py",
    "pr_number": 9359,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1651672591,
    "comment_created_at": "2024-06-24T21:39:03Z"
  },
  {
    "code": "@@ -72,12 +72,21 @@\n \n \n def dataframe_checksum(df: pd.DataFrame):\n-    original_columns = df.columns\n-    df.columns = list(range(len(df.columns)))\n-    result = hashlib.sha256(\n-        str(df.values).encode()\n-    ).hexdigest()\n-    df.columns = original_columns\n+    \"\"\"Compute efficient checksum for DataFrame without column manipulation.\"\"\"\n+    # Create a more efficient hash using pandas built-in methods\n+    try:\n+        # Use pandas util.hash_pandas_object for better performance\n+        from pandas.util import hash_pandas_object\n+        hash_values = hash_pandas_object(df, index=True).values\n+        result = hashlib.sha256(hash_values.tobytes()).hexdigest()\n+    except ImportError:\n+        # Fallback to original method if pandas util not available\n+        original_columns = df.columns\n+        df.columns = list(range(len(df.columns)))\n+        result = hashlib.sha256(\n+            str(df.values).encode()\n+        ).hexdigest()\n+        df.columns = original_columns",
    "comment": "**correctness**: `df.columns = list(range(len(df.columns)))` and `df.columns = original_columns` in the fallback path mutate the input DataFrame in-place, potentially causing data corruption for callers relying on column names.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/utilities/cache.py, lines 84-89, the fallback code for dataframe_checksum mutates the input DataFrame's columns in-place, which can corrupt the caller's DataFrame. Refactor this block to avoid any in-place mutation: make a copy of the DataFrame, change columns on the copy, and compute the hash from that. Do not modify the original DataFrame.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n    original_columns = df.columns.copy()\n    temp_df = df.copy()\n    temp_df.columns = list(range(len(temp_df.columns)))\n    result = hashlib.sha256(\n        str(temp_df.values).encode()\n    ).hexdigest()\n```\n</details>\n<!-- suggestion_end -->\n",
    "line_number": 89,
    "enriched": "File: mindsdb/utilities/cache.py\nCode: @@ -72,12 +72,21 @@\n \n \n def dataframe_checksum(df: pd.DataFrame):\n-    original_columns = df.columns\n-    df.columns = list(range(len(df.columns)))\n-    result = hashlib.sha256(\n-        str(df.values).encode()\n-    ).hexdigest()\n-    df.columns = original_columns\n+    \"\"\"Compute efficient checksum for DataFrame without column manipulation.\"\"\"\n+    # Create a more efficient hash using pandas built-in methods\n+    try:\n+        # Use pandas util.hash_pandas_object for better performance\n+        from pandas.util import hash_pandas_object\n+        hash_values = hash_pandas_object(df, index=True).values\n+        result = hashlib.sha256(hash_values.tobytes()).hexdigest()\n+    except ImportError:\n+        # Fallback to original method if pandas util not available\n+        original_columns = df.columns\n+        df.columns = list(range(len(df.columns)))\n+        result = hashlib.sha256(\n+            str(df.values).encode()\n+        ).hexdigest()\n+        df.columns = original_columns\nComment: **correctness**: `df.columns = list(range(len(df.columns)))` and `df.columns = original_columns` in the fallback path mutate the input DataFrame in-place, potentially causing data corruption for callers relying on column names.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/utilities/cache.py, lines 84-89, the fallback code for dataframe_checksum mutates the input DataFrame's columns in-place, which can corrupt the caller's DataFrame. Refactor this block to avoid any in-place mutation: make a copy of the DataFrame, change columns on the copy, and compute the hash from that. Do not modify the original DataFrame.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n    original_columns = df.columns.copy()\n    temp_df = df.copy()\n    temp_df.columns = list(range(len(temp_df.columns)))\n    result = hashlib.sha256(\n        str(temp_df.values).encode()\n    ).hexdigest()\n```\n</details>\n<!-- suggestion_end -->\n",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/utilities/cache.py",
    "pr_number": 11596,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2362892355,
    "comment_created_at": "2025-09-19T13:27:17Z"
  },
  {
    "code": "@@ -0,0 +1,92 @@\n+# NLPCloud Handler\n+\n+NLPCloud handler for MindsDB provides interfaces to connect to NLPCloud via APIs and pull repository data into MindsDB.\n+\n+---\n+\n+## Table of Contents\n+\n+- [NLPCloud Handler](#nlpcloud-handler)\n+  - [Table of Contents](#table-of-contents)\n+  - [About NLPCloud](#about-nlpcloud)\n+  - [NLPCloud Handler Implementation](#nlpcloud-handler-implementation)\n+  - [NLPCloud Handler Initialization](#nlpcloud-handler-initialization)\n+  - [Implemented Features](#implemented-features)\n+  - [TODO](#todo)\n+  - [Example Usage](#example-usage)\n+\n+---\n+\n+## About NLPCloud\n+\n+NLP Cloud is an artificial intelligence platform that allows you to use the most advanced AI engines, and even train your own engines with your own data. This platform is focused on data privacy by design so you can safely use AI in your business without compromising confidentiality. We offer both small specific AI engines and large cutting-edge generative AI engines so you can easily integrate the most advanced AI features into your application at an affordable cost.\n+\n+\n+## NLPCloud Handler Implementation\n+\n+This handler was implemented using the `nlpcloud` library that makes http calls to https://docs.nlpcloud.com/#endpoints\n+\n+## NLPCloud Handler Initialization\n+\n+The NLPCloud handler is initialized with the following parameters:\n+\n+- `token`: Auth token to connect with NLP Cloud\n+- `model`: Machine Learning Model\n+- `gpu`: Whether to use gpu or not. It's a boolean value.\n+- `lang`: Language\n+\n+Read about creating an account [here](https://nlpcloud.com/).\n+The list of models supported by nlpcloud can be found [here](https://docs.nlpcloud.com/#models).\n+\n+## Implemented Features\n+\n+- [x] NLPCloud\n+  - [x] Support SELECT\n+    - [x] Support LIMIT\n+    - [x] Support WHERE\n+    - [x] Support ORDER BY\n+    - [x] Support column selection\n+\n+## TODO",
    "comment": "Perhaps rename this section to `Use cases To-Do`. But most importantly: please list in a similar way all the currently supported use cases. From what I can tell, it would be `translation`, `summarization`, `sentiment_analysis`, `language_detection` and `named_entity_recognition`.",
    "line_number": 50,
    "enriched": "File: mindsdb/integrations/handlers/nlpcloud_handler/README.md\nCode: @@ -0,0 +1,92 @@\n+# NLPCloud Handler\n+\n+NLPCloud handler for MindsDB provides interfaces to connect to NLPCloud via APIs and pull repository data into MindsDB.\n+\n+---\n+\n+## Table of Contents\n+\n+- [NLPCloud Handler](#nlpcloud-handler)\n+  - [Table of Contents](#table-of-contents)\n+  - [About NLPCloud](#about-nlpcloud)\n+  - [NLPCloud Handler Implementation](#nlpcloud-handler-implementation)\n+  - [NLPCloud Handler Initialization](#nlpcloud-handler-initialization)\n+  - [Implemented Features](#implemented-features)\n+  - [TODO](#todo)\n+  - [Example Usage](#example-usage)\n+\n+---\n+\n+## About NLPCloud\n+\n+NLP Cloud is an artificial intelligence platform that allows you to use the most advanced AI engines, and even train your own engines with your own data. This platform is focused on data privacy by design so you can safely use AI in your business without compromising confidentiality. We offer both small specific AI engines and large cutting-edge generative AI engines so you can easily integrate the most advanced AI features into your application at an affordable cost.\n+\n+\n+## NLPCloud Handler Implementation\n+\n+This handler was implemented using the `nlpcloud` library that makes http calls to https://docs.nlpcloud.com/#endpoints\n+\n+## NLPCloud Handler Initialization\n+\n+The NLPCloud handler is initialized with the following parameters:\n+\n+- `token`: Auth token to connect with NLP Cloud\n+- `model`: Machine Learning Model\n+- `gpu`: Whether to use gpu or not. It's a boolean value.\n+- `lang`: Language\n+\n+Read about creating an account [here](https://nlpcloud.com/).\n+The list of models supported by nlpcloud can be found [here](https://docs.nlpcloud.com/#models).\n+\n+## Implemented Features\n+\n+- [x] NLPCloud\n+  - [x] Support SELECT\n+    - [x] Support LIMIT\n+    - [x] Support WHERE\n+    - [x] Support ORDER BY\n+    - [x] Support column selection\n+\n+## TODO\nComment: Perhaps rename this section to `Use cases To-Do`. But most importantly: please list in a similar way all the currently supported use cases. From what I can tell, it would be `translation`, `summarization`, `sentiment_analysis`, `language_detection` and `named_entity_recognition`.",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/nlpcloud_handler/README.md",
    "pr_number": 8025,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1382906714,
    "comment_created_at": "2023-11-06T08:10:32Z"
  },
  {
    "code": "@@ -174,3 +174,4 @@\n USER_COLUMN = \"question\"\n DEFAULT_EMBEDDINGS_MODEL_PROVIDER = \"openai\"\n DEFAULT_EMBEDDINGS_MODEL_CLASS = OpenAIEmbeddings\n+DEFAULT_TIKTOKEN_MODEL_NAME = 'gpt-4'",
    "comment": "\r\n\r\n\r\n```suggestion\r\nimport os\r\n\r\nDEFAULT_TIKTOKEN_MODEL_NAME = os.getenv(\"DEFAULT_TIKTOKEN_MODEL_NAME\", \"gpt-4\")\r\n```\r\nI think this would give us the flexibility to not rebuild an image to change this value.",
    "line_number": 177,
    "enriched": "File: mindsdb/interfaces/agents/constants.py\nCode: @@ -174,3 +174,4 @@\n USER_COLUMN = \"question\"\n DEFAULT_EMBEDDINGS_MODEL_PROVIDER = \"openai\"\n DEFAULT_EMBEDDINGS_MODEL_CLASS = OpenAIEmbeddings\n+DEFAULT_TIKTOKEN_MODEL_NAME = 'gpt-4'\nComment: \r\n\r\n\r\n```suggestion\r\nimport os\r\n\r\nDEFAULT_TIKTOKEN_MODEL_NAME = os.getenv(\"DEFAULT_TIKTOKEN_MODEL_NAME\", \"gpt-4\")\r\n```\r\nI think this would give us the flexibility to not rebuild an image to change this value.",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/interfaces/agents/constants.py",
    "pr_number": 10197,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1852751358,
    "comment_created_at": "2024-11-21T19:43:21Z"
  },
  {
    "code": "@@ -171,6 +170,20 @@ def test_create_kb(self):\n         kb_obj = self.db.session.query(KnowledgeBase).filter_by(name=\"test_kb5\").first()\n         assert kb_obj is not None\n \n+        # create a knowledge base without specifying storage and model, defaults should be used\n+\n+        # todo parser fails without USING\n+\n+        # sql = f\"\"\"\n+        #     CREATE KNOWLEDGE BASE test_kb6\n+        # \"\"\"",
    "comment": "need to fix this",
    "line_number": 179,
    "enriched": "File: tests/unit/test_knowledge_base.py\nCode: @@ -171,6 +170,20 @@ def test_create_kb(self):\n         kb_obj = self.db.session.query(KnowledgeBase).filter_by(name=\"test_kb5\").first()\n         assert kb_obj is not None\n \n+        # create a knowledge base without specifying storage and model, defaults should be used\n+\n+        # todo parser fails without USING\n+\n+        # sql = f\"\"\"\n+        #     CREATE KNOWLEDGE BASE test_kb6\n+        # \"\"\"\nComment: need to fix this",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "tests/unit/test_knowledge_base.py",
    "pr_number": 8463,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1413813752,
    "comment_created_at": "2023-12-04T12:38:02Z"
  },
  {
    "code": "@@ -164,9 +164,18 @@ def meta_get_tables(self, table_name: str, main_metadata: Dict) -> Dict:\n             \"\"\"\n             client = self.handler.connect()\n \n-            resource_metadata = next(\n-                (resource for resource in main_metadata if resource[\"name\"].lower() == resource_name),\n-            )\n+            try:\n+                resource_metadata = next(\n+                    (resource for resource in main_metadata if resource[\"name\"].lower() == resource_name),\n+                )",
    "comment": "**performance**: The repeated use of `next((resource for resource in main_metadata if resource[\"name\"].lower() == resource_name), ...)` for large `main_metadata` lists is O(n) per call and can cause significant slowdowns if called frequently or in a loop.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/integrations/handlers/salesforce_handler/salesforce_tables.py, lines 166-170, the code uses a generator expression with next() to find a resource in main_metadata by name, which is O(n) and can be slow for large lists or repeated calls. Refactor this to build a dictionary mapping lowercased resource names to resources, then use a direct lookup. Replace the generator expression with a dictionary lookup for better performance.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n\n            resource_lookup = {resource[\"name\"].lower(): resource for resource in main_metadata}\n            resource_metadata = resource_lookup.get(resource_name)\n            if resource_metadata is None:\n                raise StopIteration()\n```\n</details>\n<!-- suggestion_end -->\n",
    "line_number": 170,
    "enriched": "File: mindsdb/integrations/handlers/salesforce_handler/salesforce_tables.py\nCode: @@ -164,9 +164,18 @@ def meta_get_tables(self, table_name: str, main_metadata: Dict) -> Dict:\n             \"\"\"\n             client = self.handler.connect()\n \n-            resource_metadata = next(\n-                (resource for resource in main_metadata if resource[\"name\"].lower() == resource_name),\n-            )\n+            try:\n+                resource_metadata = next(\n+                    (resource for resource in main_metadata if resource[\"name\"].lower() == resource_name),\n+                )\nComment: **performance**: The repeated use of `next((resource for resource in main_metadata if resource[\"name\"].lower() == resource_name), ...)` for large `main_metadata` lists is O(n) per call and can cause significant slowdowns if called frequently or in a loop.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/integrations/handlers/salesforce_handler/salesforce_tables.py, lines 166-170, the code uses a generator expression with next() to find a resource in main_metadata by name, which is O(n) and can be slow for large lists or repeated calls. Refactor this to build a dictionary mapping lowercased resource names to resources, then use a direct lookup. Replace the generator expression with a dictionary lookup for better performance.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n\n            resource_lookup = {resource[\"name\"].lower(): resource for resource in main_metadata}\n            resource_metadata = resource_lookup.get(resource_name)\n            if resource_metadata is None:\n                raise StopIteration()\n```\n</details>\n<!-- suggestion_end -->\n",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/salesforce_handler/salesforce_tables.py",
    "pr_number": 11274,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2197513849,
    "comment_created_at": "2025-07-10T11:55:19Z"
  },
  {
    "code": "@@ -1300,17 +1303,35 @@ def answer_drop_tables(self, statement, database_name):\n \n         return ExecuteAnswer()\n \n-    def answer_create_view(self, statement, database_name):\n+    def answer_create_or_alter_view(self, statement: ASTNode, database_name: str) -> ExecuteAnswer:\n+        \"\"\"Process CREATE and ALTER VIEW commands\n+\n+        Args:\n+            statement (ASTNode): data for creating or altering view\n+            database_name (str): name of the current database\n+\n+        Returns:\n+            ExecuteAnswer: answer for the command\n+        \"\"\"\n         project_name = database_name\n-        # TEMP\n-        if isinstance(statement.name, Identifier):\n+\n+        if isinstance(statement.name, str):",
    "comment": "I think 'name' can't be a string here",
    "line_number": 1318,
    "enriched": "File: mindsdb/api/executor/command_executor.py\nCode: @@ -1300,17 +1303,35 @@ def answer_drop_tables(self, statement, database_name):\n \n         return ExecuteAnswer()\n \n-    def answer_create_view(self, statement, database_name):\n+    def answer_create_or_alter_view(self, statement: ASTNode, database_name: str) -> ExecuteAnswer:\n+        \"\"\"Process CREATE and ALTER VIEW commands\n+\n+        Args:\n+            statement (ASTNode): data for creating or altering view\n+            database_name (str): name of the current database\n+\n+        Returns:\n+            ExecuteAnswer: answer for the command\n+        \"\"\"\n         project_name = database_name\n-        # TEMP\n-        if isinstance(statement.name, Identifier):\n+\n+        if isinstance(statement.name, str):\nComment: I think 'name' can't be a string here",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/api/executor/command_executor.py",
    "pr_number": 10907,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2121258512,
    "comment_created_at": "2025-06-02T14:06:23Z"
  },
  {
    "code": "@@ -97,6 +97,17 @@ def get_reranking_model_from_params(reranking_model_params: dict):\n     return BaseLLMReranker(**params_copy)\n \n \n+def safe_pandas_is_datetime(value: str) -> bool:\n+    \"\"\"\n+    Check if the value can be parsed as a datetime.\n+    \"\"\"\n+    try:\n+        result = pd.api.types.is_datetime64_any_dtype(value)\n+        return result\n+    except ValueError:\n+        return False",
    "comment": "**Correctness**: The `safe_pandas_is_datetime` function incorrectly uses `pd.api.types.is_datetime64_any_dtype` which expects a pandas Series or array-like object, not individual scalar values. This will cause the function to return `False` for all scalar datetime values instead of properly detecting them.\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\ndef safe_pandas_is_datetime(value: str) -> bool:\n    \"\"\"\n    Check if the value can be parsed as a datetime.\n    \"\"\"\n    try:\n        # Check if it's already a datetime-like object\n        if isinstance(value, (pd.Timestamp, np.datetime64)):\n            return True\n        # Try to parse as datetime\n        pd.to_datetime(value)\n        return True\n    except (ValueError, TypeError):\n        return False\n```\n</details>\n<!-- suggestion_end -->\n",
    "line_number": 108,
    "enriched": "File: mindsdb/interfaces/knowledge_base/controller.py\nCode: @@ -97,6 +97,17 @@ def get_reranking_model_from_params(reranking_model_params: dict):\n     return BaseLLMReranker(**params_copy)\n \n \n+def safe_pandas_is_datetime(value: str) -> bool:\n+    \"\"\"\n+    Check if the value can be parsed as a datetime.\n+    \"\"\"\n+    try:\n+        result = pd.api.types.is_datetime64_any_dtype(value)\n+        return result\n+    except ValueError:\n+        return False\nComment: **Correctness**: The `safe_pandas_is_datetime` function incorrectly uses `pd.api.types.is_datetime64_any_dtype` which expects a pandas Series or array-like object, not individual scalar values. This will cause the function to return `False` for all scalar datetime values instead of properly detecting them.\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\ndef safe_pandas_is_datetime(value: str) -> bool:\n    \"\"\"\n    Check if the value can be parsed as a datetime.\n    \"\"\"\n    try:\n        # Check if it's already a datetime-like object\n        if isinstance(value, (pd.Timestamp, np.datetime64)):\n            return True\n        # Try to parse as datetime\n        pd.to_datetime(value)\n        return True\n    except (ValueError, TypeError):\n        return False\n```\n</details>\n<!-- suggestion_end -->\n",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/interfaces/knowledge_base/controller.py",
    "pr_number": 10931,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2113877438,
    "comment_created_at": "2025-05-29T12:49:48Z"
  },
  {
    "code": "@@ -127,39 +153,49 @@ def _load_column_statistics(self, tables: db.MetaTables, columns: db.MetaColumns\n         response = self.data_handler.meta_get_column_statistics(self.table_names)\n         df = response.data_frame\n \n-        return self._add_column_statistics(df, tables, columns)\n+        if df.empty:\n+            self.logger.info(f\"No column statistics metadata to load for {self.database_name}.\")\n+\n+        df.columns = df.columns.str.lower()\n+        self._add_column_statistics(df, tables, columns)\n+        self.logger.info(f\"Column statistics metadata loaded for {self.database_name}.\")",
    "comment": "**correctness**: `_load_column_statistics`, `_load_primary_keys`, and similar methods always lowercase DataFrame columns even if DataFrame is empty, which may cause an AttributeError if `df` is not a DataFrame (e.g., None).\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n        if df is None or df.empty:\n            self.logger.info(f\"No column statistics metadata to load for {self.database_name}.\")\n            return\n\n        df.columns = df.columns.str.lower()\n        self._add_column_statistics(df, tables, columns)\n        self.logger.info(f\"Column statistics metadata loaded for {self.database_name}.\")\n```\n</details>\n<!-- suggestion_end -->\n",
    "line_number": 161,
    "enriched": "File: mindsdb/interfaces/data_catalog/data_catalog_loader.py\nCode: @@ -127,39 +153,49 @@ def _load_column_statistics(self, tables: db.MetaTables, columns: db.MetaColumns\n         response = self.data_handler.meta_get_column_statistics(self.table_names)\n         df = response.data_frame\n \n-        return self._add_column_statistics(df, tables, columns)\n+        if df.empty:\n+            self.logger.info(f\"No column statistics metadata to load for {self.database_name}.\")\n+\n+        df.columns = df.columns.str.lower()\n+        self._add_column_statistics(df, tables, columns)\n+        self.logger.info(f\"Column statistics metadata loaded for {self.database_name}.\")\nComment: **correctness**: `_load_column_statistics`, `_load_primary_keys`, and similar methods always lowercase DataFrame columns even if DataFrame is empty, which may cause an AttributeError if `df` is not a DataFrame (e.g., None).\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n        if df is None or df.empty:\n            self.logger.info(f\"No column statistics metadata to load for {self.database_name}.\")\n            return\n\n        df.columns = df.columns.str.lower()\n        self._add_column_statistics(df, tables, columns)\n        self.logger.info(f\"Column statistics metadata loaded for {self.database_name}.\")\n```\n</details>\n<!-- suggestion_end -->\n",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/interfaces/data_catalog/data_catalog_loader.py",
    "pr_number": 11021,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2128573702,
    "comment_created_at": "2025-06-05T11:04:20Z"
  },
  {
    "code": "@@ -225,7 +225,12 @@ def get_comments(self, video_id: str, channel_id: str):\n \n         youtube_comments_df = pd.json_normalize(data, 'replies', ['comment_id', 'channel_id', 'video_id', 'user_id', 'display_name', 'comment', \"published_at\", \"updated_at\"], record_prefix='replies.')\n         youtube_comments_df = youtube_comments_df.rename(columns={'replies.user_id': 'reply_user_id', 'replies.reply_author': 'reply_author', 'replies.reply': 'reply'})\n-        return youtube_comments_df[['comment_id', 'channel_id', 'video_id', 'user_id', 'display_name', 'comment', \"published_at\", \"updated_at\", 'reply_user_id', 'reply_author', 'reply']]\n+        \n+        # check if DataFrame is empty\n+        if youtube_comments_df.empty:\n+            return youtube_comments_df\n+        else:",
    "comment": "Maybe else is not needed since we will return the df before",
    "line_number": 232,
    "enriched": "File: mindsdb/integrations/handlers/youtube_handler/youtube_tables.py\nCode: @@ -225,7 +225,12 @@ def get_comments(self, video_id: str, channel_id: str):\n \n         youtube_comments_df = pd.json_normalize(data, 'replies', ['comment_id', 'channel_id', 'video_id', 'user_id', 'display_name', 'comment', \"published_at\", \"updated_at\"], record_prefix='replies.')\n         youtube_comments_df = youtube_comments_df.rename(columns={'replies.user_id': 'reply_user_id', 'replies.reply_author': 'reply_author', 'replies.reply': 'reply'})\n-        return youtube_comments_df[['comment_id', 'channel_id', 'video_id', 'user_id', 'display_name', 'comment', \"published_at\", \"updated_at\", 'reply_user_id', 'reply_author', 'reply']]\n+        \n+        # check if DataFrame is empty\n+        if youtube_comments_df.empty:\n+            return youtube_comments_df\n+        else:\nComment: Maybe else is not needed since we will return the df before",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/youtube_handler/youtube_tables.py",
    "pr_number": 8614,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1450515144,
    "comment_created_at": "2024-01-12T14:22:07Z"
  },
  {
    "code": "@@ -1,9 +1,29 @@\n-__title__ = 'MindsDB Email handler'\n-__package_name__ = 'mindsdb_email_handler'\n-__version__ = '0.0.1'\n+\"\"\"\n+Email handler package metadata and handler UI hints.\n+Note: Keep this file import-safe and free of heavy imports.\n+\"\"\"\n+\n+from enum import Enum\n+\n+__title__ = \"MindsDB Email handler\"\n+__package_name__ = \"mindsdb_email_handler\"\n+__version__ = \"0.0.5\"\n __description__ = \"MindsDB handler for email\"\n-__author__ = 'MindsDB Inc'\n-__github__ = 'https://github.com/mindsdb/mindsdb'\n-__pypi__ = 'https://pypi.org/project/mindsdb/'\n-__license__ = 'MIT'\n-__copyright__ = 'Copyright 2022- mindsdb'\n+__author__ = \"MindsDB Inc\"\n+__github__ = \"https://github.com/mindsdb/mindsdb\"\n+__pypi__ = \"https://pypi.org/project/mindsdb/\"\n+__license__ = \"MIT\"\n+__icon_path__ = \"icon.png\"\n+# Restored for compatibility with tools that expect this metadata\n+__copyright__ = \"Copyright 2022- mindsdb\"\n+\n+# Robust, import-safe HANDLER_TYPE assignment:\n+# Prefer the enum-like source; if unavailable, fall back to a compatible enum member, not a bare string.\n+try:\n+    from mindsdb.integrations.libs.const import HANDLER_TYPE as _HANDLER_TYPE_ENUM  # type: ignore\n+\n+    HANDLER_TYPE = getattr(_HANDLER_TYPE_ENUM, \"DATA\")\n+except Exception:\n+    # Fallback to a compatible enum member to avoid breaking code that expects Enum semantics.\n+    _FallbackHT = Enum(\"HANDLER_TYPE\", {\"DATA\": \"data\"})\n+    HANDLER_TYPE = _FallbackHT.DATA",
    "comment": "**correctness**: No significant functional correctness issues found; all changes are metadata assignments and robust fallback logic for HANDLER_TYPE, which do not affect runtime behavior or program correctness.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nNo action required. The file mindsdb/integrations/handlers/email_handler/__about__.py (lines 1-29) contains only metadata and robust fallback logic, with no functional correctness issues detected.\n```\n</details>\n<!-- ai_prompt_end -->\n\n",
    "line_number": 29,
    "enriched": "File: mindsdb/integrations/handlers/email_handler/__about__.py\nCode: @@ -1,9 +1,29 @@\n-__title__ = 'MindsDB Email handler'\n-__package_name__ = 'mindsdb_email_handler'\n-__version__ = '0.0.1'\n+\"\"\"\n+Email handler package metadata and handler UI hints.\n+Note: Keep this file import-safe and free of heavy imports.\n+\"\"\"\n+\n+from enum import Enum\n+\n+__title__ = \"MindsDB Email handler\"\n+__package_name__ = \"mindsdb_email_handler\"\n+__version__ = \"0.0.5\"\n __description__ = \"MindsDB handler for email\"\n-__author__ = 'MindsDB Inc'\n-__github__ = 'https://github.com/mindsdb/mindsdb'\n-__pypi__ = 'https://pypi.org/project/mindsdb/'\n-__license__ = 'MIT'\n-__copyright__ = 'Copyright 2022- mindsdb'\n+__author__ = \"MindsDB Inc\"\n+__github__ = \"https://github.com/mindsdb/mindsdb\"\n+__pypi__ = \"https://pypi.org/project/mindsdb/\"\n+__license__ = \"MIT\"\n+__icon_path__ = \"icon.png\"\n+# Restored for compatibility with tools that expect this metadata\n+__copyright__ = \"Copyright 2022- mindsdb\"\n+\n+# Robust, import-safe HANDLER_TYPE assignment:\n+# Prefer the enum-like source; if unavailable, fall back to a compatible enum member, not a bare string.\n+try:\n+    from mindsdb.integrations.libs.const import HANDLER_TYPE as _HANDLER_TYPE_ENUM  # type: ignore\n+\n+    HANDLER_TYPE = getattr(_HANDLER_TYPE_ENUM, \"DATA\")\n+except Exception:\n+    # Fallback to a compatible enum member to avoid breaking code that expects Enum semantics.\n+    _FallbackHT = Enum(\"HANDLER_TYPE\", {\"DATA\": \"data\"})\n+    HANDLER_TYPE = _FallbackHT.DATA\nComment: **correctness**: No significant functional correctness issues found; all changes are metadata assignments and robust fallback logic for HANDLER_TYPE, which do not affect runtime behavior or program correctness.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nNo action required. The file mindsdb/integrations/handlers/email_handler/__about__.py (lines 1-29) contains only metadata and robust fallback logic, with no functional correctness issues detected.\n```\n</details>\n<!-- ai_prompt_end -->\n\n",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/email_handler/__about__.py",
    "pr_number": 11753,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2436151121,
    "comment_created_at": "2025-10-16T14:20:12Z"
  },
  {
    "code": "@@ -30,38 +24,45 @@ def list(\n         urls = []\n         crawl_depth = None\n         per_url_limit = None\n+        headers = {}\n         for condition in conditions:\n-            if condition.column == 'url':\n+            if condition.column == \"url\":\n                 if condition.op == FilterOperator.IN:\n                     urls = condition.value\n                 elif condition.op == FilterOperator.EQUAL:\n                     urls = [condition.value]\n                 condition.applied = True\n-            if condition.column == 'crawl_depth' and condition.op == FilterOperator.EQUAL:\n+            if condition.column == \"crawl_depth\" and condition.op == FilterOperator.EQUAL:\n                 crawl_depth = condition.value\n                 condition.applied = True\n-            if condition.column == 'per_url_limit' and condition.op == FilterOperator.EQUAL:\n+            if condition.column == \"per_url_limit\" and condition.op == FilterOperator.EQUAL:\n                 per_url_limit = condition.value\n                 condition.applied = True\n+            if condition.column.lower() == \"user_agent\" and condition.op == FilterOperator.EQUAL:\n+                headers[\"User-Agent\"] = condition.value\n+                condition.applied = True\n ",
    "comment": "**correctness**: `conditions` is assumed to be iterable in `CrawlerTable.list`, but if `conditions` is None (the default), a `TypeError` will occur at runtime.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/integrations/handlers/web_handler/web_handler.py, lines 28-44, the code iterates over `conditions` without checking if it is None. This will cause a TypeError if `conditions` is not provided. Please add a check to set `conditions = []` if it is None before the for loop, preserving indentation and logic.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n        if conditions is None:\n            conditions = []\n        for condition in conditions:\n            if condition.column == \"url\":\n                if condition.op == FilterOperator.IN:\n                    urls = condition.value\n                elif condition.op == FilterOperator.EQUAL:\n                    urls = [condition.value]\n                condition.applied = True\n            if condition.column == \"crawl_depth\" and condition.op == FilterOperator.EQUAL:\n                crawl_depth = condition.value\n                condition.applied = True\n            if condition.column == \"per_url_limit\" and condition.op == FilterOperator.EQUAL:\n                per_url_limit = condition.value\n                condition.applied = True\n            if condition.column.lower() == \"user_agent\" and condition.op == FilterOperator.EQUAL:\n                headers[\"User-Agent\"] = condition.value\n                condition.applied = True\n\n```\n</details>\n<!-- suggestion_end -->\n",
    "line_number": 44,
    "enriched": "File: mindsdb/integrations/handlers/web_handler/web_handler.py\nCode: @@ -30,38 +24,45 @@ def list(\n         urls = []\n         crawl_depth = None\n         per_url_limit = None\n+        headers = {}\n         for condition in conditions:\n-            if condition.column == 'url':\n+            if condition.column == \"url\":\n                 if condition.op == FilterOperator.IN:\n                     urls = condition.value\n                 elif condition.op == FilterOperator.EQUAL:\n                     urls = [condition.value]\n                 condition.applied = True\n-            if condition.column == 'crawl_depth' and condition.op == FilterOperator.EQUAL:\n+            if condition.column == \"crawl_depth\" and condition.op == FilterOperator.EQUAL:\n                 crawl_depth = condition.value\n                 condition.applied = True\n-            if condition.column == 'per_url_limit' and condition.op == FilterOperator.EQUAL:\n+            if condition.column == \"per_url_limit\" and condition.op == FilterOperator.EQUAL:\n                 per_url_limit = condition.value\n                 condition.applied = True\n+            if condition.column.lower() == \"user_agent\" and condition.op == FilterOperator.EQUAL:\n+                headers[\"User-Agent\"] = condition.value\n+                condition.applied = True\n \nComment: **correctness**: `conditions` is assumed to be iterable in `CrawlerTable.list`, but if `conditions` is None (the default), a `TypeError` will occur at runtime.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/integrations/handlers/web_handler/web_handler.py, lines 28-44, the code iterates over `conditions` without checking if it is None. This will cause a TypeError if `conditions` is not provided. Please add a check to set `conditions = []` if it is None before the for loop, preserving indentation and logic.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n        if conditions is None:\n            conditions = []\n        for condition in conditions:\n            if condition.column == \"url\":\n                if condition.op == FilterOperator.IN:\n                    urls = condition.value\n                elif condition.op == FilterOperator.EQUAL:\n                    urls = [condition.value]\n                condition.applied = True\n            if condition.column == \"crawl_depth\" and condition.op == FilterOperator.EQUAL:\n                crawl_depth = condition.value\n                condition.applied = True\n            if condition.column == \"per_url_limit\" and condition.op == FilterOperator.EQUAL:\n                per_url_limit = condition.value\n                condition.applied = True\n            if condition.column.lower() == \"user_agent\" and condition.op == FilterOperator.EQUAL:\n                headers[\"User-Agent\"] = condition.value\n                condition.applied = True\n\n```\n</details>\n<!-- suggestion_end -->\n",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/web_handler/web_handler.py",
    "pr_number": 11294,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2205072292,
    "comment_created_at": "2025-07-14T14:21:23Z"
  },
  {
    "code": "@@ -239,6 +241,136 @@ def delete(self, project_name, agent_name):\n         session.agents_controller.delete_agent(agent_name, project_name=project_name)\n         return '', HTTPStatus.NO_CONTENT\n \n+def _completion_event_generator(\n+        agent_name: str,\n+        messages: List[Dict],\n+        trace_id: str,\n+        observation_id: str,\n+        project_name: str,\n+        run_completion_span,\n+        api_trace) -> Iterable[str]:\n+    # Populate API key by default if not present.\n+    session = SessionController()\n+    existing_agent = session.agents_controller.get_agent(agent_name, project_name=project_name)\n+    if not existing_agent.params:\n+        existing_agent.params = {}\n+    existing_agent.params['openai_api_key'] = existing_agent.params.get('openai_api_key', os.getenv('OPENAI_API_KEY'))\n+    # Have to commit/flush here so DB isn't locked while streaming.\n+    db.session.commit()\n+    db.session.flush()\n+\n+    completion_stream = session.agents_controller.get_completion_stream(",
    "comment": "get_completion and get_completion_stream in controller and in langchain_agent are almost identical.\r\nmaybe we can reduce amount of code by using here: \r\n`completion_stream = session.agents_controller.get_completion(..., stream=True) `\r\n\r\nand in langchain_agent\r\n```python\r\nif stream:\r\n   return self.stream_agent(df, agent, args)\r\nelse:\r\n   return self.run_agent(df, agent, args)\r\n```",
    "line_number": 262,
    "enriched": "File: mindsdb/api/http/namespaces/agents.py\nCode: @@ -239,6 +241,136 @@ def delete(self, project_name, agent_name):\n         session.agents_controller.delete_agent(agent_name, project_name=project_name)\n         return '', HTTPStatus.NO_CONTENT\n \n+def _completion_event_generator(\n+        agent_name: str,\n+        messages: List[Dict],\n+        trace_id: str,\n+        observation_id: str,\n+        project_name: str,\n+        run_completion_span,\n+        api_trace) -> Iterable[str]:\n+    # Populate API key by default if not present.\n+    session = SessionController()\n+    existing_agent = session.agents_controller.get_agent(agent_name, project_name=project_name)\n+    if not existing_agent.params:\n+        existing_agent.params = {}\n+    existing_agent.params['openai_api_key'] = existing_agent.params.get('openai_api_key', os.getenv('OPENAI_API_KEY'))\n+    # Have to commit/flush here so DB isn't locked while streaming.\n+    db.session.commit()\n+    db.session.flush()\n+\n+    completion_stream = session.agents_controller.get_completion_stream(\nComment: get_completion and get_completion_stream in controller and in langchain_agent are almost identical.\r\nmaybe we can reduce amount of code by using here: \r\n`completion_stream = session.agents_controller.get_completion(..., stream=True) `\r\n\r\nand in langchain_agent\r\n```python\r\nif stream:\r\n   return self.stream_agent(df, agent, args)\r\nelse:\r\n   return self.run_agent(df, agent, args)\r\n```",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/api/http/namespaces/agents.py",
    "pr_number": 9488,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1678062263,
    "comment_created_at": "2024-07-15T15:56:33Z"
  },
  {
    "code": "@@ -1,9 +1,27 @@\n-__title__ = 'MindsDB Email handler'\n-__package_name__ = 'mindsdb_email_handler'\n-__version__ = '0.0.1'\n+\"\"\"\n+Email handler package metadata and handler UI hints.\n+Note: Keep this file import-safe and free of heavy imports.\n+\"\"\"\n+\n+from enum import Enum\n+\n+__title__ = \"MindsDB Email handler\"\n+__package_name__ = \"mindsdb_email_handler\"\n+__version__ = \"0.0.5\"\n __description__ = \"MindsDB handler for email\"\n-__author__ = 'MindsDB Inc'\n-__github__ = 'https://github.com/mindsdb/mindsdb'\n-__pypi__ = 'https://pypi.org/project/mindsdb/'\n-__license__ = 'MIT'\n-__copyright__ = 'Copyright 2022- mindsdb'\n+__author__ = \"MindsDB Inc\"\n+__github__ = \"https://github.com/mindsdb/mindsdb\"\n+__pypi__ = \"https://pypi.org/project/mindsdb/\"\n+__license__ = \"MIT\"\n+__icon_path__ = \"icon.png\"",
    "comment": "**correctness**: `__copyright__` variable is removed, which may break code or tools expecting this metadata; this can cause runtime errors if accessed elsewhere.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/integrations/handlers/email_handler/__about__.py, at line 16, the `__copyright__` variable was removed. This may break code or tools that expect this metadata and could cause runtime errors if accessed elsewhere. Please restore the `__copyright__` variable with the value `\"Copyright 2022- mindsdb\"` at line 16.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n__copyright__ = \"Copyright 2022- mindsdb\"\n\n```\n</details>\n<!-- suggestion_end -->\n",
    "line_number": 16,
    "enriched": "File: mindsdb/integrations/handlers/email_handler/__about__.py\nCode: @@ -1,9 +1,27 @@\n-__title__ = 'MindsDB Email handler'\n-__package_name__ = 'mindsdb_email_handler'\n-__version__ = '0.0.1'\n+\"\"\"\n+Email handler package metadata and handler UI hints.\n+Note: Keep this file import-safe and free of heavy imports.\n+\"\"\"\n+\n+from enum import Enum\n+\n+__title__ = \"MindsDB Email handler\"\n+__package_name__ = \"mindsdb_email_handler\"\n+__version__ = \"0.0.5\"\n __description__ = \"MindsDB handler for email\"\n-__author__ = 'MindsDB Inc'\n-__github__ = 'https://github.com/mindsdb/mindsdb'\n-__pypi__ = 'https://pypi.org/project/mindsdb/'\n-__license__ = 'MIT'\n-__copyright__ = 'Copyright 2022- mindsdb'\n+__author__ = \"MindsDB Inc\"\n+__github__ = \"https://github.com/mindsdb/mindsdb\"\n+__pypi__ = \"https://pypi.org/project/mindsdb/\"\n+__license__ = \"MIT\"\n+__icon_path__ = \"icon.png\"\nComment: **correctness**: `__copyright__` variable is removed, which may break code or tools expecting this metadata; this can cause runtime errors if accessed elsewhere.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/integrations/handlers/email_handler/__about__.py, at line 16, the `__copyright__` variable was removed. This may break code or tools that expect this metadata and could cause runtime errors if accessed elsewhere. Please restore the `__copyright__` variable with the value `\"Copyright 2022- mindsdb\"` at line 16.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n__copyright__ = \"Copyright 2022- mindsdb\"\n\n```\n</details>\n<!-- suggestion_end -->\n",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/email_handler/__about__.py",
    "pr_number": 11721,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2421364531,
    "comment_created_at": "2025-10-10T17:28:47Z"
  },
  {
    "code": "@@ -149,12 +150,19 @@ def process_documents(self, documents: List[Document]) -> List[ProcessedChunk]:\n         processed_chunks = []\n \n         for doc in documents:\n+            # Skip empty or whitespace-only content\n+            if not doc.content or not doc.content.strip():\n+                continue\n+\n             chunk_docs = self._split_document(doc)\n \n             # Single chunk case\n             if len(chunk_docs) == 1:\n                 context = self._generate_context(chunk_docs[0].content, doc.content)\n-                processed_content = f\"{context}\\n\\n{chunk_docs[0].content}\"\n+                if self.summarize:\n+                    processed_content = context\n+                else:",
    "comment": "To avoid unnecessary `else`, this would be simpler as:\r\n\r\n```python\r\nprocessed_content = f\"{context}\\n\\n{chunk_docs[0].content}\"\r\nif self.summarize:\r\n    processed_content = context\r\n```",
    "line_number": 164,
    "enriched": "File: mindsdb/interfaces/knowledge_base/preprocessing/document_preprocessor.py\nCode: @@ -149,12 +150,19 @@ def process_documents(self, documents: List[Document]) -> List[ProcessedChunk]:\n         processed_chunks = []\n \n         for doc in documents:\n+            # Skip empty or whitespace-only content\n+            if not doc.content or not doc.content.strip():\n+                continue\n+\n             chunk_docs = self._split_document(doc)\n \n             # Single chunk case\n             if len(chunk_docs) == 1:\n                 context = self._generate_context(chunk_docs[0].content, doc.content)\n-                processed_content = f\"{context}\\n\\n{chunk_docs[0].content}\"\n+                if self.summarize:\n+                    processed_content = context\n+                else:\nComment: To avoid unnecessary `else`, this would be simpler as:\r\n\r\n```python\r\nprocessed_content = f\"{context}\\n\\n{chunk_docs[0].content}\"\r\nif self.summarize:\r\n    processed_content = context\r\n```",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/interfaces/knowledge_base/preprocessing/document_preprocessor.py",
    "pr_number": 10232,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1868148978,
    "comment_created_at": "2024-12-03T17:38:30Z"
  },
  {
    "code": "@@ -37,9 +39,19 @@ def create(self, target: str, df: Optional[pd.DataFrame] = None, args: Optional[\n         args['target'] = target\n         self.model_storage.json_set('args', args)\n         try:\n-            resp = requests.post(args['train_url'],\n-                                 json={'df': df.to_json(orient='records'), 'target': target},\n-                                 headers={'content-type': 'application/json; format=pandas-records'})\n+            print(pd.__version__)\n+            if args.get('parquet', False):\n+                buffer = io.BytesIO()\n+                df.attrs['target'] = target\n+                df.to_parquet(buffer)",
    "comment": "Please use  the read_parquet from the FileReader https://github.com/mindsdb/mindsdb/blob/bacf861958191fe53e62369f025ebe7a1e27a94c/mindsdb/integrations/utilities/files/file_reader.py#L303",
    "line_number": 46,
    "enriched": "File: mindsdb/integrations/handlers/ray_serve_handler/ray_serve_handler.py\nCode: @@ -37,9 +39,19 @@ def create(self, target: str, df: Optional[pd.DataFrame] = None, args: Optional[\n         args['target'] = target\n         self.model_storage.json_set('args', args)\n         try:\n-            resp = requests.post(args['train_url'],\n-                                 json={'df': df.to_json(orient='records'), 'target': target},\n-                                 headers={'content-type': 'application/json; format=pandas-records'})\n+            print(pd.__version__)\n+            if args.get('parquet', False):\n+                buffer = io.BytesIO()\n+                df.attrs['target'] = target\n+                df.to_parquet(buffer)\nComment: Please use  the read_parquet from the FileReader https://github.com/mindsdb/mindsdb/blob/bacf861958191fe53e62369f025ebe7a1e27a94c/mindsdb/integrations/utilities/files/file_reader.py#L303",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/ray_serve_handler/ray_serve_handler.py",
    "pr_number": 10449,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1950768948,
    "comment_created_at": "2025-02-11T12:32:15Z"
  },
  {
    "code": "@@ -112,9 +112,8 @@ CREATE VIEW btcusdt_recent AS (\n This view is going to be joined with the model to get forecasts:\n \n ```sql\n-SELECT to_timestamp(cast(m.open_time as bigint)) AS open_time,\n-       m.open_price,\n-       m.open_price_explain\n+SELECT m.open_time ,",
    "comment": "Let's make this change in the README of TimeGPT as well.",
    "line_number": 115,
    "enriched": "File: docs/integrations/ai-engines/timegpt.mdx\nCode: @@ -112,9 +112,8 @@ CREATE VIEW btcusdt_recent AS (\n This view is going to be joined with the model to get forecasts:\n \n ```sql\n-SELECT to_timestamp(cast(m.open_time as bigint)) AS open_time,\n-       m.open_price,\n-       m.open_price_explain\n+SELECT m.open_time ,\nComment: Let's make this change in the README of TimeGPT as well.",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "docs/integrations/ai-engines/timegpt.mdx",
    "pr_number": 10478,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1958455673,
    "comment_created_at": "2025-02-17T15:53:18Z"
  },
  {
    "code": "@@ -818,7 +821,64 @@ def from_query(self, query=None):\n         else:\n             raise PlanningException(f'Unsupported query type {type(query)}')\n \n-        return self.plan\n+        plan = self.handle_partitioning(self.plan)\n+\n+        return plan\n+\n+    def handle_partitioning(self, plan):\n+        # handle fetchdataframe partitioning\n+        steps_out = []\n+\n+        partition_step = None\n+        for step in plan.steps:\n+            if isinstance(step, FetchDataframeStep) and step.params is not None:\n+                batch_size = step.params.get('batch_size')\n+                if batch_size is not None:\n+                    # found batched fetch\n+                    partition_step = FetchDataframeStepPartition(\n+                        step_num=step.step_num,\n+                        integration=step.integration,\n+                        query=step.query,\n+                        raw_query=step.raw_query,\n+                        params=step.params\n+                    )\n+                    steps_out.append(partition_step)\n+                    # mark plan\n+                    plan.is_resumable = True\n+                    continue\n+\n+            if partition_step is not None:\n+                # check and add step into partition\n+\n+                can_be_partitioned = False\n+                if isinstance(step, (JoinStep, ApplyPredictorStep, InsertToTable)):",
    "comment": "Better to make `can_be_partitioned` as property of a step",
    "line_number": 854,
    "enriched": "File: mindsdb/api/executor/planner/query_planner.py\nCode: @@ -818,7 +821,64 @@ def from_query(self, query=None):\n         else:\n             raise PlanningException(f'Unsupported query type {type(query)}')\n \n-        return self.plan\n+        plan = self.handle_partitioning(self.plan)\n+\n+        return plan\n+\n+    def handle_partitioning(self, plan):\n+        # handle fetchdataframe partitioning\n+        steps_out = []\n+\n+        partition_step = None\n+        for step in plan.steps:\n+            if isinstance(step, FetchDataframeStep) and step.params is not None:\n+                batch_size = step.params.get('batch_size')\n+                if batch_size is not None:\n+                    # found batched fetch\n+                    partition_step = FetchDataframeStepPartition(\n+                        step_num=step.step_num,\n+                        integration=step.integration,\n+                        query=step.query,\n+                        raw_query=step.raw_query,\n+                        params=step.params\n+                    )\n+                    steps_out.append(partition_step)\n+                    # mark plan\n+                    plan.is_resumable = True\n+                    continue\n+\n+            if partition_step is not None:\n+                # check and add step into partition\n+\n+                can_be_partitioned = False\n+                if isinstance(step, (JoinStep, ApplyPredictorStep, InsertToTable)):\nComment: Better to make `can_be_partitioned` as property of a step",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/api/executor/planner/query_planner.py",
    "pr_number": 10599,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2012100735,
    "comment_created_at": "2025-03-25T13:28:00Z"
  },
  {
    "code": "@@ -1,8 +1,190 @@\n ---\n-title: NeuralForecast\n+title: Nixtla's NeuralForecast Integration with MindsDB\n sidebarTitle: NeuralForecast\n ---\n \n-<Note>\n-This page is a work in progress.\n-</Note>\n+Nixtla’s NeuralForecast provides a diverse array of neural forecasting models, prioritizing their ease of use and resilience. These models encompass a spectrum of options, including traditional networks like MLP and RNNs, as well as cutting-edge innovations such as NBEATS, NHITS, TFT, and various other architectural approaches.\n+\n+You can learn more about its features [here](https://nixtla.github.io/neuralforecast/).\n+\n+## How to bring NeuralForecast Models to MindsDB\n+\n+Before creating a model, you will need to create an ML engine for NeuralForecast using the `CREATE ML_ENGINE` statement:\n+\n+```sql\n+CREATE ML_ENGINE neuralforecast\n+FROM neuralforecast;\n+```\n+\n+Once the ML engine is created, we use the `CREATE MODEL` statement to create the NeuralForecast model in MindsDB.\n+\n+```sql\n+CREATE MODEL model_name\n+FROM data_source\n+  (SELECT * FROM table_name)\n+PREDICT column_to_be_predicted\n+ORDER BY date_column\n+GROUP BY column_name, column_name, ...\n+WINDOW 12 -- model looks back at sets of 12 rows each\n+HORIZON 3 -- model forecasts the next 3 rows\n+USING ENGINE = 'neuralforecast';\n+```\n+\n+To ensure that the model is created based on the NeuralForecast engine, include the `USING` clause at the end.",
    "comment": "Maybe this can be <tip>",
    "line_number": 33,
    "enriched": "File: docs/integrations/ai-engines/neuralforecast.mdx\nCode: @@ -1,8 +1,190 @@\n ---\n-title: NeuralForecast\n+title: Nixtla's NeuralForecast Integration with MindsDB\n sidebarTitle: NeuralForecast\n ---\n \n-<Note>\n-This page is a work in progress.\n-</Note>\n+Nixtla’s NeuralForecast provides a diverse array of neural forecasting models, prioritizing their ease of use and resilience. These models encompass a spectrum of options, including traditional networks like MLP and RNNs, as well as cutting-edge innovations such as NBEATS, NHITS, TFT, and various other architectural approaches.\n+\n+You can learn more about its features [here](https://nixtla.github.io/neuralforecast/).\n+\n+## How to bring NeuralForecast Models to MindsDB\n+\n+Before creating a model, you will need to create an ML engine for NeuralForecast using the `CREATE ML_ENGINE` statement:\n+\n+```sql\n+CREATE ML_ENGINE neuralforecast\n+FROM neuralforecast;\n+```\n+\n+Once the ML engine is created, we use the `CREATE MODEL` statement to create the NeuralForecast model in MindsDB.\n+\n+```sql\n+CREATE MODEL model_name\n+FROM data_source\n+  (SELECT * FROM table_name)\n+PREDICT column_to_be_predicted\n+ORDER BY date_column\n+GROUP BY column_name, column_name, ...\n+WINDOW 12 -- model looks back at sets of 12 rows each\n+HORIZON 3 -- model forecasts the next 3 rows\n+USING ENGINE = 'neuralforecast';\n+```\n+\n+To ensure that the model is created based on the NeuralForecast engine, include the `USING` clause at the end.\nComment: Maybe this can be <tip>",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "docs/integrations/ai-engines/neuralforecast.mdx",
    "pr_number": 7308,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1323027705,
    "comment_created_at": "2023-09-12T13:15:10Z"
  },
  {
    "code": "@@ -1348,7 +1348,7 @@ def _test_reranking(self, params):\n             reranker = get_reranking_model_from_params(params)\n             reranker.get_scores(\"test\", [\"test\"])\n         except (ValueError, RuntimeError) as e:\n-            if params[\"provider\"] in (\"azure_openai\", \"openai\"):\n+            if params[\"provider\"] in (\"azure_openai\", \"openai\") and params.get(\"method\") != \"no-logprobs\":",
    "comment": "**Correctness**: Infinite recursion bug in the _test_reranking method. When a reranking configuration failed for OpenAI/Azure OpenAI providers, the code would set params[\"method\"] = \"no-logprobs\" and call itself recursively. If the \"no-logprobs\" method also failed, it would enter the same condition again, leading to infinite recursion and stack overflow.\n\n",
    "line_number": 1351,
    "enriched": "File: mindsdb/interfaces/knowledge_base/controller.py\nCode: @@ -1348,7 +1348,7 @@ def _test_reranking(self, params):\n             reranker = get_reranking_model_from_params(params)\n             reranker.get_scores(\"test\", [\"test\"])\n         except (ValueError, RuntimeError) as e:\n-            if params[\"provider\"] in (\"azure_openai\", \"openai\"):\n+            if params[\"provider\"] in (\"azure_openai\", \"openai\") and params.get(\"method\") != \"no-logprobs\":\nComment: **Correctness**: Infinite recursion bug in the _test_reranking method. When a reranking configuration failed for OpenAI/Azure OpenAI providers, the code would set params[\"method\"] = \"no-logprobs\" and call itself recursively. If the \"no-logprobs\" method also failed, it would enter the same condition again, leading to infinite recursion and stack overflow.\n\n",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/interfaces/knowledge_base/controller.py",
    "pr_number": 11758,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2440181157,
    "comment_created_at": "2025-10-17T14:13:58Z"
  },
  {
    "code": "@@ -0,0 +1,56 @@\n+from typing import List, Optional\n+from pydantic import BaseModel, Field, HttpUrl\n+\n+class CorpusFilter(BaseModel):\n+    \"\"\"Corpus filter for search.\"\"\"\n+    arxiv: bool = Field(default=True, description=\"Include arXiv papers\")\n+    patent: bool = Field(default=True, description=\"Include patents\")\n+    biorxiv: bool = Field(default=True, description=\"Include biorxiv papers\")\n+    medrxiv: bool = Field(default=True, description=\"Include medrxiv papers\")\n+    chemrxiv: bool = Field(default=True, description=\"Include chemrxiv papers\")\n+\n+class SearchFilters(BaseModel):\n+    \"\"\"Search filters.\"\"\"\n+    isHybridSearch: Optional[bool] = Field(default=True, description=\"Enable hybrid search\")\n+    alpha: Optional[float] = Field(default=0.7, ge=0.0, le=1.0, description=\"Hybrid search alpha parameter\")",
    "comment": "**correctness**: `alpha` in `SearchFilters` allows values from 0.0 to 1.0, but does not handle `None` at runtime, which may cause downstream logic to fail if `alpha` is not set.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn 'mindsdb hacktoberfest/use-cases/semantic-academic-search-engine/backend/api/models.py', line 15, the 'alpha' field in 'SearchFilters' is Optional[float] but is always expected to be a float in downstream logic. Change its type to 'float' (not Optional) to prevent runtime errors when 'alpha' is None.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n    alpha: float = Field(default=0.7, ge=0.0, le=1.0, description=\"Hybrid search alpha parameter\")\n```\n</details>\n<!-- suggestion_end -->\n",
    "line_number": 15,
    "enriched": "File: mindsdb hacktoberfest/use-cases/semantic-academic-search-engine/backend/api/models.py\nCode: @@ -0,0 +1,56 @@\n+from typing import List, Optional\n+from pydantic import BaseModel, Field, HttpUrl\n+\n+class CorpusFilter(BaseModel):\n+    \"\"\"Corpus filter for search.\"\"\"\n+    arxiv: bool = Field(default=True, description=\"Include arXiv papers\")\n+    patent: bool = Field(default=True, description=\"Include patents\")\n+    biorxiv: bool = Field(default=True, description=\"Include biorxiv papers\")\n+    medrxiv: bool = Field(default=True, description=\"Include medrxiv papers\")\n+    chemrxiv: bool = Field(default=True, description=\"Include chemrxiv papers\")\n+\n+class SearchFilters(BaseModel):\n+    \"\"\"Search filters.\"\"\"\n+    isHybridSearch: Optional[bool] = Field(default=True, description=\"Enable hybrid search\")\n+    alpha: Optional[float] = Field(default=0.7, ge=0.0, le=1.0, description=\"Hybrid search alpha parameter\")\nComment: **correctness**: `alpha` in `SearchFilters` allows values from 0.0 to 1.0, but does not handle `None` at runtime, which may cause downstream logic to fail if `alpha` is not set.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn 'mindsdb hacktoberfest/use-cases/semantic-academic-search-engine/backend/api/models.py', line 15, the 'alpha' field in 'SearchFilters' is Optional[float] but is always expected to be a float in downstream logic. Change its type to 'float' (not Optional) to prevent runtime errors when 'alpha' is None.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n    alpha: float = Field(default=0.7, ge=0.0, le=1.0, description=\"Hybrid search alpha parameter\")\n```\n</details>\n<!-- suggestion_end -->\n",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb hacktoberfest/use-cases/semantic-academic-search-engine/backend/api/models.py",
    "pr_number": 11813,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2472463187,
    "comment_created_at": "2025-10-29T10:29:23Z"
  },
  {
    "code": "@@ -61,12 +62,20 @@ def connect(self) -> ibm_db_dbi.Connection:\n         # Mandatory connection parameters.\n         if not all(key in self.connection_data for key in ['host', 'user', 'password', 'database']):\n             raise ValueError('Required parameters (host, user, password, database) must be provided.')\n+        cloud = 'databases.appdomain.cloud' in self.connection_data['host']\n+        if cloud:\n+            connection_string = f\"DATABASE={self.connection_data['database']};HOSTNAME={self.connection_data['host']};PORT={self.connection_data['port']};PROTOCOL=TCPIP;UID={self.connection_data['user']};PWD={self.connection_data['password']};SECURITY=SSL;\"\n \n-        connection_string = f\"DRIVER={'IBM DB2 ODBC DRIVER'};DATABASE={self.connection_data['database']};HOST={self.connection_data['host']};PROTOCOL=TCPIP;UID={self.connection_data['user']};PWD={self.connection_data['password']};\"\n+            if 'port' in self.connection_data:\n+                connection_string += f\"PORT={self.connection_data['port']};\"\n+                connection_string += \"SSLSERVERCERTIFICATE=;\"\n+        else:",
    "comment": "**correctness**: `connection_string` for cloud connections appends `PORT` and `SSLSERVERCERTIFICATE` twice if 'port' is present, resulting in an invalid connection string and failed connections.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/integrations/handlers/db2_handler/db2_handler.py, lines 65-72, the cloud connection string appends PORT twice if 'port' is present, resulting in an invalid connection string. Move the SSLSERVERCERTIFICATE addition outside the 'if' block so PORT is only appended once, and SSLSERVERCERTIFICATE is always appended for cloud connections. Fix the logic so the connection string is valid for cloud connections.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n        cloud = 'databases.appdomain.cloud' in self.connection_data['host']\n        if cloud:\n            connection_string = f\"DATABASE={self.connection_data['database']};HOSTNAME={self.connection_data['host']};PROTOCOL=TCPIP;UID={self.connection_data['user']};PWD={self.connection_data['password']};SECURITY=SSL;\"\n            if 'port' in self.connection_data:\n                connection_string += f\"PORT={self.connection_data['port']};\"\n            connection_string += \"SSLSERVERCERTIFICATE=;\"\n```\n</details>\n<!-- suggestion_end -->\n",
    "line_number": 72,
    "enriched": "File: mindsdb/integrations/handlers/db2_handler/db2_handler.py\nCode: @@ -61,12 +62,20 @@ def connect(self) -> ibm_db_dbi.Connection:\n         # Mandatory connection parameters.\n         if not all(key in self.connection_data for key in ['host', 'user', 'password', 'database']):\n             raise ValueError('Required parameters (host, user, password, database) must be provided.')\n+        cloud = 'databases.appdomain.cloud' in self.connection_data['host']\n+        if cloud:\n+            connection_string = f\"DATABASE={self.connection_data['database']};HOSTNAME={self.connection_data['host']};PORT={self.connection_data['port']};PROTOCOL=TCPIP;UID={self.connection_data['user']};PWD={self.connection_data['password']};SECURITY=SSL;\"\n \n-        connection_string = f\"DRIVER={'IBM DB2 ODBC DRIVER'};DATABASE={self.connection_data['database']};HOST={self.connection_data['host']};PROTOCOL=TCPIP;UID={self.connection_data['user']};PWD={self.connection_data['password']};\"\n+            if 'port' in self.connection_data:\n+                connection_string += f\"PORT={self.connection_data['port']};\"\n+                connection_string += \"SSLSERVERCERTIFICATE=;\"\n+        else:\nComment: **correctness**: `connection_string` for cloud connections appends `PORT` and `SSLSERVERCERTIFICATE` twice if 'port' is present, resulting in an invalid connection string and failed connections.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/integrations/handlers/db2_handler/db2_handler.py, lines 65-72, the cloud connection string appends PORT twice if 'port' is present, resulting in an invalid connection string. Move the SSLSERVERCERTIFICATE addition outside the 'if' block so PORT is only appended once, and SSLSERVERCERTIFICATE is always appended for cloud connections. Fix the logic so the connection string is valid for cloud connections.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n        cloud = 'databases.appdomain.cloud' in self.connection_data['host']\n        if cloud:\n            connection_string = f\"DATABASE={self.connection_data['database']};HOSTNAME={self.connection_data['host']};PROTOCOL=TCPIP;UID={self.connection_data['user']};PWD={self.connection_data['password']};SECURITY=SSL;\"\n            if 'port' in self.connection_data:\n                connection_string += f\"PORT={self.connection_data['port']};\"\n            connection_string += \"SSLSERVERCERTIFICATE=;\"\n```\n</details>\n<!-- suggestion_end -->\n",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/db2_handler/db2_handler.py",
    "pr_number": 11372,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2237812040,
    "comment_created_at": "2025-07-28T20:33:19Z"
  },
  {
    "code": "@@ -92,10 +92,10 @@ def __init__(self):\n \n     def get(self, local_name, base_dir):",
    "comment": "It is a common function, not used only by predict method\r\nI would update predict method and instead line `self.model_storage.fileStorage.pull()`:\r\n- use `self.model_storage.fileStorage.pull_path(update=False)` if it will work\r\n- or update pull method and use `self.model_storage.fileStorage.pull(update=False)`",
    "line_number": 93,
    "enriched": "File: mindsdb/interfaces/storage/fs.py\nCode: @@ -92,10 +92,10 @@ def __init__(self):\n \n     def get(self, local_name, base_dir):\nComment: It is a common function, not used only by predict method\r\nI would update predict method and instead line `self.model_storage.fileStorage.pull()`:\r\n- use `self.model_storage.fileStorage.pull_path(update=False)` if it will work\r\n- or update pull method and use `self.model_storage.fileStorage.pull(update=False)`",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/interfaces/storage/fs.py",
    "pr_number": 5631,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1168395754,
    "comment_created_at": "2023-04-17T09:05:06Z"
  },
  {
    "code": "@@ -11,10 +11,16 @@ This file contains links to applications that integrate MindsDB. Please append l\n   Link: https://ritwickrajmakhal.github.io/ytinsights\n </Card>\n \n-<Card title=\"App Name\" icon=\"link\" href=\"https://github.com/mindsdb/mindsdb\">\n-  Author: your_name\n+<Card title=\"Condense\" icon=\"link\" href=\"https://github.com/adeyinkaezra123/condense\">\n+  Author: Ezra Adeyinka\n \n-  Link: link_1, link_2, ...\n+  Link: https://condense-mindsdb.vercel.app/\n+</Card>\n+\n+<Card title=\"NpmForecast\" icon=\"link\" href=\"https://github.com/adeyinkaezra123/npmforecast\">\n+  Author: Ezra Adeyinka\n+\n+  Link: https://npmforecast.streamlit.app/",
    "comment": "Let's remove this untill it works. We released new version please check",
    "line_number": 23,
    "enriched": "File: docs/applications_showcase.mdx\nCode: @@ -11,10 +11,16 @@ This file contains links to applications that integrate MindsDB. Please append l\n   Link: https://ritwickrajmakhal.github.io/ytinsights\n </Card>\n \n-<Card title=\"App Name\" icon=\"link\" href=\"https://github.com/mindsdb/mindsdb\">\n-  Author: your_name\n+<Card title=\"Condense\" icon=\"link\" href=\"https://github.com/adeyinkaezra123/condense\">\n+  Author: Ezra Adeyinka\n \n-  Link: link_1, link_2, ...\n+  Link: https://condense-mindsdb.vercel.app/\n+</Card>\n+\n+<Card title=\"NpmForecast\" icon=\"link\" href=\"https://github.com/adeyinkaezra123/npmforecast\">\n+  Author: Ezra Adeyinka\n+\n+  Link: https://npmforecast.streamlit.app/\nComment: Let's remove this untill it works. We released new version please check",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "docs/applications_showcase.mdx",
    "pr_number": 8223,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1378826232,
    "comment_created_at": "2023-11-01T13:48:10Z"
  },
  {
    "code": "@@ -53,8 +53,10 @@ USING\n <Info>\n **Default Model**\n When you create an OpenAI model in MindsDB, it uses the `gpt-3.5-turbo` model by default. But you can use the `gpt-4` model as well by passing it to the `model-name` parameter in the `USING` clause of the `CREATE MODEL` statement.\n-</Info>\n \n+**Supported Model**\n+For full supported `model_name`, you can find [here](https://github.com/mindsdb/mindsdb/blob/staging/mindsdb/integrations/handlers/openai_handler/constants.py)\n+</Info>",
    "comment": "Let's update it to the below please:\r\n```\r\n**Supported Models**\r\nTo see all supported models, including chat models, embedding models, and more, click [here](https://github.com/mindsdb/mindsdb/blob/staging/mindsdb/integrations/handlers/openai_handler/constants.py).\r\n```",
    "line_number": 59,
    "enriched": "File: docs/integrations/ai-engines/openai.mdx\nCode: @@ -53,8 +53,10 @@ USING\n <Info>\n **Default Model**\n When you create an OpenAI model in MindsDB, it uses the `gpt-3.5-turbo` model by default. But you can use the `gpt-4` model as well by passing it to the `model-name` parameter in the `USING` clause of the `CREATE MODEL` statement.\n-</Info>\n \n+**Supported Model**\n+For full supported `model_name`, you can find [here](https://github.com/mindsdb/mindsdb/blob/staging/mindsdb/integrations/handlers/openai_handler/constants.py)\n+</Info>\nComment: Let's update it to the below please:\r\n```\r\n**Supported Models**\r\nTo see all supported models, including chat models, embedding models, and more, click [here](https://github.com/mindsdb/mindsdb/blob/staging/mindsdb/integrations/handlers/openai_handler/constants.py).\r\n```",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "docs/integrations/ai-engines/openai.mdx",
    "pr_number": 8625,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1452328136,
    "comment_created_at": "2024-01-15T12:39:15Z"
  },
  {
    "code": "@@ -56,7 +66,11 @@ def f_all_cols(node, **kwargs):\n             def inject_values(node, **kwargs):",
    "comment": "I think is better to replace `inject_values` function and use `get_fill_param_fnc' function instead (which is used by other steps to fill parameter values):\r\n```\r\nfill_params = get_fill_param_fnc(self.steps_data)\r\nquery_traversal(query, fill_params)\r\n```",
    "line_number": 66,
    "enriched": "File: mindsdb/api/executor/sql_query/steps/subselect_step.py\nCode: @@ -56,7 +66,11 @@ def f_all_cols(node, **kwargs):\n             def inject_values(node, **kwargs):\nComment: I think is better to replace `inject_values` function and use `get_fill_param_fnc' function instead (which is used by other steps to fill parameter values):\r\n```\r\nfill_params = get_fill_param_fnc(self.steps_data)\r\nquery_traversal(query, fill_params)\r\n```",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/api/executor/sql_query/steps/subselect_step.py",
    "pr_number": 11393,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2250904820,
    "comment_created_at": "2025-08-04T09:18:52Z"
  },
  {
    "code": "@@ -181,23 +181,22 @@ def get_columns(self, table_name: str) -> StatusResponse:\n         return self.native_query(query)\n \n     def _wait_for_query_to_complete(self, query_execution_id: str) -> str:\n-        \"\"\"\n-        Wait for the Athena query to complete.\n-        Args:\n-            query_execution_id (str): ID of the query to wait for\n-        Returns:\n-            str: Query execution status\n-        \"\"\"\n         while True:\n             response = self.connection.get_query_execution(QueryExecutionId=query_execution_id)\n             status = response['QueryExecution']['Status']['State']\n             if status in ['SUCCEEDED', 'FAILED', 'CANCELLED']:\n                 return status\n \n             check_interval = self.connection_data.get('check_interval', 0)\n+            if isinstance(check_interval, str):",
    "comment": "Maybe check:\r\n```\r\nif isinstance(check_interval, str) and check_interval.strip().isdigit():\r\n    check_interval = int(check_interval)\r\n```",
    "line_number": 191,
    "enriched": "File: mindsdb/integrations/handlers/athena_handler/athena_handler.py\nCode: @@ -181,23 +181,22 @@ def get_columns(self, table_name: str) -> StatusResponse:\n         return self.native_query(query)\n \n     def _wait_for_query_to_complete(self, query_execution_id: str) -> str:\n-        \"\"\"\n-        Wait for the Athena query to complete.\n-        Args:\n-            query_execution_id (str): ID of the query to wait for\n-        Returns:\n-            str: Query execution status\n-        \"\"\"\n         while True:\n             response = self.connection.get_query_execution(QueryExecutionId=query_execution_id)\n             status = response['QueryExecution']['Status']['State']\n             if status in ['SUCCEEDED', 'FAILED', 'CANCELLED']:\n                 return status\n \n             check_interval = self.connection_data.get('check_interval', 0)\n+            if isinstance(check_interval, str):\nComment: Maybe check:\r\n```\r\nif isinstance(check_interval, str) and check_interval.strip().isdigit():\r\n    check_interval = int(check_interval)\r\n```",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/athena_handler/athena_handler.py",
    "pr_number": 9865,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1800842832,
    "comment_created_at": "2024-10-15T09:58:55Z"
  },
  {
    "code": "@@ -443,6 +444,7 @@ def _register_table(self, table_name: str, table_class: Any):\n         if table_name in self._tables:\n             raise TableAlreadyExists(f\"Table with name {table_name} already exists for this handler\")\n         self._tables[table_name] = table_class\n+        self._lower_tables[table_name.lower()] = table_class",
    "comment": "it is enough to have a single index\r\n```\r\nself._tables[table_name.lower()] = table_class\r\n```",
    "line_number": 447,
    "enriched": "File: mindsdb/integrations/libs/api_handler.py\nCode: @@ -443,6 +444,7 @@ def _register_table(self, table_name: str, table_class: Any):\n         if table_name in self._tables:\n             raise TableAlreadyExists(f\"Table with name {table_name} already exists for this handler\")\n         self._tables[table_name] = table_class\n+        self._lower_tables[table_name.lower()] = table_class\nComment: it is enough to have a single index\r\n```\r\nself._tables[table_name.lower()] = table_class\r\n```",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/libs/api_handler.py",
    "pr_number": 11111,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2152419101,
    "comment_created_at": "2025-06-17T14:25:10Z"
  },
  {
    "code": "@@ -329,86 +411,45 @@ def post(self, project_name, knowledge_base_name):\n         \"\"\"\n         Add support for LLM generation on the response from knowledge base\n         \"\"\"\n-        # Check for required parameters.\n-        if 'knowledge_base' not in request.json:\n-            return http_error(\n-                HTTPStatus.BAD_REQUEST,\n-                'Missing parameter',\n-                'Must provide \"knowledge_base\" parameter in POST body'\n-            )\n-\n-        # Check for required parameters\n-        query = request.json.get('query')\n-        if query is None:\n+        if request.json.get('query') is None:\n+            # \"query\" is used for semantic search for both completion types.\n+            logger.error('Missing parameter \"query\" in POST body')\n             return http_error(\n                 HTTPStatus.BAD_REQUEST,\n                 'Missing parameter',\n                 'Must provide \"query\" parameter in POST body'\n             )\n \n-            logger.error('Missing parameter \"query\" in POST body')\n-\n-        llm_model = request.json.get('llm_model')\n-        if llm_model is None:\n-            logger.warn(f'Missing parameter \"llm_model\" in POST body, using default llm_model {DEFAULT_LLM_MODEL}')\n-\n-        prompt_template = request.json.get('prompt_template')\n-        if prompt_template is None:\n-            logger.warn(f'Missing parameter \"prompt_template\" in POST body, using default prompt template {DEFAULT_RAG_PROMPT_TEMPLATE}')\n-\n-        session = SessionController()\n         project_controller = ProjectController()\n         try:\n             project = project_controller.get(name=project_name)\n         except EntityNotExistsError:\n             # Project must exist.\n+            logger.error(\"Project not found, please check the project name exists\")\n             return http_error(\n                 HTTPStatus.NOT_FOUND,\n                 'Project not found',\n                 f'Project with name {project_name} does not exist'\n             )\n-            logger.error(\"Project not found, please check the project name exists\")\n \n+        session = SessionController()\n         # Check if knowledge base exists\n         table = session.kb_controller.get_table(knowledge_base_name, project.id)\n         if table is None:\n+            logger.error(\"Knowledge Base not found, please check the knowledge base name exists\")\n             return http_error(\n                 HTTPStatus.NOT_FOUND,\n                 'Knowledge Base not found',\n                 f'Knowledge Base with name {knowledge_base_name} does not exist'\n             )\n \n-            logger.error(\"Knowledge Base not found, please check the knowledge base name exists\")\n-\n-        # Get retrieval config, if set\n-        retrieval_config = request.json.get('retrieval_config', {})\n-        if not retrieval_config:\n-            logger.warn('No retrieval config provided, using default retrieval config')\n-\n-        # add llm model to retrieval config\n-        if llm_model is not None:\n-            retrieval_config['llm_model_name'] = llm_model\n-\n-        # add prompt template to retrieval config\n-        if prompt_template is not None:\n-            retrieval_config['rag_prompt_template'] = prompt_template\n-\n-        # add llm provider to retrieval config if set\n-        llm_provider = request.json.get('model_provider')\n-        if llm_provider is not None:\n-            retrieval_config['llm_provider'] = llm_provider\n-\n-        # build rag pipeline\n-        rag_pipeline = table.build_rag_pipeline(retrieval_config)\n-\n-        # get response from rag pipeline\n-        rag_response = rag_pipeline(query)\n-        response = {\n-            'message': {\n-                'content': rag_response.get('answer'),\n-                'context': rag_response.get('context'),\n-                'role': 'assistant'\n-            }\n-        }\n-\n-        return response\n+        completion_type = request.json.get('type', 'chat')",
    "comment": "could be good to add this behaviour to the doc string: \"unless specified otherwise by default completion_type is chat\" ",
    "line_number": 446,
    "enriched": "File: mindsdb/api/http/namespaces/knowledge_bases.py\nCode: @@ -329,86 +411,45 @@ def post(self, project_name, knowledge_base_name):\n         \"\"\"\n         Add support for LLM generation on the response from knowledge base\n         \"\"\"\n-        # Check for required parameters.\n-        if 'knowledge_base' not in request.json:\n-            return http_error(\n-                HTTPStatus.BAD_REQUEST,\n-                'Missing parameter',\n-                'Must provide \"knowledge_base\" parameter in POST body'\n-            )\n-\n-        # Check for required parameters\n-        query = request.json.get('query')\n-        if query is None:\n+        if request.json.get('query') is None:\n+            # \"query\" is used for semantic search for both completion types.\n+            logger.error('Missing parameter \"query\" in POST body')\n             return http_error(\n                 HTTPStatus.BAD_REQUEST,\n                 'Missing parameter',\n                 'Must provide \"query\" parameter in POST body'\n             )\n \n-            logger.error('Missing parameter \"query\" in POST body')\n-\n-        llm_model = request.json.get('llm_model')\n-        if llm_model is None:\n-            logger.warn(f'Missing parameter \"llm_model\" in POST body, using default llm_model {DEFAULT_LLM_MODEL}')\n-\n-        prompt_template = request.json.get('prompt_template')\n-        if prompt_template is None:\n-            logger.warn(f'Missing parameter \"prompt_template\" in POST body, using default prompt template {DEFAULT_RAG_PROMPT_TEMPLATE}')\n-\n-        session = SessionController()\n         project_controller = ProjectController()\n         try:\n             project = project_controller.get(name=project_name)\n         except EntityNotExistsError:\n             # Project must exist.\n+            logger.error(\"Project not found, please check the project name exists\")\n             return http_error(\n                 HTTPStatus.NOT_FOUND,\n                 'Project not found',\n                 f'Project with name {project_name} does not exist'\n             )\n-            logger.error(\"Project not found, please check the project name exists\")\n \n+        session = SessionController()\n         # Check if knowledge base exists\n         table = session.kb_controller.get_table(knowledge_base_name, project.id)\n         if table is None:\n+            logger.error(\"Knowledge Base not found, please check the knowledge base name exists\")\n             return http_error(\n                 HTTPStatus.NOT_FOUND,\n                 'Knowledge Base not found',\n                 f'Knowledge Base with name {knowledge_base_name} does not exist'\n             )\n \n-            logger.error(\"Knowledge Base not found, please check the knowledge base name exists\")\n-\n-        # Get retrieval config, if set\n-        retrieval_config = request.json.get('retrieval_config', {})\n-        if not retrieval_config:\n-            logger.warn('No retrieval config provided, using default retrieval config')\n-\n-        # add llm model to retrieval config\n-        if llm_model is not None:\n-            retrieval_config['llm_model_name'] = llm_model\n-\n-        # add prompt template to retrieval config\n-        if prompt_template is not None:\n-            retrieval_config['rag_prompt_template'] = prompt_template\n-\n-        # add llm provider to retrieval config if set\n-        llm_provider = request.json.get('model_provider')\n-        if llm_provider is not None:\n-            retrieval_config['llm_provider'] = llm_provider\n-\n-        # build rag pipeline\n-        rag_pipeline = table.build_rag_pipeline(retrieval_config)\n-\n-        # get response from rag pipeline\n-        rag_response = rag_pipeline(query)\n-        response = {\n-            'message': {\n-                'content': rag_response.get('answer'),\n-                'context': rag_response.get('context'),\n-                'role': 'assistant'\n-            }\n-        }\n-\n-        return response\n+        completion_type = request.json.get('type', 'chat')\nComment: could be good to add this behaviour to the doc string: \"unless specified otherwise by default completion_type is chat\" ",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/api/http/namespaces/knowledge_bases.py",
    "pr_number": 10053,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1821731244,
    "comment_created_at": "2024-10-30T01:29:18Z"
  },
  {
    "code": "@@ -577,36 +607,45 @@ def _df_to_embeddings(self, df: pd.DataFrame) -> pd.DataFrame:\n         if df.empty:\n             return pd.DataFrame([], columns=[TableField.EMBEDDINGS.value])\n \n+        # keep only content\n+        df = df[[TableField.CONTENT.value]]\n+\n         model_id = self._kb.embedding_model_id\n-        # get the input columns\n-        model_rec = db.session.query(db.Predictor).filter_by(id=model_id).first()\n+        if model_id:\n+            # get the input columns\n+            model_rec = db.session.query(db.Predictor).filter_by(id=model_id).first()\n \n-        assert model_rec is not None, f\"Model not found: {model_id}\"\n-        model_project = db.session.query(db.Project).filter_by(id=model_rec.project_id).first()\n+            assert model_rec is not None, f\"Model not found: {model_id}\"\n+            model_project = db.session.query(db.Project).filter_by(id=model_rec.project_id).first()\n \n-        project_datanode = self.session.datahub.get(model_project.name)\n+            project_datanode = self.session.datahub.get(model_project.name)\n \n-        # keep only content\n-        df = df[[TableField.CONTENT.value]]\n+            model_using = model_rec.learn_args.get('using', {})\n+            input_col = model_using.get('question_column')\n+            if input_col is None:\n+                input_col = model_using.get('input_column')\n \n-        model_using = model_rec.learn_args.get('using', {})\n-        input_col = model_using.get('question_column')\n-        if input_col is None:\n-            input_col = model_using.get('input_column')\n+            if input_col is not None and input_col != TableField.CONTENT.value:\n+                df = df.rename(columns={TableField.CONTENT.value: input_col})\n \n-        if input_col is not None and input_col != TableField.CONTENT.value:\n-            df = df.rename(columns={TableField.CONTENT.value: input_col})\n+            df_out = project_datanode.predict(\n+                model_name=model_rec.name,\n+                df=df,\n+                params=self.model_params\n+            )\n \n-        df_out = project_datanode.predict(\n-            model_name=model_rec.name,\n-            df=df,\n-            params=self.model_params\n-        )\n+            target = model_rec.to_predict[0]\n+            if target != TableField.EMBEDDINGS.value:\n+                # adapt output for vectordb\n+                df_out = df_out.rename(columns={target: TableField.EMBEDDINGS.value})\n+\n+        else:",
    "comment": "maybe\r\n```\r\nelif 'embedding_model' in self._kb.params:\r\n   ...\r\nelse: \r\n   exception\r\n```",
    "line_number": 642,
    "enriched": "File: mindsdb/interfaces/knowledge_base/controller.py\nCode: @@ -577,36 +607,45 @@ def _df_to_embeddings(self, df: pd.DataFrame) -> pd.DataFrame:\n         if df.empty:\n             return pd.DataFrame([], columns=[TableField.EMBEDDINGS.value])\n \n+        # keep only content\n+        df = df[[TableField.CONTENT.value]]\n+\n         model_id = self._kb.embedding_model_id\n-        # get the input columns\n-        model_rec = db.session.query(db.Predictor).filter_by(id=model_id).first()\n+        if model_id:\n+            # get the input columns\n+            model_rec = db.session.query(db.Predictor).filter_by(id=model_id).first()\n \n-        assert model_rec is not None, f\"Model not found: {model_id}\"\n-        model_project = db.session.query(db.Project).filter_by(id=model_rec.project_id).first()\n+            assert model_rec is not None, f\"Model not found: {model_id}\"\n+            model_project = db.session.query(db.Project).filter_by(id=model_rec.project_id).first()\n \n-        project_datanode = self.session.datahub.get(model_project.name)\n+            project_datanode = self.session.datahub.get(model_project.name)\n \n-        # keep only content\n-        df = df[[TableField.CONTENT.value]]\n+            model_using = model_rec.learn_args.get('using', {})\n+            input_col = model_using.get('question_column')\n+            if input_col is None:\n+                input_col = model_using.get('input_column')\n \n-        model_using = model_rec.learn_args.get('using', {})\n-        input_col = model_using.get('question_column')\n-        if input_col is None:\n-            input_col = model_using.get('input_column')\n+            if input_col is not None and input_col != TableField.CONTENT.value:\n+                df = df.rename(columns={TableField.CONTENT.value: input_col})\n \n-        if input_col is not None and input_col != TableField.CONTENT.value:\n-            df = df.rename(columns={TableField.CONTENT.value: input_col})\n+            df_out = project_datanode.predict(\n+                model_name=model_rec.name,\n+                df=df,\n+                params=self.model_params\n+            )\n \n-        df_out = project_datanode.predict(\n-            model_name=model_rec.name,\n-            df=df,\n-            params=self.model_params\n-        )\n+            target = model_rec.to_predict[0]\n+            if target != TableField.EMBEDDINGS.value:\n+                # adapt output for vectordb\n+                df_out = df_out.rename(columns={target: TableField.EMBEDDINGS.value})\n+\n+        else:\nComment: maybe\r\n```\r\nelif 'embedding_model' in self._kb.params:\r\n   ...\r\nelse: \r\n   exception\r\n```",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/interfaces/knowledge_base/controller.py",
    "pr_number": 10657,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2033173177,
    "comment_created_at": "2025-04-08T13:16:28Z"
  },
  {
    "code": "@@ -1,9 +1,61 @@\n-__title__ = 'MindsDB Email handler'\n-__package_name__ = 'mindsdb_email_handler'\n-__version__ = '0.0.1'\n-__description__ = \"MindsDB handler for email\"\n-__author__ = 'MindsDB Inc'\n-__github__ = 'https://github.com/mindsdb/mindsdb'\n-__pypi__ = 'https://pypi.org/project/mindsdb/'\n-__license__ = 'MIT'\n-__copyright__ = 'Copyright 2022- mindsdb'\n+# mindsdb/integrations/handlers/email_handler/__about__.py\n+\n+from mindsdb.integrations.libs.const import HANDLER_TYPE\n+\n+__title__ = \"Email\"\n+__package_name__ = \"email_handler\"\n+__version__ = \"0.0.1\"\n+__description__ = \"MindsDB handler for retrieving emails through IMAP.\"\n+__author__ = \"MindsDB Inc\"\n+__github__ = \"https://github.com/mindsdb/mindsdb\"\n+__pypi__ = \"https://pypi.org/project/mindsdb/\"\n+__license__ = \"GPL-3.0\"\n+__copyright__ = \"Copyright 2023- mindsdb\"\n+__icon_path__ = \"icon.svg\"\n+\n+# Suggestion 1: Use getattr for safer assignment\n+HANDLER_TYPE = getattr(HANDLER_TYPE, \"DATA\", \"data\")",
    "comment": "**correctness**: `HANDLER_TYPE = getattr(HANDLER_TYPE, \"DATA\", \"data\")` will always assign the string 'data' regardless of the actual value of HANDLER_TYPE, due to incorrect use of getattr on a string.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/integrations/handlers/email_handler/__about__.py, line 17, the assignment `HANDLER_TYPE = getattr(HANDLER_TYPE, \"DATA\", \"data\")` is incorrect because HANDLER_TYPE is likely a string or Enum, not an object with attributes. Replace it with a conditional that checks for the DATA attribute: `HANDLER_TYPE = HANDLER_TYPE.DATA if hasattr(HANDLER_TYPE, \"DATA\") else \"data\"`.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\nHANDLER_TYPE = HANDLER_TYPE.DATA if hasattr(HANDLER_TYPE, \"DATA\") else \"data\"\n```\n</details>\n<!-- suggestion_end -->\n",
    "line_number": 17,
    "enriched": "File: mindsdb/integrations/handlers/email_handler/__about__.py\nCode: @@ -1,9 +1,61 @@\n-__title__ = 'MindsDB Email handler'\n-__package_name__ = 'mindsdb_email_handler'\n-__version__ = '0.0.1'\n-__description__ = \"MindsDB handler for email\"\n-__author__ = 'MindsDB Inc'\n-__github__ = 'https://github.com/mindsdb/mindsdb'\n-__pypi__ = 'https://pypi.org/project/mindsdb/'\n-__license__ = 'MIT'\n-__copyright__ = 'Copyright 2022- mindsdb'\n+# mindsdb/integrations/handlers/email_handler/__about__.py\n+\n+from mindsdb.integrations.libs.const import HANDLER_TYPE\n+\n+__title__ = \"Email\"\n+__package_name__ = \"email_handler\"\n+__version__ = \"0.0.1\"\n+__description__ = \"MindsDB handler for retrieving emails through IMAP.\"\n+__author__ = \"MindsDB Inc\"\n+__github__ = \"https://github.com/mindsdb/mindsdb\"\n+__pypi__ = \"https://pypi.org/project/mindsdb/\"\n+__license__ = \"GPL-3.0\"\n+__copyright__ = \"Copyright 2023- mindsdb\"\n+__icon_path__ = \"icon.svg\"\n+\n+# Suggestion 1: Use getattr for safer assignment\n+HANDLER_TYPE = getattr(HANDLER_TYPE, \"DATA\", \"data\")\nComment: **correctness**: `HANDLER_TYPE = getattr(HANDLER_TYPE, \"DATA\", \"data\")` will always assign the string 'data' regardless of the actual value of HANDLER_TYPE, due to incorrect use of getattr on a string.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/integrations/handlers/email_handler/__about__.py, line 17, the assignment `HANDLER_TYPE = getattr(HANDLER_TYPE, \"DATA\", \"data\")` is incorrect because HANDLER_TYPE is likely a string or Enum, not an object with attributes. Replace it with a conditional that checks for the DATA attribute: `HANDLER_TYPE = HANDLER_TYPE.DATA if hasattr(HANDLER_TYPE, \"DATA\") else \"data\"`.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\nHANDLER_TYPE = HANDLER_TYPE.DATA if hasattr(HANDLER_TYPE, \"DATA\") else \"data\"\n```\n</details>\n<!-- suggestion_end -->\n",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/email_handler/__about__.py",
    "pr_number": 11676,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2401022547,
    "comment_created_at": "2025-10-03T07:16:33Z"
  },
  {
    "code": "@@ -0,0 +1,88 @@\n+import unittest\n+from mindsdb.integrations.handlers.instatus_handler.instatus_handler import InstatusHandler\n+from mindsdb.api.mysql.mysql_proxy.libs.constants.response_type import RESPONSE_TYPE\n+import pandas as pd\n+\n+\n+class InstatusHandlerTest(unittest.TestCase):\n+\n+    @classmethod\n+    def setUpClass(cls):\n+        cls.handler = InstatusHandler(name='mindsdb_instatus', connection_data={'api_key': 'd25509b171ad79395dc2c51b099ee6d0'})",
    "comment": "Can you please just change the api key to be read from env var  so we can plug it to our CI",
    "line_number": 11,
    "enriched": "File: mindsdb/integrations/handlers/instatus_handler/tests/test_instatus_handler.py\nCode: @@ -0,0 +1,88 @@\n+import unittest\n+from mindsdb.integrations.handlers.instatus_handler.instatus_handler import InstatusHandler\n+from mindsdb.api.mysql.mysql_proxy.libs.constants.response_type import RESPONSE_TYPE\n+import pandas as pd\n+\n+\n+class InstatusHandlerTest(unittest.TestCase):\n+\n+    @classmethod\n+    def setUpClass(cls):\n+        cls.handler = InstatusHandler(name='mindsdb_instatus', connection_data={'api_key': 'd25509b171ad79395dc2c51b099ee6d0'})\nComment: Can you please just change the api key to be read from env var  so we can plug it to our CI",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/instatus_handler/tests/test_instatus_handler.py",
    "pr_number": 8030,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1378769179,
    "comment_created_at": "2023-11-01T12:59:26Z"
  },
  {
    "code": "@@ -0,0 +1,185 @@\n+import requests\n+import asyncio\n+import pandas as pd\n+\n+from mindsdb.integrations.handlers.discord_handler.discord_tables import DiscordTable\n+\n+from mindsdb.utilities import log\n+\n+from mindsdb.integrations.libs.api_handler import APIHandler, FuncParser\n+from mindsdb.integrations.utilities.date_utils import parse_utc_date\n+\n+from mindsdb.integrations.libs.response import (\n+    HandlerStatusResponse as StatusResponse,\n+    HandlerResponse as Response,\n+    RESPONSE_TYPE,\n+)\n+\n+discord_bot = None\n+\n+class DiscordHandler(APIHandler):\n+    \"\"\"\n+    The Discord handler implementation.\n+    \"\"\"\n+\n+    name = 'discord'\n+\n+    def __init__(self, name: str, **kwargs):\n+        \"\"\"\n+        Initialize the handler.\n+        Args:\n+            name (str): name of particular handler instance\n+            **kwargs: arbitrary keyword arguments.\n+        \"\"\"\n+        super().__init__(name)\n+\n+        connection_data = kwargs.get(\"connection_data\", {})\n+        self.connection_data = connection_data\n+        self.kwargs = kwargs\n+\n+        self.is_connected = False\n+\n+        messages = DiscordTable(self)\n+        self._register_table('messages', messages)\n+\n+    def connect(self):\n+        \"\"\"\n+        Set up the connection required by the handler.\n+        Returns\n+        -------\n+        StatusResponse\n+            connection object\n+        \"\"\"\n+\n+        if self.is_connected:\n+            return StatusResponse(True)\n+\n+        url = f'https://discord.com/api/v10/applications/@me'\n+        result = requests.get(\n+            url,\n+            headers={\n+                'Authorization': f'Bot {self.connection_data[\"token\"]}',\n+                'Content-Type': 'application/json',\n+            },\n+        )\n+\n+        if result.status_code != 200:\n+            raise ValueError(result.text)\n+\n+        self.is_connected = True\n+\n+    def check_connection(self) -> StatusResponse:\n+        \"\"\"\n+        Check connection to the handler.\n+        Returns:\n+            HandlerStatusResponse\n+        \"\"\"\n+\n+        response = StatusResponse(False)\n+\n+        try:\n+            self.connect()\n+            response.success = True\n+        except Exception as e:\n+            response.error_message = e\n+            log.logger.error(f'Error connecting to Discord: {response.error_message}')\n+\n+        self.is_connected = response.success\n+\n+        return response\n+\n+    def native_query(self, query: str = None) -> StatusResponse:\n+        \"\"\"Receive and process a raw query.\n+        Parameters\n+        ----------\n+        query : str\n+            query in a native format\n+        Returns\n+        -------\n+        StatusResponse\n+            Request status\n+        \"\"\"\n+        method_name, params = FuncParser().from_string(query)\n+\n+        df = self.call_discord_api(method_name, params)\n+\n+        return Response(RESPONSE_TYPE.TABLE, data_frame=df)\n+    \n+    def utc_to_snowflake(self, utc_date: str) -> int:\n+        \"\"\"\n+        Convert a UTC date to a Snowflake date.\n+        Args:\n+            utc_date (str): the UTC date\n+        Returns:\n+            int\n+        \"\"\"\n+        # https://discord.com/developers/docs/reference#snowflakes\n+        return str(\n+            int(parse_utc_date(utc_date).timestamp() * 1000 - 1420070400000) << 22\n+        )\n+\n+    def call_discord_api(\n+            self, method_name: str, params: dict = None, filters: list = None",
    "comment": "Can we change the `method_name` parameter to something like `operation`? method_name makes is seem slightly 'hacky' somehow.",
    "line_number": 122,
    "enriched": "File: mindsdb/integrations/handlers/discord_handler/discord_handler.py\nCode: @@ -0,0 +1,185 @@\n+import requests\n+import asyncio\n+import pandas as pd\n+\n+from mindsdb.integrations.handlers.discord_handler.discord_tables import DiscordTable\n+\n+from mindsdb.utilities import log\n+\n+from mindsdb.integrations.libs.api_handler import APIHandler, FuncParser\n+from mindsdb.integrations.utilities.date_utils import parse_utc_date\n+\n+from mindsdb.integrations.libs.response import (\n+    HandlerStatusResponse as StatusResponse,\n+    HandlerResponse as Response,\n+    RESPONSE_TYPE,\n+)\n+\n+discord_bot = None\n+\n+class DiscordHandler(APIHandler):\n+    \"\"\"\n+    The Discord handler implementation.\n+    \"\"\"\n+\n+    name = 'discord'\n+\n+    def __init__(self, name: str, **kwargs):\n+        \"\"\"\n+        Initialize the handler.\n+        Args:\n+            name (str): name of particular handler instance\n+            **kwargs: arbitrary keyword arguments.\n+        \"\"\"\n+        super().__init__(name)\n+\n+        connection_data = kwargs.get(\"connection_data\", {})\n+        self.connection_data = connection_data\n+        self.kwargs = kwargs\n+\n+        self.is_connected = False\n+\n+        messages = DiscordTable(self)\n+        self._register_table('messages', messages)\n+\n+    def connect(self):\n+        \"\"\"\n+        Set up the connection required by the handler.\n+        Returns\n+        -------\n+        StatusResponse\n+            connection object\n+        \"\"\"\n+\n+        if self.is_connected:\n+            return StatusResponse(True)\n+\n+        url = f'https://discord.com/api/v10/applications/@me'\n+        result = requests.get(\n+            url,\n+            headers={\n+                'Authorization': f'Bot {self.connection_data[\"token\"]}',\n+                'Content-Type': 'application/json',\n+            },\n+        )\n+\n+        if result.status_code != 200:\n+            raise ValueError(result.text)\n+\n+        self.is_connected = True\n+\n+    def check_connection(self) -> StatusResponse:\n+        \"\"\"\n+        Check connection to the handler.\n+        Returns:\n+            HandlerStatusResponse\n+        \"\"\"\n+\n+        response = StatusResponse(False)\n+\n+        try:\n+            self.connect()\n+            response.success = True\n+        except Exception as e:\n+            response.error_message = e\n+            log.logger.error(f'Error connecting to Discord: {response.error_message}')\n+\n+        self.is_connected = response.success\n+\n+        return response\n+\n+    def native_query(self, query: str = None) -> StatusResponse:\n+        \"\"\"Receive and process a raw query.\n+        Parameters\n+        ----------\n+        query : str\n+            query in a native format\n+        Returns\n+        -------\n+        StatusResponse\n+            Request status\n+        \"\"\"\n+        method_name, params = FuncParser().from_string(query)\n+\n+        df = self.call_discord_api(method_name, params)\n+\n+        return Response(RESPONSE_TYPE.TABLE, data_frame=df)\n+    \n+    def utc_to_snowflake(self, utc_date: str) -> int:\n+        \"\"\"\n+        Convert a UTC date to a Snowflake date.\n+        Args:\n+            utc_date (str): the UTC date\n+        Returns:\n+            int\n+        \"\"\"\n+        # https://discord.com/developers/docs/reference#snowflakes\n+        return str(\n+            int(parse_utc_date(utc_date).timestamp() * 1000 - 1420070400000) << 22\n+        )\n+\n+    def call_discord_api(\n+            self, method_name: str, params: dict = None, filters: list = None\nComment: Can we change the `method_name` parameter to something like `operation`? method_name makes is seem slightly 'hacky' somehow.",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/discord_handler/discord_handler.py",
    "pr_number": 8170,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1384694356,
    "comment_created_at": "2023-11-07T10:21:07Z"
  },
  {
    "code": "@@ -149,12 +149,17 @@ def __init__(self, config: ContextualConfig):\n         self.summarize = self.config.summarize\n \n     def _prepare_prompts(self, chunk_contents: list[str], full_documents: list[str]) -> list[str]:\n-        prompts = [\n-            self.context_template.replace(\"{{WHOLE_DOCUMENT}}\", full_document) for full_document in full_documents\n-        ]\n-        prompts = [\n-            prompt.replace(\"{{CHUNK_CONTENT}}\", chunk_content) for prompt, chunk_content in zip(prompts, chunk_contents)\n-        ]\n+        def tag_replacer(match):",
    "comment": "is it ok if the text can have other tags apart from `chunk` and `document`? maybe we should remove every tags (open and close tags)?",
    "line_number": 152,
    "enriched": "File: mindsdb/interfaces/knowledge_base/preprocessing/document_preprocessor.py\nCode: @@ -149,12 +149,17 @@ def __init__(self, config: ContextualConfig):\n         self.summarize = self.config.summarize\n \n     def _prepare_prompts(self, chunk_contents: list[str], full_documents: list[str]) -> list[str]:\n-        prompts = [\n-            self.context_template.replace(\"{{WHOLE_DOCUMENT}}\", full_document) for full_document in full_documents\n-        ]\n-        prompts = [\n-            prompt.replace(\"{{CHUNK_CONTENT}}\", chunk_content) for prompt, chunk_content in zip(prompts, chunk_contents)\n-        ]\n+        def tag_replacer(match):\nComment: is it ok if the text can have other tags apart from `chunk` and `document`? maybe we should remove every tags (open and close tags)?",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/interfaces/knowledge_base/preprocessing/document_preprocessor.py",
    "pr_number": 11279,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2200526225,
    "comment_created_at": "2025-07-11T11:55:08Z"
  },
  {
    "code": "@@ -27,6 +27,13 @@ jobs:\n       - name: Check Version\n         run: |\n           PYTHONPATH=./ python tests/scripts/check_version.py ${{ env.CI_REF_NAME }} ${{ github.event.release.prerelease }}\n+      - name: Setup uv\n+        if: ${{ needs.changes.outputs.not-docs == 'true' }}",
    "comment": "**Correctness**: The `needs.changes` reference in the condition for the 'Setup uv' step is invalid as there is no 'changes' job defined in this workflow, causing this step to be skipped.\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n- name: Setup uv\nif: ${{ !contains(github.event.release.body, '[skip-ci]') }}\nuses: astral-sh/setup-uv@v5\n```\n</details>\n<!-- suggestion_end -->\n",
    "line_number": 31,
    "enriched": "File: .github/workflows/build_deploy_prod.yml\nCode: @@ -27,6 +27,13 @@ jobs:\n       - name: Check Version\n         run: |\n           PYTHONPATH=./ python tests/scripts/check_version.py ${{ env.CI_REF_NAME }} ${{ github.event.release.prerelease }}\n+      - name: Setup uv\n+        if: ${{ needs.changes.outputs.not-docs == 'true' }}\nComment: **Correctness**: The `needs.changes` reference in the condition for the 'Setup uv' step is invalid as there is no 'changes' job defined in this workflow, causing this step to be skipped.\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n- name: Setup uv\nif: ${{ !contains(github.event.release.body, '[skip-ci]') }}\nuses: astral-sh/setup-uv@v5\n```\n</details>\n<!-- suggestion_end -->\n",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": ".github/workflows/build_deploy_prod.yml",
    "pr_number": 10846,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2092942156,
    "comment_created_at": "2025-05-16T12:21:02Z"
  },
  {
    "code": "@@ -45,11 +34,29 @@ def create(self, target: str, df: Optional[pd.DataFrame] = None, args: Optional[\n         args['target'] = target\n         self.model_storage.json_set('args', args)\n \n-        # download model\n-        # TODO v2: point Ollama to the engine storage folder instead of their default location\n-        model_name = args['model_name']\n-        # blocking operation, finishes once model has been fully pulled and served\n-        requests.post(OllamaHandler.SERVE_URL + '/api/pull', json={'name': model_name})\n+        def _model_check():\n+            \"\"\" Checks model has been pulled and that it works correctly. \"\"\"\n+            try:\n+                return requests.post(\n+                    OllamaHandler.SERVE_URL + '/api/generate',\n+                    json={\n+                        'model': args['model_name'],\n+                        'prompt': 'Hello.',\n+                    }\n+                ).status_code\n+            except Exception:\n+                return 500\n+\n+        # check model\n+        response = _model_check()\n+        if response != 200:\n+            # pull model (blocking operation) and serve\n+            # TODO: point to the engine storage folder instead of default location\n+            requests.post(OllamaHandler.SERVE_URL + '/api/pull', json={'name': args['model_name']})\n+            # try one last time\n+            response = _model_check()\n+            if response != 200:",
    "comment": "Just to double check since I can't find it in the Ollama docs. POST request on pull endpoint returns 200 if successful or 201?",
    "line_number": 58,
    "enriched": "File: mindsdb/integrations/handlers/ollama_handler/ollama_handler.py\nCode: @@ -45,11 +34,29 @@ def create(self, target: str, df: Optional[pd.DataFrame] = None, args: Optional[\n         args['target'] = target\n         self.model_storage.json_set('args', args)\n \n-        # download model\n-        # TODO v2: point Ollama to the engine storage folder instead of their default location\n-        model_name = args['model_name']\n-        # blocking operation, finishes once model has been fully pulled and served\n-        requests.post(OllamaHandler.SERVE_URL + '/api/pull', json={'name': model_name})\n+        def _model_check():\n+            \"\"\" Checks model has been pulled and that it works correctly. \"\"\"\n+            try:\n+                return requests.post(\n+                    OllamaHandler.SERVE_URL + '/api/generate',\n+                    json={\n+                        'model': args['model_name'],\n+                        'prompt': 'Hello.',\n+                    }\n+                ).status_code\n+            except Exception:\n+                return 500\n+\n+        # check model\n+        response = _model_check()\n+        if response != 200:\n+            # pull model (blocking operation) and serve\n+            # TODO: point to the engine storage folder instead of default location\n+            requests.post(OllamaHandler.SERVE_URL + '/api/pull', json={'name': args['model_name']})\n+            # try one last time\n+            response = _model_check()\n+            if response != 200:\nComment: Just to double check since I can't find it in the Ollama docs. POST request on pull endpoint returns 200 if successful or 201?",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/ollama_handler/ollama_handler.py",
    "pr_number": 8631,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1453364397,
    "comment_created_at": "2024-01-16T12:31:49Z"
  },
  {
    "code": "@@ -286,26 +286,10 @@ def find_predictors(node, is_table, **kwargs):\n \n                 if step.result_data is not None:\n                     # save results\n-\n-                    if len(step.result_data[\"tables\"]) > 0:\n-                        table_info = step.result_data[\"tables\"][0]\n-                        columns_info = step.result_data[\"columns\"][table_info]\n-\n-                        table.columns = []\n-                        table.ds = table_info[0]\n-                        for col in columns_info:\n-                            if isinstance(col, tuple):\n-                                # is predictor\n-                                col = dict(name=col[0], type=\"str\")\n-                            table.columns.append(\n-                                Column(\n-                                    name=col[\"name\"],\n-                                    type=col[\"type\"],\n-                                )\n-                            )\n-\n-                    # map by names\n-                    table.columns_map = {i.name.upper(): i for i in table.columns}\n+                    table.columns = step.result_data.columns\n+                    table.columns_map = {\n+                        column.name: column for column in step.result_data.columns\n+                    }",
    "comment": "**correctness**: `table.columns_map` uses case-sensitive keys (`column.name`) instead of uppercased names, causing column lookups by name to fail if case does not match, leading to PlanningException or incorrect column typing.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/api/executor/planner/query_prepare.py, lines 289-292, the code sets table.columns_map using the original column names as keys, which is case-sensitive. However, later lookups use uppercased names (e.g., col_name = column.name.upper()), causing lookups to fail if the case does not match. Update the dictionary comprehension so that the keys are uppercased: use column.name.upper() as the key instead of column.name.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n                    table.columns = step.result_data.columns\n                    table.columns_map = {\n                        column.name.upper(): column for column in step.result_data.columns\n                    }\n```\n</details>\n<!-- suggestion_end -->\n",
    "line_number": 292,
    "enriched": "File: mindsdb/api/executor/planner/query_prepare.py\nCode: @@ -286,26 +286,10 @@ def find_predictors(node, is_table, **kwargs):\n \n                 if step.result_data is not None:\n                     # save results\n-\n-                    if len(step.result_data[\"tables\"]) > 0:\n-                        table_info = step.result_data[\"tables\"][0]\n-                        columns_info = step.result_data[\"columns\"][table_info]\n-\n-                        table.columns = []\n-                        table.ds = table_info[0]\n-                        for col in columns_info:\n-                            if isinstance(col, tuple):\n-                                # is predictor\n-                                col = dict(name=col[0], type=\"str\")\n-                            table.columns.append(\n-                                Column(\n-                                    name=col[\"name\"],\n-                                    type=col[\"type\"],\n-                                )\n-                            )\n-\n-                    # map by names\n-                    table.columns_map = {i.name.upper(): i for i in table.columns}\n+                    table.columns = step.result_data.columns\n+                    table.columns_map = {\n+                        column.name: column for column in step.result_data.columns\n+                    }\nComment: **correctness**: `table.columns_map` uses case-sensitive keys (`column.name`) instead of uppercased names, causing column lookups by name to fail if case does not match, leading to PlanningException or incorrect column typing.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/api/executor/planner/query_prepare.py, lines 289-292, the code sets table.columns_map using the original column names as keys, which is case-sensitive. However, later lookups use uppercased names (e.g., col_name = column.name.upper()), causing lookups to fail if the case does not match. Update the dictionary comprehension so that the keys are uppercased: use column.name.upper() as the key instead of column.name.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n                    table.columns = step.result_data.columns\n                    table.columns_map = {\n                        column.name.upper(): column for column in step.result_data.columns\n                    }\n```\n</details>\n<!-- suggestion_end -->\n",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/api/executor/planner/query_prepare.py",
    "pr_number": 11689,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2406462930,
    "comment_created_at": "2025-10-06T13:52:23Z"
  },
  {
    "code": "@@ -0,0 +1,172 @@\n+import time\n+import requests\n+from typing import Optional, Dict, Union, List\n+\n+from mindsdb.utilities import log\n+\n+logger = log.getLogger(__name__)\n+\n+\n+class MSGraphAPIClient:\n+    MICROSOFT_GRAPH_BASE_API_URL: str = \"https://graph.microsoft.com/\"\n+    MICROSOFT_GRAPH_API_VERSION: str = \"v1.0\"\n+    PAGINATION_COUNT: Optional[int] = 20\n+\n+    def __init__(self, access_token: str) -> None:\n+        self.access_token = access_token\n+        self._group_ids = None\n+\n+    def _get_api_url(self, endpoint: str) -> str:\n+        api_url = f\"{self.MICROSOFT_GRAPH_BASE_API_URL}{self.MICROSOFT_GRAPH_API_VERSION}/{endpoint}/\"\n+        return api_url\n+\n+    def _make_request(self, api_url: str, params: Optional[Dict] = None, data: Optional[Dict] = None, method: str = \"GET\") -> Union[Dict, object]:\n+        headers = {\"Authorization\": f\"Bearer {self.access_token}\"}\n+        if method == \"GET\":\n+            response = requests.get(api_url, headers=headers, params=params)\n+        elif method == \"POST\":\n+            response = requests.post(api_url, headers=headers, json=data)\n+        else:\n+            raise NotImplementedError(f\"Method {method} not implemented\")\n+        if response.status_code == 429:\n+            if \"Retry-After\" in response.headers:\n+                pause_time = float(response.headers[\"Retry-After\"])\n+                time.sleep(pause_time)\n+                response = requests.get(api_url, headers=headers, params=params)\n+        if response.status_code not in [200, 201]:\n+            raise requests.exceptions.RequestException(response.text)\n+        if response.headers[\"Content-Type\"] == \"application/octet-stream\":\n+            raw_response = response.content\n+        else:\n+            raw_response = response.json()\n+        return raw_response\n+    \n+    def _get_request_params(self, params: Optional[Dict] = None, pagination: bool = True) -> Dict:\n+        if self.PAGINATION_COUNT and pagination:\n+            params = params if params else {}\n+            if \"$top\" not in params:\n+                params[\"$top\"] = self.PAGINATION_COUNT\n+        return params\n+\n+    @staticmethod\n+    def _get_response_value_unsafe(raw_response: Dict) -> List:\n+        value = raw_response[\"value\"]\n+        return value\n+    \n+    def _fetch_data(self, endpoint: str, params: Optional[Dict] = None, pagination: bool = True):\n+        api_url = self._get_api_url(endpoint)\n+        params = self._get_request_params(params, pagination)\n+        while api_url:\n+            raw_response = self._make_request(api_url, params)\n+            value = self._get_response_value_unsafe(raw_response)\n+            params = None\n+            api_url = raw_response.get(\"@odata.nextLink\", \"\")\n+            yield value\n+\n+    def _get_group_ids(self):\n+        if not self._group_ids:\n+            api_url = self._get_api_url(\"groups\")\n+            params = {\"$select\": \"id,resourceProvisioningOptions\"}\n+            groups = self._get_response_value_unsafe(self._make_request(api_url, params=params))\n+            self._group_ids = [item[\"id\"] for item in groups if \"Team\" in item[\"resourceProvisioningOptions\"]]\n+        return self._group_ids\n+\n+    def get_channels(self):",
    "comment": "I think this method and others below should be located in ms_teams handler. They are about the chat api.\r\nWe can leave in this class _fetch_data method and other methods which implement protocol. And use it from ms teams handler as base class ",
    "line_number": 74,
    "enriched": "File: mindsdb/integrations/handlers/utilities/api_utilities/microsoft/ms_graph_api_utilities.py\nCode: @@ -0,0 +1,172 @@\n+import time\n+import requests\n+from typing import Optional, Dict, Union, List\n+\n+from mindsdb.utilities import log\n+\n+logger = log.getLogger(__name__)\n+\n+\n+class MSGraphAPIClient:\n+    MICROSOFT_GRAPH_BASE_API_URL: str = \"https://graph.microsoft.com/\"\n+    MICROSOFT_GRAPH_API_VERSION: str = \"v1.0\"\n+    PAGINATION_COUNT: Optional[int] = 20\n+\n+    def __init__(self, access_token: str) -> None:\n+        self.access_token = access_token\n+        self._group_ids = None\n+\n+    def _get_api_url(self, endpoint: str) -> str:\n+        api_url = f\"{self.MICROSOFT_GRAPH_BASE_API_URL}{self.MICROSOFT_GRAPH_API_VERSION}/{endpoint}/\"\n+        return api_url\n+\n+    def _make_request(self, api_url: str, params: Optional[Dict] = None, data: Optional[Dict] = None, method: str = \"GET\") -> Union[Dict, object]:\n+        headers = {\"Authorization\": f\"Bearer {self.access_token}\"}\n+        if method == \"GET\":\n+            response = requests.get(api_url, headers=headers, params=params)\n+        elif method == \"POST\":\n+            response = requests.post(api_url, headers=headers, json=data)\n+        else:\n+            raise NotImplementedError(f\"Method {method} not implemented\")\n+        if response.status_code == 429:\n+            if \"Retry-After\" in response.headers:\n+                pause_time = float(response.headers[\"Retry-After\"])\n+                time.sleep(pause_time)\n+                response = requests.get(api_url, headers=headers, params=params)\n+        if response.status_code not in [200, 201]:\n+            raise requests.exceptions.RequestException(response.text)\n+        if response.headers[\"Content-Type\"] == \"application/octet-stream\":\n+            raw_response = response.content\n+        else:\n+            raw_response = response.json()\n+        return raw_response\n+    \n+    def _get_request_params(self, params: Optional[Dict] = None, pagination: bool = True) -> Dict:\n+        if self.PAGINATION_COUNT and pagination:\n+            params = params if params else {}\n+            if \"$top\" not in params:\n+                params[\"$top\"] = self.PAGINATION_COUNT\n+        return params\n+\n+    @staticmethod\n+    def _get_response_value_unsafe(raw_response: Dict) -> List:\n+        value = raw_response[\"value\"]\n+        return value\n+    \n+    def _fetch_data(self, endpoint: str, params: Optional[Dict] = None, pagination: bool = True):\n+        api_url = self._get_api_url(endpoint)\n+        params = self._get_request_params(params, pagination)\n+        while api_url:\n+            raw_response = self._make_request(api_url, params)\n+            value = self._get_response_value_unsafe(raw_response)\n+            params = None\n+            api_url = raw_response.get(\"@odata.nextLink\", \"\")\n+            yield value\n+\n+    def _get_group_ids(self):\n+        if not self._group_ids:\n+            api_url = self._get_api_url(\"groups\")\n+            params = {\"$select\": \"id,resourceProvisioningOptions\"}\n+            groups = self._get_response_value_unsafe(self._make_request(api_url, params=params))\n+            self._group_ids = [item[\"id\"] for item in groups if \"Team\" in item[\"resourceProvisioningOptions\"]]\n+        return self._group_ids\n+\n+    def get_channels(self):\nComment: I think this method and others below should be located in ms_teams handler. They are about the chat api.\r\nWe can leave in this class _fetch_data method and other methods which implement protocol. And use it from ms teams handler as base class ",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/utilities/api_utilities/microsoft/ms_graph_api_utilities.py",
    "pr_number": 8479,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1420323094,
    "comment_created_at": "2023-12-08T11:41:19Z"
  },
  {
    "code": "@@ -33,8 +35,22 @@ def create(self, target: str, df: Optional[pd.DataFrame] = None, args: Optional[\n         \n \n         if df is not None:\n+            # Separate out the categorical and non-categorical columns\n+            categorical_cols = df.select_dtypes(include=['object']).columns.tolist()",
    "comment": "Let's use the types inferred by `type_infer` instead. Some non-categorical types could be force-formatted to be `objects` (or viceversa) and this filter wouldn't catch them.",
    "line_number": 39,
    "enriched": "File: mindsdb/integrations/handlers/tpot_handler/tpot_handler.py\nCode: @@ -33,8 +35,22 @@ def create(self, target: str, df: Optional[pd.DataFrame] = None, args: Optional[\n         \n \n         if df is not None:\n+            # Separate out the categorical and non-categorical columns\n+            categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\nComment: Let's use the types inferred by `type_infer` instead. Some non-categorical types could be force-formatted to be `objects` (or viceversa) and this filter wouldn't catch them.",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/tpot_handler/tpot_handler.py",
    "pr_number": 5913,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1185221986,
    "comment_created_at": "2023-05-04T15:49:18Z"
  },
  {
    "code": "@@ -0,0 +1,429 @@\n+import requests\n+from datetime import datetime, timezone\n+from typing import Text, List, Dict, Any\n+import json\n+from requests import Response\n+\n+\n+class SharepointAPI:\n+    def __init__(\n+        self, client_id: str = None, client_secret: str = None, tenant_id: str = None\n+    ):\n+        self.client_id = client_id\n+        self.client_secret = client_secret\n+        self.tenant_id = tenant_id\n+        self.bearer_token = None\n+        self.is_connected = False\n+        self.expiration_time = datetime.now(timezone.utc).timestamp()\n+\n+    def get_bearer_token(self) -> None:\n+        url = f\"https://login.microsoftonline.com/{self.tenant_id}/oauth2/token\"\n+\n+        payload = {\n+            \"client_id\": self.client_id,\n+            \"client_secret\": self.client_secret,\n+            \"redirect_uri\": \"http://localhost\",\n+            \"grant_type\": \"client_credentials\",\n+            \"resource\": \"https://graph.microsoft.com\",\n+        }\n+        files = []\n+        headers = {}\n+\n+        response = self.getresponse(\n+            request_type=\"POST\", url=url, headers=headers, payload=payload, files=files\n+        )\n+        self.bearer_token = response.json()[\"access_token\"]\n+        self.expiration_time = int(response.json()[\"expires_on\"])\n+        self.is_connected = True\n+\n+    @staticmethod\n+    def getresponse(\n+        url: str,\n+        payload: Dict[Text, Any],\n+        files: List[Any],\n+        headers: Dict[Text, Any],\n+        request_type: str,\n+    ) -> Response:\n+        response = requests.request(\n+            request_type, url, headers=headers, data=payload, files=files\n+        )\n+        status_code = response.status_code\n+\n+        if 400 <= status_code <= 499:\n+            raise Exception(\"Client error: \" + response.text)\n+\n+        if 500 <= status_code <= 599:\n+            raise Exception(\"Server error: \" + response.text)\n+        return response\n+\n+    def check_connection(self) -> bool:\n+        if (\n+            self.is_connected\n+            and datetime.now(timezone.utc).astimezone().timestamp()\n+            < self.expiration_time\n+        ):\n+            return True\n+        else:\n+            return False\n+\n+    def disconnect(self) -> None:\n+        self.bearer_token = None\n+        self.is_connected = False\n+\n+    def get_all_sites(self, limit: int = None) -> List[Dict[Text, Any]]:\n+        url = \"https://graph.microsoft.com/v1.0/sites?search=*\"\n+\n+        payload = {}\n+        headers = {\n+            \"Authorization\": f\"Bearer {self.bearer_token}\",\n+            \"Content-Type\": \"application/json\",\n+        }\n+\n+        response = self.getresponse(\n+            request_type=\"GET\", url=url, headers=headers, payload=payload, files=[]\n+        )\n+        response = response.json()[\"value\"]\n+        if limit:\n+            response = response[:limit]\n+\n+        return response\n+\n+    def update_sites(\n+        self, site_dict: List[Dict[Text, Text]], values_to_update: Dict[Text, Any]\n+    ) -> None:\n+        for site_entry in site_dict:\n+            self.update_a_site(\n+                site_id=site_entry[\"siteId\"],\n+                values_to_update=values_to_update,\n+            )\n+\n+    def update_a_site(self, site_id: str, values_to_update: Dict[Text, Any]) -> None:\n+        url = f\"https://graph.microsoft.com/v1.0/sites/{site_id}/\"\n+        payload = values_to_update\n+        payload = json.dumps(payload, indent=2)\n+        headers = {\n+            \"Authorization\": f\"Bearer {self.bearer_token}\",\n+            \"Content-Type\": \"application/json\",\n+        }\n+        self.getresponse(\n+            request_type=\"PATCH\", url=url, headers=headers, payload=payload, files=[]\n+        )\n+\n+    def get_lists_by_site(\n+        self, site_id: str, limit: int = None\n+    ) -> List[Dict[Text, Any]]:\n+        url = f\"https://graph.microsoft.com/v1.0/sites/{site_id}/lists\"\n+        payload = {}\n+        headers = {\n+            \"Authorization\": f\"Bearer {self.bearer_token}\",\n+            \"Content-Type\": \"application/json\",\n+        }\n+\n+        response = self.getresponse(\n+            request_type=\"GET\", url=url, headers=headers, payload=payload, files=[]\n+        )\n+        response = response.json()[\"value\"]\n+        if limit:\n+            response = response[:limit]\n+        return response\n+\n+    def get_all_lists(self, limit: int = None) -> List[Dict[Text, Any]]:\n+        sites = self.get_all_sites()\n+        lists = []\n+        for site in sites:\n+            for list_dict in self.get_lists_by_site(site_id=site[\"id\"].split(\",\")[1]):\n+                list_dict[\"siteName\"] = site[\"name\"]\n+                list_dict[\"siteId\"] = site[\"id\"].split(\",\")[1]\n+                lists.append(list_dict)\n+        if limit:\n+            lists = lists[:limit]\n+        return lists\n+\n+    def delete_lists(self, list_dict: List[Dict[Text, Any]]) -> None:\n+        for list_entry in list_dict:\n+            self.delete_a_list(site_id=list_entry[\"siteId\"], list_id=list_entry[\"id\"])\n+\n+    def delete_a_list(self, site_id: str, list_id: str) -> None:\n+        url = f\"https://graph.microsoft.com/v1.0/sites/{site_id}/lists/{list_id}\"\n+        payload = {}\n+        headers = {",
    "comment": "We can move this to utility function as most of the methods bellow are using it",
    "line_number": 149,
    "enriched": "File: mindsdb/integrations/handlers/sharepoint_handler/sharepoint_api.py\nCode: @@ -0,0 +1,429 @@\n+import requests\n+from datetime import datetime, timezone\n+from typing import Text, List, Dict, Any\n+import json\n+from requests import Response\n+\n+\n+class SharepointAPI:\n+    def __init__(\n+        self, client_id: str = None, client_secret: str = None, tenant_id: str = None\n+    ):\n+        self.client_id = client_id\n+        self.client_secret = client_secret\n+        self.tenant_id = tenant_id\n+        self.bearer_token = None\n+        self.is_connected = False\n+        self.expiration_time = datetime.now(timezone.utc).timestamp()\n+\n+    def get_bearer_token(self) -> None:\n+        url = f\"https://login.microsoftonline.com/{self.tenant_id}/oauth2/token\"\n+\n+        payload = {\n+            \"client_id\": self.client_id,\n+            \"client_secret\": self.client_secret,\n+            \"redirect_uri\": \"http://localhost\",\n+            \"grant_type\": \"client_credentials\",\n+            \"resource\": \"https://graph.microsoft.com\",\n+        }\n+        files = []\n+        headers = {}\n+\n+        response = self.getresponse(\n+            request_type=\"POST\", url=url, headers=headers, payload=payload, files=files\n+        )\n+        self.bearer_token = response.json()[\"access_token\"]\n+        self.expiration_time = int(response.json()[\"expires_on\"])\n+        self.is_connected = True\n+\n+    @staticmethod\n+    def getresponse(\n+        url: str,\n+        payload: Dict[Text, Any],\n+        files: List[Any],\n+        headers: Dict[Text, Any],\n+        request_type: str,\n+    ) -> Response:\n+        response = requests.request(\n+            request_type, url, headers=headers, data=payload, files=files\n+        )\n+        status_code = response.status_code\n+\n+        if 400 <= status_code <= 499:\n+            raise Exception(\"Client error: \" + response.text)\n+\n+        if 500 <= status_code <= 599:\n+            raise Exception(\"Server error: \" + response.text)\n+        return response\n+\n+    def check_connection(self) -> bool:\n+        if (\n+            self.is_connected\n+            and datetime.now(timezone.utc).astimezone().timestamp()\n+            < self.expiration_time\n+        ):\n+            return True\n+        else:\n+            return False\n+\n+    def disconnect(self) -> None:\n+        self.bearer_token = None\n+        self.is_connected = False\n+\n+    def get_all_sites(self, limit: int = None) -> List[Dict[Text, Any]]:\n+        url = \"https://graph.microsoft.com/v1.0/sites?search=*\"\n+\n+        payload = {}\n+        headers = {\n+            \"Authorization\": f\"Bearer {self.bearer_token}\",\n+            \"Content-Type\": \"application/json\",\n+        }\n+\n+        response = self.getresponse(\n+            request_type=\"GET\", url=url, headers=headers, payload=payload, files=[]\n+        )\n+        response = response.json()[\"value\"]\n+        if limit:\n+            response = response[:limit]\n+\n+        return response\n+\n+    def update_sites(\n+        self, site_dict: List[Dict[Text, Text]], values_to_update: Dict[Text, Any]\n+    ) -> None:\n+        for site_entry in site_dict:\n+            self.update_a_site(\n+                site_id=site_entry[\"siteId\"],\n+                values_to_update=values_to_update,\n+            )\n+\n+    def update_a_site(self, site_id: str, values_to_update: Dict[Text, Any]) -> None:\n+        url = f\"https://graph.microsoft.com/v1.0/sites/{site_id}/\"\n+        payload = values_to_update\n+        payload = json.dumps(payload, indent=2)\n+        headers = {\n+            \"Authorization\": f\"Bearer {self.bearer_token}\",\n+            \"Content-Type\": \"application/json\",\n+        }\n+        self.getresponse(\n+            request_type=\"PATCH\", url=url, headers=headers, payload=payload, files=[]\n+        )\n+\n+    def get_lists_by_site(\n+        self, site_id: str, limit: int = None\n+    ) -> List[Dict[Text, Any]]:\n+        url = f\"https://graph.microsoft.com/v1.0/sites/{site_id}/lists\"\n+        payload = {}\n+        headers = {\n+            \"Authorization\": f\"Bearer {self.bearer_token}\",\n+            \"Content-Type\": \"application/json\",\n+        }\n+\n+        response = self.getresponse(\n+            request_type=\"GET\", url=url, headers=headers, payload=payload, files=[]\n+        )\n+        response = response.json()[\"value\"]\n+        if limit:\n+            response = response[:limit]\n+        return response\n+\n+    def get_all_lists(self, limit: int = None) -> List[Dict[Text, Any]]:\n+        sites = self.get_all_sites()\n+        lists = []\n+        for site in sites:\n+            for list_dict in self.get_lists_by_site(site_id=site[\"id\"].split(\",\")[1]):\n+                list_dict[\"siteName\"] = site[\"name\"]\n+                list_dict[\"siteId\"] = site[\"id\"].split(\",\")[1]\n+                lists.append(list_dict)\n+        if limit:\n+            lists = lists[:limit]\n+        return lists\n+\n+    def delete_lists(self, list_dict: List[Dict[Text, Any]]) -> None:\n+        for list_entry in list_dict:\n+            self.delete_a_list(site_id=list_entry[\"siteId\"], list_id=list_entry[\"id\"])\n+\n+    def delete_a_list(self, site_id: str, list_id: str) -> None:\n+        url = f\"https://graph.microsoft.com/v1.0/sites/{site_id}/lists/{list_id}\"\n+        payload = {}\n+        headers = {\nComment: We can move this to utility function as most of the methods bellow are using it",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/sharepoint_handler/sharepoint_api.py",
    "pr_number": 7757,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1360581139,
    "comment_created_at": "2023-10-16T12:28:08Z"
  },
  {
    "code": "@@ -0,0 +1,10 @@\n+from integrations.handlers.sentence_transformer_handler.settings import (\n+    load_embeddings_model,",
    "comment": "Nit: maybe `load_embeddings_model` should be moved here, instead?",
    "line_number": 2,
    "enriched": "File: mindsdb/integrations/handlers/sentence_transformer_handler/helpers.py\nCode: @@ -0,0 +1,10 @@\n+from integrations.handlers.sentence_transformer_handler.settings import (\n+    load_embeddings_model,\nComment: Nit: maybe `load_embeddings_model` should be moved here, instead?",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/sentence_transformer_handler/helpers.py",
    "pr_number": 7028,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1287698576,
    "comment_created_at": "2023-08-08T21:17:40Z"
  },
  {
    "code": "@@ -168,3 +169,48 @@ def run(self, stop_event):\n     # def send_message(self, message: ChatBotMessage):\n     #\n     #     self.chat_task.chat_handler.realtime_send(message)\n+\n+\n+class WebhookPolling(BasePolling):\n+    \"\"\"\n+    Polling class for handling webhooks.\n+    \"\"\"\n+    def __init__(self, *args, **kwargs):\n+        super().__init__(*args, **kwargs)\n+\n+    def run(self, stop_event: threading.Event) -> None:\n+        \"\"\"\n+        Run the webhook polling.\n+        Check if a webhook token is set for the chatbot. If not, generate a new one.\n+        Then, do nothing, as the webhook is handled by a task instantiated for each request.\n+\n+        Args:\n+            stop_event (threading.Event): Event to stop the polling.       \n+        \"\"\"\n+        # If a webhook token is not set for the chatbot, generate a new one.\n+        from mindsdb.interfaces.chatbot.chatbot_controller import ChatBotController\n+\n+        chat_bot_controller = ChatBotController()\n+        chat_bot = chat_bot_controller.get_chatbot_by_id(self.chat_task.object_id)\n+\n+        if not chat_bot[\"webhook_token\"]:\n+            chat_bot_controller.update_chatbot(\n+                chatbot_name=chat_bot[\"name\"],\n+                project_name=chat_bot[\"project\"],\n+                webhook_token=secrets.token_urlsafe(16),",
    "comment": "let's make length of token bigger, around 30-50 letters",
    "line_number": 200,
    "enriched": "File: mindsdb/interfaces/chatbot/polling.py\nCode: @@ -168,3 +169,48 @@ def run(self, stop_event):\n     # def send_message(self, message: ChatBotMessage):\n     #\n     #     self.chat_task.chat_handler.realtime_send(message)\n+\n+\n+class WebhookPolling(BasePolling):\n+    \"\"\"\n+    Polling class for handling webhooks.\n+    \"\"\"\n+    def __init__(self, *args, **kwargs):\n+        super().__init__(*args, **kwargs)\n+\n+    def run(self, stop_event: threading.Event) -> None:\n+        \"\"\"\n+        Run the webhook polling.\n+        Check if a webhook token is set for the chatbot. If not, generate a new one.\n+        Then, do nothing, as the webhook is handled by a task instantiated for each request.\n+\n+        Args:\n+            stop_event (threading.Event): Event to stop the polling.       \n+        \"\"\"\n+        # If a webhook token is not set for the chatbot, generate a new one.\n+        from mindsdb.interfaces.chatbot.chatbot_controller import ChatBotController\n+\n+        chat_bot_controller = ChatBotController()\n+        chat_bot = chat_bot_controller.get_chatbot_by_id(self.chat_task.object_id)\n+\n+        if not chat_bot[\"webhook_token\"]:\n+            chat_bot_controller.update_chatbot(\n+                chatbot_name=chat_bot[\"name\"],\n+                project_name=chat_bot[\"project\"],\n+                webhook_token=secrets.token_urlsafe(16),\nComment: let's make length of token bigger, around 30-50 letters",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/interfaces/chatbot/polling.py",
    "pr_number": 9780,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1791310692,
    "comment_created_at": "2024-10-08T07:02:29Z"
  },
  {
    "code": "@@ -0,0 +1,132 @@\n+import os\n+import pandas as pd\n+import openai\n+from openai import OpenAI, NotFoundError, AuthenticationError\n+from typing import Dict, Optional\n+from mindsdb.integrations.handlers.openai_handler import Handler as OpenAIHandler\n+from mindsdb.integrations.utilities.handler_utils import get_api_key\n+from mindsdb.integrations.handlers.groq_handler.settings import groq_handler_config\n+from mindsdb.utilities import log\n+\n+logger = log.getLogger(__name__)\n+\n+\n+class GroqHandler(OpenAIHandler):\n+    \"\"\"\n+    This handler handles connection to the Groq.\n+    \"\"\"\n+\n+    name = 'groq'\n+\n+    def __init__(self, *args, **kwargs):\n+        super().__init__(*args, **kwargs)\n+        self.api_base = groq_handler_config.BASE_URL\n+        self.default_model = 'llama3-8b-8192'\n+        self.default_mode = 'default'",
    "comment": "I think we can move these defaults over to the config.",
    "line_number": 25,
    "enriched": "File: mindsdb/integrations/handlers/groq_handler/groq_handler.py\nCode: @@ -0,0 +1,132 @@\n+import os\n+import pandas as pd\n+import openai\n+from openai import OpenAI, NotFoundError, AuthenticationError\n+from typing import Dict, Optional\n+from mindsdb.integrations.handlers.openai_handler import Handler as OpenAIHandler\n+from mindsdb.integrations.utilities.handler_utils import get_api_key\n+from mindsdb.integrations.handlers.groq_handler.settings import groq_handler_config\n+from mindsdb.utilities import log\n+\n+logger = log.getLogger(__name__)\n+\n+\n+class GroqHandler(OpenAIHandler):\n+    \"\"\"\n+    This handler handles connection to the Groq.\n+    \"\"\"\n+\n+    name = 'groq'\n+\n+    def __init__(self, *args, **kwargs):\n+        super().__init__(*args, **kwargs)\n+        self.api_base = groq_handler_config.BASE_URL\n+        self.default_model = 'llama3-8b-8192'\n+        self.default_mode = 'default'\nComment: I think we can move these defaults over to the config.",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/groq_handler/groq_handler.py",
    "pr_number": 9807,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1823811306,
    "comment_created_at": "2024-10-31T06:07:47Z"
  },
  {
    "code": "@@ -77,7 +77,8 @@ def plan(self, query, integration=None):\n         if integration_to_send:\n             self.planner.prepare_integration_select(integration_to_send, query)\n \n-            last_step = self.planner.plan.add_step(FetchDataframeStep(integration=integration_to_send, query=query))\n+            fetch_params = self.planner.get_fetch_params(query.using)\n+            last_step = self.planner.plan.add_step(FetchDataframeStep(integration=integration_to_send, query=query, params=fetch_params))",
    "comment": "**security**: No input validation or sanitization is performed on user-supplied SQL identifiers or query parameters (`query`, `params`), risking SQL injection if downstream integrations do not enforce strict parameterization.\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n        if integration_to_send:\n            self.planner.prepare_integration_select(integration_to_send, query)\n            fetch_params = self.planner.get_fetch_params(query.using)\n            # Validate that all identifiers and parameters are safe before passing to integration\n            self.planner.validate_query_identifiers_and_params(query, fetch_params)\n            last_step = self.planner.plan.add_step(FetchDataframeStep(integration=integration_to_send, query=query, params=fetch_params))\n            return last_step\n```\n</details>\n<!-- suggestion_end -->\n",
    "line_number": 81,
    "enriched": "File: mindsdb/api/executor/planner/plan_join.py\nCode: @@ -77,7 +77,8 @@ def plan(self, query, integration=None):\n         if integration_to_send:\n             self.planner.prepare_integration_select(integration_to_send, query)\n \n-            last_step = self.planner.plan.add_step(FetchDataframeStep(integration=integration_to_send, query=query))\n+            fetch_params = self.planner.get_fetch_params(query.using)\n+            last_step = self.planner.plan.add_step(FetchDataframeStep(integration=integration_to_send, query=query, params=fetch_params))\nComment: **security**: No input validation or sanitization is performed on user-supplied SQL identifiers or query parameters (`query`, `params`), risking SQL injection if downstream integrations do not enforce strict parameterization.\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n        if integration_to_send:\n            self.planner.prepare_integration_select(integration_to_send, query)\n            fetch_params = self.planner.get_fetch_params(query.using)\n            # Validate that all identifiers and parameters are safe before passing to integration\n            self.planner.validate_query_identifiers_and_params(query, fetch_params)\n            last_step = self.planner.plan.add_step(FetchDataframeStep(integration=integration_to_send, query=query, params=fetch_params))\n            return last_step\n```\n</details>\n<!-- suggestion_end -->\n",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/api/executor/planner/plan_join.py",
    "pr_number": 10943,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2115429230,
    "comment_created_at": "2025-05-30T08:48:26Z"
  },
  {
    "code": "@@ -470,6 +470,8 @@ def meta_get_tables(self, table_names: Optional[List[str]] = None) -> Response:\n             query += f\" AND TABLE_NAME IN ({table_names_str})\"\n \n         result = self.native_query(query)\n+        result.data_frame[\"ROW_COUNT\"] = result.data_frame[\"ROW_COUNT\"].astype(int)",
    "comment": "**correctness**: `result.data_frame[\"ROW_COUNT\"] = result.data_frame[\"ROW_COUNT\"].astype(int)` will raise if `result.data_frame` is None or empty, causing a runtime exception.\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n        if result.data_frame is not None and not result.data_frame.empty:\n            result.data_frame[\"ROW_COUNT\"] = result.data_frame[\"ROW_COUNT\"].astype(int)\n```\n</details>\n<!-- suggestion_end -->\n",
    "line_number": 473,
    "enriched": "File: mindsdb/integrations/handlers/snowflake_handler/snowflake_handler.py\nCode: @@ -470,6 +470,8 @@ def meta_get_tables(self, table_names: Optional[List[str]] = None) -> Response:\n             query += f\" AND TABLE_NAME IN ({table_names_str})\"\n \n         result = self.native_query(query)\n+        result.data_frame[\"ROW_COUNT\"] = result.data_frame[\"ROW_COUNT\"].astype(int)\nComment: **correctness**: `result.data_frame[\"ROW_COUNT\"] = result.data_frame[\"ROW_COUNT\"].astype(int)` will raise if `result.data_frame` is None or empty, causing a runtime exception.\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n        if result.data_frame is not None and not result.data_frame.empty:\n            result.data_frame[\"ROW_COUNT\"] = result.data_frame[\"ROW_COUNT\"].astype(int)\n```\n</details>\n<!-- suggestion_end -->\n",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/snowflake_handler/snowflake_handler.py",
    "pr_number": 11020,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2128522032,
    "comment_created_at": "2025-06-05T10:33:16Z"
  },
  {
    "code": "@@ -54,5 +44,28 @@ def is_allowed_url(url):\n     :param url: The URL to check.\n     :return bool:  True if the URL is from an allowed host, False otherwise.\n     \"\"\"\n-    parsed_url = urlparse(url.lower())\n-    return parsed_url.netloc in ALLOWED_DOMAINS\n+    config = Config()\n+    parsed_url = urlparse(url)\n+    return parsed_url.netloc in config.get('file_upload_domains', [])\n+\n+\n+def validate_crawling_sites(urls):\n+    \"\"\"\n+    Checks if the provided URL is from one of the allowed sites for web crawling.\n+\n+    :param urls: The list of URLs to check.\n+    :return bool: True if the URL is from one of the allowed sites, False otherwise.\n+    \"\"\"\n+    config = Config()\n+    allowed_urls = config.get('web_crawling_allowed_sites', [])\n+    allowed_netlocs = [urlparse(allowed_url).netloc for allowed_url in allowed_urls]\n+    if not allowed_urls:\n+        return True\n+    if isinstance(urls, str):\n+        urls = [urls]\n+    # Check if all provided URLs are from the allowed sites\n+    valid = all(",
    "comment": "`all(urlparse(url).netloc in allowed_netlocs for url in urls)` will be slightly faster and laconic",
    "line_number": 67,
    "enriched": "File: mindsdb/utilities/security.py\nCode: @@ -54,5 +44,28 @@ def is_allowed_url(url):\n     :param url: The URL to check.\n     :return bool:  True if the URL is from an allowed host, False otherwise.\n     \"\"\"\n-    parsed_url = urlparse(url.lower())\n-    return parsed_url.netloc in ALLOWED_DOMAINS\n+    config = Config()\n+    parsed_url = urlparse(url)\n+    return parsed_url.netloc in config.get('file_upload_domains', [])\n+\n+\n+def validate_crawling_sites(urls):\n+    \"\"\"\n+    Checks if the provided URL is from one of the allowed sites for web crawling.\n+\n+    :param urls: The list of URLs to check.\n+    :return bool: True if the URL is from one of the allowed sites, False otherwise.\n+    \"\"\"\n+    config = Config()\n+    allowed_urls = config.get('web_crawling_allowed_sites', [])\n+    allowed_netlocs = [urlparse(allowed_url).netloc for allowed_url in allowed_urls]\n+    if not allowed_urls:\n+        return True\n+    if isinstance(urls, str):\n+        urls = [urls]\n+    # Check if all provided URLs are from the allowed sites\n+    valid = all(\nComment: `all(urlparse(url).netloc in allowed_netlocs for url in urls)` will be slightly faster and laconic",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/utilities/security.py",
    "pr_number": 9639,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1726580785,
    "comment_created_at": "2024-08-22T08:19:18Z"
  },
  {
    "code": "@@ -591,5 +593,265 @@ def test_types_casting(self):\n         # endregion\n \n \n+class TestMSSQLHandlerODBC(unittest.TestCase):\n+    \"\"\"Tests for MSSQL handler with ODBC connection\"\"\"\n+\n+    def setUp(self):\n+        self.connection_data = OrderedDict(\n+            host=\"127.0.0.1\",\n+            port=1433,\n+            user=\"example_user\",\n+            password=\"example_pass\",\n+            database=\"example_db\",\n+            driver=\"ODBC Driver 18 for SQL Server\",\n+            use_odbc=True,\n+        )\n+\n+    def test_odbc_mode_enabled_with_driver_param(self):\n+        \"\"\"Test that ODBC mode is enabled when driver parameter is provided\"\"\"\n+        handler = SqlServerHandler(\"mssql_odbc\", connection_data=self.connection_data)\n+        self.assertTrue(handler.use_odbc)\n+\n+    def test_odbc_mode_enabled_with_use_odbc_param(self):\n+        \"\"\"Test that ODBC mode is enabled when use_odbc parameter is True\"\"\"\n+        connection_data = self.connection_data.copy()\n+        del connection_data[\"driver\"]\n+        connection_data[\"use_odbc\"] = True\n+\n+        handler = SqlServerHandler(\"mssql_odbc\", connection_data=connection_data)\n+        self.assertTrue(handler.use_odbc)\n+\n+    def test_odbc_mode_disabled_by_default(self):\n+        \"\"\"Test that ODBC mode is disabled when neither driver nor use_odbc is provided\"\"\"\n+        connection_data = OrderedDict(\n+            host=\"127.0.0.1\",\n+            port=1433,\n+            user=\"example_user\",\n+            password=\"example_pass\",\n+            database=\"example_db\",\n+        )\n+        handler = SqlServerHandler(\"mssql\", connection_data=connection_data)\n+        self.assertFalse(handler.use_odbc)\n+\n+    def test_odbc_connection_string_construction(self):\n+        \"\"\"Test that ODBC connection string is constructed correctly\"\"\"\n+        mock_pyodbc = MagicMock()\n+        mock_connect = MagicMock()\n+        mock_pyodbc.connect = mock_connect\n+\n+        # Mock pyodbc in sys.modules so it can be imported\n+        sys.modules[\"pyodbc\"] = mock_pyodbc\n+\n+        try:\n+            handler = SqlServerHandler(\"mssql_odbc\", connection_data=self.connection_data)\n+            handler.connect()\n+        except Exception:\n+            pass\n+        finally:\n+            if \"pyodbc\" in sys.modules:\n+                del sys.modules[\"pyodbc\"]\n+        if mock_connect.called:\n+            call_args = mock_connect.call_args\n+            conn_str = call_args[0][0] if call_args[0] else \"\"\n+\n+            self.assertIn(\"DRIVER={ODBC Driver 18 for SQL Server}\", conn_str)\n+            self.assertIn(\"SERVER=127.0.0.1,1433\", conn_str)\n+            self.assertIn(\"DATABASE=example_db\", conn_str)\n+            self.assertIn(\"UID=example_user\", conn_str)\n+            self.assertIn(\"PWD=example_pass\", conn_str)\n+\n+    def test_odbc_connection_with_encryption_params(self):\n+        \"\"\"Test that encryption parameters are added to connection string\"\"\"\n+        mock_pyodbc = MagicMock()\n+        mock_connect = MagicMock()\n+        mock_pyodbc.connect = mock_connect\n+\n+        sys.modules[\"pyodbc\"] = mock_pyodbc\n+\n+        connection_data = self.connection_data.copy()\n+        connection_data[\"encrypt\"] = \"yes\"\n+        connection_data[\"trust_server_certificate\"] = \"yes\"\n+\n+        try:\n+            handler = SqlServerHandler(\"mssql_odbc\", connection_data=connection_data)\n+            handler.connect()\n+        except Exception:\n+            pass",
    "comment": "Can we get rid of this and the other occurrences of the same thing? This buries the actual issue when the test fails",
    "line_number": 679,
    "enriched": "File: tests/unit/handlers/test_mssql.py\nCode: @@ -591,5 +593,265 @@ def test_types_casting(self):\n         # endregion\n \n \n+class TestMSSQLHandlerODBC(unittest.TestCase):\n+    \"\"\"Tests for MSSQL handler with ODBC connection\"\"\"\n+\n+    def setUp(self):\n+        self.connection_data = OrderedDict(\n+            host=\"127.0.0.1\",\n+            port=1433,\n+            user=\"example_user\",\n+            password=\"example_pass\",\n+            database=\"example_db\",\n+            driver=\"ODBC Driver 18 for SQL Server\",\n+            use_odbc=True,\n+        )\n+\n+    def test_odbc_mode_enabled_with_driver_param(self):\n+        \"\"\"Test that ODBC mode is enabled when driver parameter is provided\"\"\"\n+        handler = SqlServerHandler(\"mssql_odbc\", connection_data=self.connection_data)\n+        self.assertTrue(handler.use_odbc)\n+\n+    def test_odbc_mode_enabled_with_use_odbc_param(self):\n+        \"\"\"Test that ODBC mode is enabled when use_odbc parameter is True\"\"\"\n+        connection_data = self.connection_data.copy()\n+        del connection_data[\"driver\"]\n+        connection_data[\"use_odbc\"] = True\n+\n+        handler = SqlServerHandler(\"mssql_odbc\", connection_data=connection_data)\n+        self.assertTrue(handler.use_odbc)\n+\n+    def test_odbc_mode_disabled_by_default(self):\n+        \"\"\"Test that ODBC mode is disabled when neither driver nor use_odbc is provided\"\"\"\n+        connection_data = OrderedDict(\n+            host=\"127.0.0.1\",\n+            port=1433,\n+            user=\"example_user\",\n+            password=\"example_pass\",\n+            database=\"example_db\",\n+        )\n+        handler = SqlServerHandler(\"mssql\", connection_data=connection_data)\n+        self.assertFalse(handler.use_odbc)\n+\n+    def test_odbc_connection_string_construction(self):\n+        \"\"\"Test that ODBC connection string is constructed correctly\"\"\"\n+        mock_pyodbc = MagicMock()\n+        mock_connect = MagicMock()\n+        mock_pyodbc.connect = mock_connect\n+\n+        # Mock pyodbc in sys.modules so it can be imported\n+        sys.modules[\"pyodbc\"] = mock_pyodbc\n+\n+        try:\n+            handler = SqlServerHandler(\"mssql_odbc\", connection_data=self.connection_data)\n+            handler.connect()\n+        except Exception:\n+            pass\n+        finally:\n+            if \"pyodbc\" in sys.modules:\n+                del sys.modules[\"pyodbc\"]\n+        if mock_connect.called:\n+            call_args = mock_connect.call_args\n+            conn_str = call_args[0][0] if call_args[0] else \"\"\n+\n+            self.assertIn(\"DRIVER={ODBC Driver 18 for SQL Server}\", conn_str)\n+            self.assertIn(\"SERVER=127.0.0.1,1433\", conn_str)\n+            self.assertIn(\"DATABASE=example_db\", conn_str)\n+            self.assertIn(\"UID=example_user\", conn_str)\n+            self.assertIn(\"PWD=example_pass\", conn_str)\n+\n+    def test_odbc_connection_with_encryption_params(self):\n+        \"\"\"Test that encryption parameters are added to connection string\"\"\"\n+        mock_pyodbc = MagicMock()\n+        mock_connect = MagicMock()\n+        mock_pyodbc.connect = mock_connect\n+\n+        sys.modules[\"pyodbc\"] = mock_pyodbc\n+\n+        connection_data = self.connection_data.copy()\n+        connection_data[\"encrypt\"] = \"yes\"\n+        connection_data[\"trust_server_certificate\"] = \"yes\"\n+\n+        try:\n+            handler = SqlServerHandler(\"mssql_odbc\", connection_data=connection_data)\n+            handler.connect()\n+        except Exception:\n+            pass\nComment: Can we get rid of this and the other occurrences of the same thing? This buries the actual issue when the test fails",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "tests/unit/handlers/test_mssql.py",
    "pr_number": 11692,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2412062749,
    "comment_created_at": "2025-10-07T22:20:24Z"
  },
  {
    "code": "@@ -216,7 +216,7 @@\n                 'Not found',\n                 'The endpoint you are trying to access does not exist on the server.'\n             )\n-        if static_root.joinpath(path).is_file():\n+        if static_root.joinpath(path).resolve().is_file():",
    "comment": "## Uncontrolled data used in path expression\n\nThis path depends on a [user-provided value](1).\n\n[Show more details](https://github.com/mindsdb/mindsdb/security/code-scanning/28)",
    "line_number": 219,
    "enriched": "File: mindsdb/api/http/initialize.py\nCode: @@ -216,7 +216,7 @@\n                 'Not found',\n                 'The endpoint you are trying to access does not exist on the server.'\n             )\n-        if static_root.joinpath(path).is_file():\n+        if static_root.joinpath(path).resolve().is_file():\nComment: ## Uncontrolled data used in path expression\n\nThis path depends on a [user-provided value](1).\n\n[Show more details](https://github.com/mindsdb/mindsdb/security/code-scanning/28)",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/api/http/initialize.py",
    "pr_number": 9700,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1747087287,
    "comment_created_at": "2024-09-06T12:59:33Z"
  },
  {
    "code": "@@ -94,14 +102,15 @@ def _dump_str(var: Any) -> str | None:\n     Returns:\n         str | None: The string representation of the value or None if the value is None\n     \"\"\"\n-    if pd.isna(var):\n+    if isinstance(var, list) is False and pd.isna(var):",
    "comment": "`pd.isna` should be called very often and could slow down mysql server, I think it works slower than regular type or value checks. \r\ncan be a better way to do this checking? ",
    "line_number": 105,
    "enriched": "File: mindsdb/api/mysql/mysql_proxy/utilities/dump.py\nCode: @@ -94,14 +102,15 @@ def _dump_str(var: Any) -> str | None:\n     Returns:\n         str | None: The string representation of the value or None if the value is None\n     \"\"\"\n-    if pd.isna(var):\n+    if isinstance(var, list) is False and pd.isna(var):\nComment: `pd.isna` should be called very often and could slow down mysql server, I think it works slower than regular type or value checks. \r\ncan be a better way to do this checking? ",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/api/mysql/mysql_proxy/utilities/dump.py",
    "pr_number": 10911,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2124358937,
    "comment_created_at": "2025-06-03T16:17:29Z"
  },
  {
    "code": "@@ -0,0 +1,487 @@\n+import pandas as pd\n+from typing import List\n+from mindsdb.integrations.libs.api_handler import APITable\n+from mindsdb.integrations.utilities.handlers.query_utilities import SELECTQueryParser, SELECTQueryExecutor\n+from mindsdb.utilities import log\n+from mindsdb_sql.parser import ast\n+\n+logger = log.getLogger(__name__)\n+\n+\n+class ZendeskUsersTable(APITable):\n+    \"\"\"Zendesk Users Table implementation\"\"\"\n+\n+    def select(self, query: ast.Select) -> pd.DataFrame:\n+        \"\"\"Pulls data from the zendesk list users API\n+\n+        Parameters\n+        ----------\n+        query : ast.Select\n+           Given SQL SELECT query\n+\n+        Returns\n+        -------\n+        pd.DataFrame\n+            Zendesk users\n+\n+        Raises\n+        ------\n+        ValueError\n+            If the query contains an unsupported condition\n+        \"\"\"\n+\n+        select_statement_parser = SELECTQueryParser(\n+            query,\n+            'users',\n+            self.get_columns()\n+        )\n+\n+        selected_columns, where_conditions, order_by_conditions, result_limit = select_statement_parser.parse_query()\n+\n+        subset_where_conditions = []\n+        for op, arg1, arg2 in where_conditions:\n+            if arg1 in self.get_columns():\n+                subset_where_conditions.append([op, arg1, arg2])\n+\n+        zen_users = list(self.handler.zen_client.users())",
    "comment": "Is it at all possible to avoid querying all of the tickets and apply the conditions + the limit clause here? This would improve performance? It would be great if the same can be done for the other resources as well.",
    "line_number": 46,
    "enriched": "File: mindsdb/integrations/handlers/zendesk_handler/zendesk_tables.py\nCode: @@ -0,0 +1,487 @@\n+import pandas as pd\n+from typing import List\n+from mindsdb.integrations.libs.api_handler import APITable\n+from mindsdb.integrations.utilities.handlers.query_utilities import SELECTQueryParser, SELECTQueryExecutor\n+from mindsdb.utilities import log\n+from mindsdb_sql.parser import ast\n+\n+logger = log.getLogger(__name__)\n+\n+\n+class ZendeskUsersTable(APITable):\n+    \"\"\"Zendesk Users Table implementation\"\"\"\n+\n+    def select(self, query: ast.Select) -> pd.DataFrame:\n+        \"\"\"Pulls data from the zendesk list users API\n+\n+        Parameters\n+        ----------\n+        query : ast.Select\n+           Given SQL SELECT query\n+\n+        Returns\n+        -------\n+        pd.DataFrame\n+            Zendesk users\n+\n+        Raises\n+        ------\n+        ValueError\n+            If the query contains an unsupported condition\n+        \"\"\"\n+\n+        select_statement_parser = SELECTQueryParser(\n+            query,\n+            'users',\n+            self.get_columns()\n+        )\n+\n+        selected_columns, where_conditions, order_by_conditions, result_limit = select_statement_parser.parse_query()\n+\n+        subset_where_conditions = []\n+        for op, arg1, arg2 in where_conditions:\n+            if arg1 in self.get_columns():\n+                subset_where_conditions.append([op, arg1, arg2])\n+\n+        zen_users = list(self.handler.zen_client.users())\nComment: Is it at all possible to avoid querying all of the tickets and apply the conditions + the limit clause here? This would improve performance? It would be great if the same can be done for the other resources as well.",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/zendesk_handler/zendesk_tables.py",
    "pr_number": 9948,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1812162065,
    "comment_created_at": "2024-10-23T08:20:48Z"
  },
  {
    "code": "@@ -461,6 +461,10 @@ def set_predictor(self, predictor):\n         self.db.session.commit()\n \n         def predict_f(_model_name, df, pred_format=\"dict\", *args, **kargs):\n+            # df is mutable and may change after 'predict' call.\n+            # This dirty hack is to save original df.\n+            df._predict_df = df[:]",
    "comment": "can be replaced by `df = df.copy()`",
    "line_number": 466,
    "enriched": "File: tests/unit/executor_test_base.py\nCode: @@ -461,6 +461,10 @@ def set_predictor(self, predictor):\n         self.db.session.commit()\n \n         def predict_f(_model_name, df, pred_format=\"dict\", *args, **kargs):\n+            # df is mutable and may change after 'predict' call.\n+            # This dirty hack is to save original df.\n+            df._predict_df = df[:]\nComment: can be replaced by `df = df.copy()`",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "tests/unit/executor_test_base.py",
    "pr_number": 10322,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1914706091,
    "comment_created_at": "2025-01-14T12:02:20Z"
  },
  {
    "code": "@@ -0,0 +1 @@\n+openbb==4.0.0a2",
    "comment": "Should we keep this since it looks like a pre-release version?",
    "line_number": 1,
    "enriched": "File: mindsdb/integrations/handlers/openbb_handler/requirements.txt\nCode: @@ -0,0 +1 @@\n+openbb==4.0.0a2\nComment: Should we keep this since it looks like a pre-release version?",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/openbb_handler/requirements.txt",
    "pr_number": 7621,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1350166782,
    "comment_created_at": "2023-10-09T11:13:07Z"
  },
  {
    "code": "@@ -641,6 +641,101 @@ def select(self, query: ast.Select) -> pd.DataFrame:\n         carrier_service_df = select_statement_executor.execute_query()\n \n         return carrier_service_df\n+    \n+    def insert(self, query: ast.Insert) -> None:\n+        \"\"\"Inserts data into the Shopify \"POST /carrier_services\" API endpoint.\n+\n+        Parameters\n+        ----------\n+        query : ast.Insert\n+           Given SQL INSERT query\n+\n+        Returns\n+        -------\n+        None\n+\n+        Raises\n+        ------\n+        ValueError\n+            If the query contains an unsupported condition\n+        \"\"\"\n+        insert_statement_parser = INSERTQueryParser(\n+            query,\n+            supported_columns=['name', 'callback_url', 'service_discovery'],\n+            mandatory_columns=['name', 'callback_url', 'service_discovery'],\n+            all_mandatory=True\n+        )\n+        carrier_service_data = insert_statement_parser.parse_query()\n+        self.create_carrier_service(carrier_service_data)\n+\n+    def delete(self, query: ast.Delete) -> None:\n+        \"\"\"\n+        Deletes data from the Shopify \"DELETE /carrier_services\" API endpoint.\n+\n+        Parameters\n+        ----------\n+        query : ast.Delete\n+           Given SQL DELETE query\n+\n+        Returns\n+        -------\n+        None\n+\n+        Raises\n+        ------\n+        ValueError\n+            If the query contains an unsupported condition\n+        \"\"\"\n+        delete_statement_parser = DELETEQueryParser(query)\n+        where_conditions = delete_statement_parser.parse_query()\n+\n+        carrier_services_df = pd.json_normalize(self.get_carrier_service())\n+\n+        delete_query_executor = DELETEQueryExecutor(\n+            carrier_services_df,\n+            where_conditions\n+        )\n+\n+        carrier_services_df = delete_query_executor.execute_query()\n+\n+        carrier_service_ids = carrier_services_df['id'].tolist()\n+        self.delete_carrier_services(carrier_service_ids)\n+\n+\n+    def update(self, query: ast.Update) -> None:\n+        \"\"\"Updates data from the Shopify \"PUT /carrier_services\" API endpoint.\n+\n+        Parameters\n+        ----------\n+        query : ast.Update\n+           Given SQL UPDATE query\n+\n+        Returns\n+        -------\n+        None\n+\n+        Raises\n+        ------\n+        ValueError\n+            If the query contains an unsupported condition\n+        \"\"\"\n+        update_statement_parser = UPDATEQueryParser(query)\n+        values_to_update, where_conditions = update_statement_parser.parse_query()\n+\n+        print(f\"values_to_update: {values_to_update}\")",
    "comment": "Please remove the print",
    "line_number": 725,
    "enriched": "File: mindsdb/integrations/handlers/shopify_handler/shopify_tables.py\nCode: @@ -641,6 +641,101 @@ def select(self, query: ast.Select) -> pd.DataFrame:\n         carrier_service_df = select_statement_executor.execute_query()\n \n         return carrier_service_df\n+    \n+    def insert(self, query: ast.Insert) -> None:\n+        \"\"\"Inserts data into the Shopify \"POST /carrier_services\" API endpoint.\n+\n+        Parameters\n+        ----------\n+        query : ast.Insert\n+           Given SQL INSERT query\n+\n+        Returns\n+        -------\n+        None\n+\n+        Raises\n+        ------\n+        ValueError\n+            If the query contains an unsupported condition\n+        \"\"\"\n+        insert_statement_parser = INSERTQueryParser(\n+            query,\n+            supported_columns=['name', 'callback_url', 'service_discovery'],\n+            mandatory_columns=['name', 'callback_url', 'service_discovery'],\n+            all_mandatory=True\n+        )\n+        carrier_service_data = insert_statement_parser.parse_query()\n+        self.create_carrier_service(carrier_service_data)\n+\n+    def delete(self, query: ast.Delete) -> None:\n+        \"\"\"\n+        Deletes data from the Shopify \"DELETE /carrier_services\" API endpoint.\n+\n+        Parameters\n+        ----------\n+        query : ast.Delete\n+           Given SQL DELETE query\n+\n+        Returns\n+        -------\n+        None\n+\n+        Raises\n+        ------\n+        ValueError\n+            If the query contains an unsupported condition\n+        \"\"\"\n+        delete_statement_parser = DELETEQueryParser(query)\n+        where_conditions = delete_statement_parser.parse_query()\n+\n+        carrier_services_df = pd.json_normalize(self.get_carrier_service())\n+\n+        delete_query_executor = DELETEQueryExecutor(\n+            carrier_services_df,\n+            where_conditions\n+        )\n+\n+        carrier_services_df = delete_query_executor.execute_query()\n+\n+        carrier_service_ids = carrier_services_df['id'].tolist()\n+        self.delete_carrier_services(carrier_service_ids)\n+\n+\n+    def update(self, query: ast.Update) -> None:\n+        \"\"\"Updates data from the Shopify \"PUT /carrier_services\" API endpoint.\n+\n+        Parameters\n+        ----------\n+        query : ast.Update\n+           Given SQL UPDATE query\n+\n+        Returns\n+        -------\n+        None\n+\n+        Raises\n+        ------\n+        ValueError\n+            If the query contains an unsupported condition\n+        \"\"\"\n+        update_statement_parser = UPDATEQueryParser(query)\n+        values_to_update, where_conditions = update_statement_parser.parse_query()\n+\n+        print(f\"values_to_update: {values_to_update}\")\nComment: Please remove the print",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/shopify_handler/shopify_tables.py",
    "pr_number": 8028,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1374542209,
    "comment_created_at": "2023-10-27T12:58:46Z"
  },
  {
    "code": "@@ -239,9 +239,9 @@ class MSTeamsHandlerConfig(BaseSettings):\n         }\n     }\n \n-    TEST_CHAT_MESSAGES_DATA = [TEST_CHAT_MESSAGE_DATA]\n+    TEST_CHAT_MESSAGES_DATA: List = [TEST_CHAT_MESSAGE_DATA]",
    "comment": "let's go with `List[dict]` instead perhaps",
    "line_number": 242,
    "enriched": "File: mindsdb/integrations/handlers/ms_teams_handler/settings.py\nCode: @@ -239,9 +239,9 @@ class MSTeamsHandlerConfig(BaseSettings):\n         }\n     }\n \n-    TEST_CHAT_MESSAGES_DATA = [TEST_CHAT_MESSAGE_DATA]\n+    TEST_CHAT_MESSAGES_DATA: List = [TEST_CHAT_MESSAGE_DATA]\nComment: let's go with `List[dict]` instead perhaps",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/ms_teams_handler/settings.py",
    "pr_number": 9186,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1595221475,
    "comment_created_at": "2024-05-09T09:40:58Z"
  },
  {
    "code": "@@ -493,64 +768,109 @@ def delete(self, query: ast.Delete) -> None:\n         delete_statement_parser = DELETEQueryParser(query)\n         where_conditions = delete_statement_parser.parse_query()\n \n-        deals_df = pd.json_normalize(self.get_deals())\n+        deals_df = pd.json_normalize(self.get_deals(limit=1000))\n+\n+        if deals_df.empty:\n+            raise ValueError(\n+                \"No deals retrieved from HubSpot to evaluate delete conditions. Verify your connection and permissions.\"\n+            )\n+\n         delete_query_executor = DELETEQueryExecutor(deals_df, where_conditions)\n+        filtered_df = delete_query_executor.execute_query()\n+\n+        if filtered_df.empty:\n+            raise ValueError(\n+                f\"No deals found matching WHERE conditions: {where_conditions}. Please verify the conditions are correct.\"\n+            )\n \n-        deals_df = delete_query_executor.execute_query()\n-        deal_ids = deals_df[\"id\"].tolist()\n+        deal_ids = filtered_df[\"id\"].astype(str).tolist()\n+        logger.info(f\"Deleting {len(deal_ids)} deal(s) matching WHERE conditions\")\n         self.delete_deals(deal_ids)\n \n     def get_columns(self) -> List[Text]:\n         return pd.json_normalize(self.get_deals(limit=1)).columns.tolist()\n \n+    @staticmethod\n+    def _get_default_deal_columns() -> List[str]:\n+        return [\n+            \"id\",\n+            \"dealname\",\n+            \"amount\",\n+            \"pipeline\",\n+            \"closedate\",\n+            \"dealstage\",\n+            \"hubspot_owner_id\",\n+            \"createdate\",\n+            \"hs_lastmodifieddate\",\n+        ]\n+\n     def get_deals(self, **kwargs) -> List[Dict]:\n         hubspot = self.handler.connect()\n         deals = hubspot.crm.deals.get_all(**kwargs)\n-        deals_dict = [\n-            {\n-                \"id\": deal.id,\n-                \"dealname\": deal.properties[\"dealname\"],\n-                \"amount\": deal.properties.get(\"amount\", None),\n-                \"pipeline\": deal.properties.get(\"pipeline\", None),\n-                \"closedate\": deal.properties.get(\"closedate\", None),\n-                \"dealstage\": deal.properties.get(\"dealstage\", None),\n-                \"hubspot_owner_id\": deal.properties.get(\"hubspot_owner_id\", None),\n-                \"createdate\": deal.properties[\"createdate\"],\n-                \"hs_lastmodifieddate\": deal.properties[\"hs_lastmodifieddate\"],\n-            }\n-            for deal in deals\n-        ]\n+        deals_dict = []\n+\n+        for deal in deals:\n+            try:\n+                deal_dict = {\n+                    \"id\": deal.id,\n+                    \"dealname\": deal.properties.get(\"dealname\", None),\n+                    \"amount\": deal.properties.get(\"amount\", None),\n+                    \"pipeline\": deal.properties.get(\"pipeline\", None),\n+                    \"closedate\": deal.properties.get(\"closedate\", None),\n+                    \"dealstage\": deal.properties.get(\"dealstage\", None),\n+                    \"hubspot_owner_id\": deal.properties.get(\"hubspot_owner_id\", None),\n+                    \"createdate\": deal.properties.get(\"createdate\", None),\n+                    \"hs_lastmodifieddate\": deal.properties.get(\"hs_lastmodifieddate\", None),\n+                }\n+                deals_dict.append(deal_dict)\n+            except Exception as e:\n+                logger.warning(f\"Error processing deal {deal.id}: {str(e)}\")",
    "comment": "in case of error a user won't see some deals in list. for the user it won't be error and they will think that everything is ok, just some deals absent in list. will it mislead them and they will try to insert them again? ",
    "line_number": 827,
    "enriched": "File: mindsdb/integrations/handlers/hubspot_handler/hubspot_tables.py\nCode: @@ -493,64 +768,109 @@ def delete(self, query: ast.Delete) -> None:\n         delete_statement_parser = DELETEQueryParser(query)\n         where_conditions = delete_statement_parser.parse_query()\n \n-        deals_df = pd.json_normalize(self.get_deals())\n+        deals_df = pd.json_normalize(self.get_deals(limit=1000))\n+\n+        if deals_df.empty:\n+            raise ValueError(\n+                \"No deals retrieved from HubSpot to evaluate delete conditions. Verify your connection and permissions.\"\n+            )\n+\n         delete_query_executor = DELETEQueryExecutor(deals_df, where_conditions)\n+        filtered_df = delete_query_executor.execute_query()\n+\n+        if filtered_df.empty:\n+            raise ValueError(\n+                f\"No deals found matching WHERE conditions: {where_conditions}. Please verify the conditions are correct.\"\n+            )\n \n-        deals_df = delete_query_executor.execute_query()\n-        deal_ids = deals_df[\"id\"].tolist()\n+        deal_ids = filtered_df[\"id\"].astype(str).tolist()\n+        logger.info(f\"Deleting {len(deal_ids)} deal(s) matching WHERE conditions\")\n         self.delete_deals(deal_ids)\n \n     def get_columns(self) -> List[Text]:\n         return pd.json_normalize(self.get_deals(limit=1)).columns.tolist()\n \n+    @staticmethod\n+    def _get_default_deal_columns() -> List[str]:\n+        return [\n+            \"id\",\n+            \"dealname\",\n+            \"amount\",\n+            \"pipeline\",\n+            \"closedate\",\n+            \"dealstage\",\n+            \"hubspot_owner_id\",\n+            \"createdate\",\n+            \"hs_lastmodifieddate\",\n+        ]\n+\n     def get_deals(self, **kwargs) -> List[Dict]:\n         hubspot = self.handler.connect()\n         deals = hubspot.crm.deals.get_all(**kwargs)\n-        deals_dict = [\n-            {\n-                \"id\": deal.id,\n-                \"dealname\": deal.properties[\"dealname\"],\n-                \"amount\": deal.properties.get(\"amount\", None),\n-                \"pipeline\": deal.properties.get(\"pipeline\", None),\n-                \"closedate\": deal.properties.get(\"closedate\", None),\n-                \"dealstage\": deal.properties.get(\"dealstage\", None),\n-                \"hubspot_owner_id\": deal.properties.get(\"hubspot_owner_id\", None),\n-                \"createdate\": deal.properties[\"createdate\"],\n-                \"hs_lastmodifieddate\": deal.properties[\"hs_lastmodifieddate\"],\n-            }\n-            for deal in deals\n-        ]\n+        deals_dict = []\n+\n+        for deal in deals:\n+            try:\n+                deal_dict = {\n+                    \"id\": deal.id,\n+                    \"dealname\": deal.properties.get(\"dealname\", None),\n+                    \"amount\": deal.properties.get(\"amount\", None),\n+                    \"pipeline\": deal.properties.get(\"pipeline\", None),\n+                    \"closedate\": deal.properties.get(\"closedate\", None),\n+                    \"dealstage\": deal.properties.get(\"dealstage\", None),\n+                    \"hubspot_owner_id\": deal.properties.get(\"hubspot_owner_id\", None),\n+                    \"createdate\": deal.properties.get(\"createdate\", None),\n+                    \"hs_lastmodifieddate\": deal.properties.get(\"hs_lastmodifieddate\", None),\n+                }\n+                deals_dict.append(deal_dict)\n+            except Exception as e:\n+                logger.warning(f\"Error processing deal {deal.id}: {str(e)}\")\nComment: in case of error a user won't see some deals in list. for the user it won't be error and they will think that everything is ok, just some deals absent in list. will it mislead them and they will try to insert them again? ",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/hubspot_handler/hubspot_tables.py",
    "pr_number": 11874,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2518333823,
    "comment_created_at": "2025-11-12T13:33:33Z"
  },
  {
    "code": "@@ -0,0 +1,33 @@\n+---\n+title: MediaWiki\n+sidebarTitle: MediaWiki\n+---\n+\n+This is the implementation of the ClickHouse data handler for MindsDB.",
    "comment": "It mentions ClickHouse",
    "line_number": 6,
    "enriched": "File: docs/integrations/app-integrations/mediawiki.mdx\nCode: @@ -0,0 +1,33 @@\n+---\n+title: MediaWiki\n+sidebarTitle: MediaWiki\n+---\n+\n+This is the implementation of the ClickHouse data handler for MindsDB.\nComment: It mentions ClickHouse",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "docs/integrations/app-integrations/mediawiki.mdx",
    "pr_number": 7390,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1332937926,
    "comment_created_at": "2023-09-21T11:57:01Z"
  },
  {
    "code": "@@ -22,169 +21,141 @@ def _merge_key_recursive(target_dict, source_dict, key):\n             _merge_key_recursive(target_dict[key], source_dict[key], k)\n \n \n-def _merge_configs(original_config, override_config):\n-    original_config = deepcopy(original_config)\n+def _merge_configs(original_config: dict, override_config: dict) -> dict:\n     for key in list(override_config.keys()):\n         _merge_key_recursive(original_config, override_config, key)\n     return original_config\n \n \n-def create_directory(path):\n-    path = Path(path)\n-    path.mkdir(mode=0o777, exist_ok=True, parents=True)\n+def create_data_dir(path: Path) -> None:\n+    \"\"\"Create a directory and checks that it is writable.\n \n+    Args:\n+        path (Path): path to create and check\n \n-def get_or_create_data_dir():\n-    data_dir = user_data_dir(\"mindsdb\", \"mindsdb\")\n-    mindsdb_data_dir = os.path.join(data_dir, \"var/\")\n-\n-    if os.path.exists(mindsdb_data_dir) is False:\n-        create_directory(mindsdb_data_dir)\n+    Raises:\n+        NotADirectoryError: if path exists, but it is not a directory\n+        PermissionError: if path exists/created, but it is not writable\n+        Exception: if directory could not be created\n+    \"\"\"\n+    if path.exists() and not path.is_dir():\n+        raise NotADirectoryError(f\"The path is not a directory: {path}\")\n \n     try:\n-        assert os.path.exists(mindsdb_data_dir)\n-        assert os.access(mindsdb_data_dir, os.W_OK) is True\n-    except Exception:\n+        path.mkdir(mode=0o777, exist_ok=True, parents=True)\n+    except Exception as e:\n         raise Exception(\n-            \"MindsDB storage directory does not exist and could not be created\"\n-        )\n+            \"MindsDB storage directory could not be created\"\n+        ) from e\n \n-    return mindsdb_data_dir\n-\n-\n-config = None\n-config_mtime = -1\n-\n-\n-class Config():\n-    def __init__(self):\n-        # initialize once\n-        global config, config_mtime\n-        self.config_path = os.environ.get('MINDSDB_CONFIG_PATH', 'absent')\n-        self.use_docker_env = os.environ.get('MINDSDB_DOCKER_ENV', False)\n-        if self.use_docker_env:\n-            self.use_docker_env = True\n-\n-        if Path(self.config_path).is_file():\n-            current_config_mtime = os.path.getmtime(self.config_path)\n-            if config_mtime != current_config_mtime:\n-                config = self.init_config()\n-                config_mtime = current_config_mtime\n-\n-        if config is None:\n-            config = self.init_config()\n-\n-        self._config = config\n-\n-    def init_config(self):\n-        if self.config_path == 'absent':\n-            self._override_config = {}\n-        else:\n-            with open(self.config_path, 'r') as fp:\n-                self._override_config = json.load(fp)\n-\n-        # region define storage dir\n-        if 'storage_dir' in self._override_config:\n-            root_storage_dir = self._override_config['storage_dir']\n-            os.environ['MINDSDB_STORAGE_DIR'] = root_storage_dir\n-        elif os.environ.get('MINDSDB_STORAGE_DIR') is not None:\n-            root_storage_dir = os.environ['MINDSDB_STORAGE_DIR']\n-        else:\n-            root_storage_dir = get_or_create_data_dir()\n-            os.environ['MINDSDB_STORAGE_DIR'] = root_storage_dir\n-        # endregion\n-\n-        # region\n-        is_storage_absent = os.environ.get('MINDSDB_STORAGE_BACKUP_DISABLED', '').lower() in ('1', 'true')\n-        if is_storage_absent is True:\n-            self._override_config['permanent_storage'] = {\n-                'location': 'absent'\n-            }\n-        # endregion\n-\n-        if os.path.isdir(root_storage_dir) is False:\n-            os.makedirs(root_storage_dir)\n-\n-        if 'storage_db' in self._override_config:\n-            os.environ['MINDSDB_DB_CON'] = self._override_config['storage_db']\n-        elif os.environ.get('MINDSDB_DB_CON', '') == '':\n-            os.environ['MINDSDB_DB_CON'] = 'sqlite:///' + os.path.join(root_storage_dir,\n-                                                                       'mindsdb.sqlite3.db') + '?check_same_thread=False&timeout=30'\n-\n-        paths = {\n-            'root': os.environ['MINDSDB_STORAGE_DIR']\n-        }\n-\n-        # content - temporary storage for entities\n-        paths['content'] = os.path.join(paths['root'], 'content')\n-        # storage - persist storage for entities\n-        paths['storage'] = os.path.join(paths['root'], 'storage')\n-        paths['static'] = os.path.join(paths['root'], 'static')\n-        paths['tmp'] = os.path.join(paths['root'], 'tmp')\n-        paths['log'] = os.path.join(paths['root'], 'log')\n-        paths['cache'] = os.path.join(paths['root'], 'cache')\n-        paths['locks'] = os.path.join(paths['root'], 'locks')\n-\n-        for path_name in paths:\n-            create_directory(paths[path_name])\n-\n-        ml_queue = {\n-            'type': 'local'\n-        }\n-\n-        if os.environ.get('MINDSDB_ML_QUEUE_TYPE', '').lower() == 'redis':\n-            ml_queue['type'] = 'redis'\n-            ml_queue['host'] = os.environ.get('MINDSDB_ML_QUEUE_HOST', 'localhost')\n-            ml_queue['port'] = int(os.environ.get('MINDSDB_ML_QUEUE_PORT', 6379))\n-            ml_queue['db'] = int(os.environ.get('MINDSDB_ML_QUEUE_DB', 0))\n-            ml_queue['username'] = os.environ.get('MINDSDB_ML_QUEUE_USERNAME')\n-            ml_queue['password'] = os.environ.get('MINDSDB_ML_QUEUE_PASSWORD')\n-\n-        # If only one of the username or password is set, raise an error.\n-        http_username = os.environ.get('MINDSDB_USERNAME')\n-        http_password = os.environ.get('MINDSDB_PASSWORD')\n-\n-        if bool(http_username) != bool(http_password):\n-            raise ValueError('Both MINDSDB_USERNAME and MINDSDB_PASSWORD must be set together and must be non-empty strings.')\n-\n-        # If both username and password are set, enable HTTP auth.\n-        if http_username and http_password:\n-            if 'auth' not in self._override_config:\n-                self._override_config['auth'] = {}\n+    if not os.access(path, os.W_OK):\n+        raise PermissionError(\n+            f\"The directory is not allowed for writing: {path}\"\n+        )\n \n-            self._override_config['auth']['http_auth_enabled'] = True\n-            self._override_config['auth']['username'] = http_username\n-            self._override_config['auth']['password'] = http_password\n \n-        # region permanent session lifetime\n-        permanent_session_lifetime = datetime.timedelta(days=31)\n-        for env_name in ('MINDSDB_HTTP_PERMANENT_SESSION_LIFETIME', 'FLASK_PERMANENT_SESSION_LIFETIME'):\n-            env_value = os.environ.get(env_name)\n-            if isinstance(env_value, str):\n-                try:\n-                    permanent_session_lifetime = int(env_value)\n-                except Exception:\n-                    logger.warning(f'Can\\'t cast env var {env_name} value to int: {env_value}')\n-                    continue\n-                break\n+class Config:\n+    \"\"\"Application config. Singletone, initialized just once. Re-initialyze if `config.auto.json` is changed.\n+    The class loads multiple configs and merge then in one. If a config option defined in multiple places (config file,\n+    env var, cmd arg, etc), then it will be resolved in following order of priority:\n+     - default config values (lowest priority)\n+     - `config.json` provided by the user\n+     - `config.auto.json`\n+     - config values collected from env vars\n+     - values from cmd args (most priority)\n+\n+    Attributes:\n+        __instance (Config): instance of 'Config' to make it singleton\n+        _config (dict): application config, the result of merging other configs\n+        _user_config (dict): config provided by the user (usually with cmd arg `--config=config.json`)\n+        _env_config (dict): config collected from different env vars\n+        _auto_config (dict): config that is editd by the app itself (e.g. when you change values in GUI)\n+        _default_config (dict): config with default values\n+        config_path (Path): path to the `config.json` provided by the user\n+        storage_root_path (Path): path to storage root folder\n+        auto_config_path (Path): path to `config.auto.json`\n+        auto_config_mtime (float): mtime of `config.auto.json` when it was loaded to `self._auto_config`\n+        _cmd_args (argparse.Namespace): cmd args\n+        use_docker_env (bool): is the app run in docker env\n+    \"\"\"\n+    __instance: 'Config' = None\n+\n+    _config: dict = None\n+    _user_config: dict = None\n+    _env_config: dict = None\n+    _auto_config: dict = None\n+    _default_config: dict = None\n+    config_path: Path = None\n+    storage_root_path: Path = None\n+    auto_config_path: Path = None\n+    auto_config_mtime: float = 0\n+    _cmd_args: argparse.Namespace = None\n+    use_docker_env: bool = os.environ.get('MINDSDB_DOCKER_ENV', False) is not False\n+\n+    def __new__(cls, *args, **kwargs) -> 'Config':\n+        \"\"\"Make class singletone and initialize config.\n+        \"\"\"\n+        if cls.__instance is not None:\n+            return cls.__instance\n+\n+        self = super().__new__(cls, *args, **kwargs)\n+        cls.__instance = self\n+\n+        self.fetch_user_config()\n+\n+        # region determine root path\n+        if self.storage_root_path is None:\n+            if isinstance(os.environ.get('MINDSDB_STORAGE_DIR'), str):\n+                self.storage_root_path = os.environ.get['MINDSDB_STORAGE_DIR']",
    "comment": "should it be?: os.environ.get('MINDSDB_STORAGE_DIR')\r\nthe same in line 248",
    "line_number": 109,
    "enriched": "File: mindsdb/utilities/config.py\nCode: @@ -22,169 +21,141 @@ def _merge_key_recursive(target_dict, source_dict, key):\n             _merge_key_recursive(target_dict[key], source_dict[key], k)\n \n \n-def _merge_configs(original_config, override_config):\n-    original_config = deepcopy(original_config)\n+def _merge_configs(original_config: dict, override_config: dict) -> dict:\n     for key in list(override_config.keys()):\n         _merge_key_recursive(original_config, override_config, key)\n     return original_config\n \n \n-def create_directory(path):\n-    path = Path(path)\n-    path.mkdir(mode=0o777, exist_ok=True, parents=True)\n+def create_data_dir(path: Path) -> None:\n+    \"\"\"Create a directory and checks that it is writable.\n \n+    Args:\n+        path (Path): path to create and check\n \n-def get_or_create_data_dir():\n-    data_dir = user_data_dir(\"mindsdb\", \"mindsdb\")\n-    mindsdb_data_dir = os.path.join(data_dir, \"var/\")\n-\n-    if os.path.exists(mindsdb_data_dir) is False:\n-        create_directory(mindsdb_data_dir)\n+    Raises:\n+        NotADirectoryError: if path exists, but it is not a directory\n+        PermissionError: if path exists/created, but it is not writable\n+        Exception: if directory could not be created\n+    \"\"\"\n+    if path.exists() and not path.is_dir():\n+        raise NotADirectoryError(f\"The path is not a directory: {path}\")\n \n     try:\n-        assert os.path.exists(mindsdb_data_dir)\n-        assert os.access(mindsdb_data_dir, os.W_OK) is True\n-    except Exception:\n+        path.mkdir(mode=0o777, exist_ok=True, parents=True)\n+    except Exception as e:\n         raise Exception(\n-            \"MindsDB storage directory does not exist and could not be created\"\n-        )\n+            \"MindsDB storage directory could not be created\"\n+        ) from e\n \n-    return mindsdb_data_dir\n-\n-\n-config = None\n-config_mtime = -1\n-\n-\n-class Config():\n-    def __init__(self):\n-        # initialize once\n-        global config, config_mtime\n-        self.config_path = os.environ.get('MINDSDB_CONFIG_PATH', 'absent')\n-        self.use_docker_env = os.environ.get('MINDSDB_DOCKER_ENV', False)\n-        if self.use_docker_env:\n-            self.use_docker_env = True\n-\n-        if Path(self.config_path).is_file():\n-            current_config_mtime = os.path.getmtime(self.config_path)\n-            if config_mtime != current_config_mtime:\n-                config = self.init_config()\n-                config_mtime = current_config_mtime\n-\n-        if config is None:\n-            config = self.init_config()\n-\n-        self._config = config\n-\n-    def init_config(self):\n-        if self.config_path == 'absent':\n-            self._override_config = {}\n-        else:\n-            with open(self.config_path, 'r') as fp:\n-                self._override_config = json.load(fp)\n-\n-        # region define storage dir\n-        if 'storage_dir' in self._override_config:\n-            root_storage_dir = self._override_config['storage_dir']\n-            os.environ['MINDSDB_STORAGE_DIR'] = root_storage_dir\n-        elif os.environ.get('MINDSDB_STORAGE_DIR') is not None:\n-            root_storage_dir = os.environ['MINDSDB_STORAGE_DIR']\n-        else:\n-            root_storage_dir = get_or_create_data_dir()\n-            os.environ['MINDSDB_STORAGE_DIR'] = root_storage_dir\n-        # endregion\n-\n-        # region\n-        is_storage_absent = os.environ.get('MINDSDB_STORAGE_BACKUP_DISABLED', '').lower() in ('1', 'true')\n-        if is_storage_absent is True:\n-            self._override_config['permanent_storage'] = {\n-                'location': 'absent'\n-            }\n-        # endregion\n-\n-        if os.path.isdir(root_storage_dir) is False:\n-            os.makedirs(root_storage_dir)\n-\n-        if 'storage_db' in self._override_config:\n-            os.environ['MINDSDB_DB_CON'] = self._override_config['storage_db']\n-        elif os.environ.get('MINDSDB_DB_CON', '') == '':\n-            os.environ['MINDSDB_DB_CON'] = 'sqlite:///' + os.path.join(root_storage_dir,\n-                                                                       'mindsdb.sqlite3.db') + '?check_same_thread=False&timeout=30'\n-\n-        paths = {\n-            'root': os.environ['MINDSDB_STORAGE_DIR']\n-        }\n-\n-        # content - temporary storage for entities\n-        paths['content'] = os.path.join(paths['root'], 'content')\n-        # storage - persist storage for entities\n-        paths['storage'] = os.path.join(paths['root'], 'storage')\n-        paths['static'] = os.path.join(paths['root'], 'static')\n-        paths['tmp'] = os.path.join(paths['root'], 'tmp')\n-        paths['log'] = os.path.join(paths['root'], 'log')\n-        paths['cache'] = os.path.join(paths['root'], 'cache')\n-        paths['locks'] = os.path.join(paths['root'], 'locks')\n-\n-        for path_name in paths:\n-            create_directory(paths[path_name])\n-\n-        ml_queue = {\n-            'type': 'local'\n-        }\n-\n-        if os.environ.get('MINDSDB_ML_QUEUE_TYPE', '').lower() == 'redis':\n-            ml_queue['type'] = 'redis'\n-            ml_queue['host'] = os.environ.get('MINDSDB_ML_QUEUE_HOST', 'localhost')\n-            ml_queue['port'] = int(os.environ.get('MINDSDB_ML_QUEUE_PORT', 6379))\n-            ml_queue['db'] = int(os.environ.get('MINDSDB_ML_QUEUE_DB', 0))\n-            ml_queue['username'] = os.environ.get('MINDSDB_ML_QUEUE_USERNAME')\n-            ml_queue['password'] = os.environ.get('MINDSDB_ML_QUEUE_PASSWORD')\n-\n-        # If only one of the username or password is set, raise an error.\n-        http_username = os.environ.get('MINDSDB_USERNAME')\n-        http_password = os.environ.get('MINDSDB_PASSWORD')\n-\n-        if bool(http_username) != bool(http_password):\n-            raise ValueError('Both MINDSDB_USERNAME and MINDSDB_PASSWORD must be set together and must be non-empty strings.')\n-\n-        # If both username and password are set, enable HTTP auth.\n-        if http_username and http_password:\n-            if 'auth' not in self._override_config:\n-                self._override_config['auth'] = {}\n+    if not os.access(path, os.W_OK):\n+        raise PermissionError(\n+            f\"The directory is not allowed for writing: {path}\"\n+        )\n \n-            self._override_config['auth']['http_auth_enabled'] = True\n-            self._override_config['auth']['username'] = http_username\n-            self._override_config['auth']['password'] = http_password\n \n-        # region permanent session lifetime\n-        permanent_session_lifetime = datetime.timedelta(days=31)\n-        for env_name in ('MINDSDB_HTTP_PERMANENT_SESSION_LIFETIME', 'FLASK_PERMANENT_SESSION_LIFETIME'):\n-            env_value = os.environ.get(env_name)\n-            if isinstance(env_value, str):\n-                try:\n-                    permanent_session_lifetime = int(env_value)\n-                except Exception:\n-                    logger.warning(f'Can\\'t cast env var {env_name} value to int: {env_value}')\n-                    continue\n-                break\n+class Config:\n+    \"\"\"Application config. Singletone, initialized just once. Re-initialyze if `config.auto.json` is changed.\n+    The class loads multiple configs and merge then in one. If a config option defined in multiple places (config file,\n+    env var, cmd arg, etc), then it will be resolved in following order of priority:\n+     - default config values (lowest priority)\n+     - `config.json` provided by the user\n+     - `config.auto.json`\n+     - config values collected from env vars\n+     - values from cmd args (most priority)\n+\n+    Attributes:\n+        __instance (Config): instance of 'Config' to make it singleton\n+        _config (dict): application config, the result of merging other configs\n+        _user_config (dict): config provided by the user (usually with cmd arg `--config=config.json`)\n+        _env_config (dict): config collected from different env vars\n+        _auto_config (dict): config that is editd by the app itself (e.g. when you change values in GUI)\n+        _default_config (dict): config with default values\n+        config_path (Path): path to the `config.json` provided by the user\n+        storage_root_path (Path): path to storage root folder\n+        auto_config_path (Path): path to `config.auto.json`\n+        auto_config_mtime (float): mtime of `config.auto.json` when it was loaded to `self._auto_config`\n+        _cmd_args (argparse.Namespace): cmd args\n+        use_docker_env (bool): is the app run in docker env\n+    \"\"\"\n+    __instance: 'Config' = None\n+\n+    _config: dict = None\n+    _user_config: dict = None\n+    _env_config: dict = None\n+    _auto_config: dict = None\n+    _default_config: dict = None\n+    config_path: Path = None\n+    storage_root_path: Path = None\n+    auto_config_path: Path = None\n+    auto_config_mtime: float = 0\n+    _cmd_args: argparse.Namespace = None\n+    use_docker_env: bool = os.environ.get('MINDSDB_DOCKER_ENV', False) is not False\n+\n+    def __new__(cls, *args, **kwargs) -> 'Config':\n+        \"\"\"Make class singletone and initialize config.\n+        \"\"\"\n+        if cls.__instance is not None:\n+            return cls.__instance\n+\n+        self = super().__new__(cls, *args, **kwargs)\n+        cls.__instance = self\n+\n+        self.fetch_user_config()\n+\n+        # region determine root path\n+        if self.storage_root_path is None:\n+            if isinstance(os.environ.get('MINDSDB_STORAGE_DIR'), str):\n+                self.storage_root_path = os.environ.get['MINDSDB_STORAGE_DIR']\nComment: should it be?: os.environ.get('MINDSDB_STORAGE_DIR')\r\nthe same in line 248",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/utilities/config.py",
    "pr_number": 10287,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1886734396,
    "comment_created_at": "2024-12-16T12:27:18Z"
  },
  {
    "code": "@@ -54,11 +55,13 @@\n     DropKnowledgeBase,\n     DropMLEngine,\n     DropPredictor,\n+    DropSkill,\n     DropTrigger,\n     Evaluate,\n     FinetunePredictor,\n     RetrainPredictor,\n     UpdateChatBot,\n+    UpdateSkill",
    "comment": "We need to cut a new release of `mindsdb_sql` to bring in https://github.com/mindsdb/mindsdb_sql/pull/317",
    "line_number": 64,
    "enriched": "File: mindsdb/api/mysql/mysql_proxy/executor/executor_commands.py\nCode: @@ -54,11 +55,13 @@\n     DropKnowledgeBase,\n     DropMLEngine,\n     DropPredictor,\n+    DropSkill,\n     DropTrigger,\n     Evaluate,\n     FinetunePredictor,\n     RetrainPredictor,\n     UpdateChatBot,\n+    UpdateSkill\nComment: We need to cut a new release of `mindsdb_sql` to bring in https://github.com/mindsdb/mindsdb_sql/pull/317",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/api/mysql/mysql_proxy/executor/executor_commands.py",
    "pr_number": 8142,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1381905832,
    "comment_created_at": "2023-11-03T15:47:04Z"
  },
  {
    "code": "@@ -58,6 +58,20 @@ JOIN files.question_table as t;\n \n ![](https://i.ibb.co/WPgXJDs/Screenshot-2023-05-30-at-7-54-32-PM.png)\n \n+## LLM conversational mode\n+```sql\n+  CREATE MODEL chatbot_agent_2\n+    PREDICT answer\n+    USING\n+    engine = 'llama_index',\n+    input_column = 'question',\n+    openai_api_key = '<key>',",
    "comment": "FYI @martyna-mindsdb we will have to update this on #8245 if merged first.",
    "line_number": 68,
    "enriched": "File: mindsdb/integrations/handlers/llama_index_handler/README.md\nCode: @@ -58,6 +58,20 @@ JOIN files.question_table as t;\n \n ![](https://i.ibb.co/WPgXJDs/Screenshot-2023-05-30-at-7-54-32-PM.png)\n \n+## LLM conversational mode\n+```sql\n+  CREATE MODEL chatbot_agent_2\n+    PREDICT answer\n+    USING\n+    engine = 'llama_index',\n+    input_column = 'question',\n+    openai_api_key = '<key>',\nComment: FYI @martyna-mindsdb we will have to update this on #8245 if merged first.",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/llama_index_handler/README.md",
    "pr_number": 8652,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1463320451,
    "comment_created_at": "2024-01-23T13:56:06Z"
  },
  {
    "code": "@@ -687,7 +688,8 @@ def insert(self, df: pd.DataFrame, params: dict = None):\n             # speed up inserting by disable checking existing records\n             db_handler.insert(self._kb.vector_database_table, df)\n         else:\n-            db_handler.do_upsert(self._kb.vector_database_table, df)\n+            skip_existing = params.get(\"skip_existing\", False) if params is not None else False",
    "comment": "This line can be simplified. The conditional check for `params is not None` is redundant since `dict.get()` safely handles None values by returning the default.",
    "line_number": 691,
    "enriched": "File: mindsdb/interfaces/knowledge_base/controller.py\nCode: @@ -687,7 +688,8 @@ def insert(self, df: pd.DataFrame, params: dict = None):\n             # speed up inserting by disable checking existing records\n             db_handler.insert(self._kb.vector_database_table, df)\n         else:\n-            db_handler.do_upsert(self._kb.vector_database_table, df)\n+            skip_existing = params.get(\"skip_existing\", False) if params is not None else False\nComment: This line can be simplified. The conditional check for `params is not None` is redundant since `dict.get()` safely handles None values by returning the default.",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/interfaces/knowledge_base/controller.py",
    "pr_number": 11532,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2328565244,
    "comment_created_at": "2025-09-07T08:01:27Z"
  },
  {
    "code": "@@ -0,0 +1,106 @@\n+from collections import OrderedDict\n+\n+from mindsdb.integrations.handlers.clipdrop_handler.clipdrop_tables import (\n+    RemoveTextTable\n+)\n+from mindsdb.integrations.handlers.clipdrop_handler.clipdrop import ClipdropClient\n+from mindsdb.integrations.libs.api_handler import APIHandler\n+from mindsdb.integrations.libs.response import (\n+    HandlerStatusResponse as StatusResponse,\n+)\n+from mindsdb.integrations.libs.const import HANDLER_CONNECTION_ARG_TYPE as ARG_TYPE\n+\n+from mindsdb.utilities.log import get_log\n+from mindsdb_sql import parse_sql\n+\n+\n+logger = get_log(\"integrations.clipdrop_handler\")\n+\n+\n+class ClipdropHandler(APIHandler):\n+    \"\"\"The Clipdrop handler implementation\"\"\"\n+\n+    def __init__(self, name: str, **kwargs):\n+        \"\"\"Initialize the Clipdrop handler.\n+\n+        Parameters\n+        ----------\n+        name : str\n+            name of a handler instance\n+        \"\"\"\n+        super().__init__(name)\n+\n+        connection_data = kwargs.get(\"connection_data\", {})\n+        self.connection_data = connection_data\n+        self.kwargs = kwargs\n+        self.client = ClipdropClient(connection_data.get(\"api_key\"), ",
    "comment": "Maybe move this to the connect method?",
    "line_number": 36,
    "enriched": "File: mindsdb/integrations/handlers/clipdrop_handler/clipdrop_handler.py\nCode: @@ -0,0 +1,106 @@\n+from collections import OrderedDict\n+\n+from mindsdb.integrations.handlers.clipdrop_handler.clipdrop_tables import (\n+    RemoveTextTable\n+)\n+from mindsdb.integrations.handlers.clipdrop_handler.clipdrop import ClipdropClient\n+from mindsdb.integrations.libs.api_handler import APIHandler\n+from mindsdb.integrations.libs.response import (\n+    HandlerStatusResponse as StatusResponse,\n+)\n+from mindsdb.integrations.libs.const import HANDLER_CONNECTION_ARG_TYPE as ARG_TYPE\n+\n+from mindsdb.utilities.log import get_log\n+from mindsdb_sql import parse_sql\n+\n+\n+logger = get_log(\"integrations.clipdrop_handler\")\n+\n+\n+class ClipdropHandler(APIHandler):\n+    \"\"\"The Clipdrop handler implementation\"\"\"\n+\n+    def __init__(self, name: str, **kwargs):\n+        \"\"\"Initialize the Clipdrop handler.\n+\n+        Parameters\n+        ----------\n+        name : str\n+            name of a handler instance\n+        \"\"\"\n+        super().__init__(name)\n+\n+        connection_data = kwargs.get(\"connection_data\", {})\n+        self.connection_data = connection_data\n+        self.kwargs = kwargs\n+        self.client = ClipdropClient(connection_data.get(\"api_key\"), \nComment: Maybe move this to the connect method?",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/clipdrop_handler/clipdrop_handler.py",
    "pr_number": 8227,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1378828516,
    "comment_created_at": "2023-11-01T13:50:00Z"
  },
  {
    "code": "@@ -202,6 +189,26 @@ def call(self, step):\n                 predictions = None\n \n             if predictions is None:\n+                # handle columns mapping to model\n+                if step.columns_map is not None:\n+                    # columns_map = {str: Identifier}\n+                    cols_to_rename = {}",
    "comment": "You can join this two lines into:\r\n```python\r\ncols_to_rename: Dict[str, Identifier] = {}\r\n```\r\n:)",
    "line_number": 195,
    "enriched": "File: mindsdb/api/executor/sql_query/steps/apply_predictor_step.py\nCode: @@ -202,6 +189,26 @@ def call(self, step):\n                 predictions = None\n \n             if predictions is None:\n+                # handle columns mapping to model\n+                if step.columns_map is not None:\n+                    # columns_map = {str: Identifier}\n+                    cols_to_rename = {}\nComment: You can join this two lines into:\r\n```python\r\ncols_to_rename: Dict[str, Identifier] = {}\r\n```\r\n:)",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/api/executor/sql_query/steps/apply_predictor_step.py",
    "pr_number": 9323,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1639381087,
    "comment_created_at": "2024-06-14T07:17:21Z"
  },
  {
    "code": "@@ -43,7 +43,7 @@ def create_map_reduce_documents_chain(summarization_config: SummarizationConfig,\n     if 'input' in map_prompt.input_variables:\n         map_prompt = map_prompt.partial(input=input)\n     # Handles summarization of individual chunks.\n-    map_chain = LLMChain(llm=summarization_llm, prompt=map_prompt)\n+    # map_chain = LLMChain(llm=summarization_llm, prompt=map_prompt)",
    "comment": "nit - Delete instead of commenting out",
    "line_number": 46,
    "enriched": "File: mindsdb/integrations/utilities/rag/chains/map_reduce_summarizer_chain.py\nCode: @@ -43,7 +43,7 @@ def create_map_reduce_documents_chain(summarization_config: SummarizationConfig,\n     if 'input' in map_prompt.input_variables:\n         map_prompt = map_prompt.partial(input=input)\n     # Handles summarization of individual chunks.\n-    map_chain = LLMChain(llm=summarization_llm, prompt=map_prompt)\n+    # map_chain = LLMChain(llm=summarization_llm, prompt=map_prompt)\nComment: nit - Delete instead of commenting out",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/utilities/rag/chains/map_reduce_summarizer_chain.py",
    "pr_number": 10392,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1924500981,
    "comment_created_at": "2025-01-21T23:46:27Z"
  },
  {
    "code": "@@ -115,6 +115,9 @@ def call(self, step):\n             if context_callback:\n                 context_callback(df, response.columns)\n \n+        # if query registered, set progress\n+        if self.sql_query.run_query is not None:\n+            self.sql_query.run_query.set_progress(df, None)",
    "comment": "**Correctness**: The code accesses `self.sql_query.run_query` without checking if `self.sql_query` exists first. This could cause an `AttributeError` if `sql_query` is `None` or undefined, leading to runtime crashes.\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n        # if query registered, set progress\n        if self.sql_query is not None and self.sql_query.run_query is not None:\n            self.sql_query.run_query.set_progress(df, None)\n```\n</details>\n<!-- suggestion_end -->\n",
    "line_number": 120,
    "enriched": "File: mindsdb/api/executor/sql_query/steps/fetch_dataframe.py\nCode: @@ -115,6 +115,9 @@ def call(self, step):\n             if context_callback:\n                 context_callback(df, response.columns)\n \n+        # if query registered, set progress\n+        if self.sql_query.run_query is not None:\n+            self.sql_query.run_query.set_progress(df, None)\nComment: **Correctness**: The code accesses `self.sql_query.run_query` without checking if `self.sql_query` exists first. This could cause an `AttributeError` if `sql_query` is `None` or undefined, leading to runtime crashes.\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n        # if query registered, set progress\n        if self.sql_query is not None and self.sql_query.run_query is not None:\n            self.sql_query.run_query.set_progress(df, None)\n```\n</details>\n<!-- suggestion_end -->\n",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/api/executor/sql_query/steps/fetch_dataframe.py",
    "pr_number": 10936,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2114283300,
    "comment_created_at": "2025-05-29T16:08:07Z"
  },
  {
    "code": "@@ -0,0 +1,46 @@\n+FROM python:3.9-slim\n+\n+# Set the working directory\n+WORKDIR /app\n+\n+# Copy the requirements file\n+COPY requirements.txt .\n+\n+# Install the dependencies\n+RUN pip install --no-cache-dir -r requirements.txt\n+\n+# Install curl for healthcheck\n+RUN apt-get update && apt-get install -y curl && rm -rf /var/lib/apt/lists/*\n+\n+# Copy the entire application code\n+COPY ./src ./src\n+COPY ./notebooks ./notebooks\n+COPY ./scripts ./scripts\n+COPY ./data ./data\n+COPY ./tests ./tests\n+COPY ./docs ./docs\n+COPY .env.example .env.example\n+\n+# Create a startup script to run both FastAPI and Streamlit\n+RUN echo '#!/bin/bash\\n\\\n+set -e\\n\\\n+\\n\\\n+echo \"Waiting for MindsDB to be ready...\"\\n\\\n+until curl -f http://${MINDSDB_HOST:-mindsdb}:${MINDSDB_PORT:-47334} > /dev/null 2>&1; do\\n\\\n+  echo \"MindsDB is unavailable - sleeping\"\\n\\\n+  sleep 2\\n\\\n+done\\n\\\n+\\n\\\n+echo \"MindsDB is ready! Starting application...\"\\n\\\n+echo \"Starting FastAPI on port 8000...\"\\n\\\n+uvicorn src.app:app --host 0.0.0.0 --port 8000 &\\n\\\n+\\n\\\n+echo \"Starting Streamlit on port 8501...\"\\n\\\n+streamlit run src/ui/streamlit_app.py --server.port=8501 --server.address=0.0.0.0\\n\\\n+' > /app/start.sh && chmod +x /app/start.sh",
    "comment": "**correctness**: `start.sh` script uses `set -e` but runs `uvicorn ... &` in background; if `uvicorn` fails, the script will not exit, potentially leaving the container running with only Streamlit and no API backend.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb hacktoberfest/use-cases/academic-research-copilot/Dockerfile, lines 25-40, the generated start.sh script uses 'set -e' but backgrounds the uvicorn process. If uvicorn fails, the script does not exit, potentially leaving the container running with only Streamlit. Update the script so both uvicorn and streamlit are started in the background, their PIDs are captured, and the script waits for both processes to exit, ensuring the container exits if either fails.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\nRUN echo '#!/bin/bash\\n\\\nset -e\\n\\\n...\\nuvicorn src.app:app --host 0.0.0.0 --port 8000 &\\n\\\nUVICORN_PID=$!\\n\\\necho \"Starting Streamlit on port 8501...\"\\n\\\nstreamlit run src/ui/streamlit_app.py --server.port=8501 --server.address=0.0.0.0 &\\n\\\nSTREAMLIT_PID=$!\\n\\\nwait $UVICORN_PID\\n\\\nwait $STREAMLIT_PID\\n\\\n' > /app/start.sh && chmod +x /app/start.sh\n```\n</details>\n<!-- suggestion_end -->\n",
    "line_number": 40,
    "enriched": "File: mindsdb hacktoberfest/use-cases/academic-research-copilot/Dockerfile\nCode: @@ -0,0 +1,46 @@\n+FROM python:3.9-slim\n+\n+# Set the working directory\n+WORKDIR /app\n+\n+# Copy the requirements file\n+COPY requirements.txt .\n+\n+# Install the dependencies\n+RUN pip install --no-cache-dir -r requirements.txt\n+\n+# Install curl for healthcheck\n+RUN apt-get update && apt-get install -y curl && rm -rf /var/lib/apt/lists/*\n+\n+# Copy the entire application code\n+COPY ./src ./src\n+COPY ./notebooks ./notebooks\n+COPY ./scripts ./scripts\n+COPY ./data ./data\n+COPY ./tests ./tests\n+COPY ./docs ./docs\n+COPY .env.example .env.example\n+\n+# Create a startup script to run both FastAPI and Streamlit\n+RUN echo '#!/bin/bash\\n\\\n+set -e\\n\\\n+\\n\\\n+echo \"Waiting for MindsDB to be ready...\"\\n\\\n+until curl -f http://${MINDSDB_HOST:-mindsdb}:${MINDSDB_PORT:-47334} > /dev/null 2>&1; do\\n\\\n+  echo \"MindsDB is unavailable - sleeping\"\\n\\\n+  sleep 2\\n\\\n+done\\n\\\n+\\n\\\n+echo \"MindsDB is ready! Starting application...\"\\n\\\n+echo \"Starting FastAPI on port 8000...\"\\n\\\n+uvicorn src.app:app --host 0.0.0.0 --port 8000 &\\n\\\n+\\n\\\n+echo \"Starting Streamlit on port 8501...\"\\n\\\n+streamlit run src/ui/streamlit_app.py --server.port=8501 --server.address=0.0.0.0\\n\\\n+' > /app/start.sh && chmod +x /app/start.sh\nComment: **correctness**: `start.sh` script uses `set -e` but runs `uvicorn ... &` in background; if `uvicorn` fails, the script will not exit, potentially leaving the container running with only Streamlit and no API backend.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb hacktoberfest/use-cases/academic-research-copilot/Dockerfile, lines 25-40, the generated start.sh script uses 'set -e' but backgrounds the uvicorn process. If uvicorn fails, the script does not exit, potentially leaving the container running with only Streamlit. Update the script so both uvicorn and streamlit are started in the background, their PIDs are captured, and the script waits for both processes to exit, ensuring the container exits if either fails.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\nRUN echo '#!/bin/bash\\n\\\nset -e\\n\\\n...\\nuvicorn src.app:app --host 0.0.0.0 --port 8000 &\\n\\\nUVICORN_PID=$!\\n\\\necho \"Starting Streamlit on port 8501...\"\\n\\\nstreamlit run src/ui/streamlit_app.py --server.port=8501 --server.address=0.0.0.0 &\\n\\\nSTREAMLIT_PID=$!\\n\\\nwait $UVICORN_PID\\n\\\nwait $STREAMLIT_PID\\n\\\n' > /app/start.sh && chmod +x /app/start.sh\n```\n</details>\n<!-- suggestion_end -->\n",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb hacktoberfest/use-cases/academic-research-copilot/Dockerfile",
    "pr_number": 11812,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2471994792,
    "comment_created_at": "2025-10-29T07:34:08Z"
  },
  {
    "code": "@@ -0,0 +1,87 @@\n+# Mendeley API Handler\n+This handler integrates with the Mendeley API\n+\n+### Connect to the Mendeley API\n+We start by creating a database to connect to the Mendeley API. In order to do that we currently don't need any parameters.\n+\n+```\n+CREATE DATABASE my_mendeley\n+WITH\n+  ENGINE = 'mendeley'\n+  PARAMETERS = {};",
    "comment": "Since by default we don't need params better to remove it from here",
    "line_number": 11,
    "enriched": "File: mindsdb/integrations/handlers/mendeley_handler/README.md\nCode: @@ -0,0 +1,87 @@\n+# Mendeley API Handler\n+This handler integrates with the Mendeley API\n+\n+### Connect to the Mendeley API\n+We start by creating a database to connect to the Mendeley API. In order to do that we currently don't need any parameters.\n+\n+```\n+CREATE DATABASE my_mendeley\n+WITH\n+  ENGINE = 'mendeley'\n+  PARAMETERS = {};\nComment: Since by default we don't need params better to remove it from here",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/mendeley_handler/README.md",
    "pr_number": 6434,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1221699217,
    "comment_created_at": "2023-06-07T14:25:20Z"
  },
  {
    "code": "@@ -38,20 +38,33 @@ env:\n   UV_LINK_MODE: \"symlink\"\n \n jobs:\n-\n+  changes:\n+    name: Filter changed files\n+    runs-on: mdb-dev\n+    outputs:\n+      not-docs: ${{ steps.filter.outputs.not-docs }}\n+    steps:\n+      - uses: dorny/paths-filter@v3\n+        id: filter\n+        with:\n+          predicate-quantifier: \"every\"\n+          filters: |\n+            not-docs:\n+              - '!docs/**'  \n+              - '!**/*.md'",
    "comment": "**Correctness**: The `predicate-quantifier: \"every\"` setting means the `not-docs` filter will only be true if *every* changed file matches the patterns. This is the opposite of the intended behavior - tests should run if *any* non-doc file changes.\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n          predicate-quantifier: \"any\"\n          filters: |\n            not-docs:\n              - '!docs/**'  \n              - '!**/*.md'\n```\n</details>\n<!-- suggestion_end -->\n",
    "line_number": 54,
    "enriched": "File: .github/workflows/test_on_deploy.yml\nCode: @@ -38,20 +38,33 @@ env:\n   UV_LINK_MODE: \"symlink\"\n \n jobs:\n-\n+  changes:\n+    name: Filter changed files\n+    runs-on: mdb-dev\n+    outputs:\n+      not-docs: ${{ steps.filter.outputs.not-docs }}\n+    steps:\n+      - uses: dorny/paths-filter@v3\n+        id: filter\n+        with:\n+          predicate-quantifier: \"every\"\n+          filters: |\n+            not-docs:\n+              - '!docs/**'  \n+              - '!**/*.md'\nComment: **Correctness**: The `predicate-quantifier: \"every\"` setting means the `not-docs` filter will only be true if *every* changed file matches the patterns. This is the opposite of the intended behavior - tests should run if *any* non-doc file changes.\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n          predicate-quantifier: \"any\"\n          filters: |\n            not-docs:\n              - '!docs/**'  \n              - '!**/*.md'\n```\n</details>\n<!-- suggestion_end -->\n",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": ".github/workflows/test_on_deploy.yml",
    "pr_number": 10834,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2089679668,
    "comment_created_at": "2025-05-14T20:19:46Z"
  },
  {
    "code": "@@ -57,6 +57,17 @@ def get(self, db_name):\n                 'engine': val.get('engine'),\n                 'deletable': val.get('deletable')\n             } for key, val in tables.items()]\n+\n+            jobs = ca.jobs_controller.get_list(db_name)\n+            tables = tables + [{",
    "comment": "wouldn't be better to add list of the jobs into [project.get_tables](https://github.com/mindsdb/mindsdb/blob/3c24a594e8b173404c18cc9ebe679d51a1617165/mindsdb/interfaces/database/projects.py#L305) instead of this one.\r\nget_tables returns objects of the project",
    "line_number": 62,
    "enriched": "File: mindsdb/api/http/namespaces/tree.py\nCode: @@ -57,6 +57,17 @@ def get(self, db_name):\n                 'engine': val.get('engine'),\n                 'deletable': val.get('deletable')\n             } for key, val in tables.items()]\n+\n+            jobs = ca.jobs_controller.get_list(db_name)\n+            tables = tables + [{\nComment: wouldn't be better to add list of the jobs into [project.get_tables](https://github.com/mindsdb/mindsdb/blob/3c24a594e8b173404c18cc9ebe679d51a1617165/mindsdb/interfaces/database/projects.py#L305) instead of this one.\r\nget_tables returns objects of the project",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/api/http/namespaces/tree.py",
    "pr_number": 11326,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2215445176,
    "comment_created_at": "2025-07-18T08:47:41Z"
  },
  {
    "code": "@@ -411,13 +411,14 @@ def get_table_info(self, table_names: Optional[List[str]] = None) -> str:\n                 if len(parts) == 1:\n                     raise ValueError(f\"Invalid table name: {name}. Expected format is 'database.table'.\")\n \n-                database_table_map[parts[0]] = database_table_map.get(parts[0], []) + [parts[1]]\n+                database_table_map.setdefault(parts[0], []).append(parts[1])\n \n             data_catalog_str = \"\"\n             for database_name, table_names in database_table_map.items():\n                 data_catalog_reader = DataCatalogReader(database_name=database_name, table_names=table_names)\n \n-                data_catalog_str += data_catalog_reader.read_metadata_as_string()\n+                result = data_catalog_reader.read_metadata_as_string()",
    "comment": "**correctness**: `data_catalog_str += data_catalog_reader.read_metadata_as_string()` can raise TypeError if `read_metadata_as_string()` returns None; concatenation with None will crash.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/interfaces/skills/sql_agent.py, lines 420-420, the code concatenates the result of data_catalog_reader.read_metadata_as_string() directly to a string. If the method returns None, this will cause a TypeError at runtime. Please update this line to ensure that None is converted to an empty string before concatenation, e.g., by assigning the result to a variable and using str(result or \"\").\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n                result = data_catalog_reader.read_metadata_as_string()\n                data_catalog_str += str(result or \"\")\n```\n</details>\n<!-- suggestion_end -->\n",
    "line_number": 420,
    "enriched": "File: mindsdb/interfaces/skills/sql_agent.py\nCode: @@ -411,13 +411,14 @@ def get_table_info(self, table_names: Optional[List[str]] = None) -> str:\n                 if len(parts) == 1:\n                     raise ValueError(f\"Invalid table name: {name}. Expected format is 'database.table'.\")\n \n-                database_table_map[parts[0]] = database_table_map.get(parts[0], []) + [parts[1]]\n+                database_table_map.setdefault(parts[0], []).append(parts[1])\n \n             data_catalog_str = \"\"\n             for database_name, table_names in database_table_map.items():\n                 data_catalog_reader = DataCatalogReader(database_name=database_name, table_names=table_names)\n \n-                data_catalog_str += data_catalog_reader.read_metadata_as_string()\n+                result = data_catalog_reader.read_metadata_as_string()\nComment: **correctness**: `data_catalog_str += data_catalog_reader.read_metadata_as_string()` can raise TypeError if `read_metadata_as_string()` returns None; concatenation with None will crash.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/interfaces/skills/sql_agent.py, lines 420-420, the code concatenates the result of data_catalog_reader.read_metadata_as_string() directly to a string. If the method returns None, this will cause a TypeError at runtime. Please update this line to ensure that None is converted to an empty string before concatenation, e.g., by assigning the result to a variable and using str(result or \"\").\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n                result = data_catalog_reader.read_metadata_as_string()\n                data_catalog_str += str(result or \"\")\n```\n</details>\n<!-- suggestion_end -->\n",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/interfaces/skills/sql_agent.py",
    "pr_number": 11177,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2166772409,
    "comment_created_at": "2025-06-25T13:50:08Z"
  },
  {
    "code": "@@ -258,8 +261,14 @@ def resolve_database_table(self, node: Identifier):\n \n         err_msg_suffix = \"\"\n         if len(parts) > 1:\n-            if parts[0].lower() in self.databases:\n-                database = parts.pop(0).lower()\n+            # if not quoted  check in lower case\n+            part = parts[0]\n+            if part not in self.databases and not node.is_quoted[0]:\n+                part = part.lower()\n+\n+            if part in self.databases:\n+                database = part\n+                parts.pop(0)",
    "comment": "will be good also pop node.is_quoted, to keep node consistent",
    "line_number": 271,
    "enriched": "File: mindsdb/api/executor/planner/query_planner.py\nCode: @@ -258,8 +261,14 @@ def resolve_database_table(self, node: Identifier):\n \n         err_msg_suffix = \"\"\n         if len(parts) > 1:\n-            if parts[0].lower() in self.databases:\n-                database = parts.pop(0).lower()\n+            # if not quoted  check in lower case\n+            part = parts[0]\n+            if part not in self.databases and not node.is_quoted[0]:\n+                part = part.lower()\n+\n+            if part in self.databases:\n+                database = part\n+                parts.pop(0)\nComment: will be good also pop node.is_quoted, to keep node consistent",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/api/executor/planner/query_planner.py",
    "pr_number": 11889,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2542333346,
    "comment_created_at": "2025-11-19T14:45:06Z"
  },
  {
    "code": "@@ -666,50 +640,51 @@ class MetaTableConstraintsTable(Table):\n     def get_data(cls, query: ASTNode = None, inf_schema=None, **kwargs):\n         databases, tables = _get_scope(query)\n \n-        records = _get_records_from_data_catalog(databases, tables)\n+        if not databases:\n+            raise ValueError(\"At least one database must be specified in the query.\")\n \n-        data = []\n-        for record in records:\n-            database_name = record.integration.name\n-            table_name = record.name\n-            primary_keys = record.meta_primary_keys\n-            foreign_keys_children = record.meta_foreign_keys_children\n-            foreign_keys_parents = record.meta_foreign_keys_parents\n-\n-            for pk in primary_keys:\n-                item = {\n-                    \"CONSTRAINT_CATALOG\": \"def\",\n-                    \"CONSTRAINT_SCHEMA\": database_name,\n-                    \"CONSTRAINT_NAME\": pk.constraint_name,\n-                    \"TABLE_SCHEMA\": database_name,\n-                    \"TABLE_NAME\": table_name,\n-                    \"CONSTRAINT_TYPE\": \"PRIMARY KEY\",\n-                }\n-                data.append(item)\n-\n-            for fk in foreign_keys_children:\n-                item = {\n-                    \"CONSTRAINT_CATALOG\": \"def\",\n-                    \"CONSTRAINT_SCHEMA\": database_name,\n-                    \"CONSTRAINT_NAME\": fk.constraint_name,\n-                    \"TABLE_SCHEMA\": database_name,\n-                    \"TABLE_NAME\": table_name,\n-                    \"CONSTRAINT_TYPE\": \"FOREIGN KEY\",\n-                }\n-                data.append(item)\n-\n-            for fk in foreign_keys_parents:\n-                item = {\n-                    \"CONSTRAINT_CATALOG\": \"def\",\n-                    \"CONSTRAINT_SCHEMA\": database_name,\n-                    \"CONSTRAINT_NAME\": fk.constraint_name,\n-                    \"TABLE_SCHEMA\": database_name,\n-                    \"TABLE_NAME\": table_name,\n-                    \"CONSTRAINT_TYPE\": \"FOREIGN KEY\",\n-                }\n-                data.append(item)\n+        df = pd.DataFrame()\n+        for database in databases:\n+            data_catalog_retriever = DataCatalogRetriever(database_name=database, table_names=tables)\n+\n+            primary_keys_df = data_catalog_retriever.retrieve_primary_keys()\n+            if not primary_keys_df.empty:\n+                primary_keys_df[\"CONSTRAINT_CATALOG\"] = \"def\"\n+                primary_keys_df[[\"CONSTRAINT_SCHEMA\", \"TABLE_SCHEMA\"]] = database\n+                primary_keys_df[\"CONSTRAINT_TYPE\"] = \"PRIMARY KEY\"",
    "comment": "**correctness**: `MetaTableConstraintsTable.get_data` and `MetaColumnUsageTable.get_data` assign a string to multiple DataFrame columns using `df[[col1, col2]] = value`, which sets each column to the string's characters, not the intended value.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/api/executor/datahub/datanodes/system_tables.py, lines 653-654, fix the assignment to DataFrame columns in MetaTableConstraintsTable.get_data so that both 'CONSTRAINT_SCHEMA' and 'TABLE_SCHEMA' are set to the full string value of 'database', not split into characters. Assign each column individually: primary_keys_df[\"CONSTRAINT_SCHEMA\"] = database and primary_keys_df[\"TABLE_SCHEMA\"] = database.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n                primary_keys_df[\"CONSTRAINT_CATALOG\"] = \"def\"\n                primary_keys_df[\"CONSTRAINT_SCHEMA\"] = database\n                primary_keys_df[\"TABLE_SCHEMA\"] = database\n```\n</details>\n<!-- suggestion_end -->\n",
    "line_number": 654,
    "enriched": "File: mindsdb/api/executor/datahub/datanodes/system_tables.py\nCode: @@ -666,50 +640,51 @@ class MetaTableConstraintsTable(Table):\n     def get_data(cls, query: ASTNode = None, inf_schema=None, **kwargs):\n         databases, tables = _get_scope(query)\n \n-        records = _get_records_from_data_catalog(databases, tables)\n+        if not databases:\n+            raise ValueError(\"At least one database must be specified in the query.\")\n \n-        data = []\n-        for record in records:\n-            database_name = record.integration.name\n-            table_name = record.name\n-            primary_keys = record.meta_primary_keys\n-            foreign_keys_children = record.meta_foreign_keys_children\n-            foreign_keys_parents = record.meta_foreign_keys_parents\n-\n-            for pk in primary_keys:\n-                item = {\n-                    \"CONSTRAINT_CATALOG\": \"def\",\n-                    \"CONSTRAINT_SCHEMA\": database_name,\n-                    \"CONSTRAINT_NAME\": pk.constraint_name,\n-                    \"TABLE_SCHEMA\": database_name,\n-                    \"TABLE_NAME\": table_name,\n-                    \"CONSTRAINT_TYPE\": \"PRIMARY KEY\",\n-                }\n-                data.append(item)\n-\n-            for fk in foreign_keys_children:\n-                item = {\n-                    \"CONSTRAINT_CATALOG\": \"def\",\n-                    \"CONSTRAINT_SCHEMA\": database_name,\n-                    \"CONSTRAINT_NAME\": fk.constraint_name,\n-                    \"TABLE_SCHEMA\": database_name,\n-                    \"TABLE_NAME\": table_name,\n-                    \"CONSTRAINT_TYPE\": \"FOREIGN KEY\",\n-                }\n-                data.append(item)\n-\n-            for fk in foreign_keys_parents:\n-                item = {\n-                    \"CONSTRAINT_CATALOG\": \"def\",\n-                    \"CONSTRAINT_SCHEMA\": database_name,\n-                    \"CONSTRAINT_NAME\": fk.constraint_name,\n-                    \"TABLE_SCHEMA\": database_name,\n-                    \"TABLE_NAME\": table_name,\n-                    \"CONSTRAINT_TYPE\": \"FOREIGN KEY\",\n-                }\n-                data.append(item)\n+        df = pd.DataFrame()\n+        for database in databases:\n+            data_catalog_retriever = DataCatalogRetriever(database_name=database, table_names=tables)\n+\n+            primary_keys_df = data_catalog_retriever.retrieve_primary_keys()\n+            if not primary_keys_df.empty:\n+                primary_keys_df[\"CONSTRAINT_CATALOG\"] = \"def\"\n+                primary_keys_df[[\"CONSTRAINT_SCHEMA\", \"TABLE_SCHEMA\"]] = database\n+                primary_keys_df[\"CONSTRAINT_TYPE\"] = \"PRIMARY KEY\"\nComment: **correctness**: `MetaTableConstraintsTable.get_data` and `MetaColumnUsageTable.get_data` assign a string to multiple DataFrame columns using `df[[col1, col2]] = value`, which sets each column to the string's characters, not the intended value.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/api/executor/datahub/datanodes/system_tables.py, lines 653-654, fix the assignment to DataFrame columns in MetaTableConstraintsTable.get_data so that both 'CONSTRAINT_SCHEMA' and 'TABLE_SCHEMA' are set to the full string value of 'database', not split into characters. Assign each column individually: primary_keys_df[\"CONSTRAINT_SCHEMA\"] = database and primary_keys_df[\"TABLE_SCHEMA\"] = database.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n                primary_keys_df[\"CONSTRAINT_CATALOG\"] = \"def\"\n                primary_keys_df[\"CONSTRAINT_SCHEMA\"] = database\n                primary_keys_df[\"TABLE_SCHEMA\"] = database\n```\n</details>\n<!-- suggestion_end -->\n",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/api/executor/datahub/datanodes/system_tables.py",
    "pr_number": 11811,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2471944812,
    "comment_created_at": "2025-10-29T07:08:46Z"
  },
  {
    "code": "@@ -275,6 +276,11 @@ def _submit_completion(model_name, prompts, api_key, api_args, args, df):\n             else:\n                 return _submit_normal_completion(kwargs, prompts, api_args)\n \n+        def _log_api_call(params, response):\n+            params2 = params.copy()\n+            params2.pop('api_key', None)\n+            log.logger.debug(f'>>>openai call: {params2}:\\n{response}')",
    "comment": "Would it perhaps be better/cleaner to do `\\t\\t\\t` instead of `>>>`?",
    "line_number": 282,
    "enriched": "File: mindsdb/integrations/handlers/openai_handler/openai_handler.py\nCode: @@ -275,6 +276,11 @@ def _submit_completion(model_name, prompts, api_key, api_args, args, df):\n             else:\n                 return _submit_normal_completion(kwargs, prompts, api_args)\n \n+        def _log_api_call(params, response):\n+            params2 = params.copy()\n+            params2.pop('api_key', None)\n+            log.logger.debug(f'>>>openai call: {params2}:\\n{response}')\nComment: Would it perhaps be better/cleaner to do `\\t\\t\\t` instead of `>>>`?",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/openai_handler/openai_handler.py",
    "pr_number": 5129,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1143981376,
    "comment_created_at": "2023-03-21T20:57:40Z"
  },
  {
    "code": "@@ -870,34 +870,42 @@ def add(\n \n         embedding_params = copy.deepcopy(config.get('default_embedding_model', {}))\n \n-        model_name = None\n-        model_project = project\n-        if embedding_model:\n-            model_name = embedding_model.parts[-1]\n-            if len(embedding_model.parts) > 1:\n-                model_project = self.session.database_controller.get_project(embedding_model.parts[-2])\n-\n-        elif 'embedding_model' in params:\n-            if isinstance(params['embedding_model'], str):\n-                # it is model name\n-                model_name = params['embedding_model']\n-            else:\n-                # it is params for model\n-                embedding_params.update(params['embedding_model'])\n-\n-        if model_name is None:\n-            model_name = self._create_embedding_model(\n-                project.name,\n-                params=embedding_params,\n-                kb_name=name,\n-            )\n-            params['created_embedding_model'] = model_name\n+        # Legacy: Allow MindsDB models to be passed as embedding_model.\n+        # model_name = None\n+        # model_project = project\n+        # if embedding_model:\n+        #     model_name = embedding_model.parts[-1]\n+        #     if len(embedding_model.parts) > 1:\n+        #         model_project = self.session.database_controller.get_project(embedding_model.parts[-2])\n+\n+        # elif 'embedding_model' in params:\n+        #     if isinstance(params['embedding_model'], str):\n+        #         # it is model name\n+        #         model_name = params['embedding_model']\n+        #     else:\n+        #         # it is params for model\n+        #         embedding_params.update(params['embedding_model'])\n+\n+        if 'embedding_model' in params:\n+            if not isinstance(params['embedding_model'], dict):\n+                raise ValueError(\n+                    \"embedding_model should be JSON object with model parameters.\"\n+                )\n+            embedding_params.update(params['embedding_model'])\n+\n+        # if model_name is None:\n+        model_name = self._create_embedding_model(\n+            project.name,\n+            params=embedding_params,\n+            kb_name=name,\n+        )\n+        params['created_embedding_model'] = model_name\n \n         embedding_model_id = None\n         if model_name is not None:",
    "comment": "**Correctness**: The `model_name` variable is used before assignment on line 905, but it's only assigned on line 897. The commented-out code previously set `model_name` to `None` initially.\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n        model_name = None\n        model_name = self._create_embedding_model(\n            project.name,\n            params=embedding_params,\n            kb_name=name,\n        )\n        params['created_embedding_model'] = model_name\n\n        embedding_model_id = None\n        if model_name is not None:\n```\n</details>\n<!-- suggestion_end -->\n",
    "line_number": 905,
    "enriched": "File: mindsdb/interfaces/knowledge_base/controller.py\nCode: @@ -870,34 +870,42 @@ def add(\n \n         embedding_params = copy.deepcopy(config.get('default_embedding_model', {}))\n \n-        model_name = None\n-        model_project = project\n-        if embedding_model:\n-            model_name = embedding_model.parts[-1]\n-            if len(embedding_model.parts) > 1:\n-                model_project = self.session.database_controller.get_project(embedding_model.parts[-2])\n-\n-        elif 'embedding_model' in params:\n-            if isinstance(params['embedding_model'], str):\n-                # it is model name\n-                model_name = params['embedding_model']\n-            else:\n-                # it is params for model\n-                embedding_params.update(params['embedding_model'])\n-\n-        if model_name is None:\n-            model_name = self._create_embedding_model(\n-                project.name,\n-                params=embedding_params,\n-                kb_name=name,\n-            )\n-            params['created_embedding_model'] = model_name\n+        # Legacy: Allow MindsDB models to be passed as embedding_model.\n+        # model_name = None\n+        # model_project = project\n+        # if embedding_model:\n+        #     model_name = embedding_model.parts[-1]\n+        #     if len(embedding_model.parts) > 1:\n+        #         model_project = self.session.database_controller.get_project(embedding_model.parts[-2])\n+\n+        # elif 'embedding_model' in params:\n+        #     if isinstance(params['embedding_model'], str):\n+        #         # it is model name\n+        #         model_name = params['embedding_model']\n+        #     else:\n+        #         # it is params for model\n+        #         embedding_params.update(params['embedding_model'])\n+\n+        if 'embedding_model' in params:\n+            if not isinstance(params['embedding_model'], dict):\n+                raise ValueError(\n+                    \"embedding_model should be JSON object with model parameters.\"\n+                )\n+            embedding_params.update(params['embedding_model'])\n+\n+        # if model_name is None:\n+        model_name = self._create_embedding_model(\n+            project.name,\n+            params=embedding_params,\n+            kb_name=name,\n+        )\n+        params['created_embedding_model'] = model_name\n \n         embedding_model_id = None\n         if model_name is not None:\nComment: **Correctness**: The `model_name` variable is used before assignment on line 905, but it's only assigned on line 897. The commented-out code previously set `model_name` to `None` initially.\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n        model_name = None\n        model_name = self._create_embedding_model(\n            project.name,\n            params=embedding_params,\n            kb_name=name,\n        )\n        params['created_embedding_model'] = model_name\n\n        embedding_model_id = None\n        if model_name is not None:\n```\n</details>\n<!-- suggestion_end -->\n",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/interfaces/knowledge_base/controller.py",
    "pr_number": 10812,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2079218595,
    "comment_created_at": "2025-05-08T08:54:50Z"
  },
  {
    "code": "@@ -24,15 +24,21 @@ def upgrade():\n         sa.Column('id', sa.Integer(), nullable=False),\n         sa.Column('company_id', sa.Integer(), nullable=True),\n         sa.Column('api_key', sa.String(), nullable=True),\n-        sa.Column('model_id', sa.Integer(), nullable=False),\n+        sa.Column('model_id', sa.Integer(), nullable=True),\n+        sa.Column('model_group', sa.String(), nullable=True),\n         sa.Column('input', sa.String(), nullable=True),\n         sa.Column('output', sa.String(), nullable=True),\n         sa.Column('start_time', sa.DateTime(), nullable=False),\n         sa.Column('end_time', sa.DateTime(), nullable=True),\n+        sa.Column('cost', sa.Numeric(), nullable=True),\n         sa.Column('prompt_tokens', sa.Integer(), nullable=True),\n         sa.Column('completion_tokens', sa.Integer(), nullable=True),\n         sa.Column('total_tokens', sa.Integer(), nullable=True),\n-        sa.Column('success', sa.Boolean(), nullable=False),\n+        sa.Column('success', sa.Boolean(), nullable=False, default=True),\n+        sa.Column('exception', sa.String(), nullable=True),\n+        sa.Column('traceback', sa.String(), nullable=True),\n+        sa.Column('stream', sa.Boolean(), default=False),\n+        sa.Column('metadata', sa.JSON(), nullable=True),",
    "comment": "I don't think this will work for existing MindsDB databases.  \r\n\r\nIIUC, all databases that have run the migration version `2024-02-12_9461892bd889_llm_log.py` will not add these changes, and it might even cause errors because this migration's checksum is different ([ref1](https://github.com/mindsdb/mindsdb/blob/main/mindsdb/migrations/versions/2024-10-07_6c57ed39a82b_added_webhook_token_to_chat_bots.py), [ref2](https://github.com/mindsdb/mindsdb/blob/main/mindsdb/migrations/versions/2024-07-19_45eb2eb61f70_add_provider_to_agent.py)).\r\n\r\nA better approach would be to add a new migration that adds these columns.\r\nI found a tutorial ([ref](https://medium.com/@content_13642/a-guide-to-maintaining-sql-tables-using-alembic-python-10c633303569)) and this is an example we have in our code.\r\n\r\n",
    "line_number": 41,
    "enriched": "File: mindsdb/migrations/versions/2024-02-12_9461892bd889_llm_log.py\nCode: @@ -24,15 +24,21 @@ def upgrade():\n         sa.Column('id', sa.Integer(), nullable=False),\n         sa.Column('company_id', sa.Integer(), nullable=True),\n         sa.Column('api_key', sa.String(), nullable=True),\n-        sa.Column('model_id', sa.Integer(), nullable=False),\n+        sa.Column('model_id', sa.Integer(), nullable=True),\n+        sa.Column('model_group', sa.String(), nullable=True),\n         sa.Column('input', sa.String(), nullable=True),\n         sa.Column('output', sa.String(), nullable=True),\n         sa.Column('start_time', sa.DateTime(), nullable=False),\n         sa.Column('end_time', sa.DateTime(), nullable=True),\n+        sa.Column('cost', sa.Numeric(), nullable=True),\n         sa.Column('prompt_tokens', sa.Integer(), nullable=True),\n         sa.Column('completion_tokens', sa.Integer(), nullable=True),\n         sa.Column('total_tokens', sa.Integer(), nullable=True),\n-        sa.Column('success', sa.Boolean(), nullable=False),\n+        sa.Column('success', sa.Boolean(), nullable=False, default=True),\n+        sa.Column('exception', sa.String(), nullable=True),\n+        sa.Column('traceback', sa.String(), nullable=True),\n+        sa.Column('stream', sa.Boolean(), default=False),\n+        sa.Column('metadata', sa.JSON(), nullable=True),\nComment: I don't think this will work for existing MindsDB databases.  \r\n\r\nIIUC, all databases that have run the migration version `2024-02-12_9461892bd889_llm_log.py` will not add these changes, and it might even cause errors because this migration's checksum is different ([ref1](https://github.com/mindsdb/mindsdb/blob/main/mindsdb/migrations/versions/2024-10-07_6c57ed39a82b_added_webhook_token_to_chat_bots.py), [ref2](https://github.com/mindsdb/mindsdb/blob/main/mindsdb/migrations/versions/2024-07-19_45eb2eb61f70_add_provider_to_agent.py)).\r\n\r\nA better approach would be to add a new migration that adds these columns.\r\nI found a tutorial ([ref](https://medium.com/@content_13642/a-guide-to-maintaining-sql-tables-using-alembic-python-10c633303569)) and this is an example we have in our code.\r\n\r\n",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/migrations/versions/2024-02-12_9461892bd889_llm_log.py",
    "pr_number": 10171,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1844342612,
    "comment_created_at": "2024-11-15T19:12:51Z"
  },
  {
    "code": "@@ -37,6 +37,10 @@ def format_docs(docs):\n \n         prompt = ChatPromptTemplate.from_template(self.prompt_template)\n \n+        # Ensure all components are not None\n+        if None in [prompt, self.llm]:\n+            raise ValueError(\"One of the required components (prompt or llm) is None\")",
    "comment": "Would be better to do individual checks for `prompt` and `self.llm` IMO to get a more detailed error message",
    "line_number": 42,
    "enriched": "File: mindsdb/integrations/utilities/rag/pipelines/rag.py\nCode: @@ -37,6 +37,10 @@ def format_docs(docs):\n \n         prompt = ChatPromptTemplate.from_template(self.prompt_template)\n \n+        # Ensure all components are not None\n+        if None in [prompt, self.llm]:\n+            raise ValueError(\"One of the required components (prompt or llm) is None\")\nComment: Would be better to do individual checks for `prompt` and `self.llm` IMO to get a more detailed error message",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/utilities/rag/pipelines/rag.py",
    "pr_number": 9525,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1686857941,
    "comment_created_at": "2024-07-22T16:55:30Z"
  },
  {
    "code": "@@ -1153,3 +1153,236 @@ def update(self, name: str, project_id: int, **kwargs) -> db.KnowledgeBase:\n         Update a knowledge base record\n         \"\"\"\n         raise NotImplementedError()\n+\n+    def evaluate(\n+            self,\n+            name: str,\n+            project_id: int,\n+            test_queries: List[Dict],\n+            metrics: List[str] = None,\n+            top_k: int = 5,\n+            test_set_name: str = None,\n+            user_notes: str = None\n+    ) -> Dict:\n+        \"\"\"Evaluate a knowledge base against a set of test queries\n+\n+        Args:\n+            name: Knowledge base name\n+            project_id: Project ID\n+            test_queries: List of test queries with expected document IDs\n+                Format: [{\"query\": \"query text\", \"expected_ids\": [\"id1\", \"id2\"]}]\n+            metrics: List of metrics to calculate (default: [\"precision\", \"recall\", \"mrr\"])\n+            top_k: Number of results to consider for metrics calculation\n+            test_set_name: Optional name for the test set\n+            user_notes: Optional notes about the evaluation\n+\n+        Returns:\n+            Dictionary with evaluation results\n+        \"\"\"\n+        from datetime import datetime\n+\n+        if metrics is None:\n+            metrics = [\"precision\", \"recall\", \"mrr\"]\n+\n+        # Get knowledge base\n+        kb = self.get(name, project_id)\n+\n+        # Run queries and get results\n+        retrieved_results = []\n+        for query in test_queries:\n+            query_text = query.get(\"query\")\n+            results = self.query(name, project_id, query_text, limit=top_k)\n+            retrieved_results.append(results)\n+\n+        # Calculate metrics\n+        metrics_results = {}\n+        per_query_results = []\n+\n+        for i, query in enumerate(test_queries):\n+            expected_ids = set(query.get(\"expected_ids\", []))\n+            retrieved_ids = [doc.get(\"id\") for doc in retrieved_results[i]]\n+\n+            # Calculate metrics for this query\n+            query_metrics = self._calculate_metrics(retrieved_ids, list(expected_ids), metrics, top_k)\n+\n+            # Store per-query results\n+            per_query_results.append({\n+                \"query\": query[\"query\"],\n+                \"expected_ids\": list(expected_ids),\n+                \"retrieved_ids\": retrieved_ids[:top_k],\n+                \"metrics\": query_metrics\n+            })\n+\n+            # Accumulate metrics\n+            for metric, value in query_metrics.items():\n+                if metric not in metrics_results:\n+                    metrics_results[metric] = 0\n+                metrics_results[metric] += value\n+\n+        # Average the metrics\n+        num_queries = len(test_queries)\n+        if num_queries > 0:\n+            for metric in metrics_results:\n+                metrics_results[metric] = metrics_results[metric] / num_queries\n+\n+        # Add per-query results\n+        metrics_results[\"per_query\"] = per_query_results\n+\n+        # Store evaluation in database\n+        with self.session_transaction() as session:\n+            evaluation = db.KnowledgeBaseEvaluation(\n+                knowledge_base_id=kb.id,\n+                project_id=project_id,\n+                test_set_name=test_set_name,\n+                metrics=metrics_results,\n+                config={\n+                    \"top_k\": top_k,\n+                    \"metrics\": metrics,\n+                    \"num_queries\": len(test_queries)\n+                },\n+                user_notes=user_notes,\n+                created_at=datetime.now()\n+            )\n+\n+            session.add(evaluation)\n+            session.flush()\n+            evaluation_id = evaluation.id\n+            session.commit()\n+\n+        # Return evaluation results\n+        return {\n+            \"evaluation_id\": evaluation_id,\n+            \"knowledge_base_id\": kb.id,\n+            \"project_id\": project_id,\n+            \"test_set_name\": test_set_name,\n+            \"created_at\": evaluation.created_at,\n+            \"metrics\": metrics_results,\n+            \"config\": {\n+                \"top_k\": top_k,\n+                \"metrics\": metrics,\n+                \"num_queries\": len(test_queries)\n+            },\n+            \"user_notes\": user_notes\n+        }\n+\n+    def _calculate_metrics(self, result_ids: List[str], expected_ids: List[str],\n+                           metrics: List[str], top_k: int\n+                           ) -> Dict:\n+        \"\"\"Calculate evaluation metrics\n+\n+        Args:\n+            result_ids: List of result document IDs\n+            expected_ids: List of expected document IDs\n+            metrics: List of metrics to calculate\n+            top_k: Number of results to consider\n+\n+        Returns:\n+            Dictionary with calculated metrics\n+        \"\"\"\n+        metrics_results = {}\n+\n+        # Limit results to top_k\n+        result_ids = result_ids[:top_k]\n+\n+        # Convert expected_ids to set for faster lookups\n+        expected_ids_set = set(expected_ids)\n+\n+        # Calculate precision: fraction of retrieved documents that are relevant\n+        if \"precision\" in metrics:\n+            if result_ids:\n+                relevant_retrieved = len([doc_id for doc_id in result_ids if doc_id in expected_ids_set])\n+                metrics_results[\"precision\"] = relevant_retrieved / len(result_ids)\n+            else:\n+                metrics_results[\"precision\"] = 0.0\n+\n+        # Calculate recall: fraction of relevant documents that are retrieved\n+        if \"recall\" in metrics:\n+            if expected_ids:\n+                relevant_retrieved = len([doc_id for doc_id in result_ids if doc_id in expected_ids_set])\n+                metrics_results[\"recall\"] = relevant_retrieved / len(expected_ids)\n+            else:\n+                metrics_results[\"recall\"] = 0.0\n+",
    "comment": "@fshabashev  calculate metrics perhaps we can just replace with the calcs in your eval script",
    "line_number": 1305,
    "enriched": "File: mindsdb/interfaces/knowledge_base/controller.py\nCode: @@ -1153,3 +1153,236 @@ def update(self, name: str, project_id: int, **kwargs) -> db.KnowledgeBase:\n         Update a knowledge base record\n         \"\"\"\n         raise NotImplementedError()\n+\n+    def evaluate(\n+            self,\n+            name: str,\n+            project_id: int,\n+            test_queries: List[Dict],\n+            metrics: List[str] = None,\n+            top_k: int = 5,\n+            test_set_name: str = None,\n+            user_notes: str = None\n+    ) -> Dict:\n+        \"\"\"Evaluate a knowledge base against a set of test queries\n+\n+        Args:\n+            name: Knowledge base name\n+            project_id: Project ID\n+            test_queries: List of test queries with expected document IDs\n+                Format: [{\"query\": \"query text\", \"expected_ids\": [\"id1\", \"id2\"]}]\n+            metrics: List of metrics to calculate (default: [\"precision\", \"recall\", \"mrr\"])\n+            top_k: Number of results to consider for metrics calculation\n+            test_set_name: Optional name for the test set\n+            user_notes: Optional notes about the evaluation\n+\n+        Returns:\n+            Dictionary with evaluation results\n+        \"\"\"\n+        from datetime import datetime\n+\n+        if metrics is None:\n+            metrics = [\"precision\", \"recall\", \"mrr\"]\n+\n+        # Get knowledge base\n+        kb = self.get(name, project_id)\n+\n+        # Run queries and get results\n+        retrieved_results = []\n+        for query in test_queries:\n+            query_text = query.get(\"query\")\n+            results = self.query(name, project_id, query_text, limit=top_k)\n+            retrieved_results.append(results)\n+\n+        # Calculate metrics\n+        metrics_results = {}\n+        per_query_results = []\n+\n+        for i, query in enumerate(test_queries):\n+            expected_ids = set(query.get(\"expected_ids\", []))\n+            retrieved_ids = [doc.get(\"id\") for doc in retrieved_results[i]]\n+\n+            # Calculate metrics for this query\n+            query_metrics = self._calculate_metrics(retrieved_ids, list(expected_ids), metrics, top_k)\n+\n+            # Store per-query results\n+            per_query_results.append({\n+                \"query\": query[\"query\"],\n+                \"expected_ids\": list(expected_ids),\n+                \"retrieved_ids\": retrieved_ids[:top_k],\n+                \"metrics\": query_metrics\n+            })\n+\n+            # Accumulate metrics\n+            for metric, value in query_metrics.items():\n+                if metric not in metrics_results:\n+                    metrics_results[metric] = 0\n+                metrics_results[metric] += value\n+\n+        # Average the metrics\n+        num_queries = len(test_queries)\n+        if num_queries > 0:\n+            for metric in metrics_results:\n+                metrics_results[metric] = metrics_results[metric] / num_queries\n+\n+        # Add per-query results\n+        metrics_results[\"per_query\"] = per_query_results\n+\n+        # Store evaluation in database\n+        with self.session_transaction() as session:\n+            evaluation = db.KnowledgeBaseEvaluation(\n+                knowledge_base_id=kb.id,\n+                project_id=project_id,\n+                test_set_name=test_set_name,\n+                metrics=metrics_results,\n+                config={\n+                    \"top_k\": top_k,\n+                    \"metrics\": metrics,\n+                    \"num_queries\": len(test_queries)\n+                },\n+                user_notes=user_notes,\n+                created_at=datetime.now()\n+            )\n+\n+            session.add(evaluation)\n+            session.flush()\n+            evaluation_id = evaluation.id\n+            session.commit()\n+\n+        # Return evaluation results\n+        return {\n+            \"evaluation_id\": evaluation_id,\n+            \"knowledge_base_id\": kb.id,\n+            \"project_id\": project_id,\n+            \"test_set_name\": test_set_name,\n+            \"created_at\": evaluation.created_at,\n+            \"metrics\": metrics_results,\n+            \"config\": {\n+                \"top_k\": top_k,\n+                \"metrics\": metrics,\n+                \"num_queries\": len(test_queries)\n+            },\n+            \"user_notes\": user_notes\n+        }\n+\n+    def _calculate_metrics(self, result_ids: List[str], expected_ids: List[str],\n+                           metrics: List[str], top_k: int\n+                           ) -> Dict:\n+        \"\"\"Calculate evaluation metrics\n+\n+        Args:\n+            result_ids: List of result document IDs\n+            expected_ids: List of expected document IDs\n+            metrics: List of metrics to calculate\n+            top_k: Number of results to consider\n+\n+        Returns:\n+            Dictionary with calculated metrics\n+        \"\"\"\n+        metrics_results = {}\n+\n+        # Limit results to top_k\n+        result_ids = result_ids[:top_k]\n+\n+        # Convert expected_ids to set for faster lookups\n+        expected_ids_set = set(expected_ids)\n+\n+        # Calculate precision: fraction of retrieved documents that are relevant\n+        if \"precision\" in metrics:\n+            if result_ids:\n+                relevant_retrieved = len([doc_id for doc_id in result_ids if doc_id in expected_ids_set])\n+                metrics_results[\"precision\"] = relevant_retrieved / len(result_ids)\n+            else:\n+                metrics_results[\"precision\"] = 0.0\n+\n+        # Calculate recall: fraction of relevant documents that are retrieved\n+        if \"recall\" in metrics:\n+            if expected_ids:\n+                relevant_retrieved = len([doc_id for doc_id in result_ids if doc_id in expected_ids_set])\n+                metrics_results[\"recall\"] = relevant_retrieved / len(expected_ids)\n+            else:\n+                metrics_results[\"recall\"] = 0.0\n+\nComment: @fshabashev  calculate metrics perhaps we can just replace with the calcs in your eval script",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/interfaces/knowledge_base/controller.py",
    "pr_number": 10788,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2070260567,
    "comment_created_at": "2025-05-01T13:19:38Z"
  },
  {
    "code": "@@ -195,12 +212,45 @@ def query_df(df, query, session=None):\n         user_functions = None\n \n     def adapt_query(node, is_table, **kwargs):\n-        if is_table:\n-            return\n+        # if is_table:\n+        #     return\n         if isinstance(node, Identifier):\n-            if len(node.parts) > 1:\n-                node.parts = [node.parts[-1]]\n-                return node\n+            if is_table:\n+                table_name = node.parts[-1].lower()\n+                if table_name in dataframe_dict:\n+                    return node\n+                elif len(dataframe_dict) == 1 and \"df\" in dataframe_dict:\n+                    node.parts = [\"df\"]\n+                    return node\n+                else:\n+                    raise QueryError(\n+                        db_type=\"DuckDB\",\n+                        db_error_msg=(\n+                            f\"Table '{table_name}' not found in provided dataframes. Available tables: {list(dataframe_dict.keys())}\",\n+                        ),\n+                        failed_query=query_str,\n+                        is_external=False,\n+                        is_expected=False,\n+                    )\n+            else:\n+                if len(node.parts) > 1:\n+                    if len(node.parts) > 2:\n+                        node.parts = node.parts[-2:]\n+\n+                    if len(dataframe_dict) == 1 and \"df\" in dataframe_dict:\n+                        table_qualifier = node.parts[0].lower()\n+\n+                        if (\n+                            table_qualifier in dataframe_dict\n+                            or table_qualifier == from_table_name\n+                            or (from_alias is not None and table_qualifier == from_alias)\n+                        ):\n+                            pass\n+                        else:\n+                            node.parts = [\"df\", node.parts[-1]]\n+\n+                    return node\n+",
    "comment": "**correctness**: `adapt_query` does not return a value for non-`Identifier` nodes, causing the AST traversal to potentially replace nodes with `None` and break query rendering.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/api/executor/utilities/sql.py, lines 214-253, the function `adapt_query` does not return a value for non-Identifier and non-Function nodes, which can cause AST traversal to replace nodes with None and break query rendering. Add a `return node` at the end of the function to ensure all code paths return the node.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n    def adapt_query(node, is_table, **kwargs):\n        # if is_table:\n        #     return\n        if isinstance(node, Identifier):\n            ...\n            return node\n        if isinstance(node, Function):\n            fnc = mysql_to_duckdb_fnc(node)\n            if fnc is not None:\n                return fnc\n        return node\n\n```\n</details>\n<!-- suggestion_end -->\n",
    "line_number": 253,
    "enriched": "File: mindsdb/api/executor/utilities/sql.py\nCode: @@ -195,12 +212,45 @@ def query_df(df, query, session=None):\n         user_functions = None\n \n     def adapt_query(node, is_table, **kwargs):\n-        if is_table:\n-            return\n+        # if is_table:\n+        #     return\n         if isinstance(node, Identifier):\n-            if len(node.parts) > 1:\n-                node.parts = [node.parts[-1]]\n-                return node\n+            if is_table:\n+                table_name = node.parts[-1].lower()\n+                if table_name in dataframe_dict:\n+                    return node\n+                elif len(dataframe_dict) == 1 and \"df\" in dataframe_dict:\n+                    node.parts = [\"df\"]\n+                    return node\n+                else:\n+                    raise QueryError(\n+                        db_type=\"DuckDB\",\n+                        db_error_msg=(\n+                            f\"Table '{table_name}' not found in provided dataframes. Available tables: {list(dataframe_dict.keys())}\",\n+                        ),\n+                        failed_query=query_str,\n+                        is_external=False,\n+                        is_expected=False,\n+                    )\n+            else:\n+                if len(node.parts) > 1:\n+                    if len(node.parts) > 2:\n+                        node.parts = node.parts[-2:]\n+\n+                    if len(dataframe_dict) == 1 and \"df\" in dataframe_dict:\n+                        table_qualifier = node.parts[0].lower()\n+\n+                        if (\n+                            table_qualifier in dataframe_dict\n+                            or table_qualifier == from_table_name\n+                            or (from_alias is not None and table_qualifier == from_alias)\n+                        ):\n+                            pass\n+                        else:\n+                            node.parts = [\"df\", node.parts[-1]]\n+\n+                    return node\n+\nComment: **correctness**: `adapt_query` does not return a value for non-`Identifier` nodes, causing the AST traversal to potentially replace nodes with `None` and break query rendering.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/api/executor/utilities/sql.py, lines 214-253, the function `adapt_query` does not return a value for non-Identifier and non-Function nodes, which can cause AST traversal to replace nodes with None and break query rendering. Add a `return node` at the end of the function to ensure all code paths return the node.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n    def adapt_query(node, is_table, **kwargs):\n        # if is_table:\n        #     return\n        if isinstance(node, Identifier):\n            ...\n            return node\n        if isinstance(node, Function):\n            fnc = mysql_to_duckdb_fnc(node)\n            if fnc is not None:\n                return fnc\n        return node\n\n```\n</details>\n<!-- suggestion_end -->\n",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/api/executor/utilities/sql.py",
    "pr_number": 11768,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2443496540,
    "comment_created_at": "2025-10-19T21:15:19Z"
  },
  {
    "code": "@@ -207,3 +209,37 @@ def json_list(self):\n \n     def json_del(self, name):\n         ...\n+\n+    def export_files(self) -> bytes:\n+        folder_path = self.folder_get('', not_empty=True)\n+        if folder_path is None:\n+            return None\n+\n+        # parent_folder = os.path.dirname(folder_path)\n+\n+        zip_fd = io.BytesIO()\n+\n+        with zipfile.ZipFile(zip_fd, 'w', zipfile.ZIP_DEFLATED) as zipf:",
    "comment": "will be good to lock dir, because files may be deleted while archiving. But let ignore it for now",
    "line_number": 222,
    "enriched": "File: mindsdb/interfaces/storage/model_fs.py\nCode: @@ -207,3 +209,37 @@ def json_list(self):\n \n     def json_del(self, name):\n         ...\n+\n+    def export_files(self) -> bytes:\n+        folder_path = self.folder_get('', not_empty=True)\n+        if folder_path is None:\n+            return None\n+\n+        # parent_folder = os.path.dirname(folder_path)\n+\n+        zip_fd = io.BytesIO()\n+\n+        with zipfile.ZipFile(zip_fd, 'w', zipfile.ZIP_DEFLATED) as zipf:\nComment: will be good to lock dir, because files may be deleted while archiving. But let ignore it for now",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/interfaces/storage/model_fs.py",
    "pr_number": 7210,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1317060056,
    "comment_created_at": "2023-09-06T10:11:54Z"
  },
  {
    "code": "@@ -121,6 +122,31 @@ def test_anyscale_provider(self):\n         )\n         assert \"stockholm\" in result_df['answer'].iloc[0].lower()\n \n+    @pytest.mark.skipif(GOOGLE_API_KEY is None, reason='Missing Google API key (GOOGLE_API_KEY env variable)')",
    "comment": "Is our unit tests sending requests to the internet or it's mocked?\r\n\r\nIf it's mocked, I would like if we could provide `GOOGLE_API_KEY` in the test. ",
    "line_number": 125,
    "enriched": "File: tests/unit/ml_handlers/test_langchain.py\nCode: @@ -121,6 +122,31 @@ def test_anyscale_provider(self):\n         )\n         assert \"stockholm\" in result_df['answer'].iloc[0].lower()\n \n+    @pytest.mark.skipif(GOOGLE_API_KEY is None, reason='Missing Google API key (GOOGLE_API_KEY env variable)')\nComment: Is our unit tests sending requests to the internet or it's mocked?\r\n\r\nIf it's mocked, I would like if we could provide `GOOGLE_API_KEY` in the test. ",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "tests/unit/ml_handlers/test_langchain.py",
    "pr_number": 10684,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2035974838,
    "comment_created_at": "2025-04-09T19:03:54Z"
  },
  {
    "code": "@@ -1185,6 +1200,89 @@ def answer_drop_view(self, statement):\n \n         return ExecuteAnswer(answer_type=ANSWER_TYPE.OK)\n \n+    def answer_create_kb(self, statement: CreateKnowledgeBase):\n+        project_name = statement.name.parts[0] if len(statement.name.parts) > 1 else self.session.database\n+        # get project id\n+        try:",
    "comment": "maybe better to move some code from this method (checks for example) into controller.add ? ",
    "line_number": 1217,
    "enriched": "File: mindsdb/api/mysql/mysql_proxy/executor/executor_commands.py\nCode: @@ -1185,6 +1200,89 @@ def answer_drop_view(self, statement):\n \n         return ExecuteAnswer(answer_type=ANSWER_TYPE.OK)\n \n+    def answer_create_kb(self, statement: CreateKnowledgeBase):\n+        project_name = statement.name.parts[0] if len(statement.name.parts) > 1 else self.session.database\n+        # get project id\n+        try:\nComment: maybe better to move some code from this method (checks for example) into controller.add ? ",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/api/mysql/mysql_proxy/executor/executor_commands.py",
    "pr_number": 7332,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1327549043,
    "comment_created_at": "2023-09-15T16:41:04Z"
  },
  {
    "code": "@@ -0,0 +1,12 @@\n+from mindsdb.integrations.libs.const import HANDLER_TYPE\n+\n+from .eventstoredb_handler import EventStoreDB as Handler\n+from .__about__ import __version__ as version\n+\n+title = 'EventStoreDB'\n+name = 'eventstoredb'\n+type = HANDLER_TYPE.DATA\n+",
    "comment": "Can we please add the icon here so it will be imported into the MindsDB Sql editor?",
    "line_number": 9,
    "enriched": "File: mindsdb/integrations/handlers/eventstoredb_handler/__init__.py\nCode: @@ -0,0 +1,12 @@\n+from mindsdb.integrations.libs.const import HANDLER_TYPE\n+\n+from .eventstoredb_handler import EventStoreDB as Handler\n+from .__about__ import __version__ as version\n+\n+title = 'EventStoreDB'\n+name = 'eventstoredb'\n+type = HANDLER_TYPE.DATA\n+\nComment: Can we please add the icon here so it will be imported into the MindsDB Sql editor?",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/eventstoredb_handler/__init__.py",
    "pr_number": 5579,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1170208124,
    "comment_created_at": "2023-04-18T15:25:58Z"
  },
  {
    "code": "@@ -0,0 +1,77 @@\n+import os\n+\n+from opentelemetry import trace\n+from opentelemetry.sdk.resources import Resource\n+from opentelemetry.sdk.trace import TracerProvider, Span\n+from opentelemetry.sdk.trace.export import BatchSpanProcessor, ConsoleSpanExporter\n+from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter\n+\n+from mindsdb.utilities import log\n+logger = log.getLogger(__name__)\n+\n+\n+# Check OpenTelemetry exporter type\n+OTEL_EXPORTER_TYPE = os.getenv(\"OTEL_EXPORTER_TYPE\", \"console\")  # console or otlp\n+\n+# Define OTLP endpoint. If not set, the default OTLP endpoint will be used\n+OTEL_OTLP_ENDPOINT = os.getenv(\"OTEL_OTLP_ENDPOINT\", \"http://localhost:4317\")\n+\n+# Define service name\n+OTEL_SERVICE_NAME = os.getenv(\"OTEL_SERVICE_NAME\", \"mindsdb\")\n+\n+# The name of the environment we\"re on, by default local for development, this is set differently per-env in our Helm chart values files\n+OTEL_SERVICE_ENVIRONMENT = os.getenv(\"OTEL_SERVICE_ENVIRONMENT\", \"local\").lower()\n+\n+# Define service release\n+OTEL_SERVICE_RELEASE = os.getenv(\"OTEL_SERVICE_RELEASE\", \"local\").lower()\n+\n+# By default we have Open Telemetry SDK enabled on all envs, except for local which is disabled by default\n+#   If you want to enable Open Telemetry on local for some reason please set OTEL_SDK_FORCE_RUN to true\n+OTEL_SDK_DISABLED = os.getenv(\"OTEL_SDK_DISABLED\", \"false\").lower() == \"true\" or os.getenv(\"OTEL_SERVICE_ENVIRONMENT\", \"local\").lower() == \"local\"\n+OTEL_SDK_FORCE_RUN = os.getenv(\"OTEL_SDK_FORCE_RUN\", \"false\").lower() == \"true\"\n+\n+# Custom span processor to add global tags to spans\n+\n+\n+class GlobalTaggingSpanProcessor(BatchSpanProcessor):\n+    def on_start(self, span: Span, parent_context):\n+        # Add environment and release to every span\n+        span.set_attribute(\"environment\", OTEL_SERVICE_ENVIRONMENT)\n+        span.set_attribute(\"release\", OTEL_SERVICE_RELEASE)\n+        super().on_start(span, parent_context)\n+\n+\n+if not OTEL_SDK_DISABLED or OTEL_SDK_FORCE_RUN:\n+    logger.info(\"OpenTelemetry enabled\")\n+    logger.info(f\"OpenTelemetry exporter type: {OTEL_EXPORTER_TYPE}\")\n+    logger.info(f\"OpenTelemetry service name: {OTEL_SERVICE_NAME}\")\n+    logger.info(f\"OpenTelemetry service environment: {OTEL_SERVICE_ENVIRONMENT}\")\n+    logger.info(f\"OpenTelemetry service release: {OTEL_SERVICE_RELEASE}\")\n+\n+    # Define OpenTelemetry resources (e.g., service name)\n+    resource = Resource(attributes={\"service.name\": OTEL_SERVICE_NAME})\n+\n+    # Set the tracer provider with the custom resource\n+    trace.set_tracer_provider(TracerProvider(resource=resource))\n+\n+    # Configure the appropriate exporter based on the environment variable\n+    if OTEL_EXPORTER_TYPE == \"otlp\":\n+        logger.info(\"OpenTelemetry is using OTLP exporter\")\n+\n+        exporter = OTLPSpanExporter(\n+            endpoint=OTEL_OTLP_ENDPOINT,  # Default OTLP endpoint\n+            insecure=True  # Disable TLS for local testing\n+        )\n+        # span_processor = BatchSpanProcessor(exporter)\n+\n+    else:\n+        logger.info(\"OpenTelemetry is using Console exporter\")\n+\n+        exporter = ConsoleSpanExporter()\n+        # span_processor = SimpleSpanProcessor(exporter)\n+\n+    # Create a batch span processor\n+    span_processor = BatchSpanProcessor(GlobalTaggingSpanProcessor(exporter))",
    "comment": "Is this used anywhere? Should it be used below?",
    "line_number": 72,
    "enriched": "File: mindsdb/utilities/otel.py\nCode: @@ -0,0 +1,77 @@\n+import os\n+\n+from opentelemetry import trace\n+from opentelemetry.sdk.resources import Resource\n+from opentelemetry.sdk.trace import TracerProvider, Span\n+from opentelemetry.sdk.trace.export import BatchSpanProcessor, ConsoleSpanExporter\n+from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter\n+\n+from mindsdb.utilities import log\n+logger = log.getLogger(__name__)\n+\n+\n+# Check OpenTelemetry exporter type\n+OTEL_EXPORTER_TYPE = os.getenv(\"OTEL_EXPORTER_TYPE\", \"console\")  # console or otlp\n+\n+# Define OTLP endpoint. If not set, the default OTLP endpoint will be used\n+OTEL_OTLP_ENDPOINT = os.getenv(\"OTEL_OTLP_ENDPOINT\", \"http://localhost:4317\")\n+\n+# Define service name\n+OTEL_SERVICE_NAME = os.getenv(\"OTEL_SERVICE_NAME\", \"mindsdb\")\n+\n+# The name of the environment we\"re on, by default local for development, this is set differently per-env in our Helm chart values files\n+OTEL_SERVICE_ENVIRONMENT = os.getenv(\"OTEL_SERVICE_ENVIRONMENT\", \"local\").lower()\n+\n+# Define service release\n+OTEL_SERVICE_RELEASE = os.getenv(\"OTEL_SERVICE_RELEASE\", \"local\").lower()\n+\n+# By default we have Open Telemetry SDK enabled on all envs, except for local which is disabled by default\n+#   If you want to enable Open Telemetry on local for some reason please set OTEL_SDK_FORCE_RUN to true\n+OTEL_SDK_DISABLED = os.getenv(\"OTEL_SDK_DISABLED\", \"false\").lower() == \"true\" or os.getenv(\"OTEL_SERVICE_ENVIRONMENT\", \"local\").lower() == \"local\"\n+OTEL_SDK_FORCE_RUN = os.getenv(\"OTEL_SDK_FORCE_RUN\", \"false\").lower() == \"true\"\n+\n+# Custom span processor to add global tags to spans\n+\n+\n+class GlobalTaggingSpanProcessor(BatchSpanProcessor):\n+    def on_start(self, span: Span, parent_context):\n+        # Add environment and release to every span\n+        span.set_attribute(\"environment\", OTEL_SERVICE_ENVIRONMENT)\n+        span.set_attribute(\"release\", OTEL_SERVICE_RELEASE)\n+        super().on_start(span, parent_context)\n+\n+\n+if not OTEL_SDK_DISABLED or OTEL_SDK_FORCE_RUN:\n+    logger.info(\"OpenTelemetry enabled\")\n+    logger.info(f\"OpenTelemetry exporter type: {OTEL_EXPORTER_TYPE}\")\n+    logger.info(f\"OpenTelemetry service name: {OTEL_SERVICE_NAME}\")\n+    logger.info(f\"OpenTelemetry service environment: {OTEL_SERVICE_ENVIRONMENT}\")\n+    logger.info(f\"OpenTelemetry service release: {OTEL_SERVICE_RELEASE}\")\n+\n+    # Define OpenTelemetry resources (e.g., service name)\n+    resource = Resource(attributes={\"service.name\": OTEL_SERVICE_NAME})\n+\n+    # Set the tracer provider with the custom resource\n+    trace.set_tracer_provider(TracerProvider(resource=resource))\n+\n+    # Configure the appropriate exporter based on the environment variable\n+    if OTEL_EXPORTER_TYPE == \"otlp\":\n+        logger.info(\"OpenTelemetry is using OTLP exporter\")\n+\n+        exporter = OTLPSpanExporter(\n+            endpoint=OTEL_OTLP_ENDPOINT,  # Default OTLP endpoint\n+            insecure=True  # Disable TLS for local testing\n+        )\n+        # span_processor = BatchSpanProcessor(exporter)\n+\n+    else:\n+        logger.info(\"OpenTelemetry is using Console exporter\")\n+\n+        exporter = ConsoleSpanExporter()\n+        # span_processor = SimpleSpanProcessor(exporter)\n+\n+    # Create a batch span processor\n+    span_processor = BatchSpanProcessor(GlobalTaggingSpanProcessor(exporter))\nComment: Is this used anywhere? Should it be used below?",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/utilities/otel.py",
    "pr_number": 9976,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1813527872,
    "comment_created_at": "2024-10-23T20:43:37Z"
  },
  {
    "code": "@@ -119,8 +109,12 @@ def get(self, name):\n         if name_lower == \"log\":\n             return self.database_controller.get_system_db(\"log\")\n \n-        if name_lower in self.persis_datanodes:\n-            return self.persis_datanodes[name_lower]\n+        if name_lower == \"files\":\n+            return IntegrationDataNode(",
    "comment": "can it lead to slowing down when files datanode is requested several times? ",
    "line_number": 113,
    "enriched": "File: mindsdb/api/executor/datahub/datanodes/information_schema_datanode.py\nCode: @@ -119,8 +109,12 @@ def get(self, name):\n         if name_lower == \"log\":\n             return self.database_controller.get_system_db(\"log\")\n \n-        if name_lower in self.persis_datanodes:\n-            return self.persis_datanodes[name_lower]\n+        if name_lower == \"files\":\n+            return IntegrationDataNode(\nComment: can it lead to slowing down when files datanode is requested several times? ",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/api/executor/datahub/datanodes/information_schema_datanode.py",
    "pr_number": 11710,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2425511601,
    "comment_created_at": "2025-10-13T08:09:26Z"
  },
  {
    "code": "@@ -90,6 +100,7 @@ def get_requirements_from_file(path):\n     \"google-auth\": [\"google\"],\n     \"google-cloud-storage\": [\"google\"],\n     \"protobuf\": [\"google\"],",
    "comment": "You can remove this line too since it looks like we don't import protobuf anywhere now.\r\n",
    "line_number": 102,
    "enriched": "File: tests/scripts/check_requirements.py\nCode: @@ -90,6 +100,7 @@ def get_requirements_from_file(path):\n     \"google-auth\": [\"google\"],\n     \"google-cloud-storage\": [\"google\"],\n     \"protobuf\": [\"google\"],\nComment: You can remove this line too since it looks like we don't import protobuf anywhere now.\r\n",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "tests/scripts/check_requirements.py",
    "pr_number": 10754,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2062721772,
    "comment_created_at": "2025-04-27T21:04:41Z"
  },
  {
    "code": "@@ -37,12 +37,14 @@ protobuf==3.20.3\n hierarchicalforecast~=0.4.0\n google-auth-oauthlib\n msal\n-langchain==0.1.11\n-langchain-core==0.1.46\n-langchain-community==0.0.27\n-langchain-openai==0.1.6\n-langchain-text-splitters==0.0.1\n-langchain-nvidia-ai-endpoints==0.1.7\n+langchain>=0.3,<0.4",
    "comment": "We should pin these to exact versions instead. (ref: https://linear.app/mindsdb/issue/ML-195/update-langchain-from-v0111-v035)",
    "line_number": 40,
    "enriched": "File: requirements/requirements.txt\nCode: @@ -37,12 +37,14 @@ protobuf==3.20.3\n hierarchicalforecast~=0.4.0\n google-auth-oauthlib\n msal\n-langchain==0.1.11\n-langchain-core==0.1.46\n-langchain-community==0.0.27\n-langchain-openai==0.1.6\n-langchain-text-splitters==0.0.1\n-langchain-nvidia-ai-endpoints==0.1.7\n+langchain>=0.3,<0.4\nComment: We should pin these to exact versions instead. (ref: https://linear.app/mindsdb/issue/ML-195/update-langchain-from-v0111-v035)",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "requirements/requirements.txt",
    "pr_number": 10140,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1835092778,
    "comment_created_at": "2024-11-08T21:50:33Z"
  },
  {
    "code": "@@ -1 +1 @@\n-oracledb==2.4.1\n+oracledb==3.3.0",
    "comment": "Not urgent, but let's try the latest 3.4.0",
    "line_number": 1,
    "enriched": "File: mindsdb/integrations/handlers/oracle_handler/requirements.txt\nCode: @@ -1 +1 @@\n-oracledb==2.4.1\n+oracledb==3.3.0\nComment: Not urgent, but let's try the latest 3.4.0",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/oracle_handler/requirements.txt",
    "pr_number": 11544,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2411028677,
    "comment_created_at": "2025-10-07T15:23:27Z"
  },
  {
    "code": "@@ -272,6 +272,9 @@ def post(self, project_name, agent_name):\n                 f'Project with name {project_name} does not exist'\n             )\n \n+        if os.getenv('OPENAI_API_KEY') is not None:\n+            existing_agent.params['openai_api_key'] = os.getenv('OPENAI_API_KEY')",
    "comment": "If the user already has their own `openai_api_key` set on an agent they create, this would overwrite it with our API key instead. I suggest:\r\n\r\n```python\r\nexisting_agent.params['openai_api_key'] = existing_agent.params.get('openai_api_key', os.getenv('OPENAI_API_KEY'))\r\n```",
    "line_number": 276,
    "enriched": "File: mindsdb/api/http/namespaces/agents.py\nCode: @@ -272,6 +272,9 @@ def post(self, project_name, agent_name):\n                 f'Project with name {project_name} does not exist'\n             )\n \n+        if os.getenv('OPENAI_API_KEY') is not None:\n+            existing_agent.params['openai_api_key'] = os.getenv('OPENAI_API_KEY')\nComment: If the user already has their own `openai_api_key` set on an agent they create, this would overwrite it with our API key instead. I suggest:\r\n\r\n```python\r\nexisting_agent.params['openai_api_key'] = existing_agent.params.get('openai_api_key', os.getenv('OPENAI_API_KEY'))\r\n```",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/api/http/namespaces/agents.py",
    "pr_number": 9329,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1635187411,
    "comment_created_at": "2024-06-11T16:38:52Z"
  },
  {
    "code": "@@ -0,0 +1,19 @@\n+from mindsdb.integrations.libs.const import HANDLER_TYPE\n+\n+from .__about__ import __version__ as version, __description__ as description\n+try:\n+    from monkeylearn_handler import monkeylearnHandler as Handler\n+    import_error = None\n+except Exception as e:\n+    Handler = None\n+    import_error = e\n+\n+title = 'MonkeyLearn'\n+name = 'monkeylearn'\n+type = HANDLER_TYPE.ML\n+permanent = True\n+execution_method = 'subprocess_keep'",
    "comment": "This handler uses API and doesn't perform prediction on client side, right?\r\nInt that case you can remove this line (execution_method=) and it will work in the same process and will do it faster",
    "line_number": 15,
    "enriched": "File: mindsdb/integrations/handlers/monkeylearn_handler/__init__.py\nCode: @@ -0,0 +1,19 @@\n+from mindsdb.integrations.libs.const import HANDLER_TYPE\n+\n+from .__about__ import __version__ as version, __description__ as description\n+try:\n+    from monkeylearn_handler import monkeylearnHandler as Handler\n+    import_error = None\n+except Exception as e:\n+    Handler = None\n+    import_error = e\n+\n+title = 'MonkeyLearn'\n+name = 'monkeylearn'\n+type = HANDLER_TYPE.ML\n+permanent = True\n+execution_method = 'subprocess_keep'\nComment: This handler uses API and doesn't perform prediction on client side, right?\r\nInt that case you can remove this line (execution_method=) and it will work in the same process and will do it faster",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/monkeylearn_handler/__init__.py",
    "pr_number": 5543,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1162435281,
    "comment_created_at": "2023-04-11T07:55:07Z"
  },
  {
    "code": "@@ -1,120 +1,60 @@\n-# Litellm Handler\n+## LlamaIndex Handler",
    "comment": "This should be LiteLLM.",
    "line_number": 1,
    "enriched": "File: mindsdb/integrations/handlers/litellm_handler/README.md\nCode: @@ -1,120 +1,60 @@\n-# Litellm Handler\n+## LlamaIndex Handler\nComment: This should be LiteLLM.",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/litellm_handler/README.md",
    "pr_number": 9473,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1675233714,
    "comment_created_at": "2024-07-12T04:05:32Z"
  },
  {
    "code": "@@ -245,76 +239,75 @@ def list(\n             op = condition.op\n \n             # Handle the column 'channel_id'.\n-            if condition.column == 'channel_id':\n+            if condition.column == \"channel_id\":\n                 if op != FilterOperator.EQUAL:\n                     raise ValueError(f\"Unsupported operator '{op}' for column 'channel_id'\")\n \n                 # Check if the provided channel exists.\n                 try:\n                     channel = SlackConversationsTable(self.handler).get_channel(value)\n-                    params['channel'] = value\n+                    params[\"channel\"] = value\n                     condition.applied = True\n                 except SlackApiError:\n                     raise ValueError(f\"Channel '{value}' not found\")\n \n             # Handle the column 'created_at'.\n-            elif condition.column == 'created_at' and value is not None:\n+            elif condition.column == \"created_at\" and value is not None:\n                 date = dt.datetime.fromisoformat(value).replace(tzinfo=dt.timezone.utc)\n                 if op == FilterOperator.GREATER_THAN:\n-                    params['oldest'] = date.timestamp() + 1\n+                    params[\"oldest\"] = date.timestamp() + 1\n                 elif op == FilterOperator.GREATER_THAN_OR_EQUAL:\n-                    params['oldest'] = date.timestamp()\n+                    params[\"oldest\"] = date.timestamp()\n                 elif op == FilterOperator.LESS_THAN_OR_EQUAL:\n-                    params['latest'] = date.timestamp()\n+                    params[\"latest\"] = date.timestamp()\n                 else:\n                     continue\n                 condition.applied = True\n \n-        if 'channel' not in params:\n+        if \"channel\" not in params:\n             raise ValueError(\"To retrieve data from Slack, you need to provide the 'channel_id' parameter.\")\n \n         # Retrieve the messages from the Slack API.\n         try:\n             # If the limit is greater than 999, paginate the results until the limit is reached.\n             if limit and limit > 999:\n+                params[\"limit\"] = 999\n                 response = client.conversations_history(**params)\n-                messages = response['messages']\n-\n-                # Paginate the results until the limit is reached.\n-                while response['response_metadata']['next_cursor']:\n-                    response = client.conversations_history(cursor=response['response_metadata']['next_cursor'])\n-                    messages.extend(response['messages'])\n+                messages = response[\"messages\"]\n+\n+                # Paginate the results until the limit is reached. response_metadata may be None.\n+                while response.get(\"response_metadata\", {}).get(\"next_cursor\"):\n+                    response = client.conversations_history(\n+                        cursor=response[\"response_metadata\"][\"next_cursor\"], **params\n+                    )\n+                    messages.extend(response[\"messages\"])",
    "comment": "**correctness**: `conversations_history` pagination does not update the `params` dict with the new cursor, causing repeated retrieval of the same page and potential infinite loop.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/integrations/handlers/slack_handler/slack_tables.py, lines 278-283, the pagination loop for `conversations_history` does not update the `params` dict with the new cursor, causing repeated retrieval of the same page and possible infinite loop. Update the code so that `params[\"cursor\"]` is set to the new cursor value before each API call, and remove the explicit `cursor=` argument from the client call. The fix should look like:\n\nwhile response.get(\"response_metadata\", {}).get(\"next_cursor\"):\n    params[\"cursor\"] = response[\"response_metadata\"][\"next_cursor\"]\n    response = client.conversations_history(**params)\n    messages.extend(response[\"messages\"])\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n                while response.get(\"response_metadata\", {}).get(\"next_cursor\"):\n                    params[\"cursor\"] = response[\"response_metadata\"][\"next_cursor\"]\n                    response = client.conversations_history(**params)\n                    messages.extend(response[\"messages\"])\n\n```\n</details>\n<!-- suggestion_end -->\n",
    "line_number": 283,
    "enriched": "File: mindsdb/integrations/handlers/slack_handler/slack_tables.py\nCode: @@ -245,76 +239,75 @@ def list(\n             op = condition.op\n \n             # Handle the column 'channel_id'.\n-            if condition.column == 'channel_id':\n+            if condition.column == \"channel_id\":\n                 if op != FilterOperator.EQUAL:\n                     raise ValueError(f\"Unsupported operator '{op}' for column 'channel_id'\")\n \n                 # Check if the provided channel exists.\n                 try:\n                     channel = SlackConversationsTable(self.handler).get_channel(value)\n-                    params['channel'] = value\n+                    params[\"channel\"] = value\n                     condition.applied = True\n                 except SlackApiError:\n                     raise ValueError(f\"Channel '{value}' not found\")\n \n             # Handle the column 'created_at'.\n-            elif condition.column == 'created_at' and value is not None:\n+            elif condition.column == \"created_at\" and value is not None:\n                 date = dt.datetime.fromisoformat(value).replace(tzinfo=dt.timezone.utc)\n                 if op == FilterOperator.GREATER_THAN:\n-                    params['oldest'] = date.timestamp() + 1\n+                    params[\"oldest\"] = date.timestamp() + 1\n                 elif op == FilterOperator.GREATER_THAN_OR_EQUAL:\n-                    params['oldest'] = date.timestamp()\n+                    params[\"oldest\"] = date.timestamp()\n                 elif op == FilterOperator.LESS_THAN_OR_EQUAL:\n-                    params['latest'] = date.timestamp()\n+                    params[\"latest\"] = date.timestamp()\n                 else:\n                     continue\n                 condition.applied = True\n \n-        if 'channel' not in params:\n+        if \"channel\" not in params:\n             raise ValueError(\"To retrieve data from Slack, you need to provide the 'channel_id' parameter.\")\n \n         # Retrieve the messages from the Slack API.\n         try:\n             # If the limit is greater than 999, paginate the results until the limit is reached.\n             if limit and limit > 999:\n+                params[\"limit\"] = 999\n                 response = client.conversations_history(**params)\n-                messages = response['messages']\n-\n-                # Paginate the results until the limit is reached.\n-                while response['response_metadata']['next_cursor']:\n-                    response = client.conversations_history(cursor=response['response_metadata']['next_cursor'])\n-                    messages.extend(response['messages'])\n+                messages = response[\"messages\"]\n+\n+                # Paginate the results until the limit is reached. response_metadata may be None.\n+                while response.get(\"response_metadata\", {}).get(\"next_cursor\"):\n+                    response = client.conversations_history(\n+                        cursor=response[\"response_metadata\"][\"next_cursor\"], **params\n+                    )\n+                    messages.extend(response[\"messages\"])\nComment: **correctness**: `conversations_history` pagination does not update the `params` dict with the new cursor, causing repeated retrieval of the same page and potential infinite loop.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/integrations/handlers/slack_handler/slack_tables.py, lines 278-283, the pagination loop for `conversations_history` does not update the `params` dict with the new cursor, causing repeated retrieval of the same page and possible infinite loop. Update the code so that `params[\"cursor\"]` is set to the new cursor value before each API call, and remove the explicit `cursor=` argument from the client call. The fix should look like:\n\nwhile response.get(\"response_metadata\", {}).get(\"next_cursor\"):\n    params[\"cursor\"] = response[\"response_metadata\"][\"next_cursor\"]\n    response = client.conversations_history(**params)\n    messages.extend(response[\"messages\"])\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n                while response.get(\"response_metadata\", {}).get(\"next_cursor\"):\n                    params[\"cursor\"] = response[\"response_metadata\"][\"next_cursor\"]\n                    response = client.conversations_history(**params)\n                    messages.extend(response[\"messages\"])\n\n```\n</details>\n<!-- suggestion_end -->\n",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/slack_handler/slack_tables.py",
    "pr_number": 10895,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2205755590,
    "comment_created_at": "2025-07-14T20:31:56Z"
  },
  {
    "code": "@@ -5,17 +5,14 @@\n \n <div align=\"center\">\n \n-\n-\n <p>\n-\t<a href=\"https://github.com/mindsdb/mindsdb/actions\"><img src=\"https://github.com/mindsdb/mindsdb/actions/workflows/release.yml/badge.svg\" alt=\"MindsDB Release\"></a>\n+\t<a href=\"https://github.com/mindsdb/mindsdb/releases\"><alt=\"MindsDB Release\"></a>",
    "comment": "This will not work since we need an image to be displayd",
    "line_number": 9,
    "enriched": "File: README.md\nCode: @@ -5,17 +5,14 @@\n \n <div align=\"center\">\n \n-\n-\n <p>\n-\t<a href=\"https://github.com/mindsdb/mindsdb/actions\"><img src=\"https://github.com/mindsdb/mindsdb/actions/workflows/release.yml/badge.svg\" alt=\"MindsDB Release\"></a>\n+\t<a href=\"https://github.com/mindsdb/mindsdb/releases\"><alt=\"MindsDB Release\"></a>\nComment: This will not work since we need an image to be displayd",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "README.md",
    "pr_number": 9055,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1560847708,
    "comment_created_at": "2024-04-11T11:21:14Z"
  },
  {
    "code": "@@ -0,0 +1,363 @@\n+import pandas as pd\n+from typing import List, Optional, Union\n+\n+from mindsdb.integrations.libs.api_handler import MetaAPIHandler\n+from mindsdb.integrations.libs.base import MetaDatabaseHandler\n+from mindsdb.integrations.libs.response import RESPONSE_TYPE\n+from mindsdb.utilities import log\n+\n+\n+logger = log.getLogger(\"mindsdb\")\n+\n+\n+class DataCatalogRetriever:\n+    \"\"\"\n+    This class is responsible for retrieving (data catalog) metadata directly from the data source via the handler.\n+    \"\"\"\n+\n+    def __init__(self, database_name: str, table_names: Optional[List[str]] = None) -> None:\n+        \"\"\"\n+        Initialize the DataCatalogRetriever.\n+\n+        Args:\n+            database_name (str): The data source to retrieve metadata from.\n+            table_names (Optional[List[str]]): The list of table names to retrieve metadata for. If None, all tables will be read.\n+        \"\"\"\n+        from mindsdb.api.executor.controllers.session_controller import (\n+            SessionController,\n+        )\n+\n+        session = SessionController()\n+\n+        self.database_name = database_name\n+        self.data_handler: Union[MetaDatabaseHandler, MetaAPIHandler] = session.integration_controller.get_data_handler(\n+            database_name\n+        )\n+        integration = session.integration_controller.get(database_name)\n+        self.integration_id = integration[\"id\"]\n+        self.integration_engine = integration[\"engine\"]\n+        # TODO: Handle situations where a schema is provided along with the database name, e.g., 'schema.table'.\n+        # TODO: Handle situations where a file path is provided with integrations like S3, e.g., 'dir/file.csv'.\n+        self.table_names = table_names\n+\n+        self.logger = logger\n+\n+    def retrieve_metadata_as_string(self) -> str:\n+        \"\"\"\n+        Retrieve the metadata as a formatted string.\n+        \"\"\"\n+        tables_df = self.retrieve_tables()\n+        if tables_df.empty:\n+            return f\"No metadata found for database '{self.database_name}'\"\n+\n+        metadata_str = \"Data Catalog: \\n\"\n+        handler_info = self.retrieve_handler_info()\n+        if handler_info:\n+            metadata_str += handler_info + \"\\n\\n\"\n+\n+        columns_df = self.retrieve_columns()\n+        column_stats_df = self.retrieve_column_statistics()\n+        primary_keys_df = self.retrieve_primary_keys()\n+        foreign_keys_df = self.retrieve_foreign_keys()\n+\n+        metadata_str += self._construct_metadata_string_for_tables(\n+            tables_df,\n+            columns_df,\n+            column_stats_df,\n+            primary_keys_df,\n+            foreign_keys_df,\n+        )\n+        return metadata_str\n+\n+    def _construct_metadata_string_for_tables(\n+        self,\n+        tables_df: pd.DataFrame,\n+        columns_df: pd.DataFrame,\n+        column_stats_df: pd.DataFrame,\n+        primary_keys_df: pd.DataFrame,\n+        foreign_keys_df: pd.DataFrame,\n+    ) -> str:\n+        \"\"\"\n+        Construct a formatted string representation of the metadata for the given tables.\n+        \"\"\"\n+        tables_metadata_str = \"\"\n+\n+        # Convert all DataFrame column names to uppercase for consistency.\n+        tables_df.columns = tables_df.columns.str.upper()\n+        columns_df.columns = columns_df.columns.str.upper()\n+        column_stats_df.columns = column_stats_df.columns.str.upper()\n+        primary_keys_df.columns = primary_keys_df.columns.str.upper()\n+        foreign_keys_df.columns = foreign_keys_df.columns.str.upper()\n+\n+        for _, table_row in tables_df.iterrows():\n+            table_columns_df = columns_df[columns_df[\"TABLE_NAME\"] == table_row[\"TABLE_NAME\"]]\n+            # If no columns are found for the table,\n+            # looking for column stats, primary keys, and foreign keys is redundant.\n+            if not table_columns_df.empty:\n+                if not column_stats_df.empty:\n+                    table_column_stats_df = column_stats_df[column_stats_df[\"TABLE_NAME\"] == table_row[\"TABLE_NAME\"]]\n+                else:\n+                    table_column_stats_df = pd.DataFrame()\n+                if not primary_keys_df.empty:\n+                    table_primary_keys_df = primary_keys_df[primary_keys_df[\"TABLE_NAME\"] == table_row[\"TABLE_NAME\"]]\n+                else:\n+                    table_primary_keys_df = pd.DataFrame()\n+                if not foreign_keys_df.empty:\n+                    table_foreign_keys_df = foreign_keys_df[foreign_keys_df[\"TABLE_NAME\"] == table_row[\"TABLE_NAME\"]]\n+                else:\n+                    table_foreign_keys_df = pd.DataFrame()\n+",
    "comment": "**performance**: `tables_df.iterrows()` and repeated DataFrame filtering inside the loop (`columns_df[columns_df[...]]`) cause O(n*m) performance for large catalogs; this can severely degrade performance as the number of tables and columns grows.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nOptimize the loop in mindsdb/interfaces/data_catalog/data_catalog_retriever.py lines 92-109. The current code repeatedly filters large DataFrames inside a for-loop, causing O(n*m) performance for large catalogs. Refactor by pre-grouping columns_df, column_stats_df, primary_keys_df, and foreign_keys_df by TABLE_NAME using groupby, and then use group lookups inside the loop. This will reduce repeated filtering and improve scalability.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n        columns_by_table = columns_df.groupby(\"TABLE_NAME\") if not columns_df.empty else {}\n        column_stats_by_table = column_stats_df.groupby(\"TABLE_NAME\") if not column_stats_df.empty else {}\n        primary_keys_by_table = primary_keys_df.groupby(\"TABLE_NAME\") if not primary_keys_df.empty else {}\n        foreign_keys_by_table = foreign_keys_df.groupby(\"TABLE_NAME\") if not foreign_keys_df.empty else {}\n        for _, table_row in tables_df.iterrows():\n            table_name = table_row[\"TABLE_NAME\"]\n            table_columns_df = columns_by_table.get_group(table_name) if table_name in columns_by_table.groups else pd.DataFrame()\n            if not table_columns_df.empty:\n                table_column_stats_df = column_stats_by_table.get_group(table_name) if table_name in column_stats_by_table.groups else pd.DataFrame()\n                table_primary_keys_df = primary_keys_by_table.get_group(table_name) if table_name in primary_keys_by_table.groups else pd.DataFrame()\n                table_foreign_keys_df = foreign_keys_by_table.get_group(table_name) if table_name in foreign_keys_by_table.groups else pd.DataFrame()\n\n```\n</details>\n<!-- suggestion_end -->\n",
    "line_number": 109,
    "enriched": "File: mindsdb/interfaces/data_catalog/data_catalog_retriever.py\nCode: @@ -0,0 +1,363 @@\n+import pandas as pd\n+from typing import List, Optional, Union\n+\n+from mindsdb.integrations.libs.api_handler import MetaAPIHandler\n+from mindsdb.integrations.libs.base import MetaDatabaseHandler\n+from mindsdb.integrations.libs.response import RESPONSE_TYPE\n+from mindsdb.utilities import log\n+\n+\n+logger = log.getLogger(\"mindsdb\")\n+\n+\n+class DataCatalogRetriever:\n+    \"\"\"\n+    This class is responsible for retrieving (data catalog) metadata directly from the data source via the handler.\n+    \"\"\"\n+\n+    def __init__(self, database_name: str, table_names: Optional[List[str]] = None) -> None:\n+        \"\"\"\n+        Initialize the DataCatalogRetriever.\n+\n+        Args:\n+            database_name (str): The data source to retrieve metadata from.\n+            table_names (Optional[List[str]]): The list of table names to retrieve metadata for. If None, all tables will be read.\n+        \"\"\"\n+        from mindsdb.api.executor.controllers.session_controller import (\n+            SessionController,\n+        )\n+\n+        session = SessionController()\n+\n+        self.database_name = database_name\n+        self.data_handler: Union[MetaDatabaseHandler, MetaAPIHandler] = session.integration_controller.get_data_handler(\n+            database_name\n+        )\n+        integration = session.integration_controller.get(database_name)\n+        self.integration_id = integration[\"id\"]\n+        self.integration_engine = integration[\"engine\"]\n+        # TODO: Handle situations where a schema is provided along with the database name, e.g., 'schema.table'.\n+        # TODO: Handle situations where a file path is provided with integrations like S3, e.g., 'dir/file.csv'.\n+        self.table_names = table_names\n+\n+        self.logger = logger\n+\n+    def retrieve_metadata_as_string(self) -> str:\n+        \"\"\"\n+        Retrieve the metadata as a formatted string.\n+        \"\"\"\n+        tables_df = self.retrieve_tables()\n+        if tables_df.empty:\n+            return f\"No metadata found for database '{self.database_name}'\"\n+\n+        metadata_str = \"Data Catalog: \\n\"\n+        handler_info = self.retrieve_handler_info()\n+        if handler_info:\n+            metadata_str += handler_info + \"\\n\\n\"\n+\n+        columns_df = self.retrieve_columns()\n+        column_stats_df = self.retrieve_column_statistics()\n+        primary_keys_df = self.retrieve_primary_keys()\n+        foreign_keys_df = self.retrieve_foreign_keys()\n+\n+        metadata_str += self._construct_metadata_string_for_tables(\n+            tables_df,\n+            columns_df,\n+            column_stats_df,\n+            primary_keys_df,\n+            foreign_keys_df,\n+        )\n+        return metadata_str\n+\n+    def _construct_metadata_string_for_tables(\n+        self,\n+        tables_df: pd.DataFrame,\n+        columns_df: pd.DataFrame,\n+        column_stats_df: pd.DataFrame,\n+        primary_keys_df: pd.DataFrame,\n+        foreign_keys_df: pd.DataFrame,\n+    ) -> str:\n+        \"\"\"\n+        Construct a formatted string representation of the metadata for the given tables.\n+        \"\"\"\n+        tables_metadata_str = \"\"\n+\n+        # Convert all DataFrame column names to uppercase for consistency.\n+        tables_df.columns = tables_df.columns.str.upper()\n+        columns_df.columns = columns_df.columns.str.upper()\n+        column_stats_df.columns = column_stats_df.columns.str.upper()\n+        primary_keys_df.columns = primary_keys_df.columns.str.upper()\n+        foreign_keys_df.columns = foreign_keys_df.columns.str.upper()\n+\n+        for _, table_row in tables_df.iterrows():\n+            table_columns_df = columns_df[columns_df[\"TABLE_NAME\"] == table_row[\"TABLE_NAME\"]]\n+            # If no columns are found for the table,\n+            # looking for column stats, primary keys, and foreign keys is redundant.\n+            if not table_columns_df.empty:\n+                if not column_stats_df.empty:\n+                    table_column_stats_df = column_stats_df[column_stats_df[\"TABLE_NAME\"] == table_row[\"TABLE_NAME\"]]\n+                else:\n+                    table_column_stats_df = pd.DataFrame()\n+                if not primary_keys_df.empty:\n+                    table_primary_keys_df = primary_keys_df[primary_keys_df[\"TABLE_NAME\"] == table_row[\"TABLE_NAME\"]]\n+                else:\n+                    table_primary_keys_df = pd.DataFrame()\n+                if not foreign_keys_df.empty:\n+                    table_foreign_keys_df = foreign_keys_df[foreign_keys_df[\"TABLE_NAME\"] == table_row[\"TABLE_NAME\"]]\n+                else:\n+                    table_foreign_keys_df = pd.DataFrame()\n+\nComment: **performance**: `tables_df.iterrows()` and repeated DataFrame filtering inside the loop (`columns_df[columns_df[...]]`) cause O(n*m) performance for large catalogs; this can severely degrade performance as the number of tables and columns grows.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nOptimize the loop in mindsdb/interfaces/data_catalog/data_catalog_retriever.py lines 92-109. The current code repeatedly filters large DataFrames inside a for-loop, causing O(n*m) performance for large catalogs. Refactor by pre-grouping columns_df, column_stats_df, primary_keys_df, and foreign_keys_df by TABLE_NAME using groupby, and then use group lookups inside the loop. This will reduce repeated filtering and improve scalability.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n        columns_by_table = columns_df.groupby(\"TABLE_NAME\") if not columns_df.empty else {}\n        column_stats_by_table = column_stats_df.groupby(\"TABLE_NAME\") if not column_stats_df.empty else {}\n        primary_keys_by_table = primary_keys_df.groupby(\"TABLE_NAME\") if not primary_keys_df.empty else {}\n        foreign_keys_by_table = foreign_keys_df.groupby(\"TABLE_NAME\") if not foreign_keys_df.empty else {}\n        for _, table_row in tables_df.iterrows():\n            table_name = table_row[\"TABLE_NAME\"]\n            table_columns_df = columns_by_table.get_group(table_name) if table_name in columns_by_table.groups else pd.DataFrame()\n            if not table_columns_df.empty:\n                table_column_stats_df = column_stats_by_table.get_group(table_name) if table_name in column_stats_by_table.groups else pd.DataFrame()\n                table_primary_keys_df = primary_keys_by_table.get_group(table_name) if table_name in primary_keys_by_table.groups else pd.DataFrame()\n                table_foreign_keys_df = foreign_keys_by_table.get_group(table_name) if table_name in foreign_keys_by_table.groups else pd.DataFrame()\n\n```\n</details>\n<!-- suggestion_end -->\n",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/interfaces/data_catalog/data_catalog_retriever.py",
    "pr_number": 11754,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2465668445,
    "comment_created_at": "2025-10-27T13:26:13Z"
  },
  {
    "code": "@@ -0,0 +1,155 @@\n+\"\"\"FastAPI application factory and startup hooks.\"\"\"\n+\n+from __future__ import annotations\n+\n+import os\n+import mindsdb_sdk\n+from fastapi import FastAPI\n+from fastapi.middleware.cors import CORSMiddleware\n+from dotenv import load_dotenv\n+\n+from . import services\n+from .api import router as api_router\n+from .db import init_postgres, DEFAULT_DB_CONFIG\n+from .jira_client import JiraClientError, build_default_client\n+from .salesforce_client import SalesforceClientError, build_default_client as build_salesforce_client\n+from .mindsdb import register_agent, clear_agents\n+\n+load_dotenv()\n+\n+# Configuration\n+MINDSDB_URL = os.getenv(\"MINDSDB_URL\", \"http://127.0.0.1:47334\")\n+AGENT_NAME = \"classification_agent\"\n+RECOMMENDATION_AGENT_NAME = \"recommendation_agent\"\n+\n+services.set_db_config(DEFAULT_DB_CONFIG)\n+\n+app = FastAPI(title=\"Banking Customer Service API\", version=\"1.0.0\")\n+\n+app.add_middleware(\n+    CORSMiddleware,\n+    allow_origins=[\"*\"],\n+    allow_credentials=True,\n+    allow_methods=[\"*\"],\n+    allow_headers=[\"*\"],\n+)\n+\n+app.include_router(api_router)\n+\n+\n+@app.on_event(\"startup\")\n+async def startup_event() -> None:\n+    print(\"\\n\" + \"=\" * 70)\n+    print(\"Starting Banking Customer Service API Server...\")\n+    print(\"=\" * 70)\n+\n+    # Step 1: Initialize PostgreSQL (conversations + analytics tables)\n+    print(\"\\nStep 1: Initializing PostgreSQL database...\")\n+    try:\n+        if init_postgres(db_config=DEFAULT_DB_CONFIG, verbose=True):\n+            print(\"✓ PostgreSQL ready\")\n+        else:\n+            print(\"⚠ PostgreSQL initialization had some issues\")\n+            print(\"  The server will start, but may encounter errors.\")\n+    except Exception as exc:  # pragma: no cover - startup diagnostics\n+        print(f\"✗ Error during PostgreSQL initialization: {exc}\")\n+        print(\"  The server will start, but may encounter errors.\")\n+\n+    # Step 2: Initialize MindsDB (create database, engine, agents)\n+    print(\"\\nStep 2: Initializing MindsDB...\")\n+    try:\n+        from .mindsdb import init_mindsdb\n+        init_success = init_mindsdb(verbose=True, init_jobs=False)\n+        if init_success:\n+            print(\"✓ MindsDB initialization completed\")\n+        else:\n+            print(\"⚠ MindsDB initialization had some issues, but continuing...\")\n+    except Exception as init_exc:  # pragma: no cover - startup diagnostics\n+        print(f\"✗ MindsDB initialization failed: {init_exc}\")\n+        print(\"  Attempting to connect to MindsDB anyway...\")\n+\n+    # Step 3: Connect to MindsDB and register agents\n+    print(\"\\nStep 3: Connecting to MindsDB and registering agents...\")\n+    mindsdb_server = None\n+    try:\n+        mindsdb_server = mindsdb_sdk.connect(MINDSDB_URL)\n+        services.set_mindsdb_server(mindsdb_server)\n+        print(\"✓ Connected to MindsDB\")\n+\n+        # Register classification agent\n+        try:\n+            classification_agent = mindsdb_server.agents.get(AGENT_NAME)\n+            register_agent(AGENT_NAME, classification_agent)\n+            services.set_agent(classification_agent)\n+            print(f\"✓ Registered agent: {AGENT_NAME}\")\n+        except Exception as agent_exc:\n+            print(f\"✗ Failed to register {AGENT_NAME}: {agent_exc}\")\n+\n+        # Register recommendation agent\n+        try:\n+            recommendation_agent = mindsdb_server.agents.get(RECOMMENDATION_AGENT_NAME)\n+            register_agent(RECOMMENDATION_AGENT_NAME, recommendation_agent)\n+            print(f\"✓ Registered agent: {RECOMMENDATION_AGENT_NAME}\")\n+        except Exception as rec_exc:\n+            print(f\"✗ Recommendation agent not available: {rec_exc}\")\n+            print(\"  Recommendation features will be disabled.\")\n+\n+        # Initialize analytics JOBs\n+        print(\"\\nStep 3.5: Initializing MindsDB analytics JOBs...\")\n+        try:\n+            from .mindsdb import init_mindsdb_jobs\n+            jobs_success = init_mindsdb_jobs(mindsdb_server, recreate=False, verbose=True)\n+            if jobs_success:\n+                print(\"✓ MindsDB JOBs initialized\")\n+            else:\n+                print(\"⚠ MindsDB JOBs had some issues\")\n+        except Exception as jobs_exc:\n+            print(f\"⚠ MindsDB JOBs initialization failed: {jobs_exc}\")\n+\n+    except Exception as exc:  # pragma: no cover - startup diagnostics\n+        print(f\"✗ Error connecting to MindsDB: {exc}\")\n+        print(\"  The server will start, but agent queries will fail.\")\n+        print(f\"  Make sure MindsDB is running at {MINDSDB_URL}\")\n+\n+    # Step 4: Initialize Jira client\n+    print(\"\\nStep 4: Initializing Jira client...\")\n+    try:\n+        jira_client = build_default_client()\n+        if jira_client:\n+            services.set_jira_client(jira_client)\n+            print(\"✓ Jira client configured\")\n+        else:\n+            print(\"✗ Jira client not configured. Missing Jira environment variables.\")\n+    except JiraClientError as exc:\n+        print(f\"✗ Jira client configuration error: {exc}\")\n+    except Exception as exc:  # pragma: no cover - startup diagnostics\n+        print(f\"✗ Unexpected Jira initialization error: {exc}\")\n+\n+    # Step 5: Initialize Salesforce client\n+    print(\"\\nStep 5: Initializing Salesforce client...\")\n+    try:\n+        salesforce_client = build_salesforce_client()\n+        if salesforce_client:\n+            services.set_salesforce_client(salesforce_client)\n+            print(\"✓ Salesforce client configured\")\n+        else:\n+            print(\"✗ Salesforce client not configured. Missing Salesforce environment variables.\")\n+    except SalesforceClientError as exc:\n+        print(f\"✗ Salesforce client configuration error: {exc}\")\n+    except Exception as exc:  # pragma: no cover - startup diagnostics\n+        print(f\"✗ Unexpected Salesforce initialization error: {exc}\")\n+\n+    print(\"\\n\" + \"=\" * 70)\n+    print(\"Server startup complete!\")\n+    print(\"=\" * 70)\n+    print(\"\\n\")\n+\n+\n+@app.on_event(\"shutdown\")\n+async def shutdown_event() -> None:\n+    \"\"\"Cleanup on server shutdown.\"\"\"\n+    services.clear_state()\n+    clear_agents()",
    "comment": "**correctness**: `services.clear_state()` and `clear_agents()` in the shutdown event are called without error handling; if either raises an exception, FastAPI shutdown may be interrupted, potentially leaving resources in an inconsistent state.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb hacktoberfest/use-cases/auto-banking-customer-service/app/__init__.py, lines 151-152, the shutdown event handler calls `services.clear_state()` and `clear_agents()` without error handling. If either raises an exception, FastAPI's shutdown process may be interrupted, potentially leaving resources in an inconsistent state. Please wrap each call in a try/except block and log any exceptions to ensure graceful shutdown.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n    try:\n        services.clear_state()\n    except Exception as exc:\n        print(f\"Warning: Exception during service state cleanup: {exc}\")\n    try:\n        clear_agents()\n    except Exception as exc:\n        print(f\"Warning: Exception during agent cleanup: {exc}\")\n```\n</details>\n<!-- suggestion_end -->\n",
    "line_number": 152,
    "enriched": "File: mindsdb hacktoberfest/use-cases/auto-banking-customer-service/app/__init__.py\nCode: @@ -0,0 +1,155 @@\n+\"\"\"FastAPI application factory and startup hooks.\"\"\"\n+\n+from __future__ import annotations\n+\n+import os\n+import mindsdb_sdk\n+from fastapi import FastAPI\n+from fastapi.middleware.cors import CORSMiddleware\n+from dotenv import load_dotenv\n+\n+from . import services\n+from .api import router as api_router\n+from .db import init_postgres, DEFAULT_DB_CONFIG\n+from .jira_client import JiraClientError, build_default_client\n+from .salesforce_client import SalesforceClientError, build_default_client as build_salesforce_client\n+from .mindsdb import register_agent, clear_agents\n+\n+load_dotenv()\n+\n+# Configuration\n+MINDSDB_URL = os.getenv(\"MINDSDB_URL\", \"http://127.0.0.1:47334\")\n+AGENT_NAME = \"classification_agent\"\n+RECOMMENDATION_AGENT_NAME = \"recommendation_agent\"\n+\n+services.set_db_config(DEFAULT_DB_CONFIG)\n+\n+app = FastAPI(title=\"Banking Customer Service API\", version=\"1.0.0\")\n+\n+app.add_middleware(\n+    CORSMiddleware,\n+    allow_origins=[\"*\"],\n+    allow_credentials=True,\n+    allow_methods=[\"*\"],\n+    allow_headers=[\"*\"],\n+)\n+\n+app.include_router(api_router)\n+\n+\n+@app.on_event(\"startup\")\n+async def startup_event() -> None:\n+    print(\"\\n\" + \"=\" * 70)\n+    print(\"Starting Banking Customer Service API Server...\")\n+    print(\"=\" * 70)\n+\n+    # Step 1: Initialize PostgreSQL (conversations + analytics tables)\n+    print(\"\\nStep 1: Initializing PostgreSQL database...\")\n+    try:\n+        if init_postgres(db_config=DEFAULT_DB_CONFIG, verbose=True):\n+            print(\"✓ PostgreSQL ready\")\n+        else:\n+            print(\"⚠ PostgreSQL initialization had some issues\")\n+            print(\"  The server will start, but may encounter errors.\")\n+    except Exception as exc:  # pragma: no cover - startup diagnostics\n+        print(f\"✗ Error during PostgreSQL initialization: {exc}\")\n+        print(\"  The server will start, but may encounter errors.\")\n+\n+    # Step 2: Initialize MindsDB (create database, engine, agents)\n+    print(\"\\nStep 2: Initializing MindsDB...\")\n+    try:\n+        from .mindsdb import init_mindsdb\n+        init_success = init_mindsdb(verbose=True, init_jobs=False)\n+        if init_success:\n+            print(\"✓ MindsDB initialization completed\")\n+        else:\n+            print(\"⚠ MindsDB initialization had some issues, but continuing...\")\n+    except Exception as init_exc:  # pragma: no cover - startup diagnostics\n+        print(f\"✗ MindsDB initialization failed: {init_exc}\")\n+        print(\"  Attempting to connect to MindsDB anyway...\")\n+\n+    # Step 3: Connect to MindsDB and register agents\n+    print(\"\\nStep 3: Connecting to MindsDB and registering agents...\")\n+    mindsdb_server = None\n+    try:\n+        mindsdb_server = mindsdb_sdk.connect(MINDSDB_URL)\n+        services.set_mindsdb_server(mindsdb_server)\n+        print(\"✓ Connected to MindsDB\")\n+\n+        # Register classification agent\n+        try:\n+            classification_agent = mindsdb_server.agents.get(AGENT_NAME)\n+            register_agent(AGENT_NAME, classification_agent)\n+            services.set_agent(classification_agent)\n+            print(f\"✓ Registered agent: {AGENT_NAME}\")\n+        except Exception as agent_exc:\n+            print(f\"✗ Failed to register {AGENT_NAME}: {agent_exc}\")\n+\n+        # Register recommendation agent\n+        try:\n+            recommendation_agent = mindsdb_server.agents.get(RECOMMENDATION_AGENT_NAME)\n+            register_agent(RECOMMENDATION_AGENT_NAME, recommendation_agent)\n+            print(f\"✓ Registered agent: {RECOMMENDATION_AGENT_NAME}\")\n+        except Exception as rec_exc:\n+            print(f\"✗ Recommendation agent not available: {rec_exc}\")\n+            print(\"  Recommendation features will be disabled.\")\n+\n+        # Initialize analytics JOBs\n+        print(\"\\nStep 3.5: Initializing MindsDB analytics JOBs...\")\n+        try:\n+            from .mindsdb import init_mindsdb_jobs\n+            jobs_success = init_mindsdb_jobs(mindsdb_server, recreate=False, verbose=True)\n+            if jobs_success:\n+                print(\"✓ MindsDB JOBs initialized\")\n+            else:\n+                print(\"⚠ MindsDB JOBs had some issues\")\n+        except Exception as jobs_exc:\n+            print(f\"⚠ MindsDB JOBs initialization failed: {jobs_exc}\")\n+\n+    except Exception as exc:  # pragma: no cover - startup diagnostics\n+        print(f\"✗ Error connecting to MindsDB: {exc}\")\n+        print(\"  The server will start, but agent queries will fail.\")\n+        print(f\"  Make sure MindsDB is running at {MINDSDB_URL}\")\n+\n+    # Step 4: Initialize Jira client\n+    print(\"\\nStep 4: Initializing Jira client...\")\n+    try:\n+        jira_client = build_default_client()\n+        if jira_client:\n+            services.set_jira_client(jira_client)\n+            print(\"✓ Jira client configured\")\n+        else:\n+            print(\"✗ Jira client not configured. Missing Jira environment variables.\")\n+    except JiraClientError as exc:\n+        print(f\"✗ Jira client configuration error: {exc}\")\n+    except Exception as exc:  # pragma: no cover - startup diagnostics\n+        print(f\"✗ Unexpected Jira initialization error: {exc}\")\n+\n+    # Step 5: Initialize Salesforce client\n+    print(\"\\nStep 5: Initializing Salesforce client...\")\n+    try:\n+        salesforce_client = build_salesforce_client()\n+        if salesforce_client:\n+            services.set_salesforce_client(salesforce_client)\n+            print(\"✓ Salesforce client configured\")\n+        else:\n+            print(\"✗ Salesforce client not configured. Missing Salesforce environment variables.\")\n+    except SalesforceClientError as exc:\n+        print(f\"✗ Salesforce client configuration error: {exc}\")\n+    except Exception as exc:  # pragma: no cover - startup diagnostics\n+        print(f\"✗ Unexpected Salesforce initialization error: {exc}\")\n+\n+    print(\"\\n\" + \"=\" * 70)\n+    print(\"Server startup complete!\")\n+    print(\"=\" * 70)\n+    print(\"\\n\")\n+\n+\n+@app.on_event(\"shutdown\")\n+async def shutdown_event() -> None:\n+    \"\"\"Cleanup on server shutdown.\"\"\"\n+    services.clear_state()\n+    clear_agents()\nComment: **correctness**: `services.clear_state()` and `clear_agents()` in the shutdown event are called without error handling; if either raises an exception, FastAPI shutdown may be interrupted, potentially leaving resources in an inconsistent state.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb hacktoberfest/use-cases/auto-banking-customer-service/app/__init__.py, lines 151-152, the shutdown event handler calls `services.clear_state()` and `clear_agents()` without error handling. If either raises an exception, FastAPI's shutdown process may be interrupted, potentially leaving resources in an inconsistent state. Please wrap each call in a try/except block and log any exceptions to ensure graceful shutdown.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n    try:\n        services.clear_state()\n    except Exception as exc:\n        print(f\"Warning: Exception during service state cleanup: {exc}\")\n    try:\n        clear_agents()\n    except Exception as exc:\n        print(f\"Warning: Exception during agent cleanup: {exc}\")\n```\n</details>\n<!-- suggestion_end -->\n",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb hacktoberfest/use-cases/auto-banking-customer-service/app/__init__.py",
    "pr_number": 11793,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2464595662,
    "comment_created_at": "2025-10-27T06:51:57Z"
  },
  {
    "code": "@@ -154,6 +154,9 @@ jobs:\n         with:\n           enable-cache: true\n           cache-dependency-glob: \"**/requirements*.txt\"\n+      - name: Install FreeTDS (macOS)",
    "comment": "Hey @ZoranPandovski,\r\nIs it OK to do this since it assumes that environments of users running MacOS will have FreeTDS installed?",
    "line_number": 157,
    "enriched": "File: .github/workflows/test_on_push.yml\nCode: @@ -154,6 +154,9 @@ jobs:\n         with:\n           enable-cache: true\n           cache-dependency-glob: \"**/requirements*.txt\"\n+      - name: Install FreeTDS (macOS)\nComment: Hey @ZoranPandovski,\r\nIs it OK to do this since it assumes that environments of users running MacOS will have FreeTDS installed?",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": ".github/workflows/test_on_push.yml",
    "pr_number": 10677,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2033672877,
    "comment_created_at": "2025-04-08T17:08:04Z"
  },
  {
    "code": "@@ -32,7 +32,7 @@ def get(self):\n                 'http_auth_enabled': config['auth']['http_auth_enabled']\n             }\n         }\n-        for key in ['default_llm', 'default_embedding_model']:\n+        for key in ['default_llm', 'default_embedding_model', 'default_reranking_model']:",
    "comment": "it is for getting reranking llm from 'default_reranking_model' instead of 'default_llm' and 'default_llm' will be use only by functions, right?",
    "line_number": 35,
    "enriched": "File: mindsdb/api/http/namespaces/config.py\nCode: @@ -32,7 +32,7 @@ def get(self):\n                 'http_auth_enabled': config['auth']['http_auth_enabled']\n             }\n         }\n-        for key in ['default_llm', 'default_embedding_model']:\n+        for key in ['default_llm', 'default_embedding_model', 'default_reranking_model']:\nComment: it is for getting reranking llm from 'default_reranking_model' instead of 'default_llm' and 'default_llm' will be use only by functions, right?",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/api/http/namespaces/config.py",
    "pr_number": 10848,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2104424034,
    "comment_created_at": "2025-05-23T11:47:54Z"
  },
  {
    "code": "@@ -0,0 +1,178 @@\n+import unittest\n+\n+import pandas as pd\n+\n+from mindsdb.integrations.libs.llm_utils import get_completed_prompts\n+from mindsdb.integrations.libs.llm_utils import ft_jsonl_validation, ft_chat_formatter, ft_chat_format_validation\n+\n+\n+class TestLLM(unittest.TestCase):\n+    def test_get_completed_prompts(self):\n+        placeholder = \"{{text}}\"\n+        prefix = \"You are a helpful assistant. Here is the user's input:\"\n+        user_inputs = [\"Hi! I would love some help.\", \"Hello! Are you sentient?\", None]\n+\n+        # send all rows at once\n+        base_template = prefix + placeholder\n+        df = pd.DataFrame({'text': user_inputs})\n+        prompts, empties = get_completed_prompts(base_template, df)\n+\n+        # should detect a single missing value in the relevant column (last row)\n+        assert empties.shape == (1, )\n+        assert empties.dtype == int\n+        assert empties[0] == 2\n+\n+        # check results\n+        for i in range(len(empties)):\n+            # in-fill\n+            assert prompts[0] == prefix + user_inputs[i]\n+\n+        # edge case - invalid template\n+        placeholder = \"\"\n+        base_template = prefix + placeholder\n+        df = pd.DataFrame({'text': user_inputs})\n+        with self.assertRaises(Exception):\n+            get_completed_prompts(base_template, df)\n+\n+    def test_ft_chat_format_validation(self):\n+        valid_chats = [\n+            # u/a pattern\n+            [\n+                {\"role\": \"user\", \"content\": \"hi\"},\n+                {\"role\": \"assistant\", \"content\": \"hello\"},\n+                {\"role\": \"user\", \"content\": \"how are you?\"},\n+                {\"role\": \"assistant\", \"content\": \"I'm good, thanks\"},\n+            ],\n+\n+            # u/a pattern\n+            [\n+                {\"role\": \"user\", \"content\": \"hi\"},\n+                {\"role\": \"assistant\", \"content\": \"hello\"},\n+                {\"role\": \"user\", \"content\": \"how are you?\"},\n+            ],\n+\n+            # s/u/a pattern\n+            [\n+                {\"role\": \"system\", \"content\": \"you are a useful assistant.\"},\n+                {\"role\": \"user\", \"content\": \"hello\"},\n+                {\"role\": \"assistant\", \"content\": \"how are you?\"},\n+            ],\n+\n+            # s/u/a pattern\n+            [\n+                {\"role\": \"system\", \"content\": \"you are a useful assistant.\"},\n+                {\"role\": \"user\", \"content\": \"hello\"},\n+                {\"role\": \"assistant\", \"content\": \"how are you?\"},\n+                {\"role\": \"user\", \"content\": \"I'm good, thanks\"},\n+            ],\n+        ]\n+\n+        invalid_chats = [\n+            # invalid - repeated user\n+            [\n+                {\"role\": \"user\", \"content\": \"hi\"},\n+                {\"role\": \"user\", \"content\": \"hello\"},  # this is invalid\n+                {\"role\": \"assistant\", \"content\": \"how are you?\"},\n+                {\"role\": \"user\", \"content\": \"I'm good, thanks\"},\n+            ],\n+\n+            # invalid - repeated assistant\n+            [\n+                {\"role\": \"user\", \"content\": \"hi\"},\n+                {\"role\": \"assistant\", \"content\": \"hello\"},\n+                {\"role\": \"assistant\", \"content\": \"how are you?\"},  # this is invalid\n+                {\"role\": \"user\", \"content\": \"I'm good, thanks\"},\n+            ],\n+\n+            # invalid - incorrect system prompt order\n+            [\n+                {\"role\": \"user\", \"content\": \"hi\"},\n+                {\"role\": \"assistant\", \"content\": \"hello\"},\n+                {\"role\": \"system\", \"content\": \"you are a useful assistant.\"},  # this is invalid\n+                {\"role\": \"user\", \"content\": \"I'm good, thanks\"},\n+            ],\n+\n+            # invalid roles\n+            [\n+                {\"role\": \"user\", \"content\": \"hi\"},\n+                {\"role\": \"invalid\", \"content\": \"this is an invalid role\"},\n+            ],\n+\n+            # invalid content\n+            [\n+                {\"role\": \"user\", \"content\": \"hi\"},\n+                {\"role\": \"assistant\", \"content\": None},  # should always be a string\n+            ],\n+\n+            # invalid - no assistant in the chat\n+            [\n+                {\"role\": \"user\", \"content\": \"hi\"},\n+            ],\n+        ]\n+",
    "comment": "minor nit: perhaps we can make invalid_chats and valid_chats a fixture. Stylistic, doesn't block merge",
    "line_number": 112,
    "enriched": "File: tests/unit/test_llm_utils.py\nCode: @@ -0,0 +1,178 @@\n+import unittest\n+\n+import pandas as pd\n+\n+from mindsdb.integrations.libs.llm_utils import get_completed_prompts\n+from mindsdb.integrations.libs.llm_utils import ft_jsonl_validation, ft_chat_formatter, ft_chat_format_validation\n+\n+\n+class TestLLM(unittest.TestCase):\n+    def test_get_completed_prompts(self):\n+        placeholder = \"{{text}}\"\n+        prefix = \"You are a helpful assistant. Here is the user's input:\"\n+        user_inputs = [\"Hi! I would love some help.\", \"Hello! Are you sentient?\", None]\n+\n+        # send all rows at once\n+        base_template = prefix + placeholder\n+        df = pd.DataFrame({'text': user_inputs})\n+        prompts, empties = get_completed_prompts(base_template, df)\n+\n+        # should detect a single missing value in the relevant column (last row)\n+        assert empties.shape == (1, )\n+        assert empties.dtype == int\n+        assert empties[0] == 2\n+\n+        # check results\n+        for i in range(len(empties)):\n+            # in-fill\n+            assert prompts[0] == prefix + user_inputs[i]\n+\n+        # edge case - invalid template\n+        placeholder = \"\"\n+        base_template = prefix + placeholder\n+        df = pd.DataFrame({'text': user_inputs})\n+        with self.assertRaises(Exception):\n+            get_completed_prompts(base_template, df)\n+\n+    def test_ft_chat_format_validation(self):\n+        valid_chats = [\n+            # u/a pattern\n+            [\n+                {\"role\": \"user\", \"content\": \"hi\"},\n+                {\"role\": \"assistant\", \"content\": \"hello\"},\n+                {\"role\": \"user\", \"content\": \"how are you?\"},\n+                {\"role\": \"assistant\", \"content\": \"I'm good, thanks\"},\n+            ],\n+\n+            # u/a pattern\n+            [\n+                {\"role\": \"user\", \"content\": \"hi\"},\n+                {\"role\": \"assistant\", \"content\": \"hello\"},\n+                {\"role\": \"user\", \"content\": \"how are you?\"},\n+            ],\n+\n+            # s/u/a pattern\n+            [\n+                {\"role\": \"system\", \"content\": \"you are a useful assistant.\"},\n+                {\"role\": \"user\", \"content\": \"hello\"},\n+                {\"role\": \"assistant\", \"content\": \"how are you?\"},\n+            ],\n+\n+            # s/u/a pattern\n+            [\n+                {\"role\": \"system\", \"content\": \"you are a useful assistant.\"},\n+                {\"role\": \"user\", \"content\": \"hello\"},\n+                {\"role\": \"assistant\", \"content\": \"how are you?\"},\n+                {\"role\": \"user\", \"content\": \"I'm good, thanks\"},\n+            ],\n+        ]\n+\n+        invalid_chats = [\n+            # invalid - repeated user\n+            [\n+                {\"role\": \"user\", \"content\": \"hi\"},\n+                {\"role\": \"user\", \"content\": \"hello\"},  # this is invalid\n+                {\"role\": \"assistant\", \"content\": \"how are you?\"},\n+                {\"role\": \"user\", \"content\": \"I'm good, thanks\"},\n+            ],\n+\n+            # invalid - repeated assistant\n+            [\n+                {\"role\": \"user\", \"content\": \"hi\"},\n+                {\"role\": \"assistant\", \"content\": \"hello\"},\n+                {\"role\": \"assistant\", \"content\": \"how are you?\"},  # this is invalid\n+                {\"role\": \"user\", \"content\": \"I'm good, thanks\"},\n+            ],\n+\n+            # invalid - incorrect system prompt order\n+            [\n+                {\"role\": \"user\", \"content\": \"hi\"},\n+                {\"role\": \"assistant\", \"content\": \"hello\"},\n+                {\"role\": \"system\", \"content\": \"you are a useful assistant.\"},  # this is invalid\n+                {\"role\": \"user\", \"content\": \"I'm good, thanks\"},\n+            ],\n+\n+            # invalid roles\n+            [\n+                {\"role\": \"user\", \"content\": \"hi\"},\n+                {\"role\": \"invalid\", \"content\": \"this is an invalid role\"},\n+            ],\n+\n+            # invalid content\n+            [\n+                {\"role\": \"user\", \"content\": \"hi\"},\n+                {\"role\": \"assistant\", \"content\": None},  # should always be a string\n+            ],\n+\n+            # invalid - no assistant in the chat\n+            [\n+                {\"role\": \"user\", \"content\": \"hi\"},\n+            ],\n+        ]\n+\nComment: minor nit: perhaps we can make invalid_chats and valid_chats a fixture. Stylistic, doesn't block merge",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "tests/unit/test_llm_utils.py",
    "pr_number": 8692,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1472514918,
    "comment_created_at": "2024-01-31T09:11:55Z"
  },
  {
    "code": "@@ -129,15 +129,23 @@ def _extract_comparison_conditions(node, **kwargs):\n \n         return conditions\n \n-    def _convert_metadata_filters(self, conditions):\n+    def _convert_metadata_filters(self, conditions, allowed_metadata_columns=None):\n         if conditions is None:\n             return\n         # try to treat conditions that are not in TableField as metadata conditions\n         for condition in conditions:\n-            if not self._is_condition_allowed(condition):\n-                condition.column = (\n-                    TableField.METADATA.value + \".\" + condition.column\n-                )\n+            if self._is_metadata_condition(condition):\n+                # check restriction\n+                if allowed_metadata_columns is not None:\n+                    # system columns are underscored, skip them\n+                    if condition.column not in allowed_metadata_columns and not condition.column.startswith(\"_\"):",
    "comment": "Should it be case-sensitive?",
    "line_number": 141,
    "enriched": "File: mindsdb/integrations/libs/vectordatabase_handler.py\nCode: @@ -129,15 +129,23 @@ def _extract_comparison_conditions(node, **kwargs):\n \n         return conditions\n \n-    def _convert_metadata_filters(self, conditions):\n+    def _convert_metadata_filters(self, conditions, allowed_metadata_columns=None):\n         if conditions is None:\n             return\n         # try to treat conditions that are not in TableField as metadata conditions\n         for condition in conditions:\n-            if not self._is_condition_allowed(condition):\n-                condition.column = (\n-                    TableField.METADATA.value + \".\" + condition.column\n-                )\n+            if self._is_metadata_condition(condition):\n+                # check restriction\n+                if allowed_metadata_columns is not None:\n+                    # system columns are underscored, skip them\n+                    if condition.column not in allowed_metadata_columns and not condition.column.startswith(\"_\"):\nComment: Should it be case-sensitive?",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/libs/vectordatabase_handler.py",
    "pr_number": 11019,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2158302072,
    "comment_created_at": "2025-06-20T08:06:07Z"
  },
  {
    "code": "@@ -1010,7 +1012,7 @@ def _create_embedding_model(self, project_name, engine=\"openai\", params: dict =\n             if 'question_column' not in params:\n                 params['question_column'] = 'content'\n             if 'api_key' in params:\n-                params[f\"{engine}_api_key\"] = params.pop('api_key')\n+                params[f\"{engine}_api_key\"] = api_key",
    "comment": "**Correctness**: The code extracts `api_key` from `params` using `pop()` which removes it, but then tries to use it again on line 1015, causing a potential KeyError since `api_key` is no longer in `params`.\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n        api_key = get_api_key(engine, params, strict=False) or params.get('api_key')\n        if 'api_key' in params:\n            params.pop('api_key')\n\n        if engine == 'azure_openai':\n            engine = 'openai'\n            params['provider'] = 'azure'\n\n            if 'question_column' not in params:\n                params['question_column'] = 'content'\n            params[f\"{engine}_api_key\"] = api_key\n```\n</details>\n<!-- suggestion_end -->\n",
    "line_number": 1015,
    "enriched": "File: mindsdb/interfaces/knowledge_base/controller.py\nCode: @@ -1010,7 +1012,7 @@ def _create_embedding_model(self, project_name, engine=\"openai\", params: dict =\n             if 'question_column' not in params:\n                 params['question_column'] = 'content'\n             if 'api_key' in params:\n-                params[f\"{engine}_api_key\"] = params.pop('api_key')\n+                params[f\"{engine}_api_key\"] = api_key\nComment: **Correctness**: The code extracts `api_key` from `params` using `pop()` which removes it, but then tries to use it again on line 1015, causing a potential KeyError since `api_key` is no longer in `params`.\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n        api_key = get_api_key(engine, params, strict=False) or params.get('api_key')\n        if 'api_key' in params:\n            params.pop('api_key')\n\n        if engine == 'azure_openai':\n            engine = 'openai'\n            params['provider'] = 'azure'\n\n            if 'question_column' not in params:\n                params['question_column'] = 'content'\n            params[f\"{engine}_api_key\"] = api_key\n```\n</details>\n<!-- suggestion_end -->\n",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/interfaces/knowledge_base/controller.py",
    "pr_number": 10811,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2079052393,
    "comment_created_at": "2025-05-08T07:14:46Z"
  },
  {
    "code": "@@ -26,7 +26,7 @@ def __init__(self, retriever_runnable, prompt_template, llm, reranker: bool = DE\n         self.prompt_template = prompt_template\n         self.llm = llm\n         if reranker:\n-            self.reranker = OpenAIReranker()\n+            self.reranker = LLMReranker()",
    "comment": "we should allow users to use non default args for reranker, so entry point here and perhaps pass in via [RagPipelineModel](https://github.com/mindsdb/mindsdb/blob/0c03525cdd8ab2c1ec17414be64aa5f11b792893/mindsdb/integrations/utilities/rag/settings.py#L104)",
    "line_number": 29,
    "enriched": "File: mindsdb/integrations/utilities/rag/pipelines/rag.py\nCode: @@ -26,7 +26,7 @@ def __init__(self, retriever_runnable, prompt_template, llm, reranker: bool = DE\n         self.prompt_template = prompt_template\n         self.llm = llm\n         if reranker:\n-            self.reranker = OpenAIReranker()\n+            self.reranker = LLMReranker()\nComment: we should allow users to use non default args for reranker, so entry point here and perhaps pass in via [RagPipelineModel](https://github.com/mindsdb/mindsdb/blob/0c03525cdd8ab2c1ec17414be64aa5f11b792893/mindsdb/integrations/utilities/rag/settings.py#L104)",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/utilities/rag/pipelines/rag.py",
    "pr_number": 10153,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1838515544,
    "comment_created_at": "2024-11-12T17:34:18Z"
  },
  {
    "code": "@@ -140,3 +140,41 @@ def test_with_exog_vars(self, mock_handler):\n \n         describe_features = self.run_sql(\"describe proj.model_exog_var.features\")\n         assert describe_features[\"exog_vars\"][0] == [\"exog_var_1\"]\n+\n+    @patch(\"mindsdb.integrations.handlers.postgres_handler.Handler\")\n+    def test_hierarchical(self, mock_handler):\n+        # create project\n+        self.run_sql(\"create database proj\")\n+        df = pd.read_csv(\"tests/unit/ml_handlers/data/house_sales.csv\")  # comes mindsdb docs forecast example\n+        self.set_handler(mock_handler, name=\"pg\", tables={\"df\": df})\n+\n+        self.run_sql(\n+            \"\"\"\n+           create model proj.model_1_group\n+           from pg (select * from df)\n+           predict ma\n+           order by saledate\n+           group by type, bedrooms\n+           horizon 4\n+           window 8\n+           using\n+             engine='neuralforecast',\n+             hierarchy=['type', 'bedrooms'],\n+             train_time=0.01\n+        \"\"\"\n+        )\n+        self.wait_predictor(\"proj\", \"model_1_group\")\n+\n+        # run predict\n+        mindsdb_result_hier = self.run_sql(",
    "comment": "I'm wondering whether it makes sense to train a non-hierarchical model in this test, and check that results are different between them? Presumably there is _some_ impact in the forecast, at least. \r\n\r\nThe big question in my mind would be whether NF is deterministic (i.e. are seeds fixed) so that we can guarantee two runs in a row for the same model config would yield equal results? If not, then this addition doesn't make sense. If yes, then I'd ask that you include this so we know there is an impact.",
    "line_number": 169,
    "enriched": "File: tests/unit/ml_handlers/test_neuralforecast.py\nCode: @@ -140,3 +140,41 @@ def test_with_exog_vars(self, mock_handler):\n \n         describe_features = self.run_sql(\"describe proj.model_exog_var.features\")\n         assert describe_features[\"exog_vars\"][0] == [\"exog_var_1\"]\n+\n+    @patch(\"mindsdb.integrations.handlers.postgres_handler.Handler\")\n+    def test_hierarchical(self, mock_handler):\n+        # create project\n+        self.run_sql(\"create database proj\")\n+        df = pd.read_csv(\"tests/unit/ml_handlers/data/house_sales.csv\")  # comes mindsdb docs forecast example\n+        self.set_handler(mock_handler, name=\"pg\", tables={\"df\": df})\n+\n+        self.run_sql(\n+            \"\"\"\n+           create model proj.model_1_group\n+           from pg (select * from df)\n+           predict ma\n+           order by saledate\n+           group by type, bedrooms\n+           horizon 4\n+           window 8\n+           using\n+             engine='neuralforecast',\n+             hierarchy=['type', 'bedrooms'],\n+             train_time=0.01\n+        \"\"\"\n+        )\n+        self.wait_predictor(\"proj\", \"model_1_group\")\n+\n+        # run predict\n+        mindsdb_result_hier = self.run_sql(\nComment: I'm wondering whether it makes sense to train a non-hierarchical model in this test, and check that results are different between them? Presumably there is _some_ impact in the forecast, at least. \r\n\r\nThe big question in my mind would be whether NF is deterministic (i.e. are seeds fixed) so that we can guarantee two runs in a row for the same model config would yield equal results? If not, then this addition doesn't make sense. If yes, then I'd ask that you include this so we know there is an impact.",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "tests/unit/ml_handlers/test_neuralforecast.py",
    "pr_number": 5605,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1164823910,
    "comment_created_at": "2023-04-13T01:09:30Z"
  },
  {
    "code": "@@ -0,0 +1,119 @@\n+/**\n+ * Este script:\n+ *  1 - cria uma release\n+ *  2 - para cada PR incluida na release, obtem o card do shortcut e coloca a label de \"Status / Live\"\n+ */\n+\n+const axios = require('axios');\n+\n+/**\n+ * @param github A pre-authenticated {@see https://octokit.github.io/rest.js} client with pagination plugins\n+ * @param context An object containing the context of the workflow run {@see https://github.com/actions/toolkit/blob/main/packages/github/src/context.ts}\n+ * @param core A reference to the @actions/core package\n+ * @return {Promise<void>}\n+ *\n+ * @see rest api docs https://docs.github.com/en/rest\n+ * @see action docs https://github.com/actions/github-script\n+ */\n+module.exports = async ({github, context}) => {\n+    const tagName = \"v\" + process.env.APP_VERSION\n+    const SHORTCUT_API_TOKEN = process.env.SHORTCUT_API_TOKEN\n+\n+    const shortcutClient = axios.create({\n+        baseURL: 'https://api.app.shortcut.com/api/v3',\n+        timeout: 5000,\n+        headers: {'Shortcut-Token': SHORTCUT_API_TOKEN}\n+    });\n+\n+    const refResponse = await github.rest.git.createRef({\n+        owner: context.repo.owner,\n+        repo: context.repo.repo,\n+        ref: \"refs/tags/\" + tagName,\n+        sha: context.sha,\n+    });\n+    const refData = refResponse.data;\n+\n+    await github.rest.git.createTag({\n+        owner: context.repo.owner,\n+        repo: context.repo.repo,\n+        tag: tagName,\n+        message: \"Version \" + tagName,\n+        object: refData.object.sha,\n+        type: refData.object.type,\n+        tagger: {\n+            name: \"Deploy Workflow\", // context.payload.pusher.name, so tem pusher se for PR\n+            email: \"tobias.ferreira@talentify.io\", //context.payload.pusher.email,\n+        }\n+    });\n+\n+    // Useful to test this script without creating a new release every time.\n+    // const {data: releaseData} = await github.rest.repos.getLatestRelease({\n+    //     owner: context.repo.owner,\n+    //     repo: context.repo.repo,\n+    // });\n+    const {data: releaseData} = await github.rest.repos.createRelease({\n+        owner: context.repo.owner,\n+        repo: context.repo.repo,\n+        tag_name: tagName,\n+        name: tagName,\n+        generate_release_notes: true,\n+    });\n+\n+    const changes = releaseData.body.split(\"\\n\");\n+    for (change of changes) {\n+        console.group(\"Changes\");\n+        // e.g.:\n+        // https://github.com/Talentify/jobs-search/pull/298 returns 298\n+        const matches = change.match(/github\\.com\\/(.*)\\/(.*)\\/pull\\/([0-9]*)/);\n+        if (!matches || !matches[3]) {\n+            console.debug(\"No reference to a Pull Request was found on the current line.\");\n+            console.groupEnd();\n+            continue;\n+        }\n+        const pullRequestNumber = matches[3];\n+\n+        console.group(\"PR-\" + pullRequestNumber);\n+        console.debug(\"Checking pull request\", pullRequestNumber);\n+\n+        const {data: comments} = await github.request('GET /repos/{owner}/{repo}/issues/{pull_number}/comments', {\n+            owner: context.repo.owner,\n+            repo: context.repo.repo,\n+            pull_number: pullRequestNumber,\n+        });\n+\n+        if (!comments) {\n+            console.info(\"No comments found\");\n+            console.groupEnd();\n+            console.groupEnd();\n+            continue;\n+        }\n+\n+        for (comment of comments) {\n+            // e.g.: https://app.shortcut.com/talentify/story/37685/some-title\n+            const shortcurtUrlParts = comment.body.match(/app\\.shortcut\\.com\\/(.*)\\/story\\/([0-9]*)/)\n+            if (!shortcurtUrlParts || !shortcurtUrlParts[2]) {\n+                console.debug(\"Comment does not contain a reference to the shortcut story.\");\n+                console.groupEnd();\n+                console.groupEnd();\n+                continue;\n+            }\n+\n+            const shortcutStoryId = shortcurtUrlParts[2];\n+            console.log(\"shortcutStoryId\", shortcutStoryId);\n+\n+            shortcutClient.put('/stories/' + shortcutStoryId, {\n+                labels: [\n+                    {\n+                        name: \"Status / Live\",\n+                    }\n+                ],\n+            }).catch(function (error) {\n+                console.log(error.toJSON());\n+            });",
    "comment": "**performance**: The script performs a separate Shortcut API `PUT` request for every comment containing a Shortcut story, potentially resulting in a large number of sequential network calls and slow execution for releases with many PRs/comments.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn .github/workflows/create_release.js, lines 104-112, the code issues a separate Shortcut API PUT request for each comment containing a Shortcut story, which can result in slow execution for releases with many PRs/comments. Refactor this section to collect all the shortcutClient.put promises in an array and use await Promise.all to execute them concurrently after the loop, significantly reducing total execution time for large numbers of updates.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n            const updatePromises = [];\n            for (comment of comments) {\n                // e.g.: https://app.shortcut.com/talentify/story/37685/some-title\n                const shortcurtUrlParts = comment.body.match(/app\\.shortcut\\.com\\/(.*)\\/story\\/([0-9]*)/)\n                if (!shortcurtUrlParts || !shortcurtUrlParts[2]) {\n                    console.debug(\"Comment does not contain a reference to the shortcut story.\");\n                    console.groupEnd();\n                    console.groupEnd();\n                    continue;\n                }\n\n                const shortcutStoryId = shortcurtUrlParts[2];\n                console.log(\"shortcutStoryId\", shortcutStoryId);\n\n                updatePromises.push(\n                    shortcutClient.put('/stories/' + shortcutStoryId, {\n                        labels: [\n                            {\n                                name: \"Status / Live\",\n                            }\n                        ],\n                    }).catch(function (error) {\n                        console.log(error.toJSON());\n                    })\n                );\n            }\n            await Promise.all(updatePromises);\n```\n</details>\n<!-- suggestion_end -->\n",
    "line_number": 112,
    "enriched": "File: .github/workflows/create_release.js\nCode: @@ -0,0 +1,119 @@\n+/**\n+ * Este script:\n+ *  1 - cria uma release\n+ *  2 - para cada PR incluida na release, obtem o card do shortcut e coloca a label de \"Status / Live\"\n+ */\n+\n+const axios = require('axios');\n+\n+/**\n+ * @param github A pre-authenticated {@see https://octokit.github.io/rest.js} client with pagination plugins\n+ * @param context An object containing the context of the workflow run {@see https://github.com/actions/toolkit/blob/main/packages/github/src/context.ts}\n+ * @param core A reference to the @actions/core package\n+ * @return {Promise<void>}\n+ *\n+ * @see rest api docs https://docs.github.com/en/rest\n+ * @see action docs https://github.com/actions/github-script\n+ */\n+module.exports = async ({github, context}) => {\n+    const tagName = \"v\" + process.env.APP_VERSION\n+    const SHORTCUT_API_TOKEN = process.env.SHORTCUT_API_TOKEN\n+\n+    const shortcutClient = axios.create({\n+        baseURL: 'https://api.app.shortcut.com/api/v3',\n+        timeout: 5000,\n+        headers: {'Shortcut-Token': SHORTCUT_API_TOKEN}\n+    });\n+\n+    const refResponse = await github.rest.git.createRef({\n+        owner: context.repo.owner,\n+        repo: context.repo.repo,\n+        ref: \"refs/tags/\" + tagName,\n+        sha: context.sha,\n+    });\n+    const refData = refResponse.data;\n+\n+    await github.rest.git.createTag({\n+        owner: context.repo.owner,\n+        repo: context.repo.repo,\n+        tag: tagName,\n+        message: \"Version \" + tagName,\n+        object: refData.object.sha,\n+        type: refData.object.type,\n+        tagger: {\n+            name: \"Deploy Workflow\", // context.payload.pusher.name, so tem pusher se for PR\n+            email: \"tobias.ferreira@talentify.io\", //context.payload.pusher.email,\n+        }\n+    });\n+\n+    // Useful to test this script without creating a new release every time.\n+    // const {data: releaseData} = await github.rest.repos.getLatestRelease({\n+    //     owner: context.repo.owner,\n+    //     repo: context.repo.repo,\n+    // });\n+    const {data: releaseData} = await github.rest.repos.createRelease({\n+        owner: context.repo.owner,\n+        repo: context.repo.repo,\n+        tag_name: tagName,\n+        name: tagName,\n+        generate_release_notes: true,\n+    });\n+\n+    const changes = releaseData.body.split(\"\\n\");\n+    for (change of changes) {\n+        console.group(\"Changes\");\n+        // e.g.:\n+        // https://github.com/Talentify/jobs-search/pull/298 returns 298\n+        const matches = change.match(/github\\.com\\/(.*)\\/(.*)\\/pull\\/([0-9]*)/);\n+        if (!matches || !matches[3]) {\n+            console.debug(\"No reference to a Pull Request was found on the current line.\");\n+            console.groupEnd();\n+            continue;\n+        }\n+        const pullRequestNumber = matches[3];\n+\n+        console.group(\"PR-\" + pullRequestNumber);\n+        console.debug(\"Checking pull request\", pullRequestNumber);\n+\n+        const {data: comments} = await github.request('GET /repos/{owner}/{repo}/issues/{pull_number}/comments', {\n+            owner: context.repo.owner,\n+            repo: context.repo.repo,\n+            pull_number: pullRequestNumber,\n+        });\n+\n+        if (!comments) {\n+            console.info(\"No comments found\");\n+            console.groupEnd();\n+            console.groupEnd();\n+            continue;\n+        }\n+\n+        for (comment of comments) {\n+            // e.g.: https://app.shortcut.com/talentify/story/37685/some-title\n+            const shortcurtUrlParts = comment.body.match(/app\\.shortcut\\.com\\/(.*)\\/story\\/([0-9]*)/)\n+            if (!shortcurtUrlParts || !shortcurtUrlParts[2]) {\n+                console.debug(\"Comment does not contain a reference to the shortcut story.\");\n+                console.groupEnd();\n+                console.groupEnd();\n+                continue;\n+            }\n+\n+            const shortcutStoryId = shortcurtUrlParts[2];\n+            console.log(\"shortcutStoryId\", shortcutStoryId);\n+\n+            shortcutClient.put('/stories/' + shortcutStoryId, {\n+                labels: [\n+                    {\n+                        name: \"Status / Live\",\n+                    }\n+                ],\n+            }).catch(function (error) {\n+                console.log(error.toJSON());\n+            });\nComment: **performance**: The script performs a separate Shortcut API `PUT` request for every comment containing a Shortcut story, potentially resulting in a large number of sequential network calls and slow execution for releases with many PRs/comments.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn .github/workflows/create_release.js, lines 104-112, the code issues a separate Shortcut API PUT request for each comment containing a Shortcut story, which can result in slow execution for releases with many PRs/comments. Refactor this section to collect all the shortcutClient.put promises in an array and use await Promise.all to execute them concurrently after the loop, significantly reducing total execution time for large numbers of updates.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n            const updatePromises = [];\n            for (comment of comments) {\n                // e.g.: https://app.shortcut.com/talentify/story/37685/some-title\n                const shortcurtUrlParts = comment.body.match(/app\\.shortcut\\.com\\/(.*)\\/story\\/([0-9]*)/)\n                if (!shortcurtUrlParts || !shortcurtUrlParts[2]) {\n                    console.debug(\"Comment does not contain a reference to the shortcut story.\");\n                    console.groupEnd();\n                    console.groupEnd();\n                    continue;\n                }\n\n                const shortcutStoryId = shortcurtUrlParts[2];\n                console.log(\"shortcutStoryId\", shortcutStoryId);\n\n                updatePromises.push(\n                    shortcutClient.put('/stories/' + shortcutStoryId, {\n                        labels: [\n                            {\n                                name: \"Status / Live\",\n                            }\n                        ],\n                    }).catch(function (error) {\n                        console.log(error.toJSON());\n                    })\n                );\n            }\n            await Promise.all(updatePromises);\n```\n</details>\n<!-- suggestion_end -->\n",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": ".github/workflows/create_release.js",
    "pr_number": 11558,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2342409787,
    "comment_created_at": "2025-09-11T21:46:39Z"
  },
  {
    "code": "@@ -521,3 +521,19 @@ def drop_table(self, table_name: str, if_exists=True):\n         \"\"\"\n         table_name = self._check_table(table_name)\n         self.raw_query(f\"DROP TABLE IF EXISTS {table_name}\")\n+\n+    def create_index(self, table_name: str, column_name: str = \"embeddings\", index_type: Literal['ivfflat', 'hnsw'] = \"hnsw\", metric_type: str = \"vector_cosine_ops\"):\n+        \"\"\"\n+        Create an index on the pgvector table.\n+        Args:\n+            table_name (str): Name of the table to create the index on.\n+            column_name (str): Name of the column to create the index on.\n+            index_type (str): Type of the index to create. Supported types are 'ivfflat' and 'hnsw'.\n+            metric_type (str): Metric type for the index. Supported types are 'vector_l2_ops', 'vector_ip_ops', and 'vector_cosine_ops'.\n+        \"\"\"\n+        # Check if the index type is supported\n+        if index_type not in ['ivfflat', 'hnsw']:\n+            raise ValueError(\"Invalid index type. Supported types are 'ivfflat' and 'hnsw'.\")\n+        table_name = self._check_table(table_name)\n+        # Create the index\n+        self.raw_query(f\"CREATE INDEX ON {table_name} USING {index_type} ({column_name} {metric_type})\")",
    "comment": "**Correctness**: The `metric_type` parameter in `create_index()` is validated in the SQL query but not validated in the method itself, potentially allowing invalid metric types to be passed to the database.\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n    def create_index(self, table_name: str, column_name: str = \"embeddings\", index_type: Literal['ivfflat', 'hnsw'] = \"hnsw\", metric_type: str = \"vector_cosine_ops\"):\n        \"\"\"\n        Create an index on the pgvector table.\n        Args:\n            table_name (str): Name of the table to create the index on.\n            column_name (str): Name of the column to create the index on.\n            index_type (str): Type of the index to create. Supported types are 'ivfflat' and 'hnsw'.\n            metric_type (str): Metric type for the index. Supported types are 'vector_l2_ops', 'vector_ip_ops', and 'vector_cosine_ops'.\n        \"\"\"\n        # Check if the index type is supported\n        if index_type not in ['ivfflat', 'hnsw']:\n            raise ValueError(\"Invalid index type. Supported types are 'ivfflat' and 'hnsw'.\")\n        # Check if the metric type is supported\n        if metric_type not in ['vector_l2_ops', 'vector_ip_ops', 'vector_cosine_ops']:\n            raise ValueError(\"Invalid metric type. Supported types are 'vector_l2_ops', 'vector_ip_ops', and 'vector_cosine_ops'.\")\n        table_name = self._check_table(table_name)\n        # Create the index\n        self.raw_query(f\"CREATE INDEX ON {table_name} USING {index_type} ({column_name} {metric_type})\")\n```\n</details>\n<!-- suggestion_end -->\n",
    "line_number": 539,
    "enriched": "File: mindsdb/integrations/handlers/pgvector_handler/pgvector_handler.py\nCode: @@ -521,3 +521,19 @@ def drop_table(self, table_name: str, if_exists=True):\n         \"\"\"\n         table_name = self._check_table(table_name)\n         self.raw_query(f\"DROP TABLE IF EXISTS {table_name}\")\n+\n+    def create_index(self, table_name: str, column_name: str = \"embeddings\", index_type: Literal['ivfflat', 'hnsw'] = \"hnsw\", metric_type: str = \"vector_cosine_ops\"):\n+        \"\"\"\n+        Create an index on the pgvector table.\n+        Args:\n+            table_name (str): Name of the table to create the index on.\n+            column_name (str): Name of the column to create the index on.\n+            index_type (str): Type of the index to create. Supported types are 'ivfflat' and 'hnsw'.\n+            metric_type (str): Metric type for the index. Supported types are 'vector_l2_ops', 'vector_ip_ops', and 'vector_cosine_ops'.\n+        \"\"\"\n+        # Check if the index type is supported\n+        if index_type not in ['ivfflat', 'hnsw']:\n+            raise ValueError(\"Invalid index type. Supported types are 'ivfflat' and 'hnsw'.\")\n+        table_name = self._check_table(table_name)\n+        # Create the index\n+        self.raw_query(f\"CREATE INDEX ON {table_name} USING {index_type} ({column_name} {metric_type})\")\nComment: **Correctness**: The `metric_type` parameter in `create_index()` is validated in the SQL query but not validated in the method itself, potentially allowing invalid metric types to be passed to the database.\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n    def create_index(self, table_name: str, column_name: str = \"embeddings\", index_type: Literal['ivfflat', 'hnsw'] = \"hnsw\", metric_type: str = \"vector_cosine_ops\"):\n        \"\"\"\n        Create an index on the pgvector table.\n        Args:\n            table_name (str): Name of the table to create the index on.\n            column_name (str): Name of the column to create the index on.\n            index_type (str): Type of the index to create. Supported types are 'ivfflat' and 'hnsw'.\n            metric_type (str): Metric type for the index. Supported types are 'vector_l2_ops', 'vector_ip_ops', and 'vector_cosine_ops'.\n        \"\"\"\n        # Check if the index type is supported\n        if index_type not in ['ivfflat', 'hnsw']:\n            raise ValueError(\"Invalid index type. Supported types are 'ivfflat' and 'hnsw'.\")\n        # Check if the metric type is supported\n        if metric_type not in ['vector_l2_ops', 'vector_ip_ops', 'vector_cosine_ops']:\n            raise ValueError(\"Invalid metric type. Supported types are 'vector_l2_ops', 'vector_ip_ops', and 'vector_cosine_ops'.\")\n        table_name = self._check_table(table_name)\n        # Create the index\n        self.raw_query(f\"CREATE INDEX ON {table_name} USING {index_type} ({column_name} {metric_type})\")\n```\n</details>\n<!-- suggestion_end -->\n",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/pgvector_handler/pgvector_handler.py",
    "pr_number": 10854,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2096092167,
    "comment_created_at": "2025-05-19T16:19:47Z"
  },
  {
    "code": "@@ -127,6 +127,8 @@ def adapt_query(node, is_table, **kwargs):\n             return\n         if isinstance(node, Identifier):\n             if len(node.parts) > 1:\n+                if isinstance(node.parts[-1], Star):",
    "comment": "is better to move it to file_handler because the reason of error: file handler doesn't support sql completely and tries to  compensate it with query_df\r\nalso changing something globally is more dangerous",
    "line_number": 130,
    "enriched": "File: mindsdb/api/executor/utilities/sql.py\nCode: @@ -127,6 +127,8 @@ def adapt_query(node, is_table, **kwargs):\n             return\n         if isinstance(node, Identifier):\n             if len(node.parts) > 1:\n+                if isinstance(node.parts[-1], Star):\nComment: is better to move it to file_handler because the reason of error: file handler doesn't support sql completely and tries to  compensate it with query_df\r\nalso changing something globally is more dangerous",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/api/executor/utilities/sql.py",
    "pr_number": 9978,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1812137159,
    "comment_created_at": "2024-10-23T08:08:52Z"
  },
  {
    "code": "@@ -126,4 +126,7 @@ ENTRYPOINT [ \"bash\", \"-c\", \"watchfiles --filter python 'python -Im mindsdb --con\n # Make sure the regular image is the default\n FROM extras\n \n+# Run GUI update during build so the final image already contains it\n+RUN python -Im mindsdb --config=/root/mindsdb_config.json --update-gui",
    "comment": "@StpMax could we put this under the 'extras' stage? That way the dev container will have up-to-date gui as well",
    "line_number": 130,
    "enriched": "File: docker/mindsdb.Dockerfile\nCode: @@ -126,4 +126,7 @@ ENTRYPOINT [ \"bash\", \"-c\", \"watchfiles --filter python 'python -Im mindsdb --con\n # Make sure the regular image is the default\n FROM extras\n \n+# Run GUI update during build so the final image already contains it\n+RUN python -Im mindsdb --config=/root/mindsdb_config.json --update-gui\nComment: @StpMax could we put this under the 'extras' stage? That way the dev container will have up-to-date gui as well",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "docker/mindsdb.Dockerfile",
    "pr_number": 11397,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2255341521,
    "comment_created_at": "2025-08-05T21:05:35Z"
  },
  {
    "code": "@@ -304,3 +282,37 @@ def _setup_index(self, documents):\n         )\n \n         return index\n+\n+    def _get_llama_index_api_key(self, args, strict=True):",
    "comment": "There is a util function for this so better to use one in all handlers https://github.com/mindsdb/mindsdb/blob/82dfb9750a3bd3dfec5adc86db5ba6c64a204fd2/mindsdb/integrations/utilities/handler_utils.py#L10",
    "line_number": 286,
    "enriched": "File: mindsdb/integrations/handlers/llama_index_handler/llama_index_handler.py\nCode: @@ -304,3 +282,37 @@ def _setup_index(self, documents):\n         )\n \n         return index\n+\n+    def _get_llama_index_api_key(self, args, strict=True):\nComment: There is a util function for this so better to use one in all handlers https://github.com/mindsdb/mindsdb/blob/82dfb9750a3bd3dfec5adc86db5ba6c64a204fd2/mindsdb/integrations/utilities/handler_utils.py#L10",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/llama_index_handler/llama_index_handler.py",
    "pr_number": 8593,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1446239774,
    "comment_created_at": "2024-01-09T15:27:45Z"
  },
  {
    "code": "@@ -62,7 +62,7 @@ def connect(self) -> StatusResponse:\n             return StatusResponse(self.name, True, 'Connected successfully')",
    "comment": "Can we please update this too? Also there are 2 more places we need to update in this class",
    "line_number": 62,
    "enriched": "File: mindsdb/integrations/handlers/athena_handler/athena_handler.py\nCode: @@ -62,7 +62,7 @@ def connect(self) -> StatusResponse:\n             return StatusResponse(self.name, True, 'Connected successfully')\nComment: Can we please update this too? Also there are 2 more places we need to update in this class",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/athena_handler/athena_handler.py",
    "pr_number": 10185,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1849964423,
    "comment_created_at": "2024-11-20T09:40:17Z"
  },
  {
    "code": "@@ -44,40 +39,99 @@ python -m mindsdb --no_studio\n \n ## Starting MindsDB with Extended Configuration\n \n-First, you should prepare a `config.json` file based on the following template; remember to substitute the values for your custom configuration.\n+Start MindsDB locally with your custom configuration by providing a path to the `config.json` file that stores custom config parameters listed in this section.\n+\n+```bash\n+python -m mindsdb --config=/path-to-the-extended-config-file/config.json\n+```\n+\n+Below are all of the custom configuration parameters that should be set according to your requirements and saved into the `config.json` file.\n \n ```bash\n {\n     \"permanent_storage\": {\n         \"location\": \"local\"\n+        \"bucket\": \"s3_bucket_name\" # optional\n     },\n+```\n+\n+The `permanent_storage` parameter defines where MindsDB stores essential user files, such as uploaded files, models, and tab content, providing an active storage mechanism. MindsDB checks the `permanent_storage` location to access the latest version of a file and updates it as needed.",
    "comment": "`permanent_storage` is place where mindsdb do copy of its data. It is not main storage.",
    "line_number": 58,
    "enriched": "File: docs/setup/custom-config.mdx\nCode: @@ -44,40 +39,99 @@ python -m mindsdb --no_studio\n \n ## Starting MindsDB with Extended Configuration\n \n-First, you should prepare a `config.json` file based on the following template; remember to substitute the values for your custom configuration.\n+Start MindsDB locally with your custom configuration by providing a path to the `config.json` file that stores custom config parameters listed in this section.\n+\n+```bash\n+python -m mindsdb --config=/path-to-the-extended-config-file/config.json\n+```\n+\n+Below are all of the custom configuration parameters that should be set according to your requirements and saved into the `config.json` file.\n \n ```bash\n {\n     \"permanent_storage\": {\n         \"location\": \"local\"\n+        \"bucket\": \"s3_bucket_name\" # optional\n     },\n+```\n+\n+The `permanent_storage` parameter defines where MindsDB stores essential user files, such as uploaded files, models, and tab content, providing an active storage mechanism. MindsDB checks the `permanent_storage` location to access the latest version of a file and updates it as needed.\nComment: `permanent_storage` is place where mindsdb do copy of its data. It is not main storage.",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "docs/setup/custom-config.mdx",
    "pr_number": 10186,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1846744903,
    "comment_created_at": "2024-11-18T15:03:53Z"
  },
  {
    "code": "@@ -1,9 +1,24 @@\n-__title__ = 'MindsDB Email handler'\n-__package_name__ = 'mindsdb_email_handler'\n-__version__ = '0.0.1'\n+\"\"\"\n+Email handler package metadata and handler UI hints.\n+Note: Keep this file import-safe and free of heavy imports.\n+\"\"\"\n+\n+__title__ = \"MindsDB Email handler\"\n+__package_name__ = \"mindsdb_email_handler\"\n+__version__ = \"0.0.2\"\n __description__ = \"MindsDB handler for email\"\n-__author__ = 'MindsDB Inc'\n-__github__ = 'https://github.com/mindsdb/mindsdb'\n-__pypi__ = 'https://pypi.org/project/mindsdb/'\n-__license__ = 'MIT'\n-__copyright__ = 'Copyright 2022- mindsdb'\n+__author__ = \"MindsDB Inc\"\n+__github__ = \"https://github.com/mindsdb/mindsdb\"\n+__pypi__ = \"https://pypi.org/project/mindsdb/\"\n+__license__ = \"MIT\"\n+__icon_path__ = \"icon.png\"  # fixed from icon.svg to match repository asset\n+\n+# Robust, import-safe HANDLER_TYPE assignment:\n+# Try to import the enum-like source; if unavailable, fall back to \"data\".\n+try:\n+    # Some handlers use a constant/enum from libs.const. Keep this resilient.\n+    from mindsdb.integrations.libs.const import HANDLER_TYPE as _HANDLER_TYPE_ENUM  # type: ignore\n+\n+    HANDLER_TYPE = getattr(_HANDLER_TYPE_ENUM, \"DATA\", \"data\")\n+except Exception:\n+    HANDLER_TYPE = \"data\"",
    "comment": "**correctness**: `HANDLER_TYPE` assignment falls back to string \"data\" if enum import fails, which may break code expecting an enum value, causing runtime errors in handler registration or type checks.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/integrations/handlers/email_handler/__about__.py, lines 22-24, the fallback assignment for HANDLER_TYPE uses the string \"data\" if the enum import fails. This can cause runtime errors if other code expects HANDLER_TYPE to be an enum value, not a string. Update the fallback so that if the enum is present, HANDLER_TYPE is always an enum value, and only use the string if the import fails entirely.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n    HANDLER_TYPE = getattr(_HANDLER_TYPE_ENUM, \"DATA\", _HANDLER_TYPE_ENUM.DATA)  # fallback to enum value if possible\nexcept Exception:\n    HANDLER_TYPE = \"data\"\n```\n</details>\n<!-- suggestion_end -->\n",
    "line_number": 24,
    "enriched": "File: mindsdb/integrations/handlers/email_handler/__about__.py\nCode: @@ -1,9 +1,24 @@\n-__title__ = 'MindsDB Email handler'\n-__package_name__ = 'mindsdb_email_handler'\n-__version__ = '0.0.1'\n+\"\"\"\n+Email handler package metadata and handler UI hints.\n+Note: Keep this file import-safe and free of heavy imports.\n+\"\"\"\n+\n+__title__ = \"MindsDB Email handler\"\n+__package_name__ = \"mindsdb_email_handler\"\n+__version__ = \"0.0.2\"\n __description__ = \"MindsDB handler for email\"\n-__author__ = 'MindsDB Inc'\n-__github__ = 'https://github.com/mindsdb/mindsdb'\n-__pypi__ = 'https://pypi.org/project/mindsdb/'\n-__license__ = 'MIT'\n-__copyright__ = 'Copyright 2022- mindsdb'\n+__author__ = \"MindsDB Inc\"\n+__github__ = \"https://github.com/mindsdb/mindsdb\"\n+__pypi__ = \"https://pypi.org/project/mindsdb/\"\n+__license__ = \"MIT\"\n+__icon_path__ = \"icon.png\"  # fixed from icon.svg to match repository asset\n+\n+# Robust, import-safe HANDLER_TYPE assignment:\n+# Try to import the enum-like source; if unavailable, fall back to \"data\".\n+try:\n+    # Some handlers use a constant/enum from libs.const. Keep this resilient.\n+    from mindsdb.integrations.libs.const import HANDLER_TYPE as _HANDLER_TYPE_ENUM  # type: ignore\n+\n+    HANDLER_TYPE = getattr(_HANDLER_TYPE_ENUM, \"DATA\", \"data\")\n+except Exception:\n+    HANDLER_TYPE = \"data\"\nComment: **correctness**: `HANDLER_TYPE` assignment falls back to string \"data\" if enum import fails, which may break code expecting an enum value, causing runtime errors in handler registration or type checks.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/integrations/handlers/email_handler/__about__.py, lines 22-24, the fallback assignment for HANDLER_TYPE uses the string \"data\" if the enum import fails. This can cause runtime errors if other code expects HANDLER_TYPE to be an enum value, not a string. Update the fallback so that if the enum is present, HANDLER_TYPE is always an enum value, and only use the string if the import fails entirely.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n    HANDLER_TYPE = getattr(_HANDLER_TYPE_ENUM, \"DATA\", _HANDLER_TYPE_ENUM.DATA)  # fallback to enum value if possible\nexcept Exception:\n    HANDLER_TYPE = \"data\"\n```\n</details>\n<!-- suggestion_end -->\n",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/email_handler/__about__.py",
    "pr_number": 11717,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2420808823,
    "comment_created_at": "2025-10-10T15:21:51Z"
  },
  {
    "code": "@@ -89,18 +89,6 @@ Here is a step-by-step guide:\n         self._tables = {}\n     ```\n \n-* Implementing the [`_register_table()`](https://github.com/mindsdb/mindsdb/blob/staging/mindsdb/integrations/libs/api_handler.py#L164) method:",
    "comment": "So this `_register_table()` function exists in the code, but it is not used anymore.\r\n\r\nWhere can I find `the list of functions that need to be implemented for an App handler` that you mention?",
    "line_number": 92,
    "enriched": "File: docs/contribute/app-handlers.mdx\nCode: @@ -89,18 +89,6 @@ Here is a step-by-step guide:\n         self._tables = {}\n     ```\n \n-* Implementing the [`_register_table()`](https://github.com/mindsdb/mindsdb/blob/staging/mindsdb/integrations/libs/api_handler.py#L164) method:\nComment: So this `_register_table()` function exists in the code, but it is not used anymore.\r\n\r\nWhere can I find `the list of functions that need to be implemented for an App handler` that you mention?",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "docs/contribute/app-handlers.mdx",
    "pr_number": 7517,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1344425922,
    "comment_created_at": "2023-10-03T16:46:07Z"
  },
  {
    "code": "@@ -1048,7 +1048,7 @@ def _create_integration(self, name: str, engine: str, connection_args: dict):\n             )\n             handler_meta = handlers_meta[engine]\n             if handler_meta.get(\"import\", {}).get(\"success\") is not True:\n-                raise ExecutorException(f\"Handler '{engine}' can not be used\")\n+                raise ExecutorException(f\"The '{engine}' handler isn't installed.\\n\" + get_handler_install_message(engine))",
    "comment": "Maybe it makes sense to add `get_handler_install_message(engine)` only if it is not a cloud env.",
    "line_number": 1051,
    "enriched": "File: mindsdb/api/executor/command_executor.py\nCode: @@ -1048,7 +1048,7 @@ def _create_integration(self, name: str, engine: str, connection_args: dict):\n             )\n             handler_meta = handlers_meta[engine]\n             if handler_meta.get(\"import\", {}).get(\"success\") is not True:\n-                raise ExecutorException(f\"Handler '{engine}' can not be used\")\n+                raise ExecutorException(f\"The '{engine}' handler isn't installed.\\n\" + get_handler_install_message(engine))\nComment: Maybe it makes sense to add `get_handler_install_message(engine)` only if it is not a cloud env.",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/api/executor/command_executor.py",
    "pr_number": 8720,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1537689916,
    "comment_created_at": "2024-03-25T14:29:24Z"
  },
  {
    "code": "@@ -95,3 +96,9 @@ def get_versions_where_predictors_become_obsolete():\n \n     versions_for_updating_predictors = [x for x in versions_for_updating_predictors if len(x) > 0]\n     return True, versions_for_updating_predictors\n+\n+\n+def init_lexer_parsers():\n+    get_lexer_parser('mindsdb')\n+    get_lexer_parser('mysql')\n+    get_lexer_parser('sqlite')",
    "comment": "We can remove sqlite and use 'mindsdb' or 'mysql' dialect in mindsdb.api.http.namespaces.analysis.QueryAnalysis.post\r\n",
    "line_number": 104,
    "enriched": "File: mindsdb/utilities/functions.py\nCode: @@ -95,3 +96,9 @@ def get_versions_where_predictors_become_obsolete():\n \n     versions_for_updating_predictors = [x for x in versions_for_updating_predictors if len(x) > 0]\n     return True, versions_for_updating_predictors\n+\n+\n+def init_lexer_parsers():\n+    get_lexer_parser('mindsdb')\n+    get_lexer_parser('mysql')\n+    get_lexer_parser('sqlite')\nComment: We can remove sqlite and use 'mindsdb' or 'mysql' dialect in mindsdb.api.http.namespaces.analysis.QueryAnalysis.post\r\n",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/utilities/functions.py",
    "pr_number": 5674,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1169757851,
    "comment_created_at": "2023-04-18T09:33:47Z"
  },
  {
    "code": "@@ -113,12 +116,12 @@ def predict(self, df: pd.DataFrame = None, args: dict = None):\n         if len(args[\"messages\"]) > 1:\n             # if more than one message, use batch completion\n             responses = batch_completion(**args)\n-            return pd.DataFrame({\"result\": [response.choices[0].message.content for response in responses]})\n+            return pd.DataFrame({target: [response.choices[0].message.content for response in responses]})\n \n         # run completion\n         response = completion(**args)\n \n-        return pd.DataFrame({\"result\": [response.choices[0].message.content]})\n+        return pd.DataFrame({target: [response.choices[0].message.content]})",
    "comment": "**correctness**: `target` column name is not used in `predict`, causing output to always be under 'result' instead of the user-specified column, breaking contract for custom column names.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/integrations/handlers/litellm_handler/litellm_handler.py, lines 119-124, ensure that the output DataFrame in the predict method uses the user-specified `target` column name instead of hardcoding 'result'. The code should return pd.DataFrame({target: ...}) in both the batch and single completion cases.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n            return pd.DataFrame({target: [response.choices[0].message.content for response in responses]})\n...\n            return pd.DataFrame({target: [response.choices[0].message.content]})\n```\n</details>\n<!-- suggestion_end -->\n",
    "line_number": 124,
    "enriched": "File: mindsdb/integrations/handlers/litellm_handler/litellm_handler.py\nCode: @@ -113,12 +116,12 @@ def predict(self, df: pd.DataFrame = None, args: dict = None):\n         if len(args[\"messages\"]) > 1:\n             # if more than one message, use batch completion\n             responses = batch_completion(**args)\n-            return pd.DataFrame({\"result\": [response.choices[0].message.content for response in responses]})\n+            return pd.DataFrame({target: [response.choices[0].message.content for response in responses]})\n \n         # run completion\n         response = completion(**args)\n \n-        return pd.DataFrame({\"result\": [response.choices[0].message.content]})\n+        return pd.DataFrame({target: [response.choices[0].message.content]})\nComment: **correctness**: `target` column name is not used in `predict`, causing output to always be under 'result' instead of the user-specified column, breaking contract for custom column names.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/integrations/handlers/litellm_handler/litellm_handler.py, lines 119-124, ensure that the output DataFrame in the predict method uses the user-specified `target` column name instead of hardcoding 'result'. The code should return pd.DataFrame({target: ...}) in both the batch and single completion cases.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n            return pd.DataFrame({target: [response.choices[0].message.content for response in responses]})\n...\n            return pd.DataFrame({target: [response.choices[0].message.content]})\n```\n</details>\n<!-- suggestion_end -->\n",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/litellm_handler/litellm_handler.py",
    "pr_number": 11309,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2207804635,
    "comment_created_at": "2025-07-15T15:14:11Z"
  },
  {
    "code": "@@ -12,26 +13,68 @@\n from mindsdb.integrations.libs.response import (\n     HandlerStatusResponse as StatusResponse,\n     HandlerResponse as Response,\n-    RESPONSE_TYPE\n+    RESPONSE_TYPE,\n )\n \n logger = log.getLogger(__name__)\n \n \n+class MindsDBClickHouseDialect(ClickHouseDialect):\n+    \"\"\"\n+    Custom ClickHouse dialect to handle MindsDB specific requirements.\n+    \"\"\"\n+\n+    driver = \"clickhouse\"\n+\n+    def __init__(self, **kwargs):\n+        super().__init__(**kwargs)\n+        self.driver = \"clickhouse\"\n+\n+\n+def convert_interval_to_clickhouse(query_str):\n+    \"\"\"\n+    Convert interval to ClickHouse compatible format.\n+    \"\"\"\n+    interval_pattern = r\"INTERVAL\\s+'([^']+)'\\s+(\\w+)\"\n+\n+    def replace_interval(match):\n+        value = match.group(1)\n+        unit = match.group(2).upper()\n+\n+        interval_mapping = {\n+            \"SECOND\": \"toIntervalSecond\",\n+            \"MINUTE\": \"toIntervalMinute\",\n+            \"HOUR\": \"toIntervalHour\",\n+            \"DAY\": \"toIntervalDay\",\n+            \"WEEK\": \"toIntervalWeek\",\n+            \"MONTH\": \"toIntervalMonth\",\n+            \"QUARTER\": \"toIntervalQuarter\",\n+            \"YEAR\": \"toIntervalYear\",\n+        }\n+\n+        if unit in interval_mapping:\n+            return f\"{interval_mapping[unit]}('{value}')\"\n+        else:\n+            return match.group(0)\n+\n+    return re.sub(interval_pattern, replace_interval, query_str,\n+                  flags=re.IGNORECASE)\n+",
    "comment": "**correctness**: `convert_interval_to_clickhouse` does not handle negative intervals, so queries like `INTERVAL '-1' DAY` will not be converted and will fail in ClickHouse.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/integrations/handlers/clickhouse_handler/clickhouse_handler.py, lines 34-62, the function `convert_interval_to_clickhouse` does not handle negative intervals (e.g., `INTERVAL '-1' DAY`). Update the regex pattern so that it matches negative numbers as well, ensuring such intervals are converted to ClickHouse format.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\ndef convert_interval_to_clickhouse(query_str):\n    \"\"\"\n    Convert interval to ClickHouse compatible format.\n    \"\"\"\n    interval_pattern = r\"INTERVAL\\s+'(-?\\d+)'\\s+(\\w+)\"\n\n    def replace_interval(match):\n        value = match.group(1)\n        unit = match.group(2).upper()\n\n        interval_mapping = {\n            \"SECOND\": \"toIntervalSecond\",\n            \"MINUTE\": \"toIntervalMinute\",\n            \"HOUR\": \"toIntervalHour\",\n            \"DAY\": \"toIntervalDay\",\n            \"WEEK\": \"toIntervalWeek\",\n            \"MONTH\": \"toIntervalMonth\",\n            \"QUARTER\": \"toIntervalQuarter\",\n            \"YEAR\": \"toIntervalYear\",\n        }\n\n        if unit in interval_mapping:\n            return f\"{interval_mapping[unit]}('{value}')\"\n        else:\n            return match.group(0)\n\n    return re.sub(interval_pattern, replace_interval, query_str,\n                  flags=re.IGNORECASE)\n```\n</details>\n<!-- suggestion_end -->\n",
    "line_number": 62,
    "enriched": "File: mindsdb/integrations/handlers/clickhouse_handler/clickhouse_handler.py\nCode: @@ -12,26 +13,68 @@\n from mindsdb.integrations.libs.response import (\n     HandlerStatusResponse as StatusResponse,\n     HandlerResponse as Response,\n-    RESPONSE_TYPE\n+    RESPONSE_TYPE,\n )\n \n logger = log.getLogger(__name__)\n \n \n+class MindsDBClickHouseDialect(ClickHouseDialect):\n+    \"\"\"\n+    Custom ClickHouse dialect to handle MindsDB specific requirements.\n+    \"\"\"\n+\n+    driver = \"clickhouse\"\n+\n+    def __init__(self, **kwargs):\n+        super().__init__(**kwargs)\n+        self.driver = \"clickhouse\"\n+\n+\n+def convert_interval_to_clickhouse(query_str):\n+    \"\"\"\n+    Convert interval to ClickHouse compatible format.\n+    \"\"\"\n+    interval_pattern = r\"INTERVAL\\s+'([^']+)'\\s+(\\w+)\"\n+\n+    def replace_interval(match):\n+        value = match.group(1)\n+        unit = match.group(2).upper()\n+\n+        interval_mapping = {\n+            \"SECOND\": \"toIntervalSecond\",\n+            \"MINUTE\": \"toIntervalMinute\",\n+            \"HOUR\": \"toIntervalHour\",\n+            \"DAY\": \"toIntervalDay\",\n+            \"WEEK\": \"toIntervalWeek\",\n+            \"MONTH\": \"toIntervalMonth\",\n+            \"QUARTER\": \"toIntervalQuarter\",\n+            \"YEAR\": \"toIntervalYear\",\n+        }\n+\n+        if unit in interval_mapping:\n+            return f\"{interval_mapping[unit]}('{value}')\"\n+        else:\n+            return match.group(0)\n+\n+    return re.sub(interval_pattern, replace_interval, query_str,\n+                  flags=re.IGNORECASE)\n+\nComment: **correctness**: `convert_interval_to_clickhouse` does not handle negative intervals, so queries like `INTERVAL '-1' DAY` will not be converted and will fail in ClickHouse.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/integrations/handlers/clickhouse_handler/clickhouse_handler.py, lines 34-62, the function `convert_interval_to_clickhouse` does not handle negative intervals (e.g., `INTERVAL '-1' DAY`). Update the regex pattern so that it matches negative numbers as well, ensuring such intervals are converted to ClickHouse format.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\ndef convert_interval_to_clickhouse(query_str):\n    \"\"\"\n    Convert interval to ClickHouse compatible format.\n    \"\"\"\n    interval_pattern = r\"INTERVAL\\s+'(-?\\d+)'\\s+(\\w+)\"\n\n    def replace_interval(match):\n        value = match.group(1)\n        unit = match.group(2).upper()\n\n        interval_mapping = {\n            \"SECOND\": \"toIntervalSecond\",\n            \"MINUTE\": \"toIntervalMinute\",\n            \"HOUR\": \"toIntervalHour\",\n            \"DAY\": \"toIntervalDay\",\n            \"WEEK\": \"toIntervalWeek\",\n            \"MONTH\": \"toIntervalMonth\",\n            \"QUARTER\": \"toIntervalQuarter\",\n            \"YEAR\": \"toIntervalYear\",\n        }\n\n        if unit in interval_mapping:\n            return f\"{interval_mapping[unit]}('{value}')\"\n        else:\n            return match.group(0)\n\n    return re.sub(interval_pattern, replace_interval, query_str,\n                  flags=re.IGNORECASE)\n```\n</details>\n<!-- suggestion_end -->\n",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/clickhouse_handler/clickhouse_handler.py",
    "pr_number": 11425,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2268002489,
    "comment_created_at": "2025-08-11T20:56:47Z"
  },
  {
    "code": "@@ -55,75 +94,211 @@ def on_tool_error(\n         action_span = self.action_uuid_to_span.get(parent_run_uuid)\n         if action_span is None:\n             return\n+\n         try:\n             error_str = str(error)\n         except Exception:\n             error_str = \"Couldn't get error string.\"\n-        action_span.update(metadata={'error_description': error_str})\n+\n+        tool_name = action_span.metadata.get('tool_name', 'unknown')\n+        if tool_name in self.tool_metrics:\n+            self.tool_metrics[tool_name]['errors'] += 1\n+            self.tool_metrics[tool_name]['last_error'] = error_str\n+\n+        metadata = {\n+            'error_description': error_str,\n+            'error_type': error.__class__.__name__,\n+            'error_time': datetime.datetime.now().isoformat()\n+        }\n+        action_span.update(metadata=metadata)\n \n     def on_chain_start(\n             self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs: Any\n     ) -> Any:\n         \"\"\"Run when chain starts running.\"\"\"\n+        if self.langfuse is None:\n+            return\n+\n         run_uuid = kwargs.get('run_id', uuid4()).hex\n \n         if serialized is None:\n             serialized = {}\n \n-        chain_span = self.langfuse.span(\n-            name=f'{serialized.get(\"name\", \"chain\")}-{run_uuid}',\n-            trace_id=self.trace_id,\n-            parent_observation_id=self.observation_id,\n-            input=str(inputs)\n-        )\n-        self.chain_uuid_to_span[run_uuid] = chain_span\n+        chain_name = serialized.get(\"name\", \"chain\")\n+        start_time = datetime.datetime.now()\n+\n+        # Initialize or update chain metrics\n+        if chain_name not in self.chain_metrics:\n+            self.chain_metrics[chain_name] = {\n+                'count': 0,\n+                'total_time': 0,\n+                'errors': 0,\n+                'last_error': None\n+            }\n+\n+        self.chain_metrics[chain_name]['count'] += 1\n+        self.current_chain = chain_name\n+\n+        try:\n+            chain_span = self.langfuse.span(\n+                name=f'{chain_name}-{run_uuid}',\n+                trace_id=self.trace_id,\n+                parent_observation_id=self.observation_id,\n+                input=str(inputs)\n+            )\n+\n+            metadata = {\n+                'chain_name': chain_name,\n+                'started': start_time.isoformat(),\n+                'start_timestamp': start_time.timestamp(),\n+                'input_size': len(str(inputs))",
    "comment": "is it correct to get length in this way if `inputs` is dict?",
    "line_number": 154,
    "enriched": "File: mindsdb/interfaces/agents/langfuse_callback_handler.py\nCode: @@ -55,75 +94,211 @@ def on_tool_error(\n         action_span = self.action_uuid_to_span.get(parent_run_uuid)\n         if action_span is None:\n             return\n+\n         try:\n             error_str = str(error)\n         except Exception:\n             error_str = \"Couldn't get error string.\"\n-        action_span.update(metadata={'error_description': error_str})\n+\n+        tool_name = action_span.metadata.get('tool_name', 'unknown')\n+        if tool_name in self.tool_metrics:\n+            self.tool_metrics[tool_name]['errors'] += 1\n+            self.tool_metrics[tool_name]['last_error'] = error_str\n+\n+        metadata = {\n+            'error_description': error_str,\n+            'error_type': error.__class__.__name__,\n+            'error_time': datetime.datetime.now().isoformat()\n+        }\n+        action_span.update(metadata=metadata)\n \n     def on_chain_start(\n             self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs: Any\n     ) -> Any:\n         \"\"\"Run when chain starts running.\"\"\"\n+        if self.langfuse is None:\n+            return\n+\n         run_uuid = kwargs.get('run_id', uuid4()).hex\n \n         if serialized is None:\n             serialized = {}\n \n-        chain_span = self.langfuse.span(\n-            name=f'{serialized.get(\"name\", \"chain\")}-{run_uuid}',\n-            trace_id=self.trace_id,\n-            parent_observation_id=self.observation_id,\n-            input=str(inputs)\n-        )\n-        self.chain_uuid_to_span[run_uuid] = chain_span\n+        chain_name = serialized.get(\"name\", \"chain\")\n+        start_time = datetime.datetime.now()\n+\n+        # Initialize or update chain metrics\n+        if chain_name not in self.chain_metrics:\n+            self.chain_metrics[chain_name] = {\n+                'count': 0,\n+                'total_time': 0,\n+                'errors': 0,\n+                'last_error': None\n+            }\n+\n+        self.chain_metrics[chain_name]['count'] += 1\n+        self.current_chain = chain_name\n+\n+        try:\n+            chain_span = self.langfuse.span(\n+                name=f'{chain_name}-{run_uuid}',\n+                trace_id=self.trace_id,\n+                parent_observation_id=self.observation_id,\n+                input=str(inputs)\n+            )\n+\n+            metadata = {\n+                'chain_name': chain_name,\n+                'started': start_time.isoformat(),\n+                'start_timestamp': start_time.timestamp(),\n+                'input_size': len(str(inputs))\nComment: is it correct to get length in this way if `inputs` is dict?",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/interfaces/agents/langfuse_callback_handler.py",
    "pr_number": 10437,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1944369716,
    "comment_created_at": "2025-02-06T09:14:58Z"
  },
  {
    "code": "@@ -109,7 +110,7 @@ class Integration(Resource):\n     def get(self, name):\n         integration = ca.integration_controller.get(name, show_secrets=False)\n         if integration is None:\n-            abort(404, f'Can\\'t find database integration: {name}')\n+            return http_error(HTTPStatus.NOT_FOUND, 'Not found', f'Can\\'t find database integration: {name}')",
    "comment": "Maybe just keep `Can\\'t find integration` since it can be API Integration",
    "line_number": 113,
    "enriched": "File: mindsdb/api/http/namespaces/config.py\nCode: @@ -109,7 +110,7 @@ class Integration(Resource):\n     def get(self, name):\n         integration = ca.integration_controller.get(name, show_secrets=False)\n         if integration is None:\n-            abort(404, f'Can\\'t find database integration: {name}')\n+            return http_error(HTTPStatus.NOT_FOUND, 'Not found', f'Can\\'t find database integration: {name}')\nComment: Maybe just keep `Can\\'t find integration` since it can be API Integration",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/api/http/namespaces/config.py",
    "pr_number": 9596,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1706992261,
    "comment_created_at": "2024-08-07T13:22:11Z"
  },
  {
    "code": "@@ -300,31 +300,7 @@\n             integration_type = handler_meta.get(\"type\")\n \n         if show_secrets is False and handler_meta is not None:\n-            connection_args = handler_meta.get(\"connection_args\", None)\n-            if isinstance(connection_args, dict):\n-                if integration_type == HANDLER_TYPE.DATA:\n-                    for key, value in connection_args.items():\n-                        if key in data and value.get(\"secret\", False) is True:\n-                            data[key] = \"******\"\n-                elif integration_type == HANDLER_TYPE.ML:\n-                    creation_args = connection_args.get(\"creation_args\")\n-                    if isinstance(creation_args, dict):\n-                        for key, value in creation_args.items():\n-                            if key in data and value.get(\"secret\", False) is True:\n-                                data[key] = \"******\"\n-                else:\n-                    raise ValueError(f\"Unexpected handler type: {integration_type}\")\n-            else:\n-                # region obsolete, del in future\n-                if \"password\" in data:\n-                    data[\"password\"] = None\n-                if (\n-                    data.get(\"type\") == \"redis\"\n-                    and isinstance(data.get(\"connection\"), dict)\n-                    and \"password\" in data[\"connection\"]\n-                ):\n-                    data[\"connection\"] = None\n-                # endregion\n+            logger.warning(f\"show_secrets parameter has been deprecated. Value of {show_secrets} will be unused.\")",
    "comment": "## Clear-text logging of sensitive information\n\nThis expression logs [sensitive data (secret)](1) as clear text.\nThis expression logs [sensitive data (secret)](2) as clear text.\nThis expression logs [sensitive data (secret)](3) as clear text.\nThis expression logs [sensitive data (secret)](4) as clear text.\nThis expression logs [sensitive data (secret)](5) as clear text.\nThis expression logs [sensitive data (secret)](6) as clear text.\nThis expression logs [sensitive data (secret)](7) as clear text.\nThis expression logs [sensitive data (secret)](8) as clear text.\n\n[Show more details](https://github.com/mindsdb/mindsdb/security/code-scanning/113)",
    "line_number": 303,
    "enriched": "File: mindsdb/interfaces/database/integrations.py\nCode: @@ -300,31 +300,7 @@\n             integration_type = handler_meta.get(\"type\")\n \n         if show_secrets is False and handler_meta is not None:\n-            connection_args = handler_meta.get(\"connection_args\", None)\n-            if isinstance(connection_args, dict):\n-                if integration_type == HANDLER_TYPE.DATA:\n-                    for key, value in connection_args.items():\n-                        if key in data and value.get(\"secret\", False) is True:\n-                            data[key] = \"******\"\n-                elif integration_type == HANDLER_TYPE.ML:\n-                    creation_args = connection_args.get(\"creation_args\")\n-                    if isinstance(creation_args, dict):\n-                        for key, value in creation_args.items():\n-                            if key in data and value.get(\"secret\", False) is True:\n-                                data[key] = \"******\"\n-                else:\n-                    raise ValueError(f\"Unexpected handler type: {integration_type}\")\n-            else:\n-                # region obsolete, del in future\n-                if \"password\" in data:\n-                    data[\"password\"] = None\n-                if (\n-                    data.get(\"type\") == \"redis\"\n-                    and isinstance(data.get(\"connection\"), dict)\n-                    and \"password\" in data[\"connection\"]\n-                ):\n-                    data[\"connection\"] = None\n-                # endregion\n+            logger.warning(f\"show_secrets parameter has been deprecated. Value of {show_secrets} will be unused.\")\nComment: ## Clear-text logging of sensitive information\n\nThis expression logs [sensitive data (secret)](1) as clear text.\nThis expression logs [sensitive data (secret)](2) as clear text.\nThis expression logs [sensitive data (secret)](3) as clear text.\nThis expression logs [sensitive data (secret)](4) as clear text.\nThis expression logs [sensitive data (secret)](5) as clear text.\nThis expression logs [sensitive data (secret)](6) as clear text.\nThis expression logs [sensitive data (secret)](7) as clear text.\nThis expression logs [sensitive data (secret)](8) as clear text.\n\n[Show more details](https://github.com/mindsdb/mindsdb/security/code-scanning/113)",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/interfaces/database/integrations.py",
    "pr_number": 11140,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2159705219,
    "comment_created_at": "2025-06-20T21:54:25Z"
  },
  {
    "code": "@@ -0,0 +1,200 @@\n+\"\"\"API routes for MindsDB REST API.\"\"\"\r\n+import logging\r\n+from typing import Any, Dict, List\r\n+from fastapi import APIRouter, HTTPException, status\r\n+\r\n+\r\n+from api.models import ChatCompletionRequest, ChatCompletionResponse, ChatInitiateResponse, ChatSessionRequest, HealthResponse, SearchRequest\r\n+from utils import build_create_agent_query, build_custom_prompt_template, build_psql_select_query, build_query_for_kb, generate_random_agent_name, replace_punctuation_with_underscore, transform_results\r\n+from config import get_config\r\n+from mindsdb import get_mindsdb_client\r\n+\r\n+router = APIRouter()\r\n+logger = logging.getLogger(__name__)\r\n+\r\n+\r\n+@router.get(\"/health\", response_model=HealthResponse)\r\n+async def health_check():\r\n+    \"\"\"Health check endpoint.\"\"\"\r\n+    client = get_mindsdb_client()\r\n+    \r\n+    return HealthResponse(\r\n+        status=\"healthy\" if client.is_connected() else \"disconnected\",\r\n+        connected=client.is_connected(),\r\n+        message=\"MindsDB client is ready\" if client.is_connected() else \"MindsDB client not connected\"\r\n+    )\r\n+\r\n+@router.post(\"/search\", response_model=List[Dict[str, Any]])\r\n+async def search_knowledge_base(request: SearchRequest):\r\n+    \"\"\"Search the knowledge base with the given query and filters.\r\n+    \r\n+    Args:\r\n+        request: Search request containing query and optional filters\r\n+        \r\n+    Returns:\r\n+        Search results from the knowledge base\r\n+        \r\n+    Example:\r\n+        ```\r\n+        POST /search\r\n+        {\r\n+          \"query\": \"machine learning\",\r\n+          \"filters\": {\r\n+            \"isHybridSearch\": true,\r\n+            \"alpha\": 0.7,\r\n+            \"corpus\": {\r\n+              \"arxiv\": true,\r\n+              \"patent\": false\r\n+            },\r\n+            \"publishedYear\": \"2024\",\r\n+            \"category\": \"cs.LG\"\r\n+          }\r\n+        }\r\n+        ```\r\n+    \"\"\"\r\n+    client = get_mindsdb_client()\r\n+    config = get_config()\r\n+    \r\n+    if not client.is_connected():\r\n+        raise HTTPException(\r\n+            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,\r\n+            detail=\"Not connected to MindsDB\"\r\n+        )\r\n+    \r\n+    # Get knowledge base name from configuration\r\n+    kb_name = config.get(\"knowledge_base.name\", \"my_pg_kb\")\r\n+    \r\n+    query_clause = f\"content = '{request.query}'\";\r\n+    clauses = [query_clause]\r\n+    if request.filters:\r\n+        filters = request.filters\r\n+\r\n+        if filters.isHybridSearch:\r\n+            clauses.append(f\"hybrid_search = true AND hybrid_search_alpha = {filters.alpha}\")\r\n+    \r\n+        corpus_filters = filters.corpus.model_dump()\r\n+        if not all(corpus_filters.values()):\r\n+            source_clause = [\r\n+                f\"source = '{source}'\" \r\n+                for source, enabled in corpus_filters.items() \r\n+                if enabled\r\n+            ]\r\n+            if source_clause:\r\n+                clauses.append(f\"({' OR '.join(source_clause)})\")\r\n+        \r\n+        if filters.publishedYear:\r\n+            clauses.append(f\"published_year = '{filters.publishedYear}'\")\r\n+        \r\n+        if filters.category:\r\n+            clauses.append(f\"categories LIKE '%{filters.category.lower()}%'\")\r\n+    \r\n+    search_query = f\"\"\"\r\n+SELECT article_id, metadata, relevance FROM {kb_name}\r\n+WHERE {\" AND \".join(clauses)};\r\n+\"\"\"\r\n+    logger.info(f\"search query - {search_query}\")\r\n+\r\n+    try:\r\n+        # Execute query\r\n+        mdb_query = client.query(search_query)\r\n+        \r\n+        # Convert result to list of dictionaries\r\n+        result = mdb_query.fetch()\r\n+        data = result.to_dict(\"records\")\r\n+        return transform_results(data)\r\n+    except Exception as e:\r\n+        raise HTTPException(\r\n+            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\r\n+            detail=f\"Search failed: {str(e)}\"\r\n+        )\r\n+\r\n+@router.post(\"/chat/initiate\", response_model=ChatInitiateResponse)\r\n+async def initiate_chat(request: ChatSessionRequest):\r\n+\r\n+    client = get_mindsdb_client()\r\n+    \r\n+    if not client.is_connected():\r\n+        raise HTTPException(\r\n+            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,\r\n+            detail=\"Not connected to MindsDB\"\r\n+        )\r\n+\r\n+    res = []\r\n+    kb_list = []\r\n+    prompt_template_list = []\r\n+    for paper in request.papers:\r\n+\r\n+        kb_name = f\"{replace_punctuation_with_underscore(paper.id)}_kb\".lower()\r\n+        create_kb_query = build_query_for_kb(kb_name, 'create')\r\n+        \r\n+        result = client.query(create_kb_query)\r\n+        _ = result.fetch()\r\n+\r\n+        insert_kb_query = build_query_for_kb(kb_name, 'insert')\r\n+        add_clause = f\"WHERE article_id = '{paper.id}' AND source = '{paper.source}'\";\r\n+        insert_kb_query = f\"{insert_kb_query} {add_clause}\"\r\n+        \r\n+        result = client.query(insert_kb_query)\r\n+        _ = result.fetch()\r\n+\r\n+        create_kb_index_query = f\"CREATE INDEX ON KNOWLEDGE_BASE {kb_name};\"\r\n+        result = client.query(create_kb_index_query)\r\n+        _ = result.fetch()\r\n+\r\n+        psql_query = build_psql_select_query()\r\n+        psql_query = f\"{psql_query} {add_clause}\"\r\n+        \r\n+        result = client.query(psql_query)\r\n+        data = result.fetch()\r\n+\r\n+        data = data.to_dict(\"records\")\r\n+\r\n+        paper_res = {}\r\n+        paper_res[\"paperUrl\"] = data[0].get(\"pdf_url\")\r\n+        paper_res[\"title\"] = data[0].get(\"title\")\r\n+        paper_res[\"source\"] = data[0].get(\"source\")\r\n+        paper_res[\"paperId\"] = paper.id\r",
    "comment": "**correctness**: `data[0]` is accessed without checking if `data` is empty, which will cause an `IndexError` if no records are returned for a paper.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb hacktoberfest/use-cases/semantic-academic-search-engine/backend/api/routes.py, lines 152-156, the code accesses data[0] without checking if data is empty, which can cause an IndexError if no records are returned for a paper. Please add a check to ensure data is not empty before accessing data[0], and handle the case where no data is found by setting the paper fields to None or appropriate defaults.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n        paper_res = {}\n        if not data:\n            logger.warning(f\"No data found for paper id {paper.id} and source {paper.source}\")\n            paper_res[\"paperUrl\"] = None\n            paper_res[\"title\"] = None\n            paper_res[\"source\"] = paper.source\n            paper_res[\"paperId\"] = paper.id\n        else:\n            paper_res[\"paperUrl\"] = data[0].get(\"pdf_url\")\n            paper_res[\"title\"] = data[0].get(\"title\")\n            paper_res[\"source\"] = data[0].get(\"source\")\n            paper_res[\"paperId\"] = paper.id\n```\n</details>\n<!-- suggestion_end -->\n",
    "line_number": 156,
    "enriched": "File: mindsdb hacktoberfest/use-cases/semantic-academic-search-engine/backend/api/routes.py\nCode: @@ -0,0 +1,200 @@\n+\"\"\"API routes for MindsDB REST API.\"\"\"\r\n+import logging\r\n+from typing import Any, Dict, List\r\n+from fastapi import APIRouter, HTTPException, status\r\n+\r\n+\r\n+from api.models import ChatCompletionRequest, ChatCompletionResponse, ChatInitiateResponse, ChatSessionRequest, HealthResponse, SearchRequest\r\n+from utils import build_create_agent_query, build_custom_prompt_template, build_psql_select_query, build_query_for_kb, generate_random_agent_name, replace_punctuation_with_underscore, transform_results\r\n+from config import get_config\r\n+from mindsdb import get_mindsdb_client\r\n+\r\n+router = APIRouter()\r\n+logger = logging.getLogger(__name__)\r\n+\r\n+\r\n+@router.get(\"/health\", response_model=HealthResponse)\r\n+async def health_check():\r\n+    \"\"\"Health check endpoint.\"\"\"\r\n+    client = get_mindsdb_client()\r\n+    \r\n+    return HealthResponse(\r\n+        status=\"healthy\" if client.is_connected() else \"disconnected\",\r\n+        connected=client.is_connected(),\r\n+        message=\"MindsDB client is ready\" if client.is_connected() else \"MindsDB client not connected\"\r\n+    )\r\n+\r\n+@router.post(\"/search\", response_model=List[Dict[str, Any]])\r\n+async def search_knowledge_base(request: SearchRequest):\r\n+    \"\"\"Search the knowledge base with the given query and filters.\r\n+    \r\n+    Args:\r\n+        request: Search request containing query and optional filters\r\n+        \r\n+    Returns:\r\n+        Search results from the knowledge base\r\n+        \r\n+    Example:\r\n+        ```\r\n+        POST /search\r\n+        {\r\n+          \"query\": \"machine learning\",\r\n+          \"filters\": {\r\n+            \"isHybridSearch\": true,\r\n+            \"alpha\": 0.7,\r\n+            \"corpus\": {\r\n+              \"arxiv\": true,\r\n+              \"patent\": false\r\n+            },\r\n+            \"publishedYear\": \"2024\",\r\n+            \"category\": \"cs.LG\"\r\n+          }\r\n+        }\r\n+        ```\r\n+    \"\"\"\r\n+    client = get_mindsdb_client()\r\n+    config = get_config()\r\n+    \r\n+    if not client.is_connected():\r\n+        raise HTTPException(\r\n+            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,\r\n+            detail=\"Not connected to MindsDB\"\r\n+        )\r\n+    \r\n+    # Get knowledge base name from configuration\r\n+    kb_name = config.get(\"knowledge_base.name\", \"my_pg_kb\")\r\n+    \r\n+    query_clause = f\"content = '{request.query}'\";\r\n+    clauses = [query_clause]\r\n+    if request.filters:\r\n+        filters = request.filters\r\n+\r\n+        if filters.isHybridSearch:\r\n+            clauses.append(f\"hybrid_search = true AND hybrid_search_alpha = {filters.alpha}\")\r\n+    \r\n+        corpus_filters = filters.corpus.model_dump()\r\n+        if not all(corpus_filters.values()):\r\n+            source_clause = [\r\n+                f\"source = '{source}'\" \r\n+                for source, enabled in corpus_filters.items() \r\n+                if enabled\r\n+            ]\r\n+            if source_clause:\r\n+                clauses.append(f\"({' OR '.join(source_clause)})\")\r\n+        \r\n+        if filters.publishedYear:\r\n+            clauses.append(f\"published_year = '{filters.publishedYear}'\")\r\n+        \r\n+        if filters.category:\r\n+            clauses.append(f\"categories LIKE '%{filters.category.lower()}%'\")\r\n+    \r\n+    search_query = f\"\"\"\r\n+SELECT article_id, metadata, relevance FROM {kb_name}\r\n+WHERE {\" AND \".join(clauses)};\r\n+\"\"\"\r\n+    logger.info(f\"search query - {search_query}\")\r\n+\r\n+    try:\r\n+        # Execute query\r\n+        mdb_query = client.query(search_query)\r\n+        \r\n+        # Convert result to list of dictionaries\r\n+        result = mdb_query.fetch()\r\n+        data = result.to_dict(\"records\")\r\n+        return transform_results(data)\r\n+    except Exception as e:\r\n+        raise HTTPException(\r\n+            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\r\n+            detail=f\"Search failed: {str(e)}\"\r\n+        )\r\n+\r\n+@router.post(\"/chat/initiate\", response_model=ChatInitiateResponse)\r\n+async def initiate_chat(request: ChatSessionRequest):\r\n+\r\n+    client = get_mindsdb_client()\r\n+    \r\n+    if not client.is_connected():\r\n+        raise HTTPException(\r\n+            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,\r\n+            detail=\"Not connected to MindsDB\"\r\n+        )\r\n+\r\n+    res = []\r\n+    kb_list = []\r\n+    prompt_template_list = []\r\n+    for paper in request.papers:\r\n+\r\n+        kb_name = f\"{replace_punctuation_with_underscore(paper.id)}_kb\".lower()\r\n+        create_kb_query = build_query_for_kb(kb_name, 'create')\r\n+        \r\n+        result = client.query(create_kb_query)\r\n+        _ = result.fetch()\r\n+\r\n+        insert_kb_query = build_query_for_kb(kb_name, 'insert')\r\n+        add_clause = f\"WHERE article_id = '{paper.id}' AND source = '{paper.source}'\";\r\n+        insert_kb_query = f\"{insert_kb_query} {add_clause}\"\r\n+        \r\n+        result = client.query(insert_kb_query)\r\n+        _ = result.fetch()\r\n+\r\n+        create_kb_index_query = f\"CREATE INDEX ON KNOWLEDGE_BASE {kb_name};\"\r\n+        result = client.query(create_kb_index_query)\r\n+        _ = result.fetch()\r\n+\r\n+        psql_query = build_psql_select_query()\r\n+        psql_query = f\"{psql_query} {add_clause}\"\r\n+        \r\n+        result = client.query(psql_query)\r\n+        data = result.fetch()\r\n+\r\n+        data = data.to_dict(\"records\")\r\n+\r\n+        paper_res = {}\r\n+        paper_res[\"paperUrl\"] = data[0].get(\"pdf_url\")\r\n+        paper_res[\"title\"] = data[0].get(\"title\")\r\n+        paper_res[\"source\"] = data[0].get(\"source\")\r\n+        paper_res[\"paperId\"] = paper.id\r\nComment: **correctness**: `data[0]` is accessed without checking if `data` is empty, which will cause an `IndexError` if no records are returned for a paper.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb hacktoberfest/use-cases/semantic-academic-search-engine/backend/api/routes.py, lines 152-156, the code accesses data[0] without checking if data is empty, which can cause an IndexError if no records are returned for a paper. Please add a check to ensure data is not empty before accessing data[0], and handle the case where no data is found by setting the paper fields to None or appropriate defaults.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n        paper_res = {}\n        if not data:\n            logger.warning(f\"No data found for paper id {paper.id} and source {paper.source}\")\n            paper_res[\"paperUrl\"] = None\n            paper_res[\"title\"] = None\n            paper_res[\"source\"] = paper.source\n            paper_res[\"paperId\"] = paper.id\n        else:\n            paper_res[\"paperUrl\"] = data[0].get(\"pdf_url\")\n            paper_res[\"title\"] = data[0].get(\"title\")\n            paper_res[\"source\"] = data[0].get(\"source\")\n            paper_res[\"paperId\"] = paper.id\n```\n</details>\n<!-- suggestion_end -->\n",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb hacktoberfest/use-cases/semantic-academic-search-engine/backend/api/routes.py",
    "pr_number": 11845,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2485139687,
    "comment_created_at": "2025-11-03T01:08:38Z"
  },
  {
    "code": "@@ -110,7 +110,8 @@ RUN python -m pip install --prefer-binary --no-cache-dir --upgrade pip==23.1.2 &\n     pip install --prefer-binary --no-cache-dir python-gitlab || true && \\\n     pip install --prefer-binary --no-cache-dir 'chromadb~=0.4.8' 'pysqlite3-binary' || true && \\\n     pip install --prefer-binary --no-cache-dir 'cohere==4.5.1' || true && \\\n-    pip install --prefer-binary --no-cache-dir 'polars' || true\n+    pip install --prefer-binary --no-cache-dir 'polars' || true && \\\n+    pip install --prefer-binary --no-cache-dir 'lightfm==1.17' 'dataprep_ml>=0.0.15' 'scipy>=1.10.1' 'pydantic~=1.10.8' || true && \\",
    "comment": "dont need `&& \\` at hte end\r\nalso need del `prints`",
    "line_number": 114,
    "enriched": "File: docker/release\nCode: @@ -110,7 +110,8 @@ RUN python -m pip install --prefer-binary --no-cache-dir --upgrade pip==23.1.2 &\n     pip install --prefer-binary --no-cache-dir python-gitlab || true && \\\n     pip install --prefer-binary --no-cache-dir 'chromadb~=0.4.8' 'pysqlite3-binary' || true && \\\n     pip install --prefer-binary --no-cache-dir 'cohere==4.5.1' || true && \\\n-    pip install --prefer-binary --no-cache-dir 'polars' || true\n+    pip install --prefer-binary --no-cache-dir 'polars' || true && \\\n+    pip install --prefer-binary --no-cache-dir 'lightfm==1.17' 'dataprep_ml>=0.0.15' 'scipy>=1.10.1' 'pydantic~=1.10.8' || true && \\\nComment: dont need `&& \\` at hte end\r\nalso need del `prints`",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "docker/release",
    "pr_number": 8334,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1390838785,
    "comment_created_at": "2023-11-13T09:36:38Z"
  },
  {
    "code": "@@ -275,12 +279,6 @@ def post(self, project_name, agent_name):\n         if 'mode' not in existing_agent.params and any(skill.type == 'retrieval' for skill in existing_agent.skills):\n             existing_agent.params['mode'] = 'retrieval'\n \n-        # get model details\n-        session = SessionController()\n-        model_name_no_version, version = db.Predictor.get_name_and_version(existing_agent.model_name)\n-        agent_model = session.model_controller.get_model(model_name_no_version, version=version, project_name=project_name)  # noqa\n-        model_using = agent_model.get('problem_definition', {}).get('using', {})\n-",
    "comment": "@paxcema  these changes were introduced as part of https://github.com/mindsdb/mindsdb/pull/9496/files cause conflict with agents not using langchain_handler model. i.e.\r\n\r\n```\r\nCREATE AGENT my_agent\r\n            USING\r\n             provider='openai',\r\n             model = \"gpt-3.5-turbo\",\r\n             openai_api_key='--',\r\n             prompt_template=\"Answer the user input in a helpful way\"\r\n```\r\n\r\nAs a temp fix, moving to line 290. It will need to be addressed in a subsequent PR.\r\n\r\nWe now have tests to avoid this happening again ",
    "line_number": 404,
    "enriched": "File: mindsdb/api/http/namespaces/agents.py\nCode: @@ -275,12 +279,6 @@ def post(self, project_name, agent_name):\n         if 'mode' not in existing_agent.params and any(skill.type == 'retrieval' for skill in existing_agent.skills):\n             existing_agent.params['mode'] = 'retrieval'\n \n-        # get model details\n-        session = SessionController()\n-        model_name_no_version, version = db.Predictor.get_name_and_version(existing_agent.model_name)\n-        agent_model = session.model_controller.get_model(model_name_no_version, version=version, project_name=project_name)  # noqa\n-        model_using = agent_model.get('problem_definition', {}).get('using', {})\n-\nComment: @paxcema  these changes were introduced as part of https://github.com/mindsdb/mindsdb/pull/9496/files cause conflict with agents not using langchain_handler model. i.e.\r\n\r\n```\r\nCREATE AGENT my_agent\r\n            USING\r\n             provider='openai',\r\n             model = \"gpt-3.5-turbo\",\r\n             openai_api_key='--',\r\n             prompt_template=\"Answer the user input in a helpful way\"\r\n```\r\n\r\nAs a temp fix, moving to line 290. It will need to be addressed in a subsequent PR.\r\n\r\nWe now have tests to avoid this happening again ",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/api/http/namespaces/agents.py",
    "pr_number": 9555,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1694763773,
    "comment_created_at": "2024-07-29T07:55:03Z"
  },
  {
    "code": "@@ -878,7 +908,11 @@ def convert_row_to_metadata(row):\n         logger.debug(f\"Output DataFrame columns: {df_out.columns}\")\n         logger.debug(f\"Output DataFrame first row: {df_out.iloc[0].to_dict() if not df_out.empty else 'Empty'}\")\n \n-        return df_out\n+        # return adapted dataframe and normalized case insensitive columns\n+        return df_out, {\n+            \"content_columns\": content_columns,\n+            \"metadata_columns\": metadata_columns,\n+        }",
    "comment": "**correctness**: `_adapt_column_names` now returns a tuple, but all callers except one expect a DataFrame only; this will cause runtime errors (e.g., unpacking or attribute errors) when those callers receive a tuple instead of a DataFrame.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/interfaces/knowledge_base/controller.py, lines 910-915, the function `_adapt_column_names` was changed to return a tuple (DataFrame, dict), but all callers except one expect only a DataFrame. This will cause runtime errors when those callers receive a tuple. Change the return statement back to `return df_out` so the function only returns the DataFrame as before.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n        return df_out\n```\n</details>\n<!-- suggestion_end -->\n",
    "line_number": 915,
    "enriched": "File: mindsdb/interfaces/knowledge_base/controller.py\nCode: @@ -878,7 +908,11 @@ def convert_row_to_metadata(row):\n         logger.debug(f\"Output DataFrame columns: {df_out.columns}\")\n         logger.debug(f\"Output DataFrame first row: {df_out.iloc[0].to_dict() if not df_out.empty else 'Empty'}\")\n \n-        return df_out\n+        # return adapted dataframe and normalized case insensitive columns\n+        return df_out, {\n+            \"content_columns\": content_columns,\n+            \"metadata_columns\": metadata_columns,\n+        }\nComment: **correctness**: `_adapt_column_names` now returns a tuple, but all callers except one expect a DataFrame only; this will cause runtime errors (e.g., unpacking or attribute errors) when those callers receive a tuple instead of a DataFrame.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/interfaces/knowledge_base/controller.py, lines 910-915, the function `_adapt_column_names` was changed to return a tuple (DataFrame, dict), but all callers except one expect only a DataFrame. This will cause runtime errors when those callers receive a tuple. Change the return statement back to `return df_out` so the function only returns the DataFrame as before.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n        return df_out\n```\n</details>\n<!-- suggestion_end -->\n",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/interfaces/knowledge_base/controller.py",
    "pr_number": 11784,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2460590359,
    "comment_created_at": "2025-10-24T13:51:44Z"
  },
  {
    "code": "@@ -125,6 +137,8 @@ def _connect_duckdb(self) -> DuckDBPyConnection:\n         \"\"\"\n         # Connect to S3 via DuckDB.\n         duckdb_conn = duckdb.connect(\":memory:\")\n+        duckdb_conn.execute(\"INSTALL httpfs\")\n+        duckdb_conn.execute(\"LOAD httpfs\")",
    "comment": "@MinuraPunchihewa \r\na question about this: should we keep it?\r\nI removed it in previous PR, but yesterday Mauricio said he had error on docker version. \r\nToday I tested docker version and it worked for me (it is latest release without these lines )",
    "line_number": 141,
    "enriched": "File: mindsdb/integrations/handlers/s3_handler/s3_handler.py\nCode: @@ -125,6 +137,8 @@ def _connect_duckdb(self) -> DuckDBPyConnection:\n         \"\"\"\n         # Connect to S3 via DuckDB.\n         duckdb_conn = duckdb.connect(\":memory:\")\n+        duckdb_conn.execute(\"INSTALL httpfs\")\n+        duckdb_conn.execute(\"LOAD httpfs\")\nComment: @MinuraPunchihewa \r\na question about this: should we keep it?\r\nI removed it in previous PR, but yesterday Mauricio said he had error on docker version. \r\nToday I tested docker version and it worked for me (it is latest release without these lines )",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/s3_handler/s3_handler.py",
    "pr_number": 9879,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1803289826,
    "comment_created_at": "2024-10-16T14:57:38Z"
  },
  {
    "code": "@@ -412,7 +417,10 @@ def process_table(self, item, query_in):\n             # not use conditions\n             conditions = []\n \n-        conditions += self.get_filters_from_join_conditions(item)\n+        # For cross-database joins, skip the IN clause optimization\n+        # Reason: We can't predict row counts, and building large IN clauses causes errors\n+        # Let the join happen in memory without filter pushdown\n+        # conditions += self.get_filters_from_join_conditions(item, query_in.using)",
    "comment": "**Security**: The line that calls get_filters_from_join_conditions() is commented out, disabling an important optimization without proper feature flagging. This creates a maintenance burden and could lead to unintended reactivation of potentially problematic code.\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n        # Option 1: Replace with feature flag approach\n        def process_table_with_feature_flag(self, item, query_in):\n            \"\"\"\n            Fixed version that replaces commented code with proper feature flag.\n            \"\"\"\n            # Use a configuration flag instead of commenting out code\n            if self.planner.config.get('enable_in_clause_optimization', False):\n                # Only apply the optimization if explicitly enabled\n                filters = self.get_filters_from_join_conditions(item, query_in)\n                if filters is not None:\n                    item['filters'].extend(filters)\n            # Rest of the function continues as normal\n```\n</details>\n<!-- suggestion_end -->\n",
    "line_number": 423,
    "enriched": "File: mindsdb/api/executor/planner/plan_join.py\nCode: @@ -412,7 +417,10 @@ def process_table(self, item, query_in):\n             # not use conditions\n             conditions = []\n \n-        conditions += self.get_filters_from_join_conditions(item)\n+        # For cross-database joins, skip the IN clause optimization\n+        # Reason: We can't predict row counts, and building large IN clauses causes errors\n+        # Let the join happen in memory without filter pushdown\n+        # conditions += self.get_filters_from_join_conditions(item, query_in.using)\nComment: **Security**: The line that calls get_filters_from_join_conditions() is commented out, disabling an important optimization without proper feature flagging. This creates a maintenance burden and could lead to unintended reactivation of potentially problematic code.\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n        # Option 1: Replace with feature flag approach\n        def process_table_with_feature_flag(self, item, query_in):\n            \"\"\"\n            Fixed version that replaces commented code with proper feature flag.\n            \"\"\"\n            # Use a configuration flag instead of commenting out code\n            if self.planner.config.get('enable_in_clause_optimization', False):\n                # Only apply the optimization if explicitly enabled\n                filters = self.get_filters_from_join_conditions(item, query_in)\n                if filters is not None:\n                    item['filters'].extend(filters)\n            # Rest of the function continues as normal\n```\n</details>\n<!-- suggestion_end -->\n",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/api/executor/planner/plan_join.py",
    "pr_number": 11771,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2444856575,
    "comment_created_at": "2025-10-20T12:22:21Z"
  },
  {
    "code": "@@ -0,0 +1,201 @@\n+import importlib\n+from typing import Dict, Union\n+\n+import pandas as pd\n+from langchain.embeddings.base import Embeddings\n+from pandas import DataFrame\n+\n+from mindsdb.integrations.libs.base import BaseMLEngine\n+from mindsdb.utilities.log import get_log\n+\n+LOG = get_log(\"langchain_embedding\")\n+\n+# construct the embedding model name to the class mapping\n+# we try to import all embedding models from langchain.embeddings\n+# for each class, we get a more user friendly name for it\n+# E.g. OpenAIEmbeddings -> OpenAI\n+# This is used for the user to select the embedding model\n+EMBEDDING_MODELS = {}\n+\n+try:\n+    module = importlib.import_module(\"langchain.embeddings\")\n+    # iterate __all__ to get all the classes\n+    for class_name in module.__all__:\n+        class_ = getattr(module, class_name)\n+        if not issubclass(class_, Embeddings):\n+            continue\n+        # convert the class name to a more user friendly name\n+        # e.g. OpenAIEmbeddings -> OpenAI\n+        user_friendly_name = class_name.replace(\"Embeddings\", \"\")\n+        EMBEDDING_MODELS[user_friendly_name] = class_name\n+        EMBEDDING_MODELS[user_friendly_name.lower()] = class_name\n+\n+except ImportError:\n+    raise Exception(\n+        \"The langchain is not installed. Please install it with `pip install langchain`.\"\n+    )\n+\n+\n+def get_langchain_class(class_name: str) -> Embeddings:\n+    \"\"\"Returns the class object of the handler class.\n+\n+    Args:\n+        class_name (str): Name of the class\n+\n+    Returns:\n+        langchain.embeddings.BaseEmbedding: The class object\n+    \"\"\"\n+    try:\n+        module = importlib.import_module(\"langchain.embeddings\")\n+        class_ = getattr(module, class_name)\n+    except ImportError:\n+        raise Exception(\n+            \"The langchain is not installed. Please install it with `pip install langchain`.\"\n+        )\n+    except AttributeError:\n+        raise Exception(\n+            f\"Could not find the class {class_name} in langchain.embeddings. Please check the class name.\"\n+        )\n+    return class_\n+\n+\n+def construct_model_from_args(args: Dict) -> Embeddings:\n+    \"\"\"\n+    Deserializes the model from the model storage\n+    \"\"\"\n+    target = args.pop(\"target\", None)\n+    class_name = args.pop(\"class\", LangchainEmbeddingHandler.DEFAULT_EMBEDDING_CLASS)\n+    if class_name in EMBEDDING_MODELS:\n+        LOG.info(\n+            f\"Mapping the user friendly name {class_name} to the class name: {EMBEDDING_MODELS[class_name]}\"\n+        )\n+        class_name = EMBEDDING_MODELS[class_name]\n+    MODEL_CLASS = get_langchain_class(class_name)\n+    serialized_dict = args\n+    model = MODEL_CLASS(**serialized_dict)\n+    if target is not None:\n+        args[\"target\"] = target\n+    args[\"class\"] = class_name\n+    return model\n+\n+\n+class LangchainEmbeddingHandler(BaseMLEngine):\n+    \"\"\"\n+    Bridge class to connect langchain.embeddings module to mindsDB\n+    \"\"\"\n+\n+    DEFAULT_EMBEDDING_CLASS = \"OpenAIEmbeddings\"\n+\n+    def __init__(self, model_storage, engine_storage, **kwargs) -> None:\n+        super().__init__(model_storage, engine_storage, **kwargs)\n+\n+    def create(\n+        self,\n+        target: str,\n+        df: Union[DataFrame, None] = None,\n+        args: Union[Dict, None] = None,\n+    ) -> None:\n+        # get the class name from the args\n+        user_args = args.get(\"using\", {})\n+\n+        # infer the input columns arg if user did not provide it\n+        # from the columns of the input dataframe if it is provided\n+        if \"input_columns\" not in user_args and df is not None:\n+            # ignore private columns starts with __mindsdb\n+            # ignore target column in the input dataframe\n+            user_args[\"input_columns\"] = [\n+                col\n+                for col in df.columns.tolist()\n+                if not col.startswith(\"__mindsdb\") and col != target\n+            ]\n+            # unquote the column names -- removing surrounding `\n+            user_args[\"input_columns\"] = [\n+                col.strip(\"`\") for col in user_args[\"input_columns\"]\n+            ]\n+\n+        elif \"input_columns\" not in user_args:\n+            # set as empty list if the input_columns is not provided\n+            user_args[\"input_columns\"] = []\n+\n+        # this may raise an exception if\n+        # the arguments are not sufficient to create such as class\n+        # due to e.g., lack of API key\n+        # But the validation logic is handled by langchain and pydantic\n+        construct_model_from_args(user_args)\n+\n+        # save the model to the model storage\n+        target = target or \"embeddings\"\n+        user_args[\n+            \"target\"\n+        ] = target  # this is the name of the column to store the embeddings\n+        self.model_storage.json_set(\"args\", user_args)\n+\n+    def predict(self, df: DataFrame, args) -> DataFrame:\n+        # reconstruct the model from the model storage\n+        user_args = self.model_storage.json_get(\"args\")\n+        model = construct_model_from_args(user_args)\n+\n+        # get the target from the model storage\n+        target = user_args[\"target\"]\n+        # run the actual embedding vector generation\n+        # TODO: need a better way to handle this\n+        # unquote the column names -- removing surrounding `\n+        cols_dfs = [col.strip(\"`\") for col in df.columns.tolist()]\n+        df.columns = cols_dfs\n+        # if input_columns is an empty list, use all the columns\n+        input_columns = user_args.get(\"input_columns\") or df.columns.tolist()\n+        # check all the input columns are in the df\n+        if not all(\n+            # ignore surrounding ` in the column names when checking\n+            [col in cols_dfs for col in input_columns]\n+        ):\n+            raise Exception(\n+                f\"Input columns {input_columns} not found in the input dataframe. Available columns are {df.columns}\"\n+            )\n+\n+        # convert each row into a document\n+        df_texts = df[input_columns].apply(self.row_to_document, axis=1)\n+        embeddings = model.embed_documents(df_texts.tolist())\n+\n+        # create a new dataframe with the embeddings\n+        df_embeddings = df.copy().assign(**{target: embeddings})\n+\n+        return df_embeddings\n+\n+    def row_to_document(self, row: pd.Series) -> str:\n+        \"\"\"\n+        Convert a row in the input dataframe into a document\n+\n+        Default implementation is to concatenate all the columns\n+        in the form of\n+        field1: value1\\nfield2: value2\\n...\n+        \"\"\"\n+        fields = row.index.tolist()\n+        values = row.values.tolist()\n+        document = \"\\n\".join(\n+            [f\"{field}: {value}\" for field, value in zip(fields, values)]\n+        )\n+        return document\n+\n+    def finetune(\n+        self, df: Union[DataFrame, None] = None, args: Union[Dict, None] = None\n+    ) -> None:\n+        # re-save the model to the model storage",
    "comment": "It feels like calling this method on an embedding engine like this one should actually finetune the underlying embedding generator rather than just reindexing based on a new `df`, no?\r\n\r\nIf that were the case, then for this particular integration it may mean we have to deactivate this method entirely.",
    "line_number": 183,
    "enriched": "File: mindsdb/integrations/handlers/langchain_embedding/langchain_embedding_handler.py\nCode: @@ -0,0 +1,201 @@\n+import importlib\n+from typing import Dict, Union\n+\n+import pandas as pd\n+from langchain.embeddings.base import Embeddings\n+from pandas import DataFrame\n+\n+from mindsdb.integrations.libs.base import BaseMLEngine\n+from mindsdb.utilities.log import get_log\n+\n+LOG = get_log(\"langchain_embedding\")\n+\n+# construct the embedding model name to the class mapping\n+# we try to import all embedding models from langchain.embeddings\n+# for each class, we get a more user friendly name for it\n+# E.g. OpenAIEmbeddings -> OpenAI\n+# This is used for the user to select the embedding model\n+EMBEDDING_MODELS = {}\n+\n+try:\n+    module = importlib.import_module(\"langchain.embeddings\")\n+    # iterate __all__ to get all the classes\n+    for class_name in module.__all__:\n+        class_ = getattr(module, class_name)\n+        if not issubclass(class_, Embeddings):\n+            continue\n+        # convert the class name to a more user friendly name\n+        # e.g. OpenAIEmbeddings -> OpenAI\n+        user_friendly_name = class_name.replace(\"Embeddings\", \"\")\n+        EMBEDDING_MODELS[user_friendly_name] = class_name\n+        EMBEDDING_MODELS[user_friendly_name.lower()] = class_name\n+\n+except ImportError:\n+    raise Exception(\n+        \"The langchain is not installed. Please install it with `pip install langchain`.\"\n+    )\n+\n+\n+def get_langchain_class(class_name: str) -> Embeddings:\n+    \"\"\"Returns the class object of the handler class.\n+\n+    Args:\n+        class_name (str): Name of the class\n+\n+    Returns:\n+        langchain.embeddings.BaseEmbedding: The class object\n+    \"\"\"\n+    try:\n+        module = importlib.import_module(\"langchain.embeddings\")\n+        class_ = getattr(module, class_name)\n+    except ImportError:\n+        raise Exception(\n+            \"The langchain is not installed. Please install it with `pip install langchain`.\"\n+        )\n+    except AttributeError:\n+        raise Exception(\n+            f\"Could not find the class {class_name} in langchain.embeddings. Please check the class name.\"\n+        )\n+    return class_\n+\n+\n+def construct_model_from_args(args: Dict) -> Embeddings:\n+    \"\"\"\n+    Deserializes the model from the model storage\n+    \"\"\"\n+    target = args.pop(\"target\", None)\n+    class_name = args.pop(\"class\", LangchainEmbeddingHandler.DEFAULT_EMBEDDING_CLASS)\n+    if class_name in EMBEDDING_MODELS:\n+        LOG.info(\n+            f\"Mapping the user friendly name {class_name} to the class name: {EMBEDDING_MODELS[class_name]}\"\n+        )\n+        class_name = EMBEDDING_MODELS[class_name]\n+    MODEL_CLASS = get_langchain_class(class_name)\n+    serialized_dict = args\n+    model = MODEL_CLASS(**serialized_dict)\n+    if target is not None:\n+        args[\"target\"] = target\n+    args[\"class\"] = class_name\n+    return model\n+\n+\n+class LangchainEmbeddingHandler(BaseMLEngine):\n+    \"\"\"\n+    Bridge class to connect langchain.embeddings module to mindsDB\n+    \"\"\"\n+\n+    DEFAULT_EMBEDDING_CLASS = \"OpenAIEmbeddings\"\n+\n+    def __init__(self, model_storage, engine_storage, **kwargs) -> None:\n+        super().__init__(model_storage, engine_storage, **kwargs)\n+\n+    def create(\n+        self,\n+        target: str,\n+        df: Union[DataFrame, None] = None,\n+        args: Union[Dict, None] = None,\n+    ) -> None:\n+        # get the class name from the args\n+        user_args = args.get(\"using\", {})\n+\n+        # infer the input columns arg if user did not provide it\n+        # from the columns of the input dataframe if it is provided\n+        if \"input_columns\" not in user_args and df is not None:\n+            # ignore private columns starts with __mindsdb\n+            # ignore target column in the input dataframe\n+            user_args[\"input_columns\"] = [\n+                col\n+                for col in df.columns.tolist()\n+                if not col.startswith(\"__mindsdb\") and col != target\n+            ]\n+            # unquote the column names -- removing surrounding `\n+            user_args[\"input_columns\"] = [\n+                col.strip(\"`\") for col in user_args[\"input_columns\"]\n+            ]\n+\n+        elif \"input_columns\" not in user_args:\n+            # set as empty list if the input_columns is not provided\n+            user_args[\"input_columns\"] = []\n+\n+        # this may raise an exception if\n+        # the arguments are not sufficient to create such as class\n+        # due to e.g., lack of API key\n+        # But the validation logic is handled by langchain and pydantic\n+        construct_model_from_args(user_args)\n+\n+        # save the model to the model storage\n+        target = target or \"embeddings\"\n+        user_args[\n+            \"target\"\n+        ] = target  # this is the name of the column to store the embeddings\n+        self.model_storage.json_set(\"args\", user_args)\n+\n+    def predict(self, df: DataFrame, args) -> DataFrame:\n+        # reconstruct the model from the model storage\n+        user_args = self.model_storage.json_get(\"args\")\n+        model = construct_model_from_args(user_args)\n+\n+        # get the target from the model storage\n+        target = user_args[\"target\"]\n+        # run the actual embedding vector generation\n+        # TODO: need a better way to handle this\n+        # unquote the column names -- removing surrounding `\n+        cols_dfs = [col.strip(\"`\") for col in df.columns.tolist()]\n+        df.columns = cols_dfs\n+        # if input_columns is an empty list, use all the columns\n+        input_columns = user_args.get(\"input_columns\") or df.columns.tolist()\n+        # check all the input columns are in the df\n+        if not all(\n+            # ignore surrounding ` in the column names when checking\n+            [col in cols_dfs for col in input_columns]\n+        ):\n+            raise Exception(\n+                f\"Input columns {input_columns} not found in the input dataframe. Available columns are {df.columns}\"\n+            )\n+\n+        # convert each row into a document\n+        df_texts = df[input_columns].apply(self.row_to_document, axis=1)\n+        embeddings = model.embed_documents(df_texts.tolist())\n+\n+        # create a new dataframe with the embeddings\n+        df_embeddings = df.copy().assign(**{target: embeddings})\n+\n+        return df_embeddings\n+\n+    def row_to_document(self, row: pd.Series) -> str:\n+        \"\"\"\n+        Convert a row in the input dataframe into a document\n+\n+        Default implementation is to concatenate all the columns\n+        in the form of\n+        field1: value1\\nfield2: value2\\n...\n+        \"\"\"\n+        fields = row.index.tolist()\n+        values = row.values.tolist()\n+        document = \"\\n\".join(\n+            [f\"{field}: {value}\" for field, value in zip(fields, values)]\n+        )\n+        return document\n+\n+    def finetune(\n+        self, df: Union[DataFrame, None] = None, args: Union[Dict, None] = None\n+    ) -> None:\n+        # re-save the model to the model storage\nComment: It feels like calling this method on an embedding engine like this one should actually finetune the underlying embedding generator rather than just reindexing based on a new `df`, no?\r\n\r\nIf that were the case, then for this particular integration it may mean we have to deactivate this method entirely.",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/langchain_embedding/langchain_embedding_handler.py",
    "pr_number": 7023,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1298913268,
    "comment_created_at": "2023-08-18T23:01:11Z"
  },
  {
    "code": "@@ -0,0 +1,18 @@\n+from mindsdb.integrations.libs.const import HANDLER_TYPE\n+\n+from mindsdb.integrations.handlers.autosklearn_handler.__about__ import __version__ as version, __description__ as description\n+try:\n+    from .huggingface_inference_api_handler import HuggingFaceInferenceAPIHandler as Handler\n+    import_error = None\n+except Exception as e:\n+    Handler = None\n+    import_error = e\n+\n+title = 'Hugging Face Inference API'\n+name = 'huggingface_inference_api'",
    "comment": "Perhaps shorten to `huggingface_api`?",
    "line_number": 12,
    "enriched": "File: mindsdb/integrations/handlers/huggingface_inference_api_handler/__init__.py\nCode: @@ -0,0 +1,18 @@\n+from mindsdb.integrations.libs.const import HANDLER_TYPE\n+\n+from mindsdb.integrations.handlers.autosklearn_handler.__about__ import __version__ as version, __description__ as description\n+try:\n+    from .huggingface_inference_api_handler import HuggingFaceInferenceAPIHandler as Handler\n+    import_error = None\n+except Exception as e:\n+    Handler = None\n+    import_error = e\n+\n+title = 'Hugging Face Inference API'\n+name = 'huggingface_inference_api'\nComment: Perhaps shorten to `huggingface_api`?",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/huggingface_inference_api_handler/__init__.py",
    "pr_number": 5609,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1164813165,
    "comment_created_at": "2023-04-13T00:52:30Z"
  },
  {
    "code": "@@ -30,8 +32,42 @@ class PgVectorHandler(VectorStoreHandler, PostgresHandler):\n     def __init__(self, name: str, **kwargs):\n \n         super().__init__(name=name, **kwargs)\n+        self._is_shared_db = False\n         self.connect()\n \n+    def _make_connection_args(self):\n+        cloud_pgvector_url = os.environ.get('KB_PGVECTOR_URL')\n+        if cloud_pgvector_url is not None:\n+            result = urlparse(cloud_pgvector_url)\n+            self.connection_args = {\n+                'host': result.hostname,\n+                'port': result.port,\n+                'user': result.username,\n+                'password': result.password,\n+                'database': result.path[1:]\n+            }\n+            self._is_shared_db = True\n+        return super()._make_connection_args()\n+\n+    def get_tables(self) -> Response:\n+        # Hide list of tables from all users\n+        if self._is_shared_db:\n+            return Response(RESPONSE_TYPE.OK)",
    "comment": "Here and below: would it be better to `raise` instead since this is technically not allowed? Silently returning nothing may be confusing.",
    "line_number": 55,
    "enriched": "File: mindsdb/integrations/handlers/pgvector_handler/pgvector_handler.py\nCode: @@ -30,8 +32,42 @@ class PgVectorHandler(VectorStoreHandler, PostgresHandler):\n     def __init__(self, name: str, **kwargs):\n \n         super().__init__(name=name, **kwargs)\n+        self._is_shared_db = False\n         self.connect()\n \n+    def _make_connection_args(self):\n+        cloud_pgvector_url = os.environ.get('KB_PGVECTOR_URL')\n+        if cloud_pgvector_url is not None:\n+            result = urlparse(cloud_pgvector_url)\n+            self.connection_args = {\n+                'host': result.hostname,\n+                'port': result.port,\n+                'user': result.username,\n+                'password': result.password,\n+                'database': result.path[1:]\n+            }\n+            self._is_shared_db = True\n+        return super()._make_connection_args()\n+\n+    def get_tables(self) -> Response:\n+        # Hide list of tables from all users\n+        if self._is_shared_db:\n+            return Response(RESPONSE_TYPE.OK)\nComment: Here and below: would it be better to `raise` instead since this is technically not allowed? Silently returning nothing may be confusing.",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/pgvector_handler/pgvector_handler.py",
    "pr_number": 9524,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1688381359,
    "comment_created_at": "2024-07-23T16:40:28Z"
  },
  {
    "code": "@@ -101,3 +101,53 @@ def get_versions_where_predictors_become_obsolete():\n def init_lexer_parsers():\n     get_lexer_parser('mindsdb')\n     get_lexer_parser('mysql')\n+\n+\n+def split_model_name(name: list) -> tuple:",
    "comment": "maybe is better to name it `resolve_model_identifier`\r\nand use Identifier as input parameter",
    "line_number": 106,
    "enriched": "File: mindsdb/utilities/functions.py\nCode: @@ -101,3 +101,53 @@ def get_versions_where_predictors_become_obsolete():\n def init_lexer_parsers():\n     get_lexer_parser('mindsdb')\n     get_lexer_parser('mysql')\n+\n+\n+def split_model_name(name: list) -> tuple:\nComment: maybe is better to name it `resolve_model_identifier`\r\nand use Identifier as input parameter",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/utilities/functions.py",
    "pr_number": 6114,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1197951405,
    "comment_created_at": "2023-05-18T15:15:32Z"
  },
  {
    "code": "@@ -134,22 +140,20 @@ def _generate_context(self, chunk_content: str, full_document: str) -> str:\n \n     def _split_document(self, doc: Document) -> List[Document]:\n         \"\"\"Split document into chunks while preserving metadata\"\"\"\n-        # Convert to langchain Document for splitting\n-        langchain_doc = LangchainDocument(\n-            page_content=doc.content,\n-            metadata=doc.metadata or {}\n-        )\n-        # Split and convert back to our Document type\n-        split_docs = self.splitter.split_documents([langchain_doc])\n-        return [Document(\n-            content=split_doc.page_content,\n-            metadata=split_doc.metadata\n-        ) for split_doc in split_docs]\n+        # Use base class implementation\n+        return super()._split_document(doc)\n \n     def process_documents(self, documents: List[Document]) -> List[ProcessedChunk]:\n         processed_chunks = []\n \n         for doc in documents:\n+            # Get content_column from metadata if available\n+            content_column = doc.metadata.get('content_column') if doc.metadata else None\n+\n+            # Ensure document has an ID\n+            if doc.id is None:",
    "comment": "I noticed in the traces in CW we had null for id on chunks",
    "line_number": 154,
    "enriched": "File: mindsdb/interfaces/knowledge_base/preprocessing/document_preprocessor.py\nCode: @@ -134,22 +140,20 @@ def _generate_context(self, chunk_content: str, full_document: str) -> str:\n \n     def _split_document(self, doc: Document) -> List[Document]:\n         \"\"\"Split document into chunks while preserving metadata\"\"\"\n-        # Convert to langchain Document for splitting\n-        langchain_doc = LangchainDocument(\n-            page_content=doc.content,\n-            metadata=doc.metadata or {}\n-        )\n-        # Split and convert back to our Document type\n-        split_docs = self.splitter.split_documents([langchain_doc])\n-        return [Document(\n-            content=split_doc.page_content,\n-            metadata=split_doc.metadata\n-        ) for split_doc in split_docs]\n+        # Use base class implementation\n+        return super()._split_document(doc)\n \n     def process_documents(self, documents: List[Document]) -> List[ProcessedChunk]:\n         processed_chunks = []\n \n         for doc in documents:\n+            # Get content_column from metadata if available\n+            content_column = doc.metadata.get('content_column') if doc.metadata else None\n+\n+            # Ensure document has an ID\n+            if doc.id is None:\nComment: I noticed in the traces in CW we had null for id on chunks",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/interfaces/knowledge_base/preprocessing/document_preprocessor.py",
    "pr_number": 10239,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1869338409,
    "comment_created_at": "2024-12-04T11:52:21Z"
  },
  {
    "code": "@@ -21,9 +21,8 @@ The required arguments to establish a connection are as follows:\n * `server` sets the current server.\n \n <Tip>\n-If you installed MindsDB locally via pip, you need to install all handler dependencies manually. To do so, go to the handler's folder (mindsdb/integrations/handlers/sqlany_handler) and run this command: `pip install -r requirements.txt`.",
    "comment": "Please undo this change.",
    "line_number": 24,
    "enriched": "File: docs/data-integrations/sap-sql-anywhere.mdx\nCode: @@ -21,9 +21,8 @@ The required arguments to establish a connection are as follows:\n * `server` sets the current server.\n \n <Tip>\n-If you installed MindsDB locally via pip, you need to install all handler dependencies manually. To do so, go to the handler's folder (mindsdb/integrations/handlers/sqlany_handler) and run this command: `pip install -r requirements.txt`.\nComment: Please undo this change.",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "docs/data-integrations/sap-sql-anywhere.mdx",
    "pr_number": 6761,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1253028835,
    "comment_created_at": "2023-07-05T12:23:14Z"
  },
  {
    "code": "@@ -121,11 +126,15 @@ def main():\n     elif method == 'predict':\n         model_state = params['model_state']\n         df = pd_decode(params['df'])\n+        args = params['args']\n \n         model = model_class()\n         model.__dict__ = decode(model_state)\n \n-        res = model.predict(df)\n+        call_args = [df]\n+        if args:\n+            call_args.append(args)",
    "comment": "```python\r\nelse:\r\n    call_args.append({})\r\n```\r\n\r\nAlternatively, replace the entire `if` with a one-liner:\r\n\r\n```python\r\ncall_args.append(args if args else {})\r\n```",
    "line_number": 136,
    "enriched": "File: mindsdb/integrations/handlers/byom_handler/proc_wrapper.py\nCode: @@ -121,11 +126,15 @@ def main():\n     elif method == 'predict':\n         model_state = params['model_state']\n         df = pd_decode(params['df'])\n+        args = params['args']\n \n         model = model_class()\n         model.__dict__ = decode(model_state)\n \n-        res = model.predict(df)\n+        call_args = [df]\n+        if args:\n+            call_args.append(args)\nComment: ```python\r\nelse:\r\n    call_args.append({})\r\n```\r\n\r\nAlternatively, replace the entire `if` with a one-liner:\r\n\r\n```python\r\ncall_args.append(args if args else {})\r\n```",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/byom_handler/proc_wrapper.py",
    "pr_number": 5996,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1274384937,
    "comment_created_at": "2023-07-26T05:16:41Z"
  },
  {
    "code": "@@ -514,6 +514,7 @@ def execute_steps(self, params=None):\n                 or isinstance(query, ast.Insert)\n                 or isinstance(query, ast.Update)\n                 or isinstance(query, ast.Delete)\n+                or isinstance(query, ast.Intersect)",
    "comment": "we can add 'Except' comand too at the same time",
    "line_number": 517,
    "enriched": "File: mindsdb/api/executor/planner/query_prepare.py\nCode: @@ -514,6 +514,7 @@ def execute_steps(self, params=None):\n                 or isinstance(query, ast.Insert)\n                 or isinstance(query, ast.Update)\n                 or isinstance(query, ast.Delete)\n+                or isinstance(query, ast.Intersect)\nComment: we can add 'Except' comand too at the same time",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/api/executor/planner/query_prepare.py",
    "pr_number": 11052,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2150264157,
    "comment_created_at": "2025-06-16T15:12:49Z"
  },
  {
    "code": "@@ -41,7 +41,7 @@ def get_integration_record(name: str) -> db.Integration:\n \n @profiler.profile()\n def get_project_record(name: str) -> db.Project:\n-    company_id = ctx.company_id if ctx.company_id is not None else 0\n+    company_id = ctx.company_id if ctx.company_id is not None else \"0\"",
    "comment": "**security**: `company_id` is set to the string \"0\" when `ctx.company_id` is None, which may allow unauthorized cross-tenant data access if \"0\" is a valid or default tenant identifier.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/interfaces/model/functions.py, line 44, the code sets `company_id` to the string \"0\" if `ctx.company_id` is None. This could allow unauthorized access to data belonging to a default or shared tenant if \"0\" is a valid company_id. Change the fallback to use SQL NULL (i.e., `null()`) instead of \"0\" to prevent cross-tenant data exposure.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n    company_id = ctx.company_id if ctx.company_id is not None else null()\n```\n</details>\n<!-- suggestion_end -->\n",
    "line_number": 36,
    "enriched": "File: mindsdb/interfaces/model/functions.py\nCode: @@ -41,7 +41,7 @@ def get_integration_record(name: str) -> db.Integration:\n \n @profiler.profile()\n def get_project_record(name: str) -> db.Project:\n-    company_id = ctx.company_id if ctx.company_id is not None else 0\n+    company_id = ctx.company_id if ctx.company_id is not None else \"0\"\nComment: **security**: `company_id` is set to the string \"0\" when `ctx.company_id` is None, which may allow unauthorized cross-tenant data access if \"0\" is a valid or default tenant identifier.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/interfaces/model/functions.py, line 44, the code sets `company_id` to the string \"0\" if `ctx.company_id` is None. This could allow unauthorized access to data belonging to a default or shared tenant if \"0\" is a valid company_id. Change the fallback to use SQL NULL (i.e., `null()`) instead of \"0\" to prevent cross-tenant data exposure.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n    company_id = ctx.company_id if ctx.company_id is not None else null()\n```\n</details>\n<!-- suggestion_end -->\n",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/interfaces/model/functions.py",
    "pr_number": 11781,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2459665228,
    "comment_created_at": "2025-10-24T10:13:07Z"
  },
  {
    "code": "@@ -0,0 +1,29 @@\n+## Testing: Hugging Face - ESG (6) Tutorial\n+\n+Testing CREATE MODEL\n+```sql\n+CREATE MODEL mindsdb.hf_esg_6\n+PREDICT PRED\n+USING\n+engine = 'huggingface',\n+task = 'text-classification',\n+model_name = 'yiyanghkust/finbert-esg',\n+input_column = 'text';\n+\n+```\n+\n+```sql\n+[export.md](https://github.com/MahenderPoshaboina/mindsdb/files/11634666/export.md)",
    "comment": "Instead of linking an `.md` file, please copy and paste its content here. (This linking could be used if you provided screenshots)",
    "line_number": 16,
    "enriched": "File: docs/manual-qa/hf_esg6_tutorial_test.mdx\nCode: @@ -0,0 +1,29 @@\n+## Testing: Hugging Face - ESG (6) Tutorial\n+\n+Testing CREATE MODEL\n+```sql\n+CREATE MODEL mindsdb.hf_esg_6\n+PREDICT PRED\n+USING\n+engine = 'huggingface',\n+task = 'text-classification',\n+model_name = 'yiyanghkust/finbert-esg',\n+input_column = 'text';\n+\n+```\n+\n+```sql\n+[export.md](https://github.com/MahenderPoshaboina/mindsdb/files/11634666/export.md)\nComment: Instead of linking an `.md` file, please copy and paste its content here. (This linking could be used if you provided screenshots)",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "docs/manual-qa/hf_esg6_tutorial_test.mdx",
    "pr_number": 6462,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1219320459,
    "comment_created_at": "2023-06-06T09:55:50Z"
  },
  {
    "code": "@@ -279,9 +283,10 @@ def select(self, query, disable_reranking=False):\n                     if not (0 <= item.value <= 1):\n                         raise ValueError(f\"Invalid hybrid_search_alpha value: {item.value}. Must be between 0 and 1.\")\n                     hybrid_search_alpha = item.value\n-                elif item.column == \"relevance\" and item.op.value != FilterOperator.GREATER_THAN_OR_EQUAL.value:\n+                elif (item.column == \"relevance\") and (item.op.value not in relevance_threshold_allowed_operators):",
    "comment": "I would move this block upper, locate 'relevance' logic together:\r\n```python\r\nif item.column == \"relevance\":\r\n   if item.op.value not in relevance_threshold_allowed_operators:\r\n       raise error\r\n   check and use relevance value\r\n```",
    "line_number": 286,
    "enriched": "File: mindsdb/interfaces/knowledge_base/controller.py\nCode: @@ -279,9 +283,10 @@ def select(self, query, disable_reranking=False):\n                     if not (0 <= item.value <= 1):\n                         raise ValueError(f\"Invalid hybrid_search_alpha value: {item.value}. Must be between 0 and 1.\")\n                     hybrid_search_alpha = item.value\n-                elif item.column == \"relevance\" and item.op.value != FilterOperator.GREATER_THAN_OR_EQUAL.value:\n+                elif (item.column == \"relevance\") and (item.op.value not in relevance_threshold_allowed_operators):\nComment: I would move this block upper, locate 'relevance' logic together:\r\n```python\r\nif item.column == \"relevance\":\r\n   if item.op.value not in relevance_threshold_allowed_operators:\r\n       raise error\r\n   check and use relevance value\r\n```",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/interfaces/knowledge_base/controller.py",
    "pr_number": 11395,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2251434016,
    "comment_created_at": "2025-08-04T13:07:44Z"
  },
  {
    "code": "@@ -194,6 +194,22 @@ def select_query(self, query: Select) -> pd.DataFrame:\n         executor = KnowledgeBaseQueryExecutor(self)\n         df = executor.run(query)\n \n+        # copy metadata to columns\n+        if \"metadata\" in df.columns:\n+            meta_columns = self._get_allowed_metadata_columns()\n+            if meta_columns:\n+                meta_data = pd.json_normalize(df[\"metadata\"])\n+                # exclude absent columns and used colunns\n+                df_columns = list(df.columns)\n+                meta_columns = list(set(meta_columns).intersection(meta_data.columns).difference(df_columns))\n+\n+                # add columns\n+                df = df.join(meta_data[meta_columns])\n+\n+                # put metadata in the end\n+                df_columns.remove(\"metadata\")\n+                df = df[df_columns + meta_columns + [\"metadata\"]]\n+",
    "comment": "**security**: `pd.json_normalize(df[\"metadata\"])` in `select_query` (lines 197-212) does not validate or sanitize metadata, allowing maliciously crafted metadata to inject unexpected columns or overwrite existing DataFrame columns, leading to potential data integrity or privilege escalation attacks.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/interfaces/knowledge_base/controller.py, lines 197-212, the code uses pd.json_normalize(df[\"metadata\"]) and joins columns from user-controlled metadata into the DataFrame without sanitization. This allows attackers to inject malicious keys in metadata that overwrite or inject DataFrame columns, leading to data integrity or privilege escalation issues. Update this block to only allow metadata keys that are valid Python identifiers, do not start with '_', and do not already exist in df.columns. Apply this sanitization before joining columns. Ensure the fix preserves the intended functionality.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n        # copy metadata to columns\n        if \"metadata\" in df.columns:\n            meta_columns = self._get_allowed_metadata_columns()\n            if meta_columns:\n                meta_data = pd.json_normalize(df[\"metadata\"])\n                # Sanitize metadata keys to prevent column injection/overwrite\n                safe_meta_columns = []\n                for col in meta_columns:\n                    if col in meta_data.columns and col not in df.columns and col.isidentifier() and not col.startswith(\"_\"):\n                        safe_meta_columns.append(col)\n                if safe_meta_columns:\n                    df = df.join(meta_data[safe_meta_columns])\n                    # put metadata in the end\n                    df_columns = list(df.columns)\n                    df_columns.remove(\"metadata\")\n                    df = df[df_columns + safe_meta_columns + [\"metadata\"]]\n\n```\n</details>\n<!-- suggestion_end -->\n",
    "line_number": 212,
    "enriched": "File: mindsdb/interfaces/knowledge_base/controller.py\nCode: @@ -194,6 +194,22 @@ def select_query(self, query: Select) -> pd.DataFrame:\n         executor = KnowledgeBaseQueryExecutor(self)\n         df = executor.run(query)\n \n+        # copy metadata to columns\n+        if \"metadata\" in df.columns:\n+            meta_columns = self._get_allowed_metadata_columns()\n+            if meta_columns:\n+                meta_data = pd.json_normalize(df[\"metadata\"])\n+                # exclude absent columns and used colunns\n+                df_columns = list(df.columns)\n+                meta_columns = list(set(meta_columns).intersection(meta_data.columns).difference(df_columns))\n+\n+                # add columns\n+                df = df.join(meta_data[meta_columns])\n+\n+                # put metadata in the end\n+                df_columns.remove(\"metadata\")\n+                df = df[df_columns + meta_columns + [\"metadata\"]]\n+\nComment: **security**: `pd.json_normalize(df[\"metadata\"])` in `select_query` (lines 197-212) does not validate or sanitize metadata, allowing maliciously crafted metadata to inject unexpected columns or overwrite existing DataFrame columns, leading to potential data integrity or privilege escalation attacks.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/interfaces/knowledge_base/controller.py, lines 197-212, the code uses pd.json_normalize(df[\"metadata\"]) and joins columns from user-controlled metadata into the DataFrame without sanitization. This allows attackers to inject malicious keys in metadata that overwrite or inject DataFrame columns, leading to data integrity or privilege escalation issues. Update this block to only allow metadata keys that are valid Python identifiers, do not start with '_', and do not already exist in df.columns. Apply this sanitization before joining columns. Ensure the fix preserves the intended functionality.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n        # copy metadata to columns\n        if \"metadata\" in df.columns:\n            meta_columns = self._get_allowed_metadata_columns()\n            if meta_columns:\n                meta_data = pd.json_normalize(df[\"metadata\"])\n                # Sanitize metadata keys to prevent column injection/overwrite\n                safe_meta_columns = []\n                for col in meta_columns:\n                    if col in meta_data.columns and col not in df.columns and col.isidentifier() and not col.startswith(\"_\"):\n                        safe_meta_columns.append(col)\n                if safe_meta_columns:\n                    df = df.join(meta_data[safe_meta_columns])\n                    # put metadata in the end\n                    df_columns = list(df.columns)\n                    df_columns.remove(\"metadata\")\n                    df = df[df_columns + safe_meta_columns + [\"metadata\"]]\n\n```\n</details>\n<!-- suggestion_end -->\n",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/interfaces/knowledge_base/controller.py",
    "pr_number": 11729,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2425676498,
    "comment_created_at": "2025-10-13T09:19:40Z"
  },
  {
    "code": "@@ -24,9 +24,12 @@ class RunningQuery:\n       Query in progres\n     \"\"\"\n \n+    OBJECT_TYPE = 'query'\n+\n     def __init__(self, record: db.Queries):\n         self.record = record\n         self.sql = record.sql\n+        self.database = record.database or 'mindsdb'",
    "comment": "`mindsdb` -> `config.get('default_project')`",
    "line_number": 32,
    "enriched": "File: mindsdb/interfaces/query_context/context_controller.py\nCode: @@ -24,9 +24,12 @@ class RunningQuery:\n       Query in progres\n     \"\"\"\n \n+    OBJECT_TYPE = 'query'\n+\n     def __init__(self, record: db.Queries):\n         self.record = record\n         self.sql = record.sql\n+        self.database = record.database or 'mindsdb'\nComment: `mindsdb` -> `config.get('default_project')`",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/interfaces/query_context/context_controller.py",
    "pr_number": 10740,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2058156036,
    "comment_created_at": "2025-04-24T11:23:49Z"
  },
  {
    "code": "@@ -0,0 +1,44 @@\n+from linkup import LinkupClient\n+from mindsdb.integrations.libs.base import BaseMLEngine\n+from typing import Dict, Optional\n+import pandas as pd\n+\n+\n+class LinkupHandler(BaseMLEngine):",
    "comment": "A couple of notes regarding model creation:\r\n1. I think it would make sense for us to allow the API key to be passed in when creating the engine, so as to avoid having to pass it in each time we create a model. This can be done by including a `create_engine()` function. Here is an example: https://github.com/mindsdb/mindsdb/blob/bacf861958191fe53e62369f025ebe7a1e27a94c/mindsdb/integrations/handlers/openai_handler/openai_handler.py#L72\r\n2. It would also be a good idea for us to validate the arguments that are passed in during model creation. I'm assuming there might be some required parameters and the like here. This can be done by including `create_validation()` function. Here is an example: https://github.com/mindsdb/mindsdb/blob/bacf861958191fe53e62369f025ebe7a1e27a94c/mindsdb/integrations/handlers/twelve_labs_handler/twelve_labs_handler.py#L28",
    "line_number": 7,
    "enriched": "File: mindsdb/integrations/handlers/linkup_handler/linkup_handler.py\nCode: @@ -0,0 +1,44 @@\n+from linkup import LinkupClient\n+from mindsdb.integrations.libs.base import BaseMLEngine\n+from typing import Dict, Optional\n+import pandas as pd\n+\n+\n+class LinkupHandler(BaseMLEngine):\nComment: A couple of notes regarding model creation:\r\n1. I think it would make sense for us to allow the API key to be passed in when creating the engine, so as to avoid having to pass it in each time we create a model. This can be done by including a `create_engine()` function. Here is an example: https://github.com/mindsdb/mindsdb/blob/bacf861958191fe53e62369f025ebe7a1e27a94c/mindsdb/integrations/handlers/openai_handler/openai_handler.py#L72\r\n2. It would also be a good idea for us to validate the arguments that are passed in during model creation. I'm assuming there might be some required parameters and the like here. This can be done by including `create_validation()` function. Here is an example: https://github.com/mindsdb/mindsdb/blob/bacf861958191fe53e62369f025ebe7a1e27a94c/mindsdb/integrations/handlers/twelve_labs_handler/twelve_labs_handler.py#L28",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/linkup_handler/linkup_handler.py",
    "pr_number": 10395,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1950326335,
    "comment_created_at": "2025-02-11T06:57:26Z"
  },
  {
    "code": "@@ -223,6 +233,11 @@ def select_query(self, query: Select) -> pd.DataFrame:\n         # filter by targets\n         if requested_kb_columns is not None:\n             df = df[requested_kb_columns]\n+\n+        # apply distinct if needed\n+        if is_distinct:\n+            df = df.drop_duplicates()",
    "comment": "I think it is better to do it with duckdb. Otherwise we have to implement every feature in custom way: distinct, group, order, functions.\r\nIf we use [query_df](https://github.com/mindsdb/mindsdb/blob/4ea0d3f57fff448a9ce12155dd146c2b26027a1d/mindsdb/api/executor/utilities/sql.py#L93) function instead we can do better feature coverage. To use it or not - we can check by query like [here](https://github.com/mindsdb/mindsdb/blob/a7526f39ddcf8c0683a8302e938cf0ac4b80c493/mindsdb/api/executor/planner/plan_join.py#L107) ",
    "line_number": 239,
    "enriched": "File: mindsdb/interfaces/knowledge_base/controller.py\nCode: @@ -223,6 +233,11 @@ def select_query(self, query: Select) -> pd.DataFrame:\n         # filter by targets\n         if requested_kb_columns is not None:\n             df = df[requested_kb_columns]\n+\n+        # apply distinct if needed\n+        if is_distinct:\n+            df = df.drop_duplicates()\nComment: I think it is better to do it with duckdb. Otherwise we have to implement every feature in custom way: distinct, group, order, functions.\r\nIf we use [query_df](https://github.com/mindsdb/mindsdb/blob/4ea0d3f57fff448a9ce12155dd146c2b26027a1d/mindsdb/api/executor/utilities/sql.py#L93) function instead we can do better feature coverage. To use it or not - we can check by query like [here](https://github.com/mindsdb/mindsdb/blob/a7526f39ddcf8c0683a8302e938cf0ac4b80c493/mindsdb/api/executor/planner/plan_join.py#L107) ",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/interfaces/knowledge_base/controller.py",
    "pr_number": 10798,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2074036142,
    "comment_created_at": "2025-05-05T19:15:04Z"
  },
  {
    "code": "@@ -37,7 +37,8 @@ def __init__(self, *args, **kwargs):\n         self.max_batch_size = 20\n         self.default_max_tokens = 100\n         self.chat_completion_models = CHAT_MODELS\n-        self.supported_ft_models = ('davinci', 'curie', 'babbage', 'ada')  # base models compatible with finetuning\n+        self.supported_ft_models = FINETUNING_LEGACY_MODELS  # base models compatible with finetuning  # TODO #7387: transition to new endpoint before 4/1/24 # noqa\n+        self.engine_storage.json_set('ft-suffix', binascii.b2a_hex(os.urandom(15)))  # user suffix for finetunes",
    "comment": "Missing a `.decode()` here to cast into string.",
    "line_number": 41,
    "enriched": "File: mindsdb/integrations/handlers/openai_handler/openai_handler.py\nCode: @@ -37,7 +37,8 @@ def __init__(self, *args, **kwargs):\n         self.max_batch_size = 20\n         self.default_max_tokens = 100\n         self.chat_completion_models = CHAT_MODELS\n-        self.supported_ft_models = ('davinci', 'curie', 'babbage', 'ada')  # base models compatible with finetuning\n+        self.supported_ft_models = FINETUNING_LEGACY_MODELS  # base models compatible with finetuning  # TODO #7387: transition to new endpoint before 4/1/24 # noqa\n+        self.engine_storage.json_set('ft-suffix', binascii.b2a_hex(os.urandom(15)))  # user suffix for finetunes\nComment: Missing a `.decode()` here to cast into string.",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/openai_handler/openai_handler.py",
    "pr_number": 7388,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1333354674,
    "comment_created_at": "2023-09-21T16:48:24Z"
  },
  {
    "code": "@@ -5,3 +5,5 @@ requires = [\n     \"setuptools\",\n     \"wheel\",\n ]\n+[tool.poetry.dependencies]",
    "comment": "We don't need the `pyprojec.toml` please remove it",
    "line_number": 8,
    "enriched": "File: pyproject.toml\nCode: @@ -5,3 +5,5 @@ requires = [\n     \"setuptools\",\n     \"wheel\",\n ]\n+[tool.poetry.dependencies]\nComment: We don't need the `pyprojec.toml` please remove it",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "pyproject.toml",
    "pr_number": 9781,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1785962167,
    "comment_created_at": "2024-10-03T10:04:50Z"
  },
  {
    "code": "@@ -1 +1,3 @@\n+thrift",
    "comment": "If this is used only for wrapping the exception maybe better to remove it?",
    "line_number": 1,
    "enriched": "File: mindsdb/integrations/handlers/hive_handler/requirements.txt\nCode: @@ -1 +1,3 @@\n+thrift\nComment: If this is used only for wrapping the exception maybe better to remove it?",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/hive_handler/requirements.txt",
    "pr_number": 9673,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1740888072,
    "comment_created_at": "2024-09-02T12:56:58Z"
  },
  {
    "code": "@@ -40,6 +40,9 @@ def _ingest_email_row(self, row: pd.Series) -> dict:\n         encoding = None\n         if isinstance(body_str, bytes):\n             encoding = chardet.detect(body_str)['encoding']\n+            if encoding is None:\n+                # Added a fallback to utf-8 since chardet failed to detect the encoding.\n+                encoding = 'utf-8'\n             if 'windows' in encoding.lower():",
    "comment": "**Correctness**: The code doesn't handle the case where `encoding` is `None` before checking if 'windows' is in it, which could cause a `TypeError: argument of type 'NoneType' is not iterable` if the added check is bypassed.\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n            if encoding is None:\n                # Added a fallback to utf-8 since chardet failed to detect the encoding.\n                encoding = 'utf-8'\n            elif 'windows' in encoding.lower():\n```\n</details>\n<!-- suggestion_end -->\n",
    "line_number": 46,
    "enriched": "File: mindsdb/integrations/handlers/email_handler/email_ingestor.py\nCode: @@ -40,6 +40,9 @@ def _ingest_email_row(self, row: pd.Series) -> dict:\n         encoding = None\n         if isinstance(body_str, bytes):\n             encoding = chardet.detect(body_str)['encoding']\n+            if encoding is None:\n+                # Added a fallback to utf-8 since chardet failed to detect the encoding.\n+                encoding = 'utf-8'\n             if 'windows' in encoding.lower():\nComment: **Correctness**: The code doesn't handle the case where `encoding` is `None` before checking if 'windows' is in it, which could cause a `TypeError: argument of type 'NoneType' is not iterable` if the added check is bypassed.\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n            if encoding is None:\n                # Added a fallback to utf-8 since chardet failed to detect the encoding.\n                encoding = 'utf-8'\n            elif 'windows' in encoding.lower():\n```\n</details>\n<!-- suggestion_end -->\n",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/email_handler/email_ingestor.py",
    "pr_number": 10903,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2107832190,
    "comment_created_at": "2025-05-26T20:33:48Z"
  },
  {
    "code": "@@ -64,29 +65,58 @@ def query_df_with_type_infer_fallback(query_str: str, dataframes: dict, user_fun\n         pandas.columns\n     \"\"\"\n \n-    with duckdb.connect(database=\":memory:\") as con:\n-        if user_functions:\n-            user_functions.register(con)\n-\n-        for name, value in dataframes.items():\n-            con.register(name, value)\n-\n-        exception = None\n-        for sample_size in [1000, 10000, 1000000]:\n-            try:\n-                con.execute(f\"set global pandas_analyze_sample={sample_size};\")\n-                result_df = con.execute(query_str).fetchdf()\n-            except InvalidInputException as e:\n-                exception = e\n+    try:\n+        with duckdb.connect(database=\":memory:\") as con:\n+            if user_functions:\n+                user_functions.register(con)\n+\n+            for name, value in dataframes.items():\n+                con.register(name, value)\n+\n+            exception = None\n+            for sample_size in [1000, 10000, 1000000]:\n+                try:\n+                    con.execute(f\"set global pandas_analyze_sample={sample_size};\")\n+                    result_df = con.execute(query_str).fetchdf()\n+                except InvalidInputException as e:\n+                    exception = e\n+                else:\n+                    break\n             else:\n-                break\n-        else:\n-            raise exception\n-        description = con.description\n+                raise exception\n+            description = con.description\n+    except Exception as e:\n+        raise Exception(format_db_error_message(\n+            db_type=\"DuckDB\",\n+            db_error_msg=str(e),\n+            failed_query=query_str,\n+            is_external=False\n+        )) from e\n \n     return result_df, description\n \n \n+_duckdb_functions_list = None\n+\n+\n+def get_duckdb_functions_list() -> list[str] | None:\n+    \"\"\"Returns a list of all functions and operations supported by DuckDB.\n+\n+    Returns:\n+        list[str] | None: List of supported functions and operations, or None if unable to retrieve the list.\n+    \"\"\"\n+    global _duckdb_functions_list\n+    if _duckdb_functions_list is None:\n+        try:\n+            df, _ = query_df_with_type_infer_fallback('pragma functions', dataframes={})\n+            df.columns = [name.lower() for name in df.columns]\n+            _duckdb_functions_list = df['name'].drop_duplicates().str.lower().to_list()\n+        except Exception as e:\n+            logger.warning('Unable to get functions list')\n+            pass\n+    return _duckdb_functions_list",
    "comment": "**correctness**: `get_duckdb_functions_list()` caches the function list globally, but if `query_df_with_type_infer_fallback('pragma functions', ...)` fails once, it sets `_duckdb_functions_list` to `None` permanently, causing all future function checks to fail silently.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/api/executor/utilities/sql.py, lines 110-117, the function get_duckdb_functions_list() sets the global _duckdb_functions_list to None if fetching the DuckDB function list fails, causing all future function checks to fail silently. Update the exception handling so that if an exception occurs, the function returns None immediately instead of leaving _duckdb_functions_list as None. This prevents permanent silent failures in function validation.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n    global _duckdb_functions_list\n    if _duckdb_functions_list is None:\n        try:\n            df, _ = query_df_with_type_infer_fallback('pragma functions', dataframes={})\n            df.columns = [name.lower() for name in df.columns]\n            _duckdb_functions_list = df['name'].drop_duplicates().str.lower().to_list()\n        except Exception as e:\n            logger.warning('Unable to get functions list')\n            return None\n    return _duckdb_functions_list\n```\n</details>\n<!-- suggestion_end -->\n",
    "line_number": 117,
    "enriched": "File: mindsdb/api/executor/utilities/sql.py\nCode: @@ -64,29 +65,58 @@ def query_df_with_type_infer_fallback(query_str: str, dataframes: dict, user_fun\n         pandas.columns\n     \"\"\"\n \n-    with duckdb.connect(database=\":memory:\") as con:\n-        if user_functions:\n-            user_functions.register(con)\n-\n-        for name, value in dataframes.items():\n-            con.register(name, value)\n-\n-        exception = None\n-        for sample_size in [1000, 10000, 1000000]:\n-            try:\n-                con.execute(f\"set global pandas_analyze_sample={sample_size};\")\n-                result_df = con.execute(query_str).fetchdf()\n-            except InvalidInputException as e:\n-                exception = e\n+    try:\n+        with duckdb.connect(database=\":memory:\") as con:\n+            if user_functions:\n+                user_functions.register(con)\n+\n+            for name, value in dataframes.items():\n+                con.register(name, value)\n+\n+            exception = None\n+            for sample_size in [1000, 10000, 1000000]:\n+                try:\n+                    con.execute(f\"set global pandas_analyze_sample={sample_size};\")\n+                    result_df = con.execute(query_str).fetchdf()\n+                except InvalidInputException as e:\n+                    exception = e\n+                else:\n+                    break\n             else:\n-                break\n-        else:\n-            raise exception\n-        description = con.description\n+                raise exception\n+            description = con.description\n+    except Exception as e:\n+        raise Exception(format_db_error_message(\n+            db_type=\"DuckDB\",\n+            db_error_msg=str(e),\n+            failed_query=query_str,\n+            is_external=False\n+        )) from e\n \n     return result_df, description\n \n \n+_duckdb_functions_list = None\n+\n+\n+def get_duckdb_functions_list() -> list[str] | None:\n+    \"\"\"Returns a list of all functions and operations supported by DuckDB.\n+\n+    Returns:\n+        list[str] | None: List of supported functions and operations, or None if unable to retrieve the list.\n+    \"\"\"\n+    global _duckdb_functions_list\n+    if _duckdb_functions_list is None:\n+        try:\n+            df, _ = query_df_with_type_infer_fallback('pragma functions', dataframes={})\n+            df.columns = [name.lower() for name in df.columns]\n+            _duckdb_functions_list = df['name'].drop_duplicates().str.lower().to_list()\n+        except Exception as e:\n+            logger.warning('Unable to get functions list')\n+            pass\n+    return _duckdb_functions_list\nComment: **correctness**: `get_duckdb_functions_list()` caches the function list globally, but if `query_df_with_type_infer_fallback('pragma functions', ...)` fails once, it sets `_duckdb_functions_list` to `None` permanently, causing all future function checks to fail silently.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/api/executor/utilities/sql.py, lines 110-117, the function get_duckdb_functions_list() sets the global _duckdb_functions_list to None if fetching the DuckDB function list fails, causing all future function checks to fail silently. Update the exception handling so that if an exception occurs, the function returns None immediately instead of leaving _duckdb_functions_list as None. This prevents permanent silent failures in function validation.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n    global _duckdb_functions_list\n    if _duckdb_functions_list is None:\n        try:\n            df, _ = query_df_with_type_infer_fallback('pragma functions', dataframes={})\n            df.columns = [name.lower() for name in df.columns]\n            _duckdb_functions_list = df['name'].drop_duplicates().str.lower().to_list()\n        except Exception as e:\n            logger.warning('Unable to get functions list')\n            return None\n    return _duckdb_functions_list\n```\n</details>\n<!-- suggestion_end -->\n",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/api/executor/utilities/sql.py",
    "pr_number": 11236,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2179891124,
    "comment_created_at": "2025-07-02T12:10:46Z"
  },
  {
    "code": "@@ -97,7 +98,10 @@ def _resolve_table_names(self, table_names: List[str], all_tables: List[Identifi\n         tables = []\n         for table_name in table_names:\n \n-            # Some LLMs (e.g. gpt-4o) may include backticks when invoking tools.\n+            # Some LLMs (e.g. gpt-4o) may include backticks or quotes when invoking tools.\n+            table_name = table_name.strip('`')",
    "comment": "it might be just:\r\n```python\r\n table_name = table_name.strip(' `\"\\'')\r\n```",
    "line_number": 102,
    "enriched": "File: mindsdb/interfaces/skills/sql_agent.py\nCode: @@ -97,7 +98,10 @@ def _resolve_table_names(self, table_names: List[str], all_tables: List[Identifi\n         tables = []\n         for table_name in table_names:\n \n-            # Some LLMs (e.g. gpt-4o) may include backticks when invoking tools.\n+            # Some LLMs (e.g. gpt-4o) may include backticks or quotes when invoking tools.\n+            table_name = table_name.strip('`')\nComment: it might be just:\r\n```python\r\n table_name = table_name.strip(' `\"\\'')\r\n```",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/interfaces/skills/sql_agent.py",
    "pr_number": 9449,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1667146459,
    "comment_created_at": "2024-07-05T20:59:18Z"
  },
  {
    "code": "@@ -0,0 +1,48 @@\n+from pydantic import BaseModel, AnyUrl, model_validator\n+from urllib.parse import urlparse\n+\n+\n+class ConnectionConfig(BaseModel):\n+    # TODO: For now validate AnyURL since MariaDBDsn wasn't working\n+    url: AnyUrl = None",
    "comment": "**correctness**: `url` field is typed as `AnyUrl = None`, but Pydantic's `AnyUrl` does not allow `None` as a valid value, causing validation errors if `url` is omitted.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/integrations/handlers/mariadb_handler/settings.py, line 7, the `url` field is typed as `AnyUrl = None`, but Pydantic's `AnyUrl` does not accept `None` unless explicitly marked as optional. Change the type annotation to `AnyUrl | None = None` to allow `url` to be omitted or set to None without validation errors.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n    url: AnyUrl | None = None\n```\n</details>\n<!-- suggestion_end -->\n",
    "line_number": 7,
    "enriched": "File: mindsdb/integrations/handlers/mariadb_handler/settings.py\nCode: @@ -0,0 +1,48 @@\n+from pydantic import BaseModel, AnyUrl, model_validator\n+from urllib.parse import urlparse\n+\n+\n+class ConnectionConfig(BaseModel):\n+    # TODO: For now validate AnyURL since MariaDBDsn wasn't working\n+    url: AnyUrl = None\nComment: **correctness**: `url` field is typed as `AnyUrl = None`, but Pydantic's `AnyUrl` does not allow `None` as a valid value, causing validation errors if `url` is omitted.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/integrations/handlers/mariadb_handler/settings.py, line 7, the `url` field is typed as `AnyUrl = None`, but Pydantic's `AnyUrl` does not accept `None` unless explicitly marked as optional. Change the type annotation to `AnyUrl | None = None` to allow `url` to be omitted or set to None without validation errors.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n    url: AnyUrl | None = None\n```\n</details>\n<!-- suggestion_end -->\n",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/mariadb_handler/settings.py",
    "pr_number": 11540,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2332112616,
    "comment_created_at": "2025-09-09T06:04:27Z"
  },
  {
    "code": "@@ -30,10 +30,16 @@ def create(self, target: str, df: Optional[pd.DataFrame] = None, args: Optional[\n         args = args['using']  # ignore the rest of the problem definition\n         args['target'] = target\n         self.model_storage.json_set('args', args)\n+        # TODO: use headers dynamically\n+        headers = {\n+            'Content-Type': 'application/json; format=pandas-records',\n+        }\n+        if 'headers' in args and 'authorization' in args['headers']:  #added header authorization key based condition",
    "comment": "The idea is to send the authorization in the USING clause as:\r\n```sql\r\nCREATE MODEL mindsdb.byom_ray_serve\r\nFROM ..\r\nPREDICT X\r\nUSING\r\n    authorization = 'Bearer xyz'\r\n```\r\nIf so, we don't need the `headers` or check for it here",
    "line_number": 37,
    "enriched": "File: mindsdb/integrations/handlers/ray_serve_handler/ray_serve_handler.py\nCode: @@ -30,10 +30,16 @@ def create(self, target: str, df: Optional[pd.DataFrame] = None, args: Optional[\n         args = args['using']  # ignore the rest of the problem definition\n         args['target'] = target\n         self.model_storage.json_set('args', args)\n+        # TODO: use headers dynamically\n+        headers = {\n+            'Content-Type': 'application/json; format=pandas-records',\n+        }\n+        if 'headers' in args and 'authorization' in args['headers']:  #added header authorization key based condition\nComment: The idea is to send the authorization in the USING clause as:\r\n```sql\r\nCREATE MODEL mindsdb.byom_ray_serve\r\nFROM ..\r\nPREDICT X\r\nUSING\r\n    authorization = 'Bearer xyz'\r\n```\r\nIf so, we don't need the `headers` or check for it here",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/ray_serve_handler/ray_serve_handler.py",
    "pr_number": 9566,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1696941794,
    "comment_created_at": "2024-07-30T13:12:46Z"
  },
  {
    "code": "@@ -114,9 +114,31 @@ jobs:\n           ref: main\n           client_payload: '{\"image-tag-prefix\": \"${{ env.CI_REF_NAME }}\", \"deploy-env\": \"prod\"}'\n \n+  trigger_dd_extension_release:\n+    # Trigger private repo to deploy to prod env\n+    runs-on: mdb-dev\n+    needs: docker_build\n+    if: github.actor != 'mindsdbadmin'\n+    environment:\n+      name: prod\n+    # We only want to run one deploy job for an env at a time\n+    # Don't cancel in progress jobs because it may be for a different PR\n+    concurrency:",
    "comment": "I think you can get rid of the concurrency group here",
    "line_number": 126,
    "enriched": "File: .github/workflows/build_deploy_prod.yml\nCode: @@ -114,9 +114,31 @@ jobs:\n           ref: main\n           client_payload: '{\"image-tag-prefix\": \"${{ env.CI_REF_NAME }}\", \"deploy-env\": \"prod\"}'\n \n+  trigger_dd_extension_release:\n+    # Trigger private repo to deploy to prod env\n+    runs-on: mdb-dev\n+    needs: docker_build\n+    if: github.actor != 'mindsdbadmin'\n+    environment:\n+      name: prod\n+    # We only want to run one deploy job for an env at a time\n+    # Don't cancel in progress jobs because it may be for a different PR\n+    concurrency:\nComment: I think you can get rid of the concurrency group here",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": ".github/workflows/build_deploy_prod.yml",
    "pr_number": 9313,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1632391270,
    "comment_created_at": "2024-06-09T20:58:45Z"
  },
  {
    "code": "@@ -411,41 +399,6 @@ def _make_handler_args(self, name: str, handler_type: str, connection_data: dict\n \n         return handler_ars\n \n-    def create_tmp_handler(self, handler_type: str, connection_data: dict) -> object:",
    "comment": "this function is used, for example [here](https://github.com/mindsdb/mindsdb/blob/11b34a7eac44cd8c50754a6895ee1e1db74bf3d8/mindsdb/api/http/namespaces/config.py#L151) ",
    "line_number": 414,
    "enriched": "File: mindsdb/interfaces/database/integrations.py\nCode: @@ -411,41 +399,6 @@ def _make_handler_args(self, name: str, handler_type: str, connection_data: dict\n \n         return handler_ars\n \n-    def create_tmp_handler(self, handler_type: str, connection_data: dict) -> object:\nComment: this function is used, for example [here](https://github.com/mindsdb/mindsdb/blob/11b34a7eac44cd8c50754a6895ee1e1db74bf3d8/mindsdb/api/http/namespaces/config.py#L151) ",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/interfaces/database/integrations.py",
    "pr_number": 9002,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1546258232,
    "comment_created_at": "2024-04-01T12:07:43Z"
  },
  {
    "code": "@@ -0,0 +1,396 @@\n+import os\n+import datetime as datetime\n+import ast\n+from typing import List\n+import pandas as pd\n+from slack_sdk import WebClient\n+from slack_sdk.errors import SlackApiError\n+from slack_sdk.web.slack_response import SlackResponse\n+\n+from mindsdb.utilities import log\n+from mindsdb.utilities.config import Config\n+\n+from mindsdb_sql.parser import ast\n+from mindsdb_sql.parser.ast import ASTNode, Update, Delete\n+from mindsdb_sql.planner.utils import query_traversal\n+\n+from mindsdb.integrations.libs.api_handler import APIHandler, APITable, FuncParser\n+from mindsdb.integrations.utilities.sql_utils import extract_comparison_conditions\n+\n+from mindsdb.integrations.libs.response import (\n+    HandlerStatusResponse as StatusResponse,\n+    HandlerResponse as Response,\n+    RESPONSE_TYPE\n+)\n+\n+class SlackChannelsTable(APITable):\n+    def __init__(self, handler):\n+        \"\"\"\n+        Checks the connection is active\n+        \"\"\"\n+        super().__init__(handler)\n+        self.client = WebClient(token=self.handler.connection_args['token'])\n+\n+    def select(self, query: ast.Select) -> Response:\n+        \"\"\"\n+        Retrieves the data from the channel using SlackAPI\n+\n+        Args:\n+            channel_name\n+\n+        Returns:\n+            conversation_history\n+        \"\"\"\n+\n+        # Get the channels list and ids\n+        channels = self.client.conversations_list(types=\"public_channel,private_channel\")['channels']\n+        channel_ids = {c['name']: c['id'] for c in channels}\n+        print(channel_ids)\n+        \n+        # Extract comparison conditions from the query\n+        conditions = extract_comparison_conditions(query.where)\n+        \n+        filters = []\n+        params = {}\n+        order_by_conditions = {}\n+        \n+        # Build the filters and parameters for the query\n+        for op, arg1, arg2 in conditions:\n+            if arg1 == 'channel':\n+                if arg2 in channel_ids:\n+                    params['channel'] = channel_ids[arg2]\n+                else:\n+                    raise ValueError(f\"Channel '{arg2}' not found\")\n+\n+            elif arg1 == 'limit':\n+                if op == '=': \n+                    params['limit'] = int(arg2)\n+                else:\n+                    raise NotImplementedError(f'Unknown op: {op}')\n+\n+            else:\n+                filters.append([op, arg1, arg2])\n+\n+        if query.limit:\n+            params['limit'] = int(query.limit.value)\n+\n+        if query.order_by and len(query.order_by) > 0:\n+            order_by_conditions[\"columns\"] = []\n+            order_by_conditions[\"ascending\"] = []\n+\n+            for an_order in query.order_by:\n+                if an_order.field.parts[1] == \"messages\":\n+                    order_by_conditions[\"columns\"].append(\"messages\")\n+\n+                    if an_order.direction == \"ASC\":\n+                        order_by_conditions[\"ascending\"].append(True)\n+                    else:\n+                        order_by_conditions[\"ascending\"].append(False)\n+                else:\n+                    raise ValueError(\n+                        f\"Order by unknown column {an_order.field.parts[1]}\"\n+                    )\n+\n+        # Retrieve the conversation history\n+        try:\n+            result = self.client.conversations_history(channel=params['channel'])\n+            conversation_history = result[\"messages\"]\n+        except SlackApiError as e:\n+            print(\"Error creating conversation: {}\".format(e))\n+        \n+        print(conversation_history)",
    "comment": "Please remove print",
    "line_number": 101,
    "enriched": "File: mindsdb/integrations/handlers/slack_handler/slack_handler.py\nCode: @@ -0,0 +1,396 @@\n+import os\n+import datetime as datetime\n+import ast\n+from typing import List\n+import pandas as pd\n+from slack_sdk import WebClient\n+from slack_sdk.errors import SlackApiError\n+from slack_sdk.web.slack_response import SlackResponse\n+\n+from mindsdb.utilities import log\n+from mindsdb.utilities.config import Config\n+\n+from mindsdb_sql.parser import ast\n+from mindsdb_sql.parser.ast import ASTNode, Update, Delete\n+from mindsdb_sql.planner.utils import query_traversal\n+\n+from mindsdb.integrations.libs.api_handler import APIHandler, APITable, FuncParser\n+from mindsdb.integrations.utilities.sql_utils import extract_comparison_conditions\n+\n+from mindsdb.integrations.libs.response import (\n+    HandlerStatusResponse as StatusResponse,\n+    HandlerResponse as Response,\n+    RESPONSE_TYPE\n+)\n+\n+class SlackChannelsTable(APITable):\n+    def __init__(self, handler):\n+        \"\"\"\n+        Checks the connection is active\n+        \"\"\"\n+        super().__init__(handler)\n+        self.client = WebClient(token=self.handler.connection_args['token'])\n+\n+    def select(self, query: ast.Select) -> Response:\n+        \"\"\"\n+        Retrieves the data from the channel using SlackAPI\n+\n+        Args:\n+            channel_name\n+\n+        Returns:\n+            conversation_history\n+        \"\"\"\n+\n+        # Get the channels list and ids\n+        channels = self.client.conversations_list(types=\"public_channel,private_channel\")['channels']\n+        channel_ids = {c['name']: c['id'] for c in channels}\n+        print(channel_ids)\n+        \n+        # Extract comparison conditions from the query\n+        conditions = extract_comparison_conditions(query.where)\n+        \n+        filters = []\n+        params = {}\n+        order_by_conditions = {}\n+        \n+        # Build the filters and parameters for the query\n+        for op, arg1, arg2 in conditions:\n+            if arg1 == 'channel':\n+                if arg2 in channel_ids:\n+                    params['channel'] = channel_ids[arg2]\n+                else:\n+                    raise ValueError(f\"Channel '{arg2}' not found\")\n+\n+            elif arg1 == 'limit':\n+                if op == '=': \n+                    params['limit'] = int(arg2)\n+                else:\n+                    raise NotImplementedError(f'Unknown op: {op}')\n+\n+            else:\n+                filters.append([op, arg1, arg2])\n+\n+        if query.limit:\n+            params['limit'] = int(query.limit.value)\n+\n+        if query.order_by and len(query.order_by) > 0:\n+            order_by_conditions[\"columns\"] = []\n+            order_by_conditions[\"ascending\"] = []\n+\n+            for an_order in query.order_by:\n+                if an_order.field.parts[1] == \"messages\":\n+                    order_by_conditions[\"columns\"].append(\"messages\")\n+\n+                    if an_order.direction == \"ASC\":\n+                        order_by_conditions[\"ascending\"].append(True)\n+                    else:\n+                        order_by_conditions[\"ascending\"].append(False)\n+                else:\n+                    raise ValueError(\n+                        f\"Order by unknown column {an_order.field.parts[1]}\"\n+                    )\n+\n+        # Retrieve the conversation history\n+        try:\n+            result = self.client.conversations_history(channel=params['channel'])\n+            conversation_history = result[\"messages\"]\n+        except SlackApiError as e:\n+            print(\"Error creating conversation: {}\".format(e))\n+        \n+        print(conversation_history)\nComment: Please remove print",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/slack_handler/slack_handler.py",
    "pr_number": 5894,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1185904523,
    "comment_created_at": "2023-05-05T09:51:58Z"
  },
  {
    "code": "@@ -0,0 +1,123 @@\n+from typing import List, Optional, Union\n+import pandas as pd\n+\n+from mindsdb.integrations.libs.api_handler import MetaAPIHandler\n+from mindsdb.integrations.libs.base import MetaDatabaseHandler\n+from mindsdb.integrations.libs.response import RESPONSE_TYPE\n+from mindsdb.utilities import log\n+\n+\n+logger = log.getLogger(\"mindsdb\")\n+\n+\n+class DataCatalogRetriever:\n+    \"\"\"\n+    This class is responsible for retrieving (data catalog) metadata directly from the data source via the handler.\n+    \"\"\"\n+\n+    def __init__(self, database_name: str, table_names: Optional[List[str]] = None) -> None:\n+        \"\"\"\n+        Initialize the DataCatalogRetriever.\n+\n+        Args:\n+            database_name (str): The data source to read/write metadata from.\n+            table_names (Optional[List[str]]): The list of table names to read or write metadata for. If None, all tables will be read or written.\n+        \"\"\"\n+        from mindsdb.api.executor.controllers.session_controller import (\n+            SessionController,\n+        )\n+\n+        session = SessionController()\n+\n+        self.database_name = database_name\n+        self.data_handler: Union[MetaDatabaseHandler, MetaAPIHandler] = session.integration_controller.get_data_handler(\n+            database_name\n+        )\n+        integration = session.integration_controller.get(database_name)\n+        self.integration_id = integration[\"id\"]\n+        self.integration_engine = integration[\"engine\"]\n+        # TODO: Handle situations where a schema is provided along with the database name, e.g., 'schema.table'.\n+        # TODO: Handle situations where a file path is provided with integrations like S3, e.g., 'dir/file.csv'.\n+        self.table_names = table_names\n+\n+    def retrieve_tables(self) -> pd.DataFrame:\n+        \"\"\"\n+        Retrieve the table metadata from the handler.\n+        \"\"\"\n+        logger.info(f\"Retrieving {', '.join(self.table_names) if self.table_names else 'all'} tables for {self.database_name}\")\n+        response = self.data_handler.meta_get_tables(self.table_names)\n+        if response.resp_type == RESPONSE_TYPE.ERROR:\n+            logger.error(f\"Failed to retrieve tables for {self.database_name}: {response.error_message}\")\n+            return pd.DataFrame()\n+        elif response.resp_type == RESPONSE_TYPE.OK:\n+            logger.error(f\"No tables found for {self.database_name} in the data source.\")\n+            return pd.DataFrame()\n+        \n+        return response.data_frame\n+\n+    def retrieve_columns(self) -> pd.DataFrame:\n+        \"\"\"\n+        Retrieve the column metadata from the handler.\n+        \"\"\"\n+        logger.info(f\"Retrieving columns for {', '.join(self.table_names) if self.table_names else 'all'} tables for {self.database_name}\")",
    "comment": "**correctness**: All similar logging statements in methods (`retrieve_columns`, `retrieve_column_statistics`, `retrieve_primary_keys`, `retrieve_foreign_keys`) use `', '.join(self.table_names)` without checking for None, risking a TypeError.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/interfaces/data_catalog/data_catalog_retriever.py, lines 62, 77, 92, and 107, the logging statements use `', '.join(self.table_names)` without checking if `self.table_names` is None, which can cause a TypeError. Update each logging statement to check for None before joining.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n        logger.info(f\"Retrieving {', '.join(self.table_names) if self.table_names is not None else 'all'} tables for {self.database_name}\")\n```\n</details>\n<!-- suggestion_end -->\n",
    "line_number": 62,
    "enriched": "File: mindsdb/interfaces/data_catalog/data_catalog_retriever.py\nCode: @@ -0,0 +1,123 @@\n+from typing import List, Optional, Union\n+import pandas as pd\n+\n+from mindsdb.integrations.libs.api_handler import MetaAPIHandler\n+from mindsdb.integrations.libs.base import MetaDatabaseHandler\n+from mindsdb.integrations.libs.response import RESPONSE_TYPE\n+from mindsdb.utilities import log\n+\n+\n+logger = log.getLogger(\"mindsdb\")\n+\n+\n+class DataCatalogRetriever:\n+    \"\"\"\n+    This class is responsible for retrieving (data catalog) metadata directly from the data source via the handler.\n+    \"\"\"\n+\n+    def __init__(self, database_name: str, table_names: Optional[List[str]] = None) -> None:\n+        \"\"\"\n+        Initialize the DataCatalogRetriever.\n+\n+        Args:\n+            database_name (str): The data source to read/write metadata from.\n+            table_names (Optional[List[str]]): The list of table names to read or write metadata for. If None, all tables will be read or written.\n+        \"\"\"\n+        from mindsdb.api.executor.controllers.session_controller import (\n+            SessionController,\n+        )\n+\n+        session = SessionController()\n+\n+        self.database_name = database_name\n+        self.data_handler: Union[MetaDatabaseHandler, MetaAPIHandler] = session.integration_controller.get_data_handler(\n+            database_name\n+        )\n+        integration = session.integration_controller.get(database_name)\n+        self.integration_id = integration[\"id\"]\n+        self.integration_engine = integration[\"engine\"]\n+        # TODO: Handle situations where a schema is provided along with the database name, e.g., 'schema.table'.\n+        # TODO: Handle situations where a file path is provided with integrations like S3, e.g., 'dir/file.csv'.\n+        self.table_names = table_names\n+\n+    def retrieve_tables(self) -> pd.DataFrame:\n+        \"\"\"\n+        Retrieve the table metadata from the handler.\n+        \"\"\"\n+        logger.info(f\"Retrieving {', '.join(self.table_names) if self.table_names else 'all'} tables for {self.database_name}\")\n+        response = self.data_handler.meta_get_tables(self.table_names)\n+        if response.resp_type == RESPONSE_TYPE.ERROR:\n+            logger.error(f\"Failed to retrieve tables for {self.database_name}: {response.error_message}\")\n+            return pd.DataFrame()\n+        elif response.resp_type == RESPONSE_TYPE.OK:\n+            logger.error(f\"No tables found for {self.database_name} in the data source.\")\n+            return pd.DataFrame()\n+        \n+        return response.data_frame\n+\n+    def retrieve_columns(self) -> pd.DataFrame:\n+        \"\"\"\n+        Retrieve the column metadata from the handler.\n+        \"\"\"\n+        logger.info(f\"Retrieving columns for {', '.join(self.table_names) if self.table_names else 'all'} tables for {self.database_name}\")\nComment: **correctness**: All similar logging statements in methods (`retrieve_columns`, `retrieve_column_statistics`, `retrieve_primary_keys`, `retrieve_foreign_keys`) use `', '.join(self.table_names)` without checking for None, risking a TypeError.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/interfaces/data_catalog/data_catalog_retriever.py, lines 62, 77, 92, and 107, the logging statements use `', '.join(self.table_names)` without checking if `self.table_names` is None, which can cause a TypeError. Update each logging statement to check for None before joining.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n        logger.info(f\"Retrieving {', '.join(self.table_names) if self.table_names is not None else 'all'} tables for {self.database_name}\")\n```\n</details>\n<!-- suggestion_end -->\n",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/interfaces/data_catalog/data_catalog_retriever.py",
    "pr_number": 11516,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2322065874,
    "comment_created_at": "2025-09-04T13:03:22Z"
  },
  {
    "code": "@@ -105,20 +105,30 @@ def post(self, project_name):\n                 f'Knowledge Base with name {kb_name} already exists'\n             )\n \n-        embedding_model_identifier = None\n-        if knowledge_base.get('model'):\n-            embedding_model_identifier = Identifier(parts=[knowledge_base['model']])\n+        # Legacy: Support for embedding model identifier.\n+        # embedding_model_identifier = None\n+        # if knowledge_base.get('model'):\n+        #     embedding_model_identifier = Identifier(parts=[knowledge_base['model']])\n \n         storage = knowledge_base.get('storage')\n         embedding_table_identifier = None\n         if storage is not None:\n             embedding_table_identifier = Identifier(parts=[storage['database'], storage['table']])\n \n+        params = knowledge_base.get('params', {}),",
    "comment": "**Correctness**: The `params` variable is incorrectly defined with a trailing comma, creating a tuple instead of a dictionary. This will cause issues when trying to update it with embedding and reranking models.\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n        params = knowledge_base.get('params', {})\n```\n</details>\n<!-- suggestion_end -->\n",
    "line_number": 118,
    "enriched": "File: mindsdb/api/http/namespaces/knowledge_bases.py\nCode: @@ -105,20 +105,30 @@ def post(self, project_name):\n                 f'Knowledge Base with name {kb_name} already exists'\n             )\n \n-        embedding_model_identifier = None\n-        if knowledge_base.get('model'):\n-            embedding_model_identifier = Identifier(parts=[knowledge_base['model']])\n+        # Legacy: Support for embedding model identifier.\n+        # embedding_model_identifier = None\n+        # if knowledge_base.get('model'):\n+        #     embedding_model_identifier = Identifier(parts=[knowledge_base['model']])\n \n         storage = knowledge_base.get('storage')\n         embedding_table_identifier = None\n         if storage is not None:\n             embedding_table_identifier = Identifier(parts=[storage['database'], storage['table']])\n \n+        params = knowledge_base.get('params', {}),\nComment: **Correctness**: The `params` variable is incorrectly defined with a trailing comma, creating a tuple instead of a dictionary. This will cause issues when trying to update it with embedding and reranking models.\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n        params = knowledge_base.get('params', {})\n```\n</details>\n<!-- suggestion_end -->\n",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/api/http/namespaces/knowledge_bases.py",
    "pr_number": 10813,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2079310948,
    "comment_created_at": "2025-05-08T09:32:26Z"
  },
  {
    "code": "@@ -110,28 +114,29 @@ def create_validation(target, args=None, **kwargs):\n ",
    "comment": "Looking a lot better, one minor nit pick then should be good to go\r\n\r\nOne cool feature about `BaseModel` is that it supports the option to prevent extra arguments being passed. So we can remove lines 107-114.\r\n\r\nto set up see comment above for how to implement",
    "line_number": 114,
    "enriched": "File: mindsdb/integrations/handlers/palm_handler/palm_handler.py\nCode: @@ -110,28 +114,29 @@ def create_validation(target, args=None, **kwargs):\n \nComment: Looking a lot better, one minor nit pick then should be good to go\r\n\r\nOne cool feature about `BaseModel` is that it supports the option to prevent extra arguments being passed. So we can remove lines 107-114.\r\n\r\nto set up see comment above for how to implement",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/palm_handler/palm_handler.py",
    "pr_number": 7356,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1331438911,
    "comment_created_at": "2023-09-20T10:49:21Z"
  },
  {
    "code": "@@ -1,56 +1,71 @@\n-from mindsdb_sql.parser.ast.base import ASTNode\n-from mindsdb.integrations.libs.base import MachineLearningHandler\n-from mindsdb.utilities import log\n-\n-import darts\n+import pandas as pd\n+from typing import Optional, Dict\n from darts import TimeSeries\n from darts.models import ARIMA\n-from darts.dataprocessing.transformers import Scaler\n+from mindsdb.integrations.libs.base import BaseMLEngine\n+from mindsdb.integrations.libs.response import HandlerResponse\n+\n+class DartsModel(BaseMLEngine):\n+    \n+    name = \"dartsmodel\"\n+\n+    def _init_(self, model_storage, engine_storage) -> None:\n+        super()._init_(model_storage, engine_storage)\n+\n+    def create(self, target: str, df: Optional[pd.DataFrame] = None, args: Optional[Dict] = None) -> None:\n+        \"\"\"\n+        Train an ARIMA model and save it for later usage.\n+\n+        Args:\n+            target (str): Name of the target column.\n+            df (pd.DataFrame): Input DataFrame containing the time series data.\n+            args (Dict): Additional arguments (if needed).\n+        \"\"\"\n+        # Ensure df contains a datetime index\n+        df.set_index('timestamp', inplace=True)\n+\n+        # Convert DataFrame to Darts TimeSeries\n+        time_series = TimeSeries.from_dataframe(df, time_col='timestamp', value_cols=[target])\n \n-class DartsHandler(MachineLearningHandler):\n+        # Create and train ARIMA model\n+        arima_model = ARIMA(order=(1, 1, 1))  # Can adjust the order as needed\n+        arima_model.fit(time_series)\n \n-    name = 'darts'\n+        # Save the trained model to model_storage\n+        self.model_storage.save_model('arima_model', arima_model)\n \n-    def __init__(self, name: str, connection_data: dict, **kwargs):\n-        super().__init__(name)\n-        self.connection_data = connection_data\n-        self.model = None\n+    def predict(self, df: pd.DataFrame, args: Optional[Dict] = None) -> HandlerResponse:",
    "comment": "`predict()` should return a pandas dataframe. The ML engine executor will then wrap it with a HandlerResponse object, no need for this to happen here. Revert to:\r\n\r\n```\r\ndef predict(self, df: pd.DataFrame, args: Optional[Dict] = None) -> pd.DataFrame:\r\n```",
    "line_number": 37,
    "enriched": "File: mindsdb/integrations/handlers/darts_handler/darts_handler.py\nCode: @@ -1,56 +1,71 @@\n-from mindsdb_sql.parser.ast.base import ASTNode\n-from mindsdb.integrations.libs.base import MachineLearningHandler\n-from mindsdb.utilities import log\n-\n-import darts\n+import pandas as pd\n+from typing import Optional, Dict\n from darts import TimeSeries\n from darts.models import ARIMA\n-from darts.dataprocessing.transformers import Scaler\n+from mindsdb.integrations.libs.base import BaseMLEngine\n+from mindsdb.integrations.libs.response import HandlerResponse\n+\n+class DartsModel(BaseMLEngine):\n+    \n+    name = \"dartsmodel\"\n+\n+    def _init_(self, model_storage, engine_storage) -> None:\n+        super()._init_(model_storage, engine_storage)\n+\n+    def create(self, target: str, df: Optional[pd.DataFrame] = None, args: Optional[Dict] = None) -> None:\n+        \"\"\"\n+        Train an ARIMA model and save it for later usage.\n+\n+        Args:\n+            target (str): Name of the target column.\n+            df (pd.DataFrame): Input DataFrame containing the time series data.\n+            args (Dict): Additional arguments (if needed).\n+        \"\"\"\n+        # Ensure df contains a datetime index\n+        df.set_index('timestamp', inplace=True)\n+\n+        # Convert DataFrame to Darts TimeSeries\n+        time_series = TimeSeries.from_dataframe(df, time_col='timestamp', value_cols=[target])\n \n-class DartsHandler(MachineLearningHandler):\n+        # Create and train ARIMA model\n+        arima_model = ARIMA(order=(1, 1, 1))  # Can adjust the order as needed\n+        arima_model.fit(time_series)\n \n-    name = 'darts'\n+        # Save the trained model to model_storage\n+        self.model_storage.save_model('arima_model', arima_model)\n \n-    def __init__(self, name: str, connection_data: dict, **kwargs):\n-        super().__init__(name)\n-        self.connection_data = connection_data\n-        self.model = None\n+    def predict(self, df: pd.DataFrame, args: Optional[Dict] = None) -> HandlerResponse:\nComment: `predict()` should return a pandas dataframe. The ML engine executor will then wrap it with a HandlerResponse object, no need for this to happen here. Revert to:\r\n\r\n```\r\ndef predict(self, df: pd.DataFrame, args: Optional[Dict] = None) -> pd.DataFrame:\r\n```",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/darts_handler/darts_handler.py",
    "pr_number": 7575,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1360835302,
    "comment_created_at": "2023-10-16T15:14:00Z"
  },
  {
    "code": "@@ -374,6 +374,7 @@ class LLMConfig(BaseModel):\n         description=\"LLM model provider to use for generation\",\n     )\n     params: Dict[str, Any] = Field(default_factory=dict)\n+    model_config = ConfigDict(protected_namespaces=())",
    "comment": "**correctness**: `model_config = ConfigDict(protected_namespaces=())` is added to `LLMConfig`, but not to `BaseLLMConfig`, which may cause the Pydantic warning to persist if `BaseLLMConfig` is instantiated directly.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/integrations/utilities/rag/settings.py, add `model_config = ConfigDict(protected_namespaces=())` to the `BaseLLMConfig` class (in addition to `LLMConfig`) to ensure the Pydantic protected namespace warning is silenced for all relevant config classes. Place this assignment as the first line inside the `BaseLLMConfig` class body.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n    model_config = ConfigDict(protected_namespaces=())\n\n\nclass BaseLLMConfig(BaseModel):\n    model_config = ConfigDict(protected_namespaces=())\n```\n</details>\n<!-- suggestion_end -->\n",
    "line_number": 377,
    "enriched": "File: mindsdb/integrations/utilities/rag/settings.py\nCode: @@ -374,6 +374,7 @@ class LLMConfig(BaseModel):\n         description=\"LLM model provider to use for generation\",\n     )\n     params: Dict[str, Any] = Field(default_factory=dict)\n+    model_config = ConfigDict(protected_namespaces=())\nComment: **correctness**: `model_config = ConfigDict(protected_namespaces=())` is added to `LLMConfig`, but not to `BaseLLMConfig`, which may cause the Pydantic warning to persist if `BaseLLMConfig` is instantiated directly.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/integrations/utilities/rag/settings.py, add `model_config = ConfigDict(protected_namespaces=())` to the `BaseLLMConfig` class (in addition to `LLMConfig`) to ensure the Pydantic protected namespace warning is silenced for all relevant config classes. Place this assignment as the first line inside the `BaseLLMConfig` class body.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n    model_config = ConfigDict(protected_namespaces=())\n\n\nclass BaseLLMConfig(BaseModel):\n    model_config = ConfigDict(protected_namespaces=())\n```\n</details>\n<!-- suggestion_end -->\n",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/utilities/rag/settings.py",
    "pr_number": 11365,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2232037496,
    "comment_created_at": "2025-07-25T21:27:02Z"
  },
  {
    "code": "@@ -44,20 +45,21 @@ def select(self, query: ast.Select) -> pd.DataFrame:\n             select_statement_parser.parse_query()\n         )\n \n-        subset_where_conditions = {}\n+        subset_where_conditions = []\n+        api_filters = {}\n         for op, arg1, arg2 in where_conditions:\n             if arg1 in self.get_columns():\n                 if op != '=':\n                     raise NotImplementedError(f\"Unknown op: {op}. Only '=' is supported.\")\n-                subset_where_conditions[arg1] = arg2\n+                api_filters[arg1] = arg2\n+                subset_where_conditions.append([op, arg1, arg2])\n \n-        count = 0\n-        result = self.handler.zen_client.users(**subset_where_conditions)\n+\n+        result = self.handler.zen_client.users(**api_filters)\n         response = []\n         if isinstance(result, zenpy.lib.generator.BaseResultGenerator):\n-            while count <= result_limit:\n-                response.append(result.next().to_dict())\n-                count += 1\n+            for item in islice(result, result_limit):\n+                response.append(item.to_dict())",
    "comment": "**correctness**: `result_limit` is not checked for None or negative values, which may cause `islice` to raise a TypeError or return all results unintentionally, leading to unexpected behavior or memory issues.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/integrations/handlers/zendesk_handler/zendesk_tables.py, lines 61-62, the code uses `islice(result, result_limit)` without checking if `result_limit` is None or negative. This can cause a TypeError or unintended behavior. Please add a check to ensure `result_limit` is a positive integer before passing it to `islice`, defaulting to 0 or a safe value if not.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n            limit = result_limit if result_limit is not None and result_limit > 0 else 0\n            for item in islice(result, limit):\n                response.append(item.to_dict())\n```\n</details>\n<!-- suggestion_end -->\n",
    "line_number": 62,
    "enriched": "File: mindsdb/integrations/handlers/zendesk_handler/zendesk_tables.py\nCode: @@ -44,20 +45,21 @@ def select(self, query: ast.Select) -> pd.DataFrame:\n             select_statement_parser.parse_query()\n         )\n \n-        subset_where_conditions = {}\n+        subset_where_conditions = []\n+        api_filters = {}\n         for op, arg1, arg2 in where_conditions:\n             if arg1 in self.get_columns():\n                 if op != '=':\n                     raise NotImplementedError(f\"Unknown op: {op}. Only '=' is supported.\")\n-                subset_where_conditions[arg1] = arg2\n+                api_filters[arg1] = arg2\n+                subset_where_conditions.append([op, arg1, arg2])\n \n-        count = 0\n-        result = self.handler.zen_client.users(**subset_where_conditions)\n+\n+        result = self.handler.zen_client.users(**api_filters)\n         response = []\n         if isinstance(result, zenpy.lib.generator.BaseResultGenerator):\n-            while count <= result_limit:\n-                response.append(result.next().to_dict())\n-                count += 1\n+            for item in islice(result, result_limit):\n+                response.append(item.to_dict())\nComment: **correctness**: `result_limit` is not checked for None or negative values, which may cause `islice` to raise a TypeError or return all results unintentionally, leading to unexpected behavior or memory issues.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/integrations/handlers/zendesk_handler/zendesk_tables.py, lines 61-62, the code uses `islice(result, result_limit)` without checking if `result_limit` is None or negative. This can cause a TypeError or unintended behavior. Please add a check to ensure `result_limit` is a positive integer before passing it to `islice`, defaulting to 0 or a safe value if not.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n            limit = result_limit if result_limit is not None and result_limit > 0 else 0\n            for item in islice(result, limit):\n                response.append(item.to_dict())\n```\n</details>\n<!-- suggestion_end -->\n",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/zendesk_handler/zendesk_tables.py",
    "pr_number": 11666,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2399222126,
    "comment_created_at": "2025-10-02T15:27:38Z"
  },
  {
    "code": "@@ -35,9 +36,16 @@ def __init__(\n             **openai_kwargs\n         )\n \n-    def _chunk_list(self, texts: List[str], chunk_size: int) -> List[List[str]]:\n-        \"\"\"Split list into chunks of specified size.\"\"\"\n-        return [texts[i:i + chunk_size] for i in range(0, len(texts), chunk_size)]\n+    def _format_text(self, text: str, is_query: bool = False) -> str:\n+        \"\"\"\n+        Format text according to nomic-embed requirements if using nomic model.\n+        e.g. see here for more details: https://huggingface.co/nomic-ai/nomic-embed-text-v1.5#task-instruction-prefixes\n+        \"\"\"\n+\n+        if not self.is_nomic:",
    "comment": "We should really have a better way for special handling of embedding models. Many different models have different requirements like this, and a bunch of member variables & `if` statements can get out of hand.\r\n\r\nFor short term though this is fine since we don't have time for the alternative",
    "line_number": 45,
    "enriched": "File: mindsdb/integrations/handlers/langchain_embedding_handler/vllm_embeddings.py\nCode: @@ -35,9 +36,16 @@ def __init__(\n             **openai_kwargs\n         )\n \n-    def _chunk_list(self, texts: List[str], chunk_size: int) -> List[List[str]]:\n-        \"\"\"Split list into chunks of specified size.\"\"\"\n-        return [texts[i:i + chunk_size] for i in range(0, len(texts), chunk_size)]\n+    def _format_text(self, text: str, is_query: bool = False) -> str:\n+        \"\"\"\n+        Format text according to nomic-embed requirements if using nomic model.\n+        e.g. see here for more details: https://huggingface.co/nomic-ai/nomic-embed-text-v1.5#task-instruction-prefixes\n+        \"\"\"\n+\n+        if not self.is_nomic:\nComment: We should really have a better way for special handling of embedding models. Many different models have different requirements like this, and a bunch of member variables & `if` statements can get out of hand.\r\n\r\nFor short term though this is fine since we don't have time for the alternative",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/langchain_embedding_handler/vllm_embeddings.py",
    "pr_number": 10284,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1882604849,
    "comment_created_at": "2024-12-12T17:34:49Z"
  },
  {
    "code": "@@ -291,86 +291,71 @@ def __init__(self, config: Optional[TextChunkingConfig] = None):\n         \"\"\"Initialize with text chunking configuration\"\"\"\n         super().__init__()\n         self.config = config or TextChunkingConfig()\n-        self.splitter = RecursiveCharacterTextSplitter(\n+        self.splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n+            encoding_name=self.config.encoding_name,\n             chunk_size=self.config.chunk_size,\n             chunk_overlap=self.config.chunk_overlap,\n             length_function=self.config.length_function,\n+            disallowed_special=(),  # Allow all special tokens\n             separators=self.config.separators,\n         )\n \n     def _split_document(self, doc: Document) -> List[Document]:\n         \"\"\"Split document into chunks while preserving metadata\"\"\"\n         # Use base class implementation\n-        return super()._split_document(doc)\n+        if self.splitter is None:",
    "comment": "Why not just use base class?",
    "line_number": 306,
    "enriched": "File: mindsdb/interfaces/knowledge_base/preprocessing/document_preprocessor.py\nCode: @@ -291,86 +291,71 @@ def __init__(self, config: Optional[TextChunkingConfig] = None):\n         \"\"\"Initialize with text chunking configuration\"\"\"\n         super().__init__()\n         self.config = config or TextChunkingConfig()\n-        self.splitter = RecursiveCharacterTextSplitter(\n+        self.splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n+            encoding_name=self.config.encoding_name,\n             chunk_size=self.config.chunk_size,\n             chunk_overlap=self.config.chunk_overlap,\n             length_function=self.config.length_function,\n+            disallowed_special=(),  # Allow all special tokens\n             separators=self.config.separators,\n         )\n \n     def _split_document(self, doc: Document) -> List[Document]:\n         \"\"\"Split document into chunks while preserving metadata\"\"\"\n         # Use base class implementation\n-        return super()._split_document(doc)\n+        if self.splitter is None:\nComment: Why not just use base class?",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/interfaces/knowledge_base/preprocessing/document_preprocessor.py",
    "pr_number": 10638,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2024620489,
    "comment_created_at": "2025-04-02T11:19:21Z"
  },
  {
    "code": "@@ -113,10 +113,18 @@ def on_agent_action(self, action: AgentAction, **kwargs: Any) -> Any:\n         '''Run on agent action.'''\n         self.logger.debug(f'Running tool {action.tool} with input:')\n         self.logger.debug(action.tool_input)\n+\n+        stop_block = 'Observation: '",
    "comment": "@dusvyat, I'm not sure about this. Maybe there is a better place to locate stop word",
    "line_number": 117,
    "enriched": "File: mindsdb/interfaces/agents/callback_handlers.py\nCode: @@ -113,10 +113,18 @@ def on_agent_action(self, action: AgentAction, **kwargs: Any) -> Any:\n         '''Run on agent action.'''\n         self.logger.debug(f'Running tool {action.tool} with input:')\n         self.logger.debug(action.tool_input)\n+\n+        stop_block = 'Observation: '\nComment: @dusvyat, I'm not sure about this. Maybe there is a better place to locate stop word",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/interfaces/agents/callback_handlers.py",
    "pr_number": 9753,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1778357278,
    "comment_created_at": "2024-09-27T09:56:54Z"
  },
  {
    "code": "@@ -180,6 +181,14 @@ def predict_text_classification(self, pipeline, item, args):\n         final[f\"{args['target']}_explain\"] = explain\n         return final\n \n+    def predict_text_generation(self, pipeline, item, args):\n+        result = pipeline([item], max_length=args[\"max_length\"])[0]\n+\n+        final = {}",
    "comment": "alternative (nit): \r\n\r\n```python\r\nfinal = {args[\"target\"]: result[\"generated_text\"]}\r\n```",
    "line_number": 187,
    "enriched": "File: mindsdb/integrations/handlers/huggingface_handler/huggingface_handler.py\nCode: @@ -180,6 +181,14 @@ def predict_text_classification(self, pipeline, item, args):\n         final[f\"{args['target']}_explain\"] = explain\n         return final\n \n+    def predict_text_generation(self, pipeline, item, args):\n+        result = pipeline([item], max_length=args[\"max_length\"])[0]\n+\n+        final = {}\nComment: alternative (nit): \r\n\r\n```python\r\nfinal = {args[\"target\"]: result[\"generated_text\"]}\r\n```",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/huggingface_handler/huggingface_handler.py",
    "pr_number": 8513,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1452775557,
    "comment_created_at": "2024-01-15T22:21:28Z"
  },
  {
    "code": "@@ -0,0 +1,31 @@\n+from mindsdb.integrations.libs.const import HANDLER_TYPE\n+from mindsdb.utilities.log import get_log\n+\n+logger = get_log()\n+\n+from .__about__ import __version__ as version, __description__ as description\n+\n+try:\n+    from .influxdb_handler import InfluxDBHandler as Handler\n+    logger.error(\"No error Importing InfluxDB Handler\")",
    "comment": "I think this should not be an error. It is confusing.",
    "line_number": 10,
    "enriched": "File: mindsdb/integrations/handlers/influxdb_handler/__init__.py\nCode: @@ -0,0 +1,31 @@\n+from mindsdb.integrations.libs.const import HANDLER_TYPE\n+from mindsdb.utilities.log import get_log\n+\n+logger = get_log()\n+\n+from .__about__ import __version__ as version, __description__ as description\n+\n+try:\n+    from .influxdb_handler import InfluxDBHandler as Handler\n+    logger.error(\"No error Importing InfluxDB Handler\")\nComment: I think this should not be an error. It is confusing.",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/influxdb_handler/__init__.py",
    "pr_number": 5754,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1174912258,
    "comment_created_at": "2023-04-24T07:52:45Z"
  },
  {
    "code": "@@ -112,6 +114,8 @@ An agent can be created, deleted, queried, and updated. Here is how you can do t\n You can query all agents using this command:\n \n ```sql\n+SHOW AGENTS;\n+",
    "comment": "Just in case I will do fix which allow:\r\nSHOW AGENTS From project name;",
    "line_number": 118,
    "enriched": "File: docs/mindsdb_sql/agents/agent.mdx\nCode: @@ -112,6 +114,8 @@ An agent can be created, deleted, queried, and updated. Here is how you can do t\n You can query all agents using this command:\n \n ```sql\n+SHOW AGENTS;\n+\nComment: Just in case I will do fix which allow:\r\nSHOW AGENTS From project name;",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "docs/mindsdb_sql/agents/agent.mdx",
    "pr_number": 8939,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1535818077,
    "comment_created_at": "2024-03-22T15:59:38Z"
  },
  {
    "code": "@@ -5,6 +5,22 @@ sidebarTitle: Persisting Predictions\n \n MindsDB provides a range of options for persisting predictions and forecasts. Let's explore all possibilities to save the prediction results.\n \n+<Note>\n+**Reasons to Save Predictions**\n+\n+Every time you want to get predictions, you need to query the model, usually joined with an input data table, like this:",
    "comment": "Maybe we can explain that querying the model by default returns the result set which is not persistent. For future use, it is better to persist the result-set instead of querying the model again with the same data. What do you think?",
    "line_number": 11,
    "enriched": "File: docs/faqs/persist-predictions.mdx\nCode: @@ -5,6 +5,22 @@ sidebarTitle: Persisting Predictions\n \n MindsDB provides a range of options for persisting predictions and forecasts. Let's explore all possibilities to save the prediction results.\n \n+<Note>\n+**Reasons to Save Predictions**\n+\n+Every time you want to get predictions, you need to query the model, usually joined with an input data table, like this:\nComment: Maybe we can explain that querying the model by default returns the result set which is not persistent. For future use, it is better to persist the result-set instead of querying the model again with the same data. What do you think?",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "docs/faqs/persist-predictions.mdx",
    "pr_number": 6374,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1211405505,
    "comment_created_at": "2023-05-31T09:55:43Z"
  },
  {
    "code": "@@ -0,0 +1,418 @@\n+import json\n+\n+import pandas as pd\n+from mindsdb_sql.parser.ast import BinaryOperation, Constant, Select\n+from mindsdb_sql.parser.ast.base import ASTNode\n+\n+from mindsdb.interfaces.agents.agents_controller import AgentsController\n+from mindsdb.interfaces.jobs.jobs_controller import JobsController\n+from mindsdb.interfaces.skills.skills_controller import SkillsController\n+from mindsdb.interfaces.database.views import ViewController\n+from mindsdb.interfaces.database.projects import ProjectController\n+\n+\n+from .system_tables import Table",
    "comment": "It is better to use absolute imports where possible",
    "line_number": 14,
    "enriched": "File: mindsdb/api/executor/datahub/datanodes/mindsdb_tables.py\nCode: @@ -0,0 +1,418 @@\n+import json\n+\n+import pandas as pd\n+from mindsdb_sql.parser.ast import BinaryOperation, Constant, Select\n+from mindsdb_sql.parser.ast.base import ASTNode\n+\n+from mindsdb.interfaces.agents.agents_controller import AgentsController\n+from mindsdb.interfaces.jobs.jobs_controller import JobsController\n+from mindsdb.interfaces.skills.skills_controller import SkillsController\n+from mindsdb.interfaces.database.views import ViewController\n+from mindsdb.interfaces.database.projects import ProjectController\n+\n+\n+from .system_tables import Table\nComment: It is better to use absolute imports where possible",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/api/executor/datahub/datanodes/mindsdb_tables.py",
    "pr_number": 9001,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1579271641,
    "comment_created_at": "2024-04-25T10:51:07Z"
  },
  {
    "code": "@@ -8,9 +8,88 @@\n import pandas as pd\n \n from tests.unit.executor_test_base import BaseExecutorDummyML\n+from mindsdb.utilities.exception import QueryError\n \n \n class TestFiles(BaseExecutorDummyML):\n+\n+    @pytest.fixture(autouse=True)\n+    def setup_and_teardown(self):\n+        \"\"\"Setup and teardown for each test\"\"\"\n+        yield\n+        try:\n+            for file_name in list(self.file_controller.list_files()):\n+                try:\n+                    self.file_controller.delete_file(file_name)\n+                except Exception:\n+                    pass\n+        except Exception:\n+            pass\n+\n+    @pytest.fixture(autouse=True)\n+    def create_tables_and_data(self):",
    "comment": "these data are not used by every test, it is more test specific data\r\nmaybe is better to create separated class (with all tests that use these files) and create these files once in `setup_class`?",
    "line_number": 30,
    "enriched": "File: tests/unit/executor/test_files.py\nCode: @@ -8,9 +8,88 @@\n import pandas as pd\n \n from tests.unit.executor_test_base import BaseExecutorDummyML\n+from mindsdb.utilities.exception import QueryError\n \n \n class TestFiles(BaseExecutorDummyML):\n+\n+    @pytest.fixture(autouse=True)\n+    def setup_and_teardown(self):\n+        \"\"\"Setup and teardown for each test\"\"\"\n+        yield\n+        try:\n+            for file_name in list(self.file_controller.list_files()):\n+                try:\n+                    self.file_controller.delete_file(file_name)\n+                except Exception:\n+                    pass\n+        except Exception:\n+            pass\n+\n+    @pytest.fixture(autouse=True)\n+    def create_tables_and_data(self):\nComment: these data are not used by every test, it is more test specific data\r\nmaybe is better to create separated class (with all tests that use these files) and create these files once in `setup_class`?",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "tests/unit/executor/test_files.py",
    "pr_number": 11773,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2450836733,
    "comment_created_at": "2025-10-22T08:10:27Z"
  },
  {
    "code": "@@ -31,6 +31,7 @@ pytz\n botocore\n boto3\n python-dateutil\n+pymupdf",
    "comment": "this is already included in `mindsdb_gateway` config.json fyi",
    "line_number": 34,
    "enriched": "File: requirements/requirements.txt\nCode: @@ -31,6 +31,7 @@ pytz\n botocore\n boto3\n python-dateutil\n+pymupdf\nComment: this is already included in `mindsdb_gateway` config.json fyi",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "requirements/requirements.txt",
    "pr_number": 8589,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1446105469,
    "comment_created_at": "2024-01-09T13:44:43Z"
  },
  {
    "code": "@@ -94,6 +94,8 @@ def connect(self):\n                 config[\"ssl_key\"] = ssl_key\n         if 'collation' not in config:\n             config['collation'] = 'utf8mb4_general_ci'\n+        if 'use_pure' not in config:",
    "comment": "Maybe we add this to docs so the users know it exists?",
    "line_number": 97,
    "enriched": "File: mindsdb/integrations/handlers/mysql_handler/mysql_handler.py\nCode: @@ -94,6 +94,8 @@ def connect(self):\n                 config[\"ssl_key\"] = ssl_key\n         if 'collation' not in config:\n             config['collation'] = 'utf8mb4_general_ci'\n+        if 'use_pure' not in config:\nComment: Maybe we add this to docs so the users know it exists?",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/mysql_handler/mysql_handler.py",
    "pr_number": 10565,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1998774540,
    "comment_created_at": "2025-03-17T13:49:42Z"
  },
  {
    "code": "@@ -82,32 +75,10 @@ def get_columns(self):\n \n \n class WebHandler(APIHandler):",
    "comment": "The `APIHandler` still requires a `check_connection()` method from what I can understand. The command executor still looks for it [here](https://github.com/mindsdb/mindsdb/blob/e6867f1ef42a0ff55e77f46c3ff3c44a8c96e825/mindsdb/api/executor/command_executor.py#L1099).\r\n\r\nI think this is the reason why I am not able to execute a `CREATE DATABASE` command successfully,\r\n![image](https://github.com/mindsdb/mindsdb/assets/49385643/c072e416-d99e-448f-b8b9-2cd3bbc4e157)\r\n",
    "line_number": 77,
    "enriched": "File: mindsdb/integrations/handlers/web_handler/web_handler.py\nCode: @@ -82,32 +75,10 @@ def get_columns(self):\n \n \n class WebHandler(APIHandler):\nComment: The `APIHandler` still requires a `check_connection()` method from what I can understand. The command executor still looks for it [here](https://github.com/mindsdb/mindsdb/blob/e6867f1ef42a0ff55e77f46c3ff3c44a8c96e825/mindsdb/api/executor/command_executor.py#L1099).\r\n\r\nI think this is the reason why I am not able to execute a `CREATE DATABASE` command successfully,\r\n![image](https://github.com/mindsdb/mindsdb/assets/49385643/c072e416-d99e-448f-b8b9-2cd3bbc4e157)\r\n",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/web_handler/web_handler.py",
    "pr_number": 9190,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1597928030,
    "comment_created_at": "2024-05-13T06:18:04Z"
  },
  {
    "code": "@@ -241,22 +238,30 @@\n                 \"The endpoint you are trying to access does not exist on the server.\",\n             )\n \n-        # Normalize the path.\n-        full_path = os.path.normpath(os.path.join(static_root, path))\n+        try:\n+            # Ensure the requested path is within the static directory\n+            # https://docs.python.org/3/library/pathlib.html#pathlib.PurePath.is_relative_to\n+            requested_path = (static_root / path).resolve()",
    "comment": "## Uncontrolled data used in path expression\n\nThis path depends on a [user-provided value](1).\n\n[Show more details](https://github.com/mindsdb/mindsdb/security/code-scanning/121)",
    "line_number": 244,
    "enriched": "File: mindsdb/api/http/initialize.py\nCode: @@ -241,22 +238,30 @@\n                 \"The endpoint you are trying to access does not exist on the server.\",\n             )\n \n-        # Normalize the path.\n-        full_path = os.path.normpath(os.path.join(static_root, path))\n+        try:\n+            # Ensure the requested path is within the static directory\n+            # https://docs.python.org/3/library/pathlib.html#pathlib.PurePath.is_relative_to\n+            requested_path = (static_root / path).resolve()\nComment: ## Uncontrolled data used in path expression\n\nThis path depends on a [user-provided value](1).\n\n[Show more details](https://github.com/mindsdb/mindsdb/security/code-scanning/121)",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/api/http/initialize.py",
    "pr_number": 11762,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2440682136,
    "comment_created_at": "2025-10-17T17:29:22Z"
  },
  {
    "code": "@@ -4,10 +4,13 @@ permissions:\n   contents: read\n \n on:\n-  pull_request_target:\n+  pull_request:",
    "comment": "**Security**: Changed trigger from `pull_request_target` to `pull_request`, which introduces a critical security vulnerability. This allows workflows to run in the context of the fork/PR with access to repository secrets, enabling potential secret exfiltration by malicious actors.\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n  pull_request_target:\n```\n</details>\n<!-- suggestion_end -->\n",
    "line_number": 7,
    "enriched": "File: .github/workflows/build_deploy_dev.yml\nCode: @@ -4,10 +4,13 @@ permissions:\n   contents: read\n \n on:\n-  pull_request_target:\n+  pull_request:\nComment: **Security**: Changed trigger from `pull_request_target` to `pull_request`, which introduces a critical security vulnerability. This allows workflows to run in the context of the fork/PR with access to repository secrets, enabling potential secret exfiltration by malicious actors.\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n  pull_request_target:\n```\n</details>\n<!-- suggestion_end -->\n",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": ".github/workflows/build_deploy_dev.yml",
    "pr_number": 11765,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2441498995,
    "comment_created_at": "2025-10-18T01:13:00Z"
  },
  {
    "code": "@@ -45,27 +46,54 @@ class SkillData:\n     agent_tables_list: Optional[List[str]]\n \n     @property\n-    def tables_list(self) -> List[str]:\n-        \"\"\"List of tables which may use this skill. If the list is empty, there are no restrictions.\n-        The result list is a combination of skill's and agent's tables lists.\n+    def restriction_on_tables(self) -> Optional[Dict[str, set]]:\n+        \"\"\"Schemas and tables which agent+skill may use. The result is intersections of skill's and agent's tables lists.\n \n         Returns:\n-            List[str]: List of tables.\n+            Optional[Dict[str, set]]: allowed schemas and tables. Schemas - are keys in dict, tables - are values.\n+                if result is None, then there are no restrictions\n \n         Raises:\n             ValueError: if there is no intersection between skill's and agent's list.\n                 This means that all tables restricted for use.\n         \"\"\"\n-        agent_tables_list = self.agent_tables_list or []\n-        skill_tables_list = self.params.get('tables', [])\n-        if len(skill_tables_list) > 0 and len(agent_tables_list) > 0:\n-            diff = set(skill_tables_list) & set(agent_tables_list)\n-            if len(diff) == 0:\n-                raise ValueError(\"There are no tables allowed for use.\")\n-            return list(diff)\n-        if len(skill_tables_list) > 0:\n-            return skill_tables_list\n-        return agent_tables_list\n+        def list_to_map(input: List) -> Dict:\n+            agent_tables_map = defaultdict(set)\n+            for x in input:\n+                if isinstance(x, str):\n+                    table_name = x\n+                    schema_name = None\n+                elif isinstance(x, dict):\n+                    table_name = x['table']\n+                    schema_name = x.get('schema')\n+                else:\n+                    raise ValueError(f'Unexpected value in tables list: {x}')\n+                agent_tables_map[schema_name].add(table_name)\n+            return agent_tables_map\n+\n+        agent_tables_map = list_to_map(self.agent_tables_list or [])\n+        skill_tables_map = list_to_map(self.params.get('tables', []))\n+\n+        if len(agent_tables_map) > 0 and len(skill_tables_map) > 0:\n+            if len(set(agent_tables_map) & set(skill_tables_map)) == 0:\n+                raise ValueError(\"Skill's and agent's allowed tables list have no shared schemas.\")\n+\n+            intersection_tables_map = defaultdict(set)\n+            has_intersection = False\n+            for schema_name in agent_tables_map:\n+                if schema_name not in skill_tables_map:\n+                    continue\n+                intersection_tables_map[schema_name] = agent_tables_map[schema_name] & skill_tables_map[schema_name]\n+                if len(intersection_tables_map[schema_name]) > 0:\n+                    has_intersection = True\n+            if has_intersection is False:",
    "comment": "instead of has_intersection variable this condition can be used:\r\nlen(intersection_tables_map[schema_name])>0",
    "line_number": 89,
    "enriched": "File: mindsdb/interfaces/skills/skill_tool.py\nCode: @@ -45,27 +46,54 @@ class SkillData:\n     agent_tables_list: Optional[List[str]]\n \n     @property\n-    def tables_list(self) -> List[str]:\n-        \"\"\"List of tables which may use this skill. If the list is empty, there are no restrictions.\n-        The result list is a combination of skill's and agent's tables lists.\n+    def restriction_on_tables(self) -> Optional[Dict[str, set]]:\n+        \"\"\"Schemas and tables which agent+skill may use. The result is intersections of skill's and agent's tables lists.\n \n         Returns:\n-            List[str]: List of tables.\n+            Optional[Dict[str, set]]: allowed schemas and tables. Schemas - are keys in dict, tables - are values.\n+                if result is None, then there are no restrictions\n \n         Raises:\n             ValueError: if there is no intersection between skill's and agent's list.\n                 This means that all tables restricted for use.\n         \"\"\"\n-        agent_tables_list = self.agent_tables_list or []\n-        skill_tables_list = self.params.get('tables', [])\n-        if len(skill_tables_list) > 0 and len(agent_tables_list) > 0:\n-            diff = set(skill_tables_list) & set(agent_tables_list)\n-            if len(diff) == 0:\n-                raise ValueError(\"There are no tables allowed for use.\")\n-            return list(diff)\n-        if len(skill_tables_list) > 0:\n-            return skill_tables_list\n-        return agent_tables_list\n+        def list_to_map(input: List) -> Dict:\n+            agent_tables_map = defaultdict(set)\n+            for x in input:\n+                if isinstance(x, str):\n+                    table_name = x\n+                    schema_name = None\n+                elif isinstance(x, dict):\n+                    table_name = x['table']\n+                    schema_name = x.get('schema')\n+                else:\n+                    raise ValueError(f'Unexpected value in tables list: {x}')\n+                agent_tables_map[schema_name].add(table_name)\n+            return agent_tables_map\n+\n+        agent_tables_map = list_to_map(self.agent_tables_list or [])\n+        skill_tables_map = list_to_map(self.params.get('tables', []))\n+\n+        if len(agent_tables_map) > 0 and len(skill_tables_map) > 0:\n+            if len(set(agent_tables_map) & set(skill_tables_map)) == 0:\n+                raise ValueError(\"Skill's and agent's allowed tables list have no shared schemas.\")\n+\n+            intersection_tables_map = defaultdict(set)\n+            has_intersection = False\n+            for schema_name in agent_tables_map:\n+                if schema_name not in skill_tables_map:\n+                    continue\n+                intersection_tables_map[schema_name] = agent_tables_map[schema_name] & skill_tables_map[schema_name]\n+                if len(intersection_tables_map[schema_name]) > 0:\n+                    has_intersection = True\n+            if has_intersection is False:\nComment: instead of has_intersection variable this condition can be used:\r\nlen(intersection_tables_map[schema_name])>0",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/interfaces/skills/skill_tool.py",
    "pr_number": 10194,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1912962629,
    "comment_created_at": "2025-01-13T10:21:18Z"
  },
  {
    "code": "@@ -0,0 +1,68 @@\n+---\n+title: PyPI\n+sidebarTitle: PyPI\n+---\n+\n+In this section, we present how to connect PyPI to MindsDB.\n+\n+[PyPI](https://pypi.org) is a host for maintaining and storing Python packages. It's a good place for publishing your Python packages in different versions and releases.\n+\n+Data from PyPI can be utilized within MindsDB to train models and make predictions about your Python packages.\n+\n+## Connection\n+\n+This handler is implemented using the standard Python `requests` library. It is used to connect to the RESTful service that [pypistats.org](https://pypistats.org) is serving.\n+\n+There are no connection arguments required to initialize the handler.\n+\n+To connect to PyPI using MindsDB, the following CREATE DATABASE statement can be used:\n+\n+```sql\n+CREATE DATABASE mediawiki_datasource",
    "comment": "Let's call it `pypi_datasource`.",
    "line_number": 21,
    "enriched": "File: docs/integrations/app-integrations/pypi.mdx\nCode: @@ -0,0 +1,68 @@\n+---\n+title: PyPI\n+sidebarTitle: PyPI\n+---\n+\n+In this section, we present how to connect PyPI to MindsDB.\n+\n+[PyPI](https://pypi.org) is a host for maintaining and storing Python packages. It's a good place for publishing your Python packages in different versions and releases.\n+\n+Data from PyPI can be utilized within MindsDB to train models and make predictions about your Python packages.\n+\n+## Connection\n+\n+This handler is implemented using the standard Python `requests` library. It is used to connect to the RESTful service that [pypistats.org](https://pypistats.org) is serving.\n+\n+There are no connection arguments required to initialize the handler.\n+\n+To connect to PyPI using MindsDB, the following CREATE DATABASE statement can be used:\n+\n+```sql\n+CREATE DATABASE mediawiki_datasource\nComment: Let's call it `pypi_datasource`.",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "docs/integrations/app-integrations/pypi.mdx",
    "pr_number": 8044,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1370278711,
    "comment_created_at": "2023-10-24T14:24:39Z"
  },
  {
    "code": "@@ -0,0 +1,103 @@\n+from http import HTTPStatus\n+from urllib.parse import urljoin\n+\n+import requests\n+\n+\n+class BigCommerceAPIClient:\n+    def __init__(self, url: str, access_token: str):\n+        # we have to use both endpoints: v2/ and v3/, so delete it from base url\n+        self.base_url = url.rstrip(\"/\")\n+        self.base_url = self.base_url.rstrip(\"v2\")\n+        self.base_url = self.base_url.rstrip(\"v3\")\n+        if not self.base_url.endswith(\"/\"):\n+            self.base_url += \"/\"\n+\n+        self.access_token = access_token\n+        self.session = requests.Session()\n+        self.session.headers.update(\n+            {\n+                \"X-Auth-Token\": self.access_token,\n+                \"Content-Type\": \"application/json\",\n+                \"Accept\": \"application/json\",\n+            }\n+        )\n+\n+    def get_products(\n+        self,\n+        filter: dict = None,\n+        sort_condition: dict = None,\n+        limit: int = 999999999,\n+    ):\n+        # doc: https://developer.bigcommerce.com/docs/rest-catalog/products#get-all-products\n+        params = {\n+            \"limit\": limit,\n+        }\n+        if filter is not None:\n+            params.update(filter)\n+        if sort_condition is not None:\n+            params.update(sort_condition)\n+        return self._make_request_v3(\"GET\", \"catalog/products\", params=params)\n+\n+    def get_customers(\n+        self,\n+        filter: dict = None,\n+        sort_condition: dict = None,\n+        limit: int = 999999999,\n+    ):\n+        # doc: https://developer.bigcommerce.com/docs/rest-management/customers#get-all-customers\n+        params = {\n+            \"limit\": 999999999,\n+        }\n+        if filter:\n+            params.update(filter)\n+        if sort_condition:\n+            params[\"sort\"] = sort_condition\n+        return self._make_request_v3(\"GET\", \"customers\", params=params)\n+\n+    def get_orders(\n+        self,\n+        filter: dict = None,\n+        sort_condition: str = None,\n+        limit: int = 999999999,\n+    ):\n+        # doc: https://developer.bigcommerce.com/docs/rest-management/orders#get-all-orders\n+        params = {\"limit\": limit}\n+        if filter:\n+            params.update(filter)\n+        if sort_condition:\n+            params[\"sort\"] = sort_condition\n+\n+        return self._make_request_v2(\"GET\", \"orders\", params=filter)",
    "comment": "**correctness**: `get_orders` ignores the `limit` and `sort_condition` parameters and always passes `filter` as `params`, causing incorrect results and potentially returning too many or unsorted orders.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/integrations/handlers/bigcommerce_handler/bigcommerce_api_client.py, lines 65-71, the `get_orders` method incorrectly passes only the `filter` dictionary as `params` to `_make_request_v2`, ignoring the constructed `params` dict that includes `limit` and `sort_condition`. Update the last line to pass `params=params` instead of `params=filter` to ensure correct query parameter handling.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n        params = {\"limit\": limit}\n        if filter:\n            params.update(filter)\n        if sort_condition:\n            params[\"sort\"] = sort_condition\n\n        return self._make_request_v2(\"GET\", \"orders\", params=params)\n```\n</details>\n<!-- suggestion_end -->\n",
    "line_number": 71,
    "enriched": "File: mindsdb/integrations/handlers/bigcommerce_handler/bigcommerce_api_client.py\nCode: @@ -0,0 +1,103 @@\n+from http import HTTPStatus\n+from urllib.parse import urljoin\n+\n+import requests\n+\n+\n+class BigCommerceAPIClient:\n+    def __init__(self, url: str, access_token: str):\n+        # we have to use both endpoints: v2/ and v3/, so delete it from base url\n+        self.base_url = url.rstrip(\"/\")\n+        self.base_url = self.base_url.rstrip(\"v2\")\n+        self.base_url = self.base_url.rstrip(\"v3\")\n+        if not self.base_url.endswith(\"/\"):\n+            self.base_url += \"/\"\n+\n+        self.access_token = access_token\n+        self.session = requests.Session()\n+        self.session.headers.update(\n+            {\n+                \"X-Auth-Token\": self.access_token,\n+                \"Content-Type\": \"application/json\",\n+                \"Accept\": \"application/json\",\n+            }\n+        )\n+\n+    def get_products(\n+        self,\n+        filter: dict = None,\n+        sort_condition: dict = None,\n+        limit: int = 999999999,\n+    ):\n+        # doc: https://developer.bigcommerce.com/docs/rest-catalog/products#get-all-products\n+        params = {\n+            \"limit\": limit,\n+        }\n+        if filter is not None:\n+            params.update(filter)\n+        if sort_condition is not None:\n+            params.update(sort_condition)\n+        return self._make_request_v3(\"GET\", \"catalog/products\", params=params)\n+\n+    def get_customers(\n+        self,\n+        filter: dict = None,\n+        sort_condition: dict = None,\n+        limit: int = 999999999,\n+    ):\n+        # doc: https://developer.bigcommerce.com/docs/rest-management/customers#get-all-customers\n+        params = {\n+            \"limit\": 999999999,\n+        }\n+        if filter:\n+            params.update(filter)\n+        if sort_condition:\n+            params[\"sort\"] = sort_condition\n+        return self._make_request_v3(\"GET\", \"customers\", params=params)\n+\n+    def get_orders(\n+        self,\n+        filter: dict = None,\n+        sort_condition: str = None,\n+        limit: int = 999999999,\n+    ):\n+        # doc: https://developer.bigcommerce.com/docs/rest-management/orders#get-all-orders\n+        params = {\"limit\": limit}\n+        if filter:\n+            params.update(filter)\n+        if sort_condition:\n+            params[\"sort\"] = sort_condition\n+\n+        return self._make_request_v2(\"GET\", \"orders\", params=filter)\nComment: **correctness**: `get_orders` ignores the `limit` and `sort_condition` parameters and always passes `filter` as `params`, causing incorrect results and potentially returning too many or unsorted orders.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/integrations/handlers/bigcommerce_handler/bigcommerce_api_client.py, lines 65-71, the `get_orders` method incorrectly passes only the `filter` dictionary as `params` to `_make_request_v2`, ignoring the constructed `params` dict that includes `limit` and `sort_condition`. Update the last line to pass `params=params` instead of `params=filter` to ensure correct query parameter handling.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n        params = {\"limit\": limit}\n        if filter:\n            params.update(filter)\n        if sort_condition:\n            params[\"sort\"] = sort_condition\n\n        return self._make_request_v2(\"GET\", \"orders\", params=params)\n```\n</details>\n<!-- suggestion_end -->\n",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/bigcommerce_handler/bigcommerce_api_client.py",
    "pr_number": 11785,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2461013386,
    "comment_created_at": "2025-10-24T15:23:49Z"
  },
  {
    "code": "@@ -61,12 +62,20 @@ def connect(self) -> ibm_db_dbi.Connection:\n         # Mandatory connection parameters.\n         if not all(key in self.connection_data for key in ['host', 'user', 'password', 'database']):\n             raise ValueError('Required parameters (host, user, password, database) must be provided.')\n+        cloud = 'databases.appdomain.cloud' in self.connection_data['host']\n+        if cloud:\n+            connection_string = f\"DATABASE={self.connection_data['database']};HOSTNAME={self.connection_data['host']};PORT={self.connection_data['port']};PROTOCOL=TCPIP;UID={self.connection_data['user']};PWD={self.connection_data['password']};SECURITY=SSL;\"",
    "comment": "**correctness**: `self.connection_data['port']` is used unconditionally in the cloud connection string, but 'port' may not exist, causing a `KeyError` and runtime failure.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/integrations/handlers/db2_handler/db2_handler.py, lines 65-67, the code unconditionally uses self.connection_data['port'] in the cloud connection string, which will cause a KeyError if 'port' is missing. Refactor this block so that 'PORT=...' is only added if 'port' exists in self.connection_data, matching the handling in the non-cloud branch.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n        cloud = 'databases.appdomain.cloud' in self.connection_data['host']\n        if cloud:\n            connection_string = f\"DATABASE={self.connection_data['database']};HOSTNAME={self.connection_data['host']};PROTOCOL=TCPIP;UID={self.connection_data['user']};PWD={self.connection_data['password']};SECURITY=SSL;\"\n            if 'port' in self.connection_data:\n                connection_string += f\"PORT={self.connection_data['port']};\"\n                connection_string += \"SSLSERVERCERTIFICATE=;\"\n```\n</details>\n<!-- suggestion_end -->\n",
    "line_number": 67,
    "enriched": "File: mindsdb/integrations/handlers/db2_handler/db2_handler.py\nCode: @@ -61,12 +62,20 @@ def connect(self) -> ibm_db_dbi.Connection:\n         # Mandatory connection parameters.\n         if not all(key in self.connection_data for key in ['host', 'user', 'password', 'database']):\n             raise ValueError('Required parameters (host, user, password, database) must be provided.')\n+        cloud = 'databases.appdomain.cloud' in self.connection_data['host']\n+        if cloud:\n+            connection_string = f\"DATABASE={self.connection_data['database']};HOSTNAME={self.connection_data['host']};PORT={self.connection_data['port']};PROTOCOL=TCPIP;UID={self.connection_data['user']};PWD={self.connection_data['password']};SECURITY=SSL;\"\nComment: **correctness**: `self.connection_data['port']` is used unconditionally in the cloud connection string, but 'port' may not exist, causing a `KeyError` and runtime failure.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/integrations/handlers/db2_handler/db2_handler.py, lines 65-67, the code unconditionally uses self.connection_data['port'] in the cloud connection string, which will cause a KeyError if 'port' is missing. Refactor this block so that 'PORT=...' is only added if 'port' exists in self.connection_data, matching the handling in the non-cloud branch.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n        cloud = 'databases.appdomain.cloud' in self.connection_data['host']\n        if cloud:\n            connection_string = f\"DATABASE={self.connection_data['database']};HOSTNAME={self.connection_data['host']};PROTOCOL=TCPIP;UID={self.connection_data['user']};PWD={self.connection_data['password']};SECURITY=SSL;\"\n            if 'port' in self.connection_data:\n                connection_string += f\"PORT={self.connection_data['port']};\"\n                connection_string += \"SSLSERVERCERTIFICATE=;\"\n```\n</details>\n<!-- suggestion_end -->\n",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/db2_handler/db2_handler.py",
    "pr_number": 11423,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2266964177,
    "comment_created_at": "2025-08-11T14:30:36Z"
  },
  {
    "code": "@@ -174,4 +174,42 @@ def test_hf_translation(self, mock_handler):\n         '''\n         self.hf_test_run(mock_handler, model_name, create_sql, predict_sql)\n \n+    @patch('mindsdb.integrations.handlers.postgres_handler.Handler')\n+    def test_hf_text2text(self, mock_handler):\n+\n+        # create predictor\n+        create_sql = '''                \n+        CREATE MODEL mindsdb.text_generator\n+        predict PREDICTION\n+        USING\n+            engine='huggingface',\n+            join_learn_process=true,\n+            task = \"text2text-generation\",\n+            model_name = \"google/flan-t5-base\",\n+            input_column = \"text_short\"\n+        '''\n+\n+        model_name = 'text_generator'\n+\n+        predict_sql = '''",
    "comment": "Does it make sense to run the text generator on the input defined for `pg.df` at the start of this file? Perhaps you can change this into the `SELECT * FROM ... WHERE` query you've left as a comment in line 214, instead.",
    "line_number": 194,
    "enriched": "File: tests/unit/ml_handlers/test_huggingface.py\nCode: @@ -174,4 +174,42 @@ def test_hf_translation(self, mock_handler):\n         '''\n         self.hf_test_run(mock_handler, model_name, create_sql, predict_sql)\n \n+    @patch('mindsdb.integrations.handlers.postgres_handler.Handler')\n+    def test_hf_text2text(self, mock_handler):\n+\n+        # create predictor\n+        create_sql = '''                \n+        CREATE MODEL mindsdb.text_generator\n+        predict PREDICTION\n+        USING\n+            engine='huggingface',\n+            join_learn_process=true,\n+            task = \"text2text-generation\",\n+            model_name = \"google/flan-t5-base\",\n+            input_column = \"text_short\"\n+        '''\n+\n+        model_name = 'text_generator'\n+\n+        predict_sql = '''\nComment: Does it make sense to run the text generator on the input defined for `pg.df` at the start of this file? Perhaps you can change this into the `SELECT * FROM ... WHERE` query you've left as a comment in line 214, instead.",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "tests/unit/ml_handlers/test_huggingface.py",
    "pr_number": 5624,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1171589255,
    "comment_created_at": "2023-04-19T16:30:41Z"
  },
  {
    "code": "@@ -401,10 +401,7 @@ def drop_table(self, table_name: str, if_exists=True):\n             if if_exists:\n                 return Response(resp_type=RESPONSE_TYPE.OK)",
    "comment": "We can also remove 'Response(..' and leave just 'return' ",
    "line_number": 402,
    "enriched": "File: mindsdb/integrations/handlers/chromadb_handler/chromadb_handler.py\nCode: @@ -401,10 +401,7 @@ def drop_table(self, table_name: str, if_exists=True):\n             if if_exists:\n                 return Response(resp_type=RESPONSE_TYPE.OK)\nComment: We can also remove 'Response(..' and leave just 'return' ",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/chromadb_handler/chromadb_handler.py",
    "pr_number": 8612,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1457587995,
    "comment_created_at": "2024-01-18T15:14:14Z"
  },
  {
    "code": "@@ -0,0 +1,75 @@\n+from typing import Optional\n+\n+import pandas as pd\n+\n+from mindsdb.integrations.handlers.sentence_transformers_handler.settings import Parameters\n+\n+from mindsdb.integrations.handlers.rag_handler.settings import load_embeddings_model, df_to_documents\n+\n+\n+from mindsdb.integrations.libs.base import BaseMLEngine\n+from mindsdb.utilities import log\n+\n+logger = log.getLogger(__name__)\n+\n+\n+class SentenceTransformersHandler(BaseMLEngine):\n+    name = \"sentence transformers\"\n+\n+    def __init__(self, model_storage, engine_storage, **kwargs) -> None:\n+        super().__init__(model_storage, engine_storage, **kwargs)\n+        self.generative = True\n+\n+    def create(self, target, df=None, args=None, **kwargs):\n+        \"\"\"creates embeddings model and persists\"\"\"\n+\n+        args = args[\"using\"]\n+\n+        valid_args = Parameters(**args)\n+        self.model_storage.json_set(\"args\", valid_args.dict())\n+\n+    def predict(self, df, args=None):\n+        \"\"\"loads persisted embeddings model and gets embeddings on input text column(s)\"\"\"\n+\n+        args = self.model_storage.json_get(\"args\")\n+\n+        if isinstance(df['content'].iloc[0], list) and len(df['content']) == 1:\n+            # allow user to pass in a list of strings in where clause\n+            # i.e where content = ['hello', 'world'] or where content = (select content from some_db.some_table)\n+            input_df = df.copy()\n+            df = pd.DataFrame(data={\"content\": input_df['content'].iloc[0]})\n+\n+        # get text columns if specified\n+        if isinstance(args['text_columns'], str):\n+            columns = [args['text_columns']]\n+\n+        elif isinstance(args['text_columns'], list):\n+            columns = args['text_columns']\n+\n+        elif args['text_columns'] is None:\n+            # assume all columns are text columns\n+            logger.info(\"No text columns specified, assuming all columns are text columns\")\n+            columns = df.columns.tolist()\n+\n+        else:\n+            raise ValueError(f\"Invalid value for text_columns: {args['text_columns']}\")\n+\n+        documents = df_to_documents(df=df, page_content_columns=columns)\n+\n+        content = [doc.page_content for doc in documents]\n+        metadata = [doc.metadata for doc in documents]\n+\n+        model = load_embeddings_model(args['embeddings_model_name'])",
    "comment": "I see this has been moved from `create` to `predict`. I'm guessing HF loading is probably faster than our vanilla serialization right? So hopefully no negative impact in terms of runtime when under intense prediction load.",
    "line_number": 62,
    "enriched": "File: mindsdb/integrations/handlers/sentence_transformers_handler/sentence_transformers_handler.py\nCode: @@ -0,0 +1,75 @@\n+from typing import Optional\n+\n+import pandas as pd\n+\n+from mindsdb.integrations.handlers.sentence_transformers_handler.settings import Parameters\n+\n+from mindsdb.integrations.handlers.rag_handler.settings import load_embeddings_model, df_to_documents\n+\n+\n+from mindsdb.integrations.libs.base import BaseMLEngine\n+from mindsdb.utilities import log\n+\n+logger = log.getLogger(__name__)\n+\n+\n+class SentenceTransformersHandler(BaseMLEngine):\n+    name = \"sentence transformers\"\n+\n+    def __init__(self, model_storage, engine_storage, **kwargs) -> None:\n+        super().__init__(model_storage, engine_storage, **kwargs)\n+        self.generative = True\n+\n+    def create(self, target, df=None, args=None, **kwargs):\n+        \"\"\"creates embeddings model and persists\"\"\"\n+\n+        args = args[\"using\"]\n+\n+        valid_args = Parameters(**args)\n+        self.model_storage.json_set(\"args\", valid_args.dict())\n+\n+    def predict(self, df, args=None):\n+        \"\"\"loads persisted embeddings model and gets embeddings on input text column(s)\"\"\"\n+\n+        args = self.model_storage.json_get(\"args\")\n+\n+        if isinstance(df['content'].iloc[0], list) and len(df['content']) == 1:\n+            # allow user to pass in a list of strings in where clause\n+            # i.e where content = ['hello', 'world'] or where content = (select content from some_db.some_table)\n+            input_df = df.copy()\n+            df = pd.DataFrame(data={\"content\": input_df['content'].iloc[0]})\n+\n+        # get text columns if specified\n+        if isinstance(args['text_columns'], str):\n+            columns = [args['text_columns']]\n+\n+        elif isinstance(args['text_columns'], list):\n+            columns = args['text_columns']\n+\n+        elif args['text_columns'] is None:\n+            # assume all columns are text columns\n+            logger.info(\"No text columns specified, assuming all columns are text columns\")\n+            columns = df.columns.tolist()\n+\n+        else:\n+            raise ValueError(f\"Invalid value for text_columns: {args['text_columns']}\")\n+\n+        documents = df_to_documents(df=df, page_content_columns=columns)\n+\n+        content = [doc.page_content for doc in documents]\n+        metadata = [doc.metadata for doc in documents]\n+\n+        model = load_embeddings_model(args['embeddings_model_name'])\nComment: I see this has been moved from `create` to `predict`. I'm guessing HF loading is probably faster than our vanilla serialization right? So hopefully no negative impact in terms of runtime when under intense prediction load.",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/sentence_transformers_handler/sentence_transformers_handler.py",
    "pr_number": 8464,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1418890982,
    "comment_created_at": "2023-12-07T12:28:48Z"
  },
  {
    "code": "@@ -0,0 +1,171 @@\n+from mindsdb.integrations.libs.base import BaseMLEngine\n+import pandas as pd\n+from mindsdb.integrations.handlers.anomaly_detection_handler.utils import (\n+    train_unsupervised,\n+    train_supervised,\n+    train_semisupervised,\n+)\n+from joblib import dump, load\n+from pyod.models.ecod import ECOD  # unsupervised default\n+from pyod.models.xgbod import XGBOD  # semi-supervised default\n+from catboost import CatBoostClassifier  # supervised default\n+from pyod.models.lof import LOF\n+from pyod.models.knn import KNN\n+from pyod.models.pca import PCA\n+from xgboost import XGBClassifier\n+from sklearn.naive_bayes import GaussianNB\n+import numpy as np\n+\n+\n+MODELS = {\n+    \"supervised\": {\n+        \"catboost\": CatBoostClassifier(logging_level=\"Silent\"),\n+        \"xgb\": XGBClassifier(),\n+        \"nb\": GaussianNB(),\n+    },\n+    \"semi-supervised\": {\n+        \"xgbod\": XGBOD(estimator_list=[ECOD()]),\n+    },\n+    \"unsupervised\": {\n+        \"ecod\": ECOD(),\n+        \"knn\": KNN(),\n+        \"pca\": PCA(),\n+        \"lof\": LOF(),\n+    },\n+}\n+\n+\n+def choose_model_type(training_df, model_type=None, target=None, supervised_threshold=3000):\n+    \"\"\"Choose the model type based on the presence of labels and size of the dataset\"\"\"\n+    if model_type is None:\n+        if target is None:\n+            model_type = \"unsupervised\"\n+        else:\n+            model_type = \"supervised\" if len(training_df) > supervised_threshold else \"semi-supervised\"\n+    assert model_type in [\n+        \"supervised\",\n+        \"semi-supervised\",\n+        \"unsupervised\",\n+    ], \"model type must be 'supervised', 'semi-supervised', or 'unsupervised'\"\n+    return model_type\n+\n+\n+def choose_model(df, model_name=None, model_type=None, target=None, supervised_threshold=3000):\n+    \"\"\"Choose the best model based on the size of the dataset and the model type\"\"\"\n+    training_df = preprocess_data(df)\n+    model_type = choose_model_type(training_df, model_type, target, supervised_threshold)\n+    if model_name is not None:\n+        assert model_name in MODELS[model_type], f\"model name must be one of {list(MODELS[model_type].keys())}\"\n+        model = MODELS[model_type][model_name]\n+    else:\n+        model = None\n+    if model_type == \"unsupervised\":\n+        return train_unsupervised(training_df, model=model)\n+\n+    X_train = training_df.drop(target, axis=1)\n+    y_train = training_df[target].astype(int)\n+\n+    if model_type == \"supervised\":\n+        return train_supervised(X_train, y_train, model=model)\n+    elif model_type == \"semi-supervised\":\n+        return train_semisupervised(X_train, y_train)  # Only one semi-supervised model available\n+\n+\n+def anomaly_type_to_model_name(anomaly_type):\n+    \"\"\"Choose the best model name based on the anomaly type\"\"\"\n+    assert anomaly_type in [\n+        \"local\",\n+        \"global\",\n+        \"clustered\",\n+        \"dependency\",\n+    ], \"anomaly type must be 'local', 'global', 'clustered', or 'dependency'\"\n+    anomaly_type_dict = {\n+        \"local\": \"lof\",\n+        \"global\": \"knn\",\n+        \"clustered\": \"pca\",\n+        \"dependency\": \"knn\",\n+    }\n+    return anomaly_type_dict[anomaly_type]\n+\n+\n+def preprocess_data(df):\n+    \"\"\"Preprocess the data by one-hot encoding categorical columns and scaling numeric columns\"\"\"\n+    # one-hot encode categorical columns\n+    categorical_columns = list(df.select_dtypes(include=[\"object\"]).columns.values)\n+    df[categorical_columns] = df[categorical_columns].astype(\"category\")\n+    df[categorical_columns] = df[categorical_columns].apply(lambda x: x.cat.codes)\n+    df = pd.get_dummies(df, columns=categorical_columns)\n+    # scale numeric columns to have mean 0 and std 1\n+    numeric_columns = list(df.select_dtypes(include=[\"float64\", \"int64\"]).columns.values)\n+    df[numeric_columns] = (df[numeric_columns] - df[numeric_columns].mean()) / df[numeric_columns].std()\n+    return df\n+\n+\n+def get_model_names(using_args):\n+    \"\"\"Get the model names from the using_args. Model names is a list of model names to train.\n+    If the model is not an ensemble, it only contains one model\"\"\"\n+    model_names = anomaly_type_to_model_name(using_args[\"anomaly_type\"]) if \"anomaly_type\" in using_args else None\n+    model_names = using_args[\"model_name\"] if \"model_name\" in using_args else model_names\n+    model_names = using_args[\"ensemble_models\"] if \"ensemble_models\" in using_args else model_names\n+    model_names = [model_names] if model_names is None else model_names\n+    model_names = [model_names] if type(model_names) is str else model_names\n+    return model_names\n+\n+\n+class AnomalyDetectionHandler(BaseMLEngine):\n+    \"\"\"Integration with the PyOD and CatBoost libraries for\n+    anomaly detection. Both supervised and unsupervised.\n+    \"\"\"\n+\n+    name = \"anomaly_detection\"\n+\n+    def __init__(self, *args, **kwargs):\n+        super().__init__(*args, **kwargs)\n+        self.generative = True  # makes unsupervised learning work\n+\n+    def create(self, target, df, args={}):\n+        \"\"\"Train a model and save it to the model storage\"\"\"\n+        using_args = args[\"using\"]\n+        model_type = using_args[\"type\"] if \"type\" in using_args else None\n+\n+        model_names = get_model_names(using_args)\n+\n+        model_save_paths = []\n+        model_targets = []\n+        model_class_names = []\n+        for model_name in model_names:\n+            model = choose_model(df, model_name=model_name, model_type=model_type, target=target)\n+            this_model_target = \"outlier\" if target is None else target  # output column name for unsupervised learning",
    "comment": "It may be better to opt for \"anomaly\" instead of outlier, to be consistent with other engines. It's a minor thing though. As long as we document this clearly for the user, shouldn't be a problem.",
    "line_number": 138,
    "enriched": "File: mindsdb/integrations/handlers/anomaly_detection_handler/anomaly_detection_handler.py\nCode: @@ -0,0 +1,171 @@\n+from mindsdb.integrations.libs.base import BaseMLEngine\n+import pandas as pd\n+from mindsdb.integrations.handlers.anomaly_detection_handler.utils import (\n+    train_unsupervised,\n+    train_supervised,\n+    train_semisupervised,\n+)\n+from joblib import dump, load\n+from pyod.models.ecod import ECOD  # unsupervised default\n+from pyod.models.xgbod import XGBOD  # semi-supervised default\n+from catboost import CatBoostClassifier  # supervised default\n+from pyod.models.lof import LOF\n+from pyod.models.knn import KNN\n+from pyod.models.pca import PCA\n+from xgboost import XGBClassifier\n+from sklearn.naive_bayes import GaussianNB\n+import numpy as np\n+\n+\n+MODELS = {\n+    \"supervised\": {\n+        \"catboost\": CatBoostClassifier(logging_level=\"Silent\"),\n+        \"xgb\": XGBClassifier(),\n+        \"nb\": GaussianNB(),\n+    },\n+    \"semi-supervised\": {\n+        \"xgbod\": XGBOD(estimator_list=[ECOD()]),\n+    },\n+    \"unsupervised\": {\n+        \"ecod\": ECOD(),\n+        \"knn\": KNN(),\n+        \"pca\": PCA(),\n+        \"lof\": LOF(),\n+    },\n+}\n+\n+\n+def choose_model_type(training_df, model_type=None, target=None, supervised_threshold=3000):\n+    \"\"\"Choose the model type based on the presence of labels and size of the dataset\"\"\"\n+    if model_type is None:\n+        if target is None:\n+            model_type = \"unsupervised\"\n+        else:\n+            model_type = \"supervised\" if len(training_df) > supervised_threshold else \"semi-supervised\"\n+    assert model_type in [\n+        \"supervised\",\n+        \"semi-supervised\",\n+        \"unsupervised\",\n+    ], \"model type must be 'supervised', 'semi-supervised', or 'unsupervised'\"\n+    return model_type\n+\n+\n+def choose_model(df, model_name=None, model_type=None, target=None, supervised_threshold=3000):\n+    \"\"\"Choose the best model based on the size of the dataset and the model type\"\"\"\n+    training_df = preprocess_data(df)\n+    model_type = choose_model_type(training_df, model_type, target, supervised_threshold)\n+    if model_name is not None:\n+        assert model_name in MODELS[model_type], f\"model name must be one of {list(MODELS[model_type].keys())}\"\n+        model = MODELS[model_type][model_name]\n+    else:\n+        model = None\n+    if model_type == \"unsupervised\":\n+        return train_unsupervised(training_df, model=model)\n+\n+    X_train = training_df.drop(target, axis=1)\n+    y_train = training_df[target].astype(int)\n+\n+    if model_type == \"supervised\":\n+        return train_supervised(X_train, y_train, model=model)\n+    elif model_type == \"semi-supervised\":\n+        return train_semisupervised(X_train, y_train)  # Only one semi-supervised model available\n+\n+\n+def anomaly_type_to_model_name(anomaly_type):\n+    \"\"\"Choose the best model name based on the anomaly type\"\"\"\n+    assert anomaly_type in [\n+        \"local\",\n+        \"global\",\n+        \"clustered\",\n+        \"dependency\",\n+    ], \"anomaly type must be 'local', 'global', 'clustered', or 'dependency'\"\n+    anomaly_type_dict = {\n+        \"local\": \"lof\",\n+        \"global\": \"knn\",\n+        \"clustered\": \"pca\",\n+        \"dependency\": \"knn\",\n+    }\n+    return anomaly_type_dict[anomaly_type]\n+\n+\n+def preprocess_data(df):\n+    \"\"\"Preprocess the data by one-hot encoding categorical columns and scaling numeric columns\"\"\"\n+    # one-hot encode categorical columns\n+    categorical_columns = list(df.select_dtypes(include=[\"object\"]).columns.values)\n+    df[categorical_columns] = df[categorical_columns].astype(\"category\")\n+    df[categorical_columns] = df[categorical_columns].apply(lambda x: x.cat.codes)\n+    df = pd.get_dummies(df, columns=categorical_columns)\n+    # scale numeric columns to have mean 0 and std 1\n+    numeric_columns = list(df.select_dtypes(include=[\"float64\", \"int64\"]).columns.values)\n+    df[numeric_columns] = (df[numeric_columns] - df[numeric_columns].mean()) / df[numeric_columns].std()\n+    return df\n+\n+\n+def get_model_names(using_args):\n+    \"\"\"Get the model names from the using_args. Model names is a list of model names to train.\n+    If the model is not an ensemble, it only contains one model\"\"\"\n+    model_names = anomaly_type_to_model_name(using_args[\"anomaly_type\"]) if \"anomaly_type\" in using_args else None\n+    model_names = using_args[\"model_name\"] if \"model_name\" in using_args else model_names\n+    model_names = using_args[\"ensemble_models\"] if \"ensemble_models\" in using_args else model_names\n+    model_names = [model_names] if model_names is None else model_names\n+    model_names = [model_names] if type(model_names) is str else model_names\n+    return model_names\n+\n+\n+class AnomalyDetectionHandler(BaseMLEngine):\n+    \"\"\"Integration with the PyOD and CatBoost libraries for\n+    anomaly detection. Both supervised and unsupervised.\n+    \"\"\"\n+\n+    name = \"anomaly_detection\"\n+\n+    def __init__(self, *args, **kwargs):\n+        super().__init__(*args, **kwargs)\n+        self.generative = True  # makes unsupervised learning work\n+\n+    def create(self, target, df, args={}):\n+        \"\"\"Train a model and save it to the model storage\"\"\"\n+        using_args = args[\"using\"]\n+        model_type = using_args[\"type\"] if \"type\" in using_args else None\n+\n+        model_names = get_model_names(using_args)\n+\n+        model_save_paths = []\n+        model_targets = []\n+        model_class_names = []\n+        for model_name in model_names:\n+            model = choose_model(df, model_name=model_name, model_type=model_type, target=target)\n+            this_model_target = \"outlier\" if target is None else target  # output column name for unsupervised learning\nComment: It may be better to opt for \"anomaly\" instead of outlier, to be consistent with other engines. It's a minor thing though. As long as we document this clearly for the user, shouldn't be a problem.",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/anomaly_detection_handler/anomaly_detection_handler.py",
    "pr_number": 8032,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1373604413,
    "comment_created_at": "2023-10-26T18:30:36Z"
  },
  {
    "code": "@@ -178,7 +178,7 @@ def select_query(self, query: Select) -> pd.DataFrame:\n         query_conditions = db_handler.extract_conditions(query.where)\n         if query_conditions is not None:\n             for item in query_conditions:\n-                if item.column == \"relevance_threshold\" and item.op.value == \"=\":\n+                if item.column == \"relevance\" and item.op.value == FilterOperator.GREATER_THAN_OR_EQUAL:\n                     try:\n                         relevance_threshold = float(item.value)\n                         # Validate range: must be between 0 and 1",
    "comment": "**security**: `item.value` from user-controlled query is directly cast to `float` for `relevance_threshold` without type or range validation, allowing injection of non-numeric or malicious values that could cause exceptions or logic bypass.\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n                        if not isinstance(item.value, (int, float, str)) or isinstance(item.value, bool):\n                            raise ValueError(f\"Invalid type for relevance_threshold: {type(item.value)}\")\n                        try:\n                            relevance_threshold = float(item.value)\n                        except (ValueError, TypeError):\n                            raise ValueError(f\"relevance_threshold must be a number, got: {item.value}\")\n                        # Validate range: must be between 0 and 1\n```\n</details>\n<!-- suggestion_end -->\n",
    "line_number": 184,
    "enriched": "File: mindsdb/interfaces/knowledge_base/controller.py\nCode: @@ -178,7 +178,7 @@ def select_query(self, query: Select) -> pd.DataFrame:\n         query_conditions = db_handler.extract_conditions(query.where)\n         if query_conditions is not None:\n             for item in query_conditions:\n-                if item.column == \"relevance_threshold\" and item.op.value == \"=\":\n+                if item.column == \"relevance\" and item.op.value == FilterOperator.GREATER_THAN_OR_EQUAL:\n                     try:\n                         relevance_threshold = float(item.value)\n                         # Validate range: must be between 0 and 1\nComment: **security**: `item.value` from user-controlled query is directly cast to `float` for `relevance_threshold` without type or range validation, allowing injection of non-numeric or malicious values that could cause exceptions or logic bypass.\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n                        if not isinstance(item.value, (int, float, str)) or isinstance(item.value, bool):\n                            raise ValueError(f\"Invalid type for relevance_threshold: {type(item.value)}\")\n                        try:\n                            relevance_threshold = float(item.value)\n                        except (ValueError, TypeError):\n                            raise ValueError(f\"relevance_threshold must be a number, got: {item.value}\")\n                        # Validate range: must be between 0 and 1\n```\n</details>\n<!-- suggestion_end -->\n",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/interfaces/knowledge_base/controller.py",
    "pr_number": 10948,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2116021739,
    "comment_created_at": "2025-05-30T14:19:52Z"
  },
  {
    "code": "@@ -185,6 +185,13 @@ def put(self, name):\n                 export = decrypt(storage.encode(), secret_key)\n                 handler.handler_storage.import_files(export)\n \n+        except ValueError as e:\n+            # Return a 400 Bad Request for validation errors (e.g. name must be lowercase)\n+            logger.error(str(e))\n+            if temp_dir is not None:\n+                shutil.rmtree(temp_dir)",
    "comment": "**Correctness**: The new ValueError exception handler is placed after the generic Exception handler in the try-catch chain. Since ValueError is a subclass of Exception, the generic handler will catch ValueError exceptions first, making the new code unreachable.\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n        except ValueError as e:\n            # Return a 400 Bad Request for validation errors (e.g. name must be lowercase)\n            logger.error(str(e))\n            if temp_dir is not None:\n                shutil.rmtree(temp_dir)\n            return http_error(HTTPStatus.BAD_REQUEST, \"Wrong argument\", f\"Error during config update: {str(e)}\")\n\n        except Exception as e:\n            logger.error(str(e))\n            if temp_dir is not None:\n                shutil.rmtree(temp_dir)\n            return http_error(HTTPStatus.INTERNAL_SERVER_ERROR, \"Exception\", str(e))\n```\n</details>\n<!-- suggestion_end -->\n",
    "line_number": 192,
    "enriched": "File: mindsdb/api/http/namespaces/config.py\nCode: @@ -185,6 +185,13 @@ def put(self, name):\n                 export = decrypt(storage.encode(), secret_key)\n                 handler.handler_storage.import_files(export)\n \n+        except ValueError as e:\n+            # Return a 400 Bad Request for validation errors (e.g. name must be lowercase)\n+            logger.error(str(e))\n+            if temp_dir is not None:\n+                shutil.rmtree(temp_dir)\nComment: **Correctness**: The new ValueError exception handler is placed after the generic Exception handler in the try-catch chain. Since ValueError is a subclass of Exception, the generic handler will catch ValueError exceptions first, making the new code unreachable.\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n        except ValueError as e:\n            # Return a 400 Bad Request for validation errors (e.g. name must be lowercase)\n            logger.error(str(e))\n            if temp_dir is not None:\n                shutil.rmtree(temp_dir)\n            return http_error(HTTPStatus.BAD_REQUEST, \"Wrong argument\", f\"Error during config update: {str(e)}\")\n\n        except Exception as e:\n            logger.error(str(e))\n            if temp_dir is not None:\n                shutil.rmtree(temp_dir)\n            return http_error(HTTPStatus.INTERNAL_SERVER_ERROR, \"Exception\", str(e))\n```\n</details>\n<!-- suggestion_end -->\n",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/api/http/namespaces/config.py",
    "pr_number": 11742,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2431892844,
    "comment_created_at": "2025-10-15T09:46:08Z"
  },
  {
    "code": "@@ -392,7 +397,16 @@ def initialize_flask():\n     app.config[\"SWAGGER_HOST\"] = \"http://localhost:8000/mindsdb\"\n     app.json = ORJSONProvider(app)\n \n-    authorizations = {\"apikey\": {\"type\": \"apiKey\", \"in\": \"header\", \"name\": \"Authorization\"}}\n+    http_auth_type = config[\"auth\"][\"http_auth_type\"]\n+    if http_auth_type == HTTP_AUTH_TYPE.SESSION:\n+        app.config[\"SECRET_KEY\"] = os.environ.get(\"FLASK_SECRET_KEY\", secrets.token_hex(32))\n+        app.config[\"SESSION_COOKIE_NAME\"] = \"session\"",
    "comment": "**security**: `SECRET_KEY` is set using `secrets.token_hex(32)` if `FLASK_SECRET_KEY` is unset, causing a new key on every restart and invalidating all sessions, enabling session fixation and hijacking attacks.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/api/http/initialize.py, lines 402-403, the session-based authentication sets SECRET_KEY to a random value on each start if FLASK_SECRET_KEY is not set. This breaks session integrity and enables session fixation/hijacking. Change the code to require FLASK_SECRET_KEY to be set in the environment, and raise an error if it is missing. Do not generate a random key at runtime.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n        app.config[\"SECRET_KEY\"] = os.environ.get(\"FLASK_SECRET_KEY\")\n        if not app.config[\"SECRET_KEY\"]:\n            raise RuntimeError(\"FLASK_SECRET_KEY environment variable must be set for session-based authentication.\")\n        app.config[\"SESSION_COOKIE_NAME\"] = \"session\"\n```\n</details>\n<!-- suggestion_end -->\n",
    "line_number": 405,
    "enriched": "File: mindsdb/api/http/initialize.py\nCode: @@ -392,7 +397,16 @@ def initialize_flask():\n     app.config[\"SWAGGER_HOST\"] = \"http://localhost:8000/mindsdb\"\n     app.json = ORJSONProvider(app)\n \n-    authorizations = {\"apikey\": {\"type\": \"apiKey\", \"in\": \"header\", \"name\": \"Authorization\"}}\n+    http_auth_type = config[\"auth\"][\"http_auth_type\"]\n+    if http_auth_type == HTTP_AUTH_TYPE.SESSION:\n+        app.config[\"SECRET_KEY\"] = os.environ.get(\"FLASK_SECRET_KEY\", secrets.token_hex(32))\n+        app.config[\"SESSION_COOKIE_NAME\"] = \"session\"\nComment: **security**: `SECRET_KEY` is set using `secrets.token_hex(32)` if `FLASK_SECRET_KEY` is unset, causing a new key on every restart and invalidating all sessions, enabling session fixation and hijacking attacks.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/api/http/initialize.py, lines 402-403, the session-based authentication sets SECRET_KEY to a random value on each start if FLASK_SECRET_KEY is not set. This breaks session integrity and enables session fixation/hijacking. Change the code to require FLASK_SECRET_KEY to be set in the environment, and raise an error if it is missing. Do not generate a random key at runtime.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n        app.config[\"SECRET_KEY\"] = os.environ.get(\"FLASK_SECRET_KEY\")\n        if not app.config[\"SECRET_KEY\"]:\n            raise RuntimeError(\"FLASK_SECRET_KEY environment variable must be set for session-based authentication.\")\n        app.config[\"SESSION_COOKIE_NAME\"] = \"session\"\n```\n</details>\n<!-- suggestion_end -->\n",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/api/http/initialize.py",
    "pr_number": 11911,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2542226445,
    "comment_created_at": "2025-11-19T14:21:47Z"
  },
  {
    "code": "@@ -81,15 +81,16 @@ Required connection parameters include one of the following:\n         - Search for YouTube Data API v3 and enable it.\n \n       - Go to OAuth consent screen:\n-        - Choose user type as External and click on Create.\n+        - Click on GET STARTED.\n         - Provide app name and support email.\n-        - In Scopes, add the following scopes: `../auth/youtube`, `.../auth/youtube.force-ssl`, `.../auth/youtubepartner`.\n-        - Next, add test users.\n-        - Save and continue.\n+        - Choose Audience based on who will be using the app.\n+        - Add the Contact Information (email address) of the developer.\n+        - Agree to the terms and click on CONTINUE.",
    "comment": "You can add here: \"...and then click on Create.\"",
    "line_number": 88,
    "enriched": "File: docs/integrations/app-integrations/youtube.mdx\nCode: @@ -81,15 +81,16 @@ Required connection parameters include one of the following:\n         - Search for YouTube Data API v3 and enable it.\n \n       - Go to OAuth consent screen:\n-        - Choose user type as External and click on Create.\n+        - Click on GET STARTED.\n         - Provide app name and support email.\n-        - In Scopes, add the following scopes: `../auth/youtube`, `.../auth/youtube.force-ssl`, `.../auth/youtubepartner`.\n-        - Next, add test users.\n-        - Save and continue.\n+        - Choose Audience based on who will be using the app.\n+        - Add the Contact Information (email address) of the developer.\n+        - Agree to the terms and click on CONTINUE.\nComment: You can add here: \"...and then click on Create.\"",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "docs/integrations/app-integrations/youtube.mdx",
    "pr_number": 10515,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2010042949,
    "comment_created_at": "2025-03-24T12:01:13Z"
  },
  {
    "code": "@@ -43,6 +44,19 @@ def calc_entropy(values: List[float]) -> float:\n     return -sum([pk * math.log(pk) for pk in values])\n \n \n+def sanitize_json_response(response: str) -> str:\n+    \"\"\"Remove markdown code block formatting from JSON response.\"\"\"\n+    # Remove leading/trailing whitespace\n+    # Try to find the first JSON object using regex (this assumes a flat or nested dict)\n+    json_match = re.search(r\"\\{.*?\\}\", response, re.DOTALL)",
    "comment": "for json like `{\"a\": {\"b\": 1}}` this regexp will return `{\"a\": {\"b\": 1}`\r\nmaybe better to look for first `{` and last `}` symbols? \r\n",
    "line_number": 51,
    "enriched": "File: mindsdb/interfaces/knowledge_base/evaluate.py\nCode: @@ -43,6 +44,19 @@ def calc_entropy(values: List[float]) -> float:\n     return -sum([pk * math.log(pk) for pk in values])\n \n \n+def sanitize_json_response(response: str) -> str:\n+    \"\"\"Remove markdown code block formatting from JSON response.\"\"\"\n+    # Remove leading/trailing whitespace\n+    # Try to find the first JSON object using regex (this assumes a flat or nested dict)\n+    json_match = re.search(r\"\\{.*?\\}\", response, re.DOTALL)\nComment: for json like `{\"a\": {\"b\": 1}}` this regexp will return `{\"a\": {\"b\": 1}`\r\nmaybe better to look for first `{` and last `}` symbols? \r\n",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/interfaces/knowledge_base/evaluate.py",
    "pr_number": 11178,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2174788062,
    "comment_created_at": "2025-06-30T10:50:47Z"
  },
  {
    "code": "@@ -61,6 +64,16 @@ def connect(self):\n         self.connection = conf\n         self.is_connected = True\n         return self.connection\n+    \n+    def validate_connection_data(self):\n+        if self.connection_data.get('url') == \"\":\n+            raise ValueError(\"`url` must be a valid non-empty url\")\n+\n+        if self.connection_data.get('username') == \"\":",
    "comment": "maybe check as:\r\n\r\n```\r\n if not username:\r\n```\r\n\r\nNot only for empty string",
    "line_number": 72,
    "enriched": "File: mindsdb/integrations/handlers/confluence_handler/confluence_handler.py\nCode: @@ -61,6 +64,16 @@ def connect(self):\n         self.connection = conf\n         self.is_connected = True\n         return self.connection\n+    \n+    def validate_connection_data(self):\n+        if self.connection_data.get('url') == \"\":\n+            raise ValueError(\"`url` must be a valid non-empty url\")\n+\n+        if self.connection_data.get('username') == \"\":\nComment: maybe check as:\r\n\r\n```\r\n if not username:\r\n```\r\n\r\nNot only for empty string",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/confluence_handler/confluence_handler.py",
    "pr_number": 10135,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1838112525,
    "comment_created_at": "2024-11-12T13:34:59Z"
  },
  {
    "code": "@@ -68,6 +68,9 @@ def _missing_(cls, value):\n         sys.exit(1)\n \n \n+HTTP_OPTIONAL_APIS = {\"a2a\", \"mcp\"}",
    "comment": "not in use",
    "line_number": 71,
    "enriched": "File: mindsdb/__main__.py\nCode: @@ -68,6 +68,9 @@ def _missing_(cls, value):\n         sys.exit(1)\n \n \n+HTTP_OPTIONAL_APIS = {\"a2a\", \"mcp\"}\nComment: not in use",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/__main__.py",
    "pr_number": 11962,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2606794139,
    "comment_created_at": "2025-12-10T14:04:58Z"
  },
  {
    "code": "@@ -0,0 +1,402 @@\n+from collections import OrderedDict\n+from datetime import datetime\n+from typing import List, Optional\n+\n+import weaviate\n+import pandas as pd\n+\n+from mindsdb.integrations.libs.const import HANDLER_CONNECTION_ARG_TYPE as ARG_TYPE\n+from mindsdb.integrations.libs.response import RESPONSE_TYPE\n+from mindsdb.integrations.libs.response import HandlerResponse\n+from mindsdb.integrations.libs.response import HandlerResponse as Response\n+from mindsdb.integrations.libs.response import HandlerStatusResponse as StatusResponse\n+from mindsdb.integrations.libs.vectordatabase_handler import (\n+    FilterCondition,\n+    FilterOperator,\n+    TableField,\n+    VectorStoreHandler,\n+)\n+from mindsdb.utilities import log\n+\n+\n+class WeaviateDBHandler(VectorStoreHandler):\n+    \"\"\"This handler handles connection and execution of the Weaviate statements.\"\"\"\n+\n+    name = \"weaviate\"\n+\n+    def __init__(self, name: str, **kwargs):\n+        super().__init__(name)\n+\n+        self._connection_data = kwargs.get(\"connection_data\")\n+\n+        self._client_config = {\n+            \"weaviate_url\": self._connection_data.get(\"weaviate_url\"),\n+            \"weaviate_api_key\": self._connection_data.get(\n+                \"weaviate_api_key\"\n+            ),\n+            \"persist_directory\": self._connection_data.get(\"persist_directory\"),\n+        }\n+\n+        # either host + port or persist_directory is required\n+        # but not both\n+        if (\n+                self._client_config[\"weaviate_url\"] is None\n+                or self._client_config[\"weaviate_api_key\"] is None\n+        ):\n+            raise Exception(\n+                \"Either url + auth client secret is required for weaviate connection!\"\n+            )\n+\n+        self._client = None\n+        self.is_connected = False",
    "comment": "It would be much better with an annotation\r\nself.is_connected: bool = False",
    "line_number": 51,
    "enriched": "File: mindsdb/integrations/handlers/weaviate_handler/weaviate_handler.py\nCode: @@ -0,0 +1,402 @@\n+from collections import OrderedDict\n+from datetime import datetime\n+from typing import List, Optional\n+\n+import weaviate\n+import pandas as pd\n+\n+from mindsdb.integrations.libs.const import HANDLER_CONNECTION_ARG_TYPE as ARG_TYPE\n+from mindsdb.integrations.libs.response import RESPONSE_TYPE\n+from mindsdb.integrations.libs.response import HandlerResponse\n+from mindsdb.integrations.libs.response import HandlerResponse as Response\n+from mindsdb.integrations.libs.response import HandlerStatusResponse as StatusResponse\n+from mindsdb.integrations.libs.vectordatabase_handler import (\n+    FilterCondition,\n+    FilterOperator,\n+    TableField,\n+    VectorStoreHandler,\n+)\n+from mindsdb.utilities import log\n+\n+\n+class WeaviateDBHandler(VectorStoreHandler):\n+    \"\"\"This handler handles connection and execution of the Weaviate statements.\"\"\"\n+\n+    name = \"weaviate\"\n+\n+    def __init__(self, name: str, **kwargs):\n+        super().__init__(name)\n+\n+        self._connection_data = kwargs.get(\"connection_data\")\n+\n+        self._client_config = {\n+            \"weaviate_url\": self._connection_data.get(\"weaviate_url\"),\n+            \"weaviate_api_key\": self._connection_data.get(\n+                \"weaviate_api_key\"\n+            ),\n+            \"persist_directory\": self._connection_data.get(\"persist_directory\"),\n+        }\n+\n+        # either host + port or persist_directory is required\n+        # but not both\n+        if (\n+                self._client_config[\"weaviate_url\"] is None\n+                or self._client_config[\"weaviate_api_key\"] is None\n+        ):\n+            raise Exception(\n+                \"Either url + auth client secret is required for weaviate connection!\"\n+            )\n+\n+        self._client = None\n+        self.is_connected = False\nComment: It would be much better with an annotation\r\nself.is_connected: bool = False",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/weaviate_handler/weaviate_handler.py",
    "pr_number": 7593,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1349314956,
    "comment_created_at": "2023-10-06T20:56:38Z"
  },
  {
    "code": "@@ -352,6 +352,8 @@ def prepare_env_config(self) -> None:\n         if os.environ.get('MINDSDB_DB_CON', '') != '':\n             self._env_config['storage_db'] = os.environ['MINDSDB_DB_CON']\n \n+        self._env_config['default_project'] = os.environ.get('MINDSDB_DEFAULT_PROJECT', 'mindsdb')",
    "comment": "Please add to `_default_config[default_project]` = `mindsdb`. For convenience it will be good to have all possible values in default config.",
    "line_number": 355,
    "enriched": "File: mindsdb/utilities/config.py\nCode: @@ -352,6 +352,8 @@ def prepare_env_config(self) -> None:\n         if os.environ.get('MINDSDB_DB_CON', '') != '':\n             self._env_config['storage_db'] = os.environ['MINDSDB_DB_CON']\n \n+        self._env_config['default_project'] = os.environ.get('MINDSDB_DEFAULT_PROJECT', 'mindsdb')\nComment: Please add to `_default_config[default_project]` = `mindsdb`. For convenience it will be good to have all possible values in default config.",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/utilities/config.py",
    "pr_number": 10333,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1946429023,
    "comment_created_at": "2025-02-07T12:05:48Z"
  },
  {
    "code": "@@ -15,6 +15,8 @@ XXX Test Failed: <link to the test results in your GitHub repository>, <link to\n </br>\n \n \n+AI_WORKFLOW_SECTION (DEV_ENV) Test Passed: https://github.com/Daddypastor/mindsdb_test",
    "comment": "Please changeAI_WORKFLOW_SECTION (DEV_ENV) to  Create AI Engines section using SQL",
    "line_number": 18,
    "enriched": "File: docs/Docs_Manual_QA.md\nCode: @@ -15,6 +15,8 @@ XXX Test Failed: <link to the test results in your GitHub repository>, <link to\n </br>\n \n \n+AI_WORKFLOW_SECTION (DEV_ENV) Test Passed: https://github.com/Daddypastor/mindsdb_test\nComment: Please changeAI_WORKFLOW_SECTION (DEV_ENV) to  Create AI Engines section using SQL",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "docs/Docs_Manual_QA.md",
    "pr_number": 7584,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1348390098,
    "comment_created_at": "2023-10-06T08:10:57Z"
  },
  {
    "code": "@@ -1502,6 +1502,16 @@ def answer_create_predictor(self, statement):\n             ml_integration_name\n         )\n \n+        if getattr(statement, \"is_replace\", False) is True:\n+            # try to delete\n+            try:\n+                self.session.model_controller.delete_model(\n+                    model_name,\n+                    project_name=integration_name\n+                )\n+            except Exception:",
    "comment": "`Except EntityNotExistsError:` would be better",
    "line_number": 1512,
    "enriched": "File: mindsdb/api/executor/command_executor.py\nCode: @@ -1502,6 +1502,16 @@ def answer_create_predictor(self, statement):\n             ml_integration_name\n         )\n \n+        if getattr(statement, \"is_replace\", False) is True:\n+            # try to delete\n+            try:\n+                self.session.model_controller.delete_model(\n+                    model_name,\n+                    project_name=integration_name\n+                )\n+            except Exception:\nComment: `Except EntityNotExistsError:` would be better",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/api/executor/command_executor.py",
    "pr_number": 8721,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1478084749,
    "comment_created_at": "2024-02-05T11:55:34Z"
  },
  {
    "code": "@@ -76,10 +81,7 @@ def __init__(self, dialect_name):\n             # update version for support float cast\n             self.dialect.server_version_info = (8, 0, 17)\n \n-        self.types_map = {}\n-        for type_name in sa_type_names:\n-            self.types_map[type_name.upper()] = getattr(sa.types, type_name)\n-        self.types_map['BOOL'] = self.types_map['BOOLEAN']\n+        self.types_map = types_map",
    "comment": "it is used only once in line 366\r\n```\r\n        type = self.types_map[typename]\r\n```\r\nmaybe replace it to  `type = types_map[typename]`?\r\n",
    "line_number": 84,
    "enriched": "File: mindsdb/utilities/render/sqlalchemy_render.py\nCode: @@ -76,10 +81,7 @@ def __init__(self, dialect_name):\n             # update version for support float cast\n             self.dialect.server_version_info = (8, 0, 17)\n \n-        self.types_map = {}\n-        for type_name in sa_type_names:\n-            self.types_map[type_name.upper()] = getattr(sa.types, type_name)\n-        self.types_map['BOOL'] = self.types_map['BOOLEAN']\n+        self.types_map = types_map\nComment: it is used only once in line 366\r\n```\r\n        type = self.types_map[typename]\r\n```\r\nmaybe replace it to  `type = types_map[typename]`?\r\n",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/utilities/render/sqlalchemy_render.py",
    "pr_number": 10303,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1895758992,
    "comment_created_at": "2024-12-23T13:32:52Z"
  },
  {
    "code": "@@ -1,9 +1,60 @@\n-__title__ = 'MindsDB Email handler'\n-__package_name__ = 'mindsdb_email_handler'\n-__version__ = '0.0.1'\n-__description__ = \"MindsDB handler for email\"\n-__author__ = 'MindsDB Inc'\n-__github__ = 'https://github.com/mindsdb/mindsdb'\n-__pypi__ = 'https://pypi.org/project/mindsdb/'\n-__license__ = 'MIT'\n-__copyright__ = 'Copyright 2022- mindsdb'\n+# mindsdb/integrations/handlers/email_handler/__about__.py\n+\n+from mindsdb.integrations.libs.const import HANDLER_TYPE\n+\n+__title__ = \"Email\"\n+__package_name__ = \"email_handler\"\n+__version__ = \"0.0.1\"\n+__description__ = \"MindsDB handler for retrieving emails through IMAP.\"\n+__author__ = \"MindsDB Inc\"\n+__github__ = \"https://github.com/mindsdb/mindsdb\"\n+__pypi__ = \"https://pypi.org/project/mindsdb/\"\n+__license__ = \"GPL-3.0\"\n+__copyright__ = \"Copyright 2023- mindsdb\"\n+__icon_path__ = \"icon.svg\"\n+\n+HANDLER_TYPE = HANDLER_TYPE.DATA",
    "comment": "**correctness**: `HANDLER_TYPE` is assigned as `HANDLER_TYPE.DATA` but if `HANDLER_TYPE` is not an enum or does not have a `DATA` attribute, this will cause an `AttributeError` at runtime.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/integrations/handlers/email_handler/__about__.py, line 16, the code assigns HANDLER_TYPE = HANDLER_TYPE.DATA. If HANDLER_TYPE is not an enum or does not have a DATA attribute, this will cause an AttributeError at runtime. Change this line to use getattr to safely assign HANDLER_TYPE, like: HANDLER_TYPE = getattr(HANDLER_TYPE, \"DATA\", HANDLER_TYPE).\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\nHANDLER_TYPE = getattr(HANDLER_TYPE, \"DATA\", HANDLER_TYPE)\n```\n</details>\n<!-- suggestion_end -->\n",
    "line_number": 16,
    "enriched": "File: mindsdb/integrations/handlers/email_handler/__about__.py\nCode: @@ -1,9 +1,60 @@\n-__title__ = 'MindsDB Email handler'\n-__package_name__ = 'mindsdb_email_handler'\n-__version__ = '0.0.1'\n-__description__ = \"MindsDB handler for email\"\n-__author__ = 'MindsDB Inc'\n-__github__ = 'https://github.com/mindsdb/mindsdb'\n-__pypi__ = 'https://pypi.org/project/mindsdb/'\n-__license__ = 'MIT'\n-__copyright__ = 'Copyright 2022- mindsdb'\n+# mindsdb/integrations/handlers/email_handler/__about__.py\n+\n+from mindsdb.integrations.libs.const import HANDLER_TYPE\n+\n+__title__ = \"Email\"\n+__package_name__ = \"email_handler\"\n+__version__ = \"0.0.1\"\n+__description__ = \"MindsDB handler for retrieving emails through IMAP.\"\n+__author__ = \"MindsDB Inc\"\n+__github__ = \"https://github.com/mindsdb/mindsdb\"\n+__pypi__ = \"https://pypi.org/project/mindsdb/\"\n+__license__ = \"GPL-3.0\"\n+__copyright__ = \"Copyright 2023- mindsdb\"\n+__icon_path__ = \"icon.svg\"\n+\n+HANDLER_TYPE = HANDLER_TYPE.DATA\nComment: **correctness**: `HANDLER_TYPE` is assigned as `HANDLER_TYPE.DATA` but if `HANDLER_TYPE` is not an enum or does not have a `DATA` attribute, this will cause an `AttributeError` at runtime.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/integrations/handlers/email_handler/__about__.py, line 16, the code assigns HANDLER_TYPE = HANDLER_TYPE.DATA. If HANDLER_TYPE is not an enum or does not have a DATA attribute, this will cause an AttributeError at runtime. Change this line to use getattr to safely assign HANDLER_TYPE, like: HANDLER_TYPE = getattr(HANDLER_TYPE, \"DATA\", HANDLER_TYPE).\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\nHANDLER_TYPE = getattr(HANDLER_TYPE, \"DATA\", HANDLER_TYPE)\n```\n</details>\n<!-- suggestion_end -->\n",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/email_handler/__about__.py",
    "pr_number": 11674,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2400857766,
    "comment_created_at": "2025-10-03T05:44:45Z"
  },
  {
    "code": "@@ -0,0 +1,197 @@\n+import os\n+import pandas as pd\n+from pandas import DataFrame\n+from google.oauth2 import service_account\n+from google.auth.transport.requests import Request\n+from google.oauth2.credentials import Credentials\n+from googleapiclient.discovery import build\n+from mindsdb.api.mysql.mysql_proxy.libs.constants.response_type import RESPONSE_TYPE\n+from .google_search_tables import SearchAnalyticsTable, SiteMapsTable\n+from mindsdb.integrations.libs.api_handler import APIHandler, FuncParser\n+from mindsdb.integrations.libs.response import (\n+    HandlerStatusResponse as StatusResponse,\n+    HandlerResponse as Response,\n+)\n+from mindsdb.utilities import log\n+\n+\n+class GoogleSearchConsoleHandler(APIHandler):\n+    \"\"\"\n+        A class for handling connections and interactions with the Google Search Console API.\n+    \"\"\"\n+    name = 'google_search'\n+\n+    def __init__(self, name: str, **kwargs):\n+        \"\"\"\n+        Initialize the Google Search Console API handler.\n+        Args:\n+            name (str): name of the handler\n+            kwargs (dict): additional arguments\n+        \"\"\"\n+        super().__init__(name)\n+        self.token = None\n+        self.service = None\n+        self.connection_data = kwargs.get('connection_data', {})\n+        self.credentials_file = self.connection_data.get('credentials', None)\n+        self.credentials = None\n+        self.scopes = ['https://www.googleapis.com/auth/webmasters.readonly',\n+                       'https://www.googleapis.com/auth/webmasters']\n+        self.is_connected = False\n+        analytics = SearchAnalyticsTable(self)\n+        self.analytics = analytics\n+        self._register_table('Analytics', analytics)\n+        sitemaps = SiteMapsTable(self)\n+        self.sitemaps = sitemaps\n+        self._register_table('Sitemaps', sitemaps)\n+\n+    def connect(self):\n+        \"\"\"\n+        Set up any connections required by the handler\n+        Should return output of check_connection() method after attempting\n+        connection. Should switch self.is_connected.\n+        Returns:\n+            HandlerStatusResponse\n+        \"\"\"\n+        if self.is_connected is True:\n+            return self.service\n+        if self.credentials_file:\n+            if os.path.exists('token_search.json'):\n+                self.credentials = Credentials.from_authorized_user_file('token_search.json', self.scopes)\n+            if not self.credentials or not self.credentials.valid:\n+                if self.credentials and self.credentials.expired and self.credentials.refresh_token:\n+                    self.credentials.refresh(Request())\n+                else:\n+                    self.credentials = service_account.Credentials.from_service_account_file(\n+                        self.credentials_file, scopes=self.scopes)\n+            # Save the credentials for the next run\n+            with open('token_search.json', 'w') as token:",
    "comment": "It is not save to store credentials in common file. It can be used by different users (in cloud) or different GoogleSearch integrations on the same computer.\r\nWe can left only 'Credentials.from_authorized_user_file'\r\n",
    "line_number": 67,
    "enriched": "File: mindsdb/integrations/handlers/google_search_handler/google_search_handler.py\nCode: @@ -0,0 +1,197 @@\n+import os\n+import pandas as pd\n+from pandas import DataFrame\n+from google.oauth2 import service_account\n+from google.auth.transport.requests import Request\n+from google.oauth2.credentials import Credentials\n+from googleapiclient.discovery import build\n+from mindsdb.api.mysql.mysql_proxy.libs.constants.response_type import RESPONSE_TYPE\n+from .google_search_tables import SearchAnalyticsTable, SiteMapsTable\n+from mindsdb.integrations.libs.api_handler import APIHandler, FuncParser\n+from mindsdb.integrations.libs.response import (\n+    HandlerStatusResponse as StatusResponse,\n+    HandlerResponse as Response,\n+)\n+from mindsdb.utilities import log\n+\n+\n+class GoogleSearchConsoleHandler(APIHandler):\n+    \"\"\"\n+        A class for handling connections and interactions with the Google Search Console API.\n+    \"\"\"\n+    name = 'google_search'\n+\n+    def __init__(self, name: str, **kwargs):\n+        \"\"\"\n+        Initialize the Google Search Console API handler.\n+        Args:\n+            name (str): name of the handler\n+            kwargs (dict): additional arguments\n+        \"\"\"\n+        super().__init__(name)\n+        self.token = None\n+        self.service = None\n+        self.connection_data = kwargs.get('connection_data', {})\n+        self.credentials_file = self.connection_data.get('credentials', None)\n+        self.credentials = None\n+        self.scopes = ['https://www.googleapis.com/auth/webmasters.readonly',\n+                       'https://www.googleapis.com/auth/webmasters']\n+        self.is_connected = False\n+        analytics = SearchAnalyticsTable(self)\n+        self.analytics = analytics\n+        self._register_table('Analytics', analytics)\n+        sitemaps = SiteMapsTable(self)\n+        self.sitemaps = sitemaps\n+        self._register_table('Sitemaps', sitemaps)\n+\n+    def connect(self):\n+        \"\"\"\n+        Set up any connections required by the handler\n+        Should return output of check_connection() method after attempting\n+        connection. Should switch self.is_connected.\n+        Returns:\n+            HandlerStatusResponse\n+        \"\"\"\n+        if self.is_connected is True:\n+            return self.service\n+        if self.credentials_file:\n+            if os.path.exists('token_search.json'):\n+                self.credentials = Credentials.from_authorized_user_file('token_search.json', self.scopes)\n+            if not self.credentials or not self.credentials.valid:\n+                if self.credentials and self.credentials.expired and self.credentials.refresh_token:\n+                    self.credentials.refresh(Request())\n+                else:\n+                    self.credentials = service_account.Credentials.from_service_account_file(\n+                        self.credentials_file, scopes=self.scopes)\n+            # Save the credentials for the next run\n+            with open('token_search.json', 'w') as token:\nComment: It is not save to store credentials in common file. It can be used by different users (in cloud) or different GoogleSearch integrations on the same computer.\r\nWe can left only 'Credentials.from_authorized_user_file'\r\n",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/google_search_handler/google_search_handler.py",
    "pr_number": 5823,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1183832765,
    "comment_created_at": "2023-05-03T15:14:43Z"
  },
  {
    "code": "@@ -189,6 +190,12 @@ def select_query(self, query: Select) -> pd.DataFrame:\n                         error_msg = f\"Invalid relevance_threshold value: {item.value}. {str(e)}\"\n                         logger.error(error_msg)\n                         raise ValueError(error_msg)\n+                elif item.column == \"reranking\":\n+                    reranking_flag = item.value\n+                    # cast to boolean\n+                    if isinstance(reranking_flag, str):",
    "comment": "is it for the case when value is a string\r\n`where reranking=\"false\"`",
    "line_number": 196,
    "enriched": "File: mindsdb/interfaces/knowledge_base/controller.py\nCode: @@ -189,6 +190,12 @@ def select_query(self, query: Select) -> pd.DataFrame:\n                         error_msg = f\"Invalid relevance_threshold value: {item.value}. {str(e)}\"\n                         logger.error(error_msg)\n                         raise ValueError(error_msg)\n+                elif item.column == \"reranking\":\n+                    reranking_flag = item.value\n+                    # cast to boolean\n+                    if isinstance(reranking_flag, str):\nComment: is it for the case when value is a string\r\n`where reranking=\"false\"`",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/interfaces/knowledge_base/controller.py",
    "pr_number": 10940,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2115166213,
    "comment_created_at": "2025-05-30T05:27:35Z"
  },
  {
    "code": "@@ -242,22 +242,30 @@ def root_index(path):\n                 \"The endpoint you are trying to access does not exist on the server.\",\n             )\n \n-        # Normalize the path.\n-        full_path = os.path.normpath(os.path.join(static_root, path))\n+        try:\n+            # Ensure the requested path is within the static directory\n+            # https://docs.python.org/3/library/pathlib.html#pathlib.PurePath.is_relative_to\n+            requested_path = (static_root / path).resolve()\n+\n+            if not requested_path.is_relative_to(static_root.resolve()):",
    "comment": "**correctness**: `requested_path.is_relative_to(static_root.resolve())` will raise AttributeError on Python <3.9, causing runtime crashes if the environment is not guaranteed to be 3.9+.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/api/http/initialize.py, line 250, the use of `requested_path.is_relative_to(static_root.resolve())` will cause an AttributeError on Python versions below 3.9, leading to runtime crashes. Replace this line with a check that works on all supported Python versions, such as comparing the string prefix of the resolved paths.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n            if not str(requested_path).startswith(str(static_root.resolve()) + os.sep):\n```\n</details>\n<!-- suggestion_end -->\n",
    "line_number": 250,
    "enriched": "File: mindsdb/api/http/initialize.py\nCode: @@ -242,22 +242,30 @@ def root_index(path):\n                 \"The endpoint you are trying to access does not exist on the server.\",\n             )\n \n-        # Normalize the path.\n-        full_path = os.path.normpath(os.path.join(static_root, path))\n+        try:\n+            # Ensure the requested path is within the static directory\n+            # https://docs.python.org/3/library/pathlib.html#pathlib.PurePath.is_relative_to\n+            requested_path = (static_root / path).resolve()\n+\n+            if not requested_path.is_relative_to(static_root.resolve()):\nComment: **correctness**: `requested_path.is_relative_to(static_root.resolve())` will raise AttributeError on Python <3.9, causing runtime crashes if the environment is not guaranteed to be 3.9+.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/api/http/initialize.py, line 250, the use of `requested_path.is_relative_to(static_root.resolve())` will cause an AttributeError on Python versions below 3.9, leading to runtime crashes. Replace this line with a check that works on all supported Python versions, such as comparing the string prefix of the resolved paths.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n            if not str(requested_path).startswith(str(static_root.resolve()) + os.sep):\n```\n</details>\n<!-- suggestion_end -->\n",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/api/http/initialize.py",
    "pr_number": 11680,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2403479708,
    "comment_created_at": "2025-10-03T22:11:48Z"
  },
  {
    "code": "@@ -116,9 +118,22 @@ def get_tables(self) -> Response:\n             return Response(RESPONSE_TYPE.OK)\n         return super().get_tables()\n \n-    def native_query(self, query, params=None) -> Response:\n+    def query(self, query: ASTNode) -> Response:\n+        # Option to drop table of shared pgvector connection\n+        if isinstance(query, DropTables):\n+            query.tables = [self._check_table(table.parts[-1]) for table in query.tables]\n+            query_str, params = self.renderer.get_exec_params(query, with_failback=True)\n+            return self.native_query(query_str, params, no_restrict=True)\n+        return super().query(query)",
    "comment": "**security**: `native_query` allows execution of arbitrary SQL queries on shared pgvector DB if `no_restrict=True`, but `query` method allows `DropTables` AST to bypass restrictions, enabling privilege escalation or unauthorized table drops.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/integrations/handlers/pgvector_handler/pgvector_handler.py, lines 122-127, the `query` method allows `DropTables` AST to bypass native query restrictions on shared pgvector DBs, enabling unauthorized table drops. Update this block to explicitly block `DropTables` when `self._is_shared_db` is True, returning an error response instead of executing the drop.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n        if isinstance(query, DropTables):\n            if not self._is_shared_db:\n                query.tables = [self._check_table(table.parts[-1]) for table in query.tables]\n                query_str, params = self.renderer.get_exec_params(query, with_failback=True)\n                return self.native_query(query_str, params, no_restrict=True)\n            else:\n                return Response(RESPONSE_TYPE.ERROR, error_message=\"DROP TABLE is not allowed on shared pgvector connection.\")\n```\n</details>\n<!-- suggestion_end -->\n",
    "line_number": 127,
    "enriched": "File: mindsdb/integrations/handlers/pgvector_handler/pgvector_handler.py\nCode: @@ -116,9 +118,22 @@ def get_tables(self) -> Response:\n             return Response(RESPONSE_TYPE.OK)\n         return super().get_tables()\n \n-    def native_query(self, query, params=None) -> Response:\n+    def query(self, query: ASTNode) -> Response:\n+        # Option to drop table of shared pgvector connection\n+        if isinstance(query, DropTables):\n+            query.tables = [self._check_table(table.parts[-1]) for table in query.tables]\n+            query_str, params = self.renderer.get_exec_params(query, with_failback=True)\n+            return self.native_query(query_str, params, no_restrict=True)\n+        return super().query(query)\nComment: **security**: `native_query` allows execution of arbitrary SQL queries on shared pgvector DB if `no_restrict=True`, but `query` method allows `DropTables` AST to bypass restrictions, enabling privilege escalation or unauthorized table drops.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/integrations/handlers/pgvector_handler/pgvector_handler.py, lines 122-127, the `query` method allows `DropTables` AST to bypass native query restrictions on shared pgvector DBs, enabling unauthorized table drops. Update this block to explicitly block `DropTables` when `self._is_shared_db` is True, returning an error response instead of executing the drop.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n        if isinstance(query, DropTables):\n            if not self._is_shared_db:\n                query.tables = [self._check_table(table.parts[-1]) for table in query.tables]\n                query_str, params = self.renderer.get_exec_params(query, with_failback=True)\n                return self.native_query(query_str, params, no_restrict=True)\n            else:\n                return Response(RESPONSE_TYPE.ERROR, error_message=\"DROP TABLE is not allowed on shared pgvector connection.\")\n```\n</details>\n<!-- suggestion_end -->\n",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/pgvector_handler/pgvector_handler.py",
    "pr_number": 11542,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2332428541,
    "comment_created_at": "2025-09-09T08:09:24Z"
  },
  {
    "code": "@@ -28,88 +20,6 @@\n \n logger = log.getLogger(__name__)\n \n-_DEFAULT_MARKDOWN_HEADERS_TO_SPLIT_ON = [\n-    (\"#\", \"Header 1\"),\n-    (\"##\", \"Header 2\"),\n-    (\"###\", \"Header 3\"),\n-]\n-\n-_DEFAULT_WEB_CRAWL_LIMIT = 100\n-\n-\n-def _insert_file_into_knowledge_base(table: KnowledgeBaseTable, file_name: str):\n-    file_controller = FileController()\n-    splitter = FileSplitter(FileSplitterConfig())\n-    file_path = file_controller.get_file_path(file_name)\n-    loader = FileLoader(file_path)\n-    split_docs = []\n-    for doc in loader.lazy_load():\n-        split_docs += splitter.split_documents([doc])\n-    doc_objs = []\n-    for split_doc in split_docs:\n-        doc_objs.append({\n-            'content': split_doc.page_content,\n-        })\n-    docs_df = pd.DataFrame.from_records(doc_objs)\n-    # Insert documents into KB\n-    table.insert(docs_df)\n-\n-\n-def _insert_web_pages_into_knowledge_base(table: KnowledgeBaseTable, urls: List[str], crawl_depth: int = 1, filters: List[str] = None):\n-    try:\n-        # To prevent dependency on langchain_text_splitters unless needed.\n-        from langchain_text_splitters import MarkdownHeaderTextSplitter\n-    except ImportError as e:\n-        logger.error(f'Error importing langchain_text_splitters to insert web page into knowledge base: {e}')\n-        raise e\n-\n-    websites_df = get_all_websites(urls, limit=_DEFAULT_WEB_CRAWL_LIMIT, crawl_depth=crawl_depth, filters=filters)\n-    # Text content is treated as markdown.\n-    markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=_DEFAULT_MARKDOWN_HEADERS_TO_SPLIT_ON)\n-\n-    def append_row_documents(row, all_docs):\n-        docs = markdown_splitter.split_text(row['text_content'])\n-        for doc in docs:\n-            # Link the URL to each web page chunk.\n-            doc.metadata['url'] = row['url']\n-        all_docs += docs\n-\n-    all_docs = []\n-    websites_df.apply(lambda row: append_row_documents(row, all_docs), axis=1)\n-    # Convert back to a DF.\n-    doc_objs = []\n-    for doc in all_docs:\n-        doc_objs.append({\n-            'content': doc.page_content,\n-            'url': doc.metadata['url']\n-        })\n-    docs_df = pd.DataFrame.from_records(doc_objs)\n-    # Insert documents into KB.\n-    table.insert(docs_df)\n-\n-\n-def _insert_select_query_result_into_knowledge_base(query: str, table: KnowledgeBaseTable, project_name: str):\n-    if not query:\n-        return\n-    mysql_proxy = FakeMysqlProxy()\n-    query_result = mysql_proxy.process_query(query)\n-    # Use same project as API request for SQL context.\n-    mysql_proxy.set_context({'db': project_name})\n-    if query_result.type != SQL_RESPONSE_TYPE.TABLE:\n-        raise ExecutorException('Query returned no data')\n-\n-    # Check column name aliases.\n-    column_names = [c.get('alias', c.get('name')) for c in query_result.columns]\n-    df_to_insert = pd.DataFrame.from_records(query_result.data, columns=column_names)\n-    table.insert(df_to_insert)\n-\n-\n-def _insert_rows_into_knowledge_base(rows: List[Dict], table: KnowledgeBaseTable):\n-    if not rows:\n-        return\n-    df_to_insert = pd.DataFrame.from_records(rows)\n-    table.insert(df_to_insert)",
    "comment": "moving this into `preprocess.py` module.\r\n\r\nThis will allow us to support different preprocessing and chunking strategies",
    "line_number": 111,
    "enriched": "File: mindsdb/api/http/namespaces/knowledge_bases.py\nCode: @@ -28,88 +20,6 @@\n \n logger = log.getLogger(__name__)\n \n-_DEFAULT_MARKDOWN_HEADERS_TO_SPLIT_ON = [\n-    (\"#\", \"Header 1\"),\n-    (\"##\", \"Header 2\"),\n-    (\"###\", \"Header 3\"),\n-]\n-\n-_DEFAULT_WEB_CRAWL_LIMIT = 100\n-\n-\n-def _insert_file_into_knowledge_base(table: KnowledgeBaseTable, file_name: str):\n-    file_controller = FileController()\n-    splitter = FileSplitter(FileSplitterConfig())\n-    file_path = file_controller.get_file_path(file_name)\n-    loader = FileLoader(file_path)\n-    split_docs = []\n-    for doc in loader.lazy_load():\n-        split_docs += splitter.split_documents([doc])\n-    doc_objs = []\n-    for split_doc in split_docs:\n-        doc_objs.append({\n-            'content': split_doc.page_content,\n-        })\n-    docs_df = pd.DataFrame.from_records(doc_objs)\n-    # Insert documents into KB\n-    table.insert(docs_df)\n-\n-\n-def _insert_web_pages_into_knowledge_base(table: KnowledgeBaseTable, urls: List[str], crawl_depth: int = 1, filters: List[str] = None):\n-    try:\n-        # To prevent dependency on langchain_text_splitters unless needed.\n-        from langchain_text_splitters import MarkdownHeaderTextSplitter\n-    except ImportError as e:\n-        logger.error(f'Error importing langchain_text_splitters to insert web page into knowledge base: {e}')\n-        raise e\n-\n-    websites_df = get_all_websites(urls, limit=_DEFAULT_WEB_CRAWL_LIMIT, crawl_depth=crawl_depth, filters=filters)\n-    # Text content is treated as markdown.\n-    markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=_DEFAULT_MARKDOWN_HEADERS_TO_SPLIT_ON)\n-\n-    def append_row_documents(row, all_docs):\n-        docs = markdown_splitter.split_text(row['text_content'])\n-        for doc in docs:\n-            # Link the URL to each web page chunk.\n-            doc.metadata['url'] = row['url']\n-        all_docs += docs\n-\n-    all_docs = []\n-    websites_df.apply(lambda row: append_row_documents(row, all_docs), axis=1)\n-    # Convert back to a DF.\n-    doc_objs = []\n-    for doc in all_docs:\n-        doc_objs.append({\n-            'content': doc.page_content,\n-            'url': doc.metadata['url']\n-        })\n-    docs_df = pd.DataFrame.from_records(doc_objs)\n-    # Insert documents into KB.\n-    table.insert(docs_df)\n-\n-\n-def _insert_select_query_result_into_knowledge_base(query: str, table: KnowledgeBaseTable, project_name: str):\n-    if not query:\n-        return\n-    mysql_proxy = FakeMysqlProxy()\n-    query_result = mysql_proxy.process_query(query)\n-    # Use same project as API request for SQL context.\n-    mysql_proxy.set_context({'db': project_name})\n-    if query_result.type != SQL_RESPONSE_TYPE.TABLE:\n-        raise ExecutorException('Query returned no data')\n-\n-    # Check column name aliases.\n-    column_names = [c.get('alias', c.get('name')) for c in query_result.columns]\n-    df_to_insert = pd.DataFrame.from_records(query_result.data, columns=column_names)\n-    table.insert(df_to_insert)\n-\n-\n-def _insert_rows_into_knowledge_base(rows: List[Dict], table: KnowledgeBaseTable):\n-    if not rows:\n-        return\n-    df_to_insert = pd.DataFrame.from_records(rows)\n-    table.insert(df_to_insert)\nComment: moving this into `preprocess.py` module.\r\n\r\nThis will allow us to support different preprocessing and chunking strategies",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/api/http/namespaces/knowledge_bases.py",
    "pr_number": 10035,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1815516813,
    "comment_created_at": "2024-10-24T18:14:13Z"
  },
  {
    "code": "@@ -111,3 +116,43 @@ def _convert(v):\n         axis='columns'\n     )\n     return result_df\n+\n+\n+def infer_column_type(column):\n+    if not pd.api.types.is_object_dtype(column):",
    "comment": "@StpMax @ea-rus this doesnt make sense, the whole point is to fix the fact that pandas sucks at detecting types, \r\nand asumes everything is a string, even columns that are clearly numbers.\r\n\r\ntake for example:\r\n\r\n```\r\nCREATE DATABASE mybinance\r\nWITH\r\n  ENGINE = 'binance'\r\n  PARAMETERS = {};\r\n```\r\n  \r\nthen if you try to do:\r\n\r\n```\r\nSELECT TO_TIMESTAMP(open_time), open_price from(\r\nSELECT open_time, open_price\r\nFROM mybinance.aggregated_trade_data\r\nWHERE symbol = 'BTCUSDT'); \r\n```\r\n you get:\r\n\r\n```\r\nBinder Error: No function matches the given name and argument types 'to_timestamp(DOUBLE)'. You might need to add explicit type casts.\r\n\tCandidate functions:\r\n\tto_timestamp(BIGINT) -> TIMESTAMP\r\n```\r\n\t\r\n\t\r\nbut this is bad behavior because clearly open_time IS an int, if you remove if not pd.api.types.is_object_dtype(column):\r\n \r\nfrom infer_column_type, int columns will be int columns, float columns will be float columns, etc, etc, which is what we want! ",
    "line_number": 122,
    "enriched": "File: mindsdb/api/mysql/mysql_proxy/utilities/sql.py\nCode: @@ -111,3 +116,43 @@ def _convert(v):\n         axis='columns'\n     )\n     return result_df\n+\n+\n+def infer_column_type(column):\n+    if not pd.api.types.is_object_dtype(column):\nComment: @StpMax @ea-rus this doesnt make sense, the whole point is to fix the fact that pandas sucks at detecting types, \r\nand asumes everything is a string, even columns that are clearly numbers.\r\n\r\ntake for example:\r\n\r\n```\r\nCREATE DATABASE mybinance\r\nWITH\r\n  ENGINE = 'binance'\r\n  PARAMETERS = {};\r\n```\r\n  \r\nthen if you try to do:\r\n\r\n```\r\nSELECT TO_TIMESTAMP(open_time), open_price from(\r\nSELECT open_time, open_price\r\nFROM mybinance.aggregated_trade_data\r\nWHERE symbol = 'BTCUSDT'); \r\n```\r\n you get:\r\n\r\n```\r\nBinder Error: No function matches the given name and argument types 'to_timestamp(DOUBLE)'. You might need to add explicit type casts.\r\n\tCandidate functions:\r\n\tto_timestamp(BIGINT) -> TIMESTAMP\r\n```\r\n\t\r\n\t\r\nbut this is bad behavior because clearly open_time IS an int, if you remove if not pd.api.types.is_object_dtype(column):\r\n \r\nfrom infer_column_type, int columns will be int columns, float columns will be float columns, etc, etc, which is what we want! ",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/api/mysql/mysql_proxy/utilities/sql.py",
    "pr_number": 7074,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1295638383,
    "comment_created_at": "2023-08-16T09:35:58Z"
  },
  {
    "code": "@@ -346,38 +347,131 @@ def describe(self, attribute: Optional[str] = None) -> pd.DataFrame:\n             tables = [\"args\", \"metadata\"]\n             return pd.DataFrame(tables, columns=[\"tables\"])\n \n-    def finetune(\n-        self, df: Optional[pd.DataFrame] = None, args: Optional[Dict] = None\n-    ) -> None:\n-        finetune_args = args if args else {}\n-        args = self.base_model_storage.json_get(\"args\")\n-        args.update(finetune_args)\n+    def finetune(self, df: Optional[pd.DataFrame] = None, args: Optional[Dict] = None) -> None:\n+        print(\"Starting finetune method\")\n+        self.print_debug_info()\n+\n+        if df is None or df.empty:\n+            raise ValueError(\"No data provided for fine-tuning\")\n+\n+        print(\"Input DataFrame shape:\", df.shape)\n+        print(\"Input DataFrame columns:\", df.columns)\n+\n+        # Retrieve stored args, defaulting to an empty dict if None\n+        stored_args = self.model_storage.json_get(\"args\") or {}\n+        print(\"Stored args:\", stored_args)\n+        \n+        # Merge stored args with provided args, prioritizing provided args\n+        finetune_args = {**stored_args, **(args or {})}\n+        print(\"Combined finetune args:\", finetune_args)\n+\n+        # Check if 'using' key exists and extract its contents\n+        if 'using' in finetune_args:\n+            finetune_args.update(finetune_args['using'])\n+            del finetune_args['using']\n+\n+        model_name = finetune_args.get(\"model_name\")\n+        if not model_name:\n+            raise ValueError(\"Model name not found in arguments. Please ensure the model was created correctly.\")\n+\n+        print(\"Model name:\", model_name)\n \n-        model_name = args[\"model_name\"]\n         model_folder = self.model_storage.folder_get(model_name)\n-        args[\"model_folder\"] = model_folder\n+        finetune_args[\"model_folder\"] = model_folder\n         model_folder_name = model_folder.split(\"/\")[-1]\n-        task = args[\"task\"]\n+        task = finetune_args.get(\"task\")\n+        if not task:\n+            raise ValueError(\"Task not specified in arguments\")\n \n         if task not in FINETUNE_MAP:\n             raise KeyError(\n                 f\"{task} is not currently supported, please choose a supported task - {', '.join(FINETUNE_MAP)}\"\n             )\n \n-        tokenizer, trainer = FINETUNE_MAP[task](df, args)\n+        # Ensure the input column and target column are correctly specified\n+        input_column = finetune_args.get(\"input_column\")\n+        target_column = finetune_args.get(\"target\")\n+\n+        if not input_column or not target_column:\n+            raise ValueError(\"Input column or target column not specified in arguments\")\n+\n+        if input_column not in df.columns or target_column not in df.columns:\n+            raise ValueError(f\"Input column '{input_column}' or target column '{target_column}' not found in the dataset\")\n+\n+        # Prepare the dataset\n+        df = df.rename(columns={input_column: \"text\", target_column: \"labels\"})\n+        \n+        # Convert labels to integers if they're not already\n+        labels = finetune_args.get(\"labels\")\n+        if not labels:\n+            raise ValueError(\"Labels not specified in arguments\")\n+        \n+        # Create labels_map if it doesn't exist\n+        if \"labels_map\" not in finetune_args:\n+            finetune_args[\"labels_map\"] = {label: i for i, label in enumerate(labels)}\n+        \n+        label_map = finetune_args[\"labels_map\"]\n+        df[\"labels\"] = df[\"labels\"].map(label_map)\n+\n+        print(\"Finetune args after processing:\", finetune_args)\n+        print(\"DataFrame head:\", df.head())\n+        print(\"DataFrame info:\", df.info())\n+\n+        tokenizer, trainer = FINETUNE_MAP[task](df, finetune_args)\n \n         try:\n-            trainer.train()\n-            trainer.save_model(\n-                model_folder\n-            )  # TODO: save entire pipeline instead https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.Pipeline.save_pretrained\n+            # Checks Transformers version\n+            transformers_version = version.parse(transformers.__version__)\n+            print(f\"Transformers version: {transformers_version}\")\n+\n+            if transformers_version >= version.parse(\"4.0.0\"):\n+                # Set up early stopping for newer versions\n+                early_stopping = transformers.EarlyStoppingCallback(early_stopping_patience=3)\n+                try:\n+                    trainer.train(callbacks=[early_stopping])\n+                except TypeError:\n+                    print(\"Warning: Callbacks not supported in this version of Transformers. Training without early stopping.\")\n+                    trainer.train()\n+            else:\n+                # For older versions, train without callbacks\n+                print(\"Warning: Using an older version of Transformers. Early stopping is not available.\")\n+                trainer.train()\n+            \n+            eval_results = trainer.evaluate()\n+            print(f\"Evaluation results: {eval_results}\")\n+\n+            trainer.save_model(model_folder) \n+            # TODO: save entire pipeline instead https://huggingface.co/docs/transformers/main_classes/\n+            # pipelines#transformers.Pipeline.save_pretrained\n             tokenizer.save_pretrained(model_folder)\n \n+            finetune_args[\"fine_tuned\"] = True\n+            finetune_args[\"eval_results\"] = eval_results\n             # persist changes\n-            self.model_storage.json_set(\"args\", args)\n+            self.model_storage.json_set(\"args\", finetune_args)\n             self.model_storage.folder_sync(model_folder_name)\n \n+            print(\"Fine-tuning completed successfully\")\n+            print(\"Final evaluation results:\", eval_results)\n+\n         except Exception as e:\n             err_str = f\"Finetune failed with error: {str(e)}\"\n-            logger.debug(err_str)\n+            print(err_str)\n+            logger.error(err_str)\n             raise Exception(err_str)\n+    \n+    def print_debug_info(self):",
    "comment": "To improve maintainability and allow better control over log levels, could you please replace the print statements with a logger? Also instead of placing a logger call on every line, feel free to group related log messages where appropriate to avoid excessive logging.",
    "line_number": 463,
    "enriched": "File: mindsdb/integrations/handlers/huggingface_handler/huggingface_handler.py\nCode: @@ -346,38 +347,131 @@ def describe(self, attribute: Optional[str] = None) -> pd.DataFrame:\n             tables = [\"args\", \"metadata\"]\n             return pd.DataFrame(tables, columns=[\"tables\"])\n \n-    def finetune(\n-        self, df: Optional[pd.DataFrame] = None, args: Optional[Dict] = None\n-    ) -> None:\n-        finetune_args = args if args else {}\n-        args = self.base_model_storage.json_get(\"args\")\n-        args.update(finetune_args)\n+    def finetune(self, df: Optional[pd.DataFrame] = None, args: Optional[Dict] = None) -> None:\n+        print(\"Starting finetune method\")\n+        self.print_debug_info()\n+\n+        if df is None or df.empty:\n+            raise ValueError(\"No data provided for fine-tuning\")\n+\n+        print(\"Input DataFrame shape:\", df.shape)\n+        print(\"Input DataFrame columns:\", df.columns)\n+\n+        # Retrieve stored args, defaulting to an empty dict if None\n+        stored_args = self.model_storage.json_get(\"args\") or {}\n+        print(\"Stored args:\", stored_args)\n+        \n+        # Merge stored args with provided args, prioritizing provided args\n+        finetune_args = {**stored_args, **(args or {})}\n+        print(\"Combined finetune args:\", finetune_args)\n+\n+        # Check if 'using' key exists and extract its contents\n+        if 'using' in finetune_args:\n+            finetune_args.update(finetune_args['using'])\n+            del finetune_args['using']\n+\n+        model_name = finetune_args.get(\"model_name\")\n+        if not model_name:\n+            raise ValueError(\"Model name not found in arguments. Please ensure the model was created correctly.\")\n+\n+        print(\"Model name:\", model_name)\n \n-        model_name = args[\"model_name\"]\n         model_folder = self.model_storage.folder_get(model_name)\n-        args[\"model_folder\"] = model_folder\n+        finetune_args[\"model_folder\"] = model_folder\n         model_folder_name = model_folder.split(\"/\")[-1]\n-        task = args[\"task\"]\n+        task = finetune_args.get(\"task\")\n+        if not task:\n+            raise ValueError(\"Task not specified in arguments\")\n \n         if task not in FINETUNE_MAP:\n             raise KeyError(\n                 f\"{task} is not currently supported, please choose a supported task - {', '.join(FINETUNE_MAP)}\"\n             )\n \n-        tokenizer, trainer = FINETUNE_MAP[task](df, args)\n+        # Ensure the input column and target column are correctly specified\n+        input_column = finetune_args.get(\"input_column\")\n+        target_column = finetune_args.get(\"target\")\n+\n+        if not input_column or not target_column:\n+            raise ValueError(\"Input column or target column not specified in arguments\")\n+\n+        if input_column not in df.columns or target_column not in df.columns:\n+            raise ValueError(f\"Input column '{input_column}' or target column '{target_column}' not found in the dataset\")\n+\n+        # Prepare the dataset\n+        df = df.rename(columns={input_column: \"text\", target_column: \"labels\"})\n+        \n+        # Convert labels to integers if they're not already\n+        labels = finetune_args.get(\"labels\")\n+        if not labels:\n+            raise ValueError(\"Labels not specified in arguments\")\n+        \n+        # Create labels_map if it doesn't exist\n+        if \"labels_map\" not in finetune_args:\n+            finetune_args[\"labels_map\"] = {label: i for i, label in enumerate(labels)}\n+        \n+        label_map = finetune_args[\"labels_map\"]\n+        df[\"labels\"] = df[\"labels\"].map(label_map)\n+\n+        print(\"Finetune args after processing:\", finetune_args)\n+        print(\"DataFrame head:\", df.head())\n+        print(\"DataFrame info:\", df.info())\n+\n+        tokenizer, trainer = FINETUNE_MAP[task](df, finetune_args)\n \n         try:\n-            trainer.train()\n-            trainer.save_model(\n-                model_folder\n-            )  # TODO: save entire pipeline instead https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.Pipeline.save_pretrained\n+            # Checks Transformers version\n+            transformers_version = version.parse(transformers.__version__)\n+            print(f\"Transformers version: {transformers_version}\")\n+\n+            if transformers_version >= version.parse(\"4.0.0\"):\n+                # Set up early stopping for newer versions\n+                early_stopping = transformers.EarlyStoppingCallback(early_stopping_patience=3)\n+                try:\n+                    trainer.train(callbacks=[early_stopping])\n+                except TypeError:\n+                    print(\"Warning: Callbacks not supported in this version of Transformers. Training without early stopping.\")\n+                    trainer.train()\n+            else:\n+                # For older versions, train without callbacks\n+                print(\"Warning: Using an older version of Transformers. Early stopping is not available.\")\n+                trainer.train()\n+            \n+            eval_results = trainer.evaluate()\n+            print(f\"Evaluation results: {eval_results}\")\n+\n+            trainer.save_model(model_folder) \n+            # TODO: save entire pipeline instead https://huggingface.co/docs/transformers/main_classes/\n+            # pipelines#transformers.Pipeline.save_pretrained\n             tokenizer.save_pretrained(model_folder)\n \n+            finetune_args[\"fine_tuned\"] = True\n+            finetune_args[\"eval_results\"] = eval_results\n             # persist changes\n-            self.model_storage.json_set(\"args\", args)\n+            self.model_storage.json_set(\"args\", finetune_args)\n             self.model_storage.folder_sync(model_folder_name)\n \n+            print(\"Fine-tuning completed successfully\")\n+            print(\"Final evaluation results:\", eval_results)\n+\n         except Exception as e:\n             err_str = f\"Finetune failed with error: {str(e)}\"\n-            logger.debug(err_str)\n+            print(err_str)\n+            logger.error(err_str)\n             raise Exception(err_str)\n+    \n+    def print_debug_info(self):\nComment: To improve maintainability and allow better control over log levels, could you please replace the print statements with a logger? Also instead of placing a logger call on every line, feel free to group related log messages where appropriate to avoid excessive logging.",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/huggingface_handler/huggingface_handler.py",
    "pr_number": 9821,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1793320159,
    "comment_created_at": "2024-10-09T11:05:02Z"
  },
  {
    "code": "@@ -1,16 +1,11 @@\n exclude: \"^databricks_src|^data_engineering/\"\n repos:\n-  - repo: https://github.com/ambv/black\n-    rev: 22.3.0\n-    hooks:\n-      - id: black\n-        args: ['--skip-string-normalization'] #prevents conversion of single to double quotes see https://safjan.com/black-keep-single-quotes-strings/\n   - repo: https://github.com/pycqa/flake8\n     rev: 4.0.1\n     hooks:\n       - id: flake8\n         additional_dependencies: [flake8-typing-imports==1.10.0]\n-        args: ['--max-line-length=88', '--extend-ignore=E203,E704']\n+        args: ['--config', '.flake8']",
    "comment": "using flake8 config",
    "line_number": 8,
    "enriched": "File: .pre-commit-config.yaml\nCode: @@ -1,16 +1,11 @@\n exclude: \"^databricks_src|^data_engineering/\"\n repos:\n-  - repo: https://github.com/ambv/black\n-    rev: 22.3.0\n-    hooks:\n-      - id: black\n-        args: ['--skip-string-normalization'] #prevents conversion of single to double quotes see https://safjan.com/black-keep-single-quotes-strings/\n   - repo: https://github.com/pycqa/flake8\n     rev: 4.0.1\n     hooks:\n       - id: flake8\n         additional_dependencies: [flake8-typing-imports==1.10.0]\n-        args: ['--max-line-length=88', '--extend-ignore=E203,E704']\n+        args: ['--config', '.flake8']\nComment: using flake8 config",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": ".pre-commit-config.yaml",
    "pr_number": 8297,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1392362892,
    "comment_created_at": "2023-11-14T10:32:51Z"
  },
  {
    "code": "@@ -448,19 +448,53 @@ class Agents(Base):\n     deleted_at = Column(DateTime)\n \n     def as_dict(self) -> Dict:\n-        return {\n+        skills = []\n+        skills_extra_parameters = {}\n+        for rel in self.skills_relationships:\n+            skill = rel.skill\n+            # Skip auto-generated SQL skills\n+            if skill.params.get(\"description\", \"\").startswith(\"Auto-generated SQL skill for agent\"):\n+                continue\n+            skills.append(skill.as_dict())\n+            skills_extra_parameters[skill.name] = rel.parameters or {}\n+\n+        params = self.params.copy()\n+\n+        agent_dict = {\n             \"id\": self.id,\n             \"name\": self.name,\n             \"project_id\": self.project_id,\n-            \"model_name\": self.model_name,\n-            \"skills\": [rel.skill.as_dict() for rel in self.skills_relationships],\n-            \"skills_extra_parameters\": {rel.skill.name: (rel.parameters or {}) for rel in self.skills_relationships},\n-            \"provider\": self.provider,\n-            \"params\": self.params,\n             \"updated_at\": self.updated_at,\n             \"created_at\": self.created_at,\n         }\n \n+        if self.model_name:\n+            agent_dict[\"model_name\"] = self.model_name\n+\n+        if self.provider:\n+            agent_dict[\"provider\"] = self.provider\n+\n+        # Since skills were depreciated, they are only used with Minds\n+        # Minds expects the parameters to be provided as is without breaking them down\n+        if skills:\n+            agent_dict[\"skills\"] = skills\n+            agent_dict[\"skills_extra_parameters\"] = skills_extra_parameters\n+            agent_dict[\"params\"] = params\n+        else:\n+            data = params.pop(\"data\", {})\n+            model = params.pop(\"model\", {})\n+            prompt_template = params.pop(\"prompt_template\", None)\n+            if data:\n+                agent_dict[\"data\"] = data\n+            if model:\n+                agent_dict[\"model\"] = model\n+            if prompt_template:\n+                agent_dict[\"prompt_template\"] = prompt_template\n+            if params:\n+                agent_dict[\"params\"] = params",
    "comment": "**correctness**: `params.pop()` in `Agents.as_dict` mutates the original `params` dict, causing loss of data for subsequent calls or ORM objects sharing the same dict.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/interfaces/storage/db.py, lines 484-494, the code uses `params.pop()` to extract 'data', 'model', and 'prompt_template' from the params dict. This mutates the original params dict, which can cause data loss or unexpected behavior if the same dict is reused elsewhere (e.g., by SQLAlchemy ORM). Refactor this block to use `params.get()` for extraction, and only remove these keys from a copy of the dict when assigning to 'params'. Ensure the original params dict is never mutated. Replace the block with the following:\n\ndata = params.get(\"data\", {})\nmodel = params.get(\"model\", {})\nprompt_template = params.get(\"prompt_template\", None)\nif data:\n    agent_dict[\"data\"] = data\nif model:\n    agent_dict[\"model\"] = model\nif prompt_template:\n    agent_dict[\"prompt_template\"] = prompt_template\nparams_copy = params.copy()\nparams_copy.pop(\"data\", None)\nparams_copy.pop(\"model\", None)\nparams_copy.pop(\"prompt_template\", None)\nif params_copy:\n    agent_dict[\"params\"] = params_copy\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n            data = params.get(\"data\", {})\n            model = params.get(\"model\", {})\n            prompt_template = params.get(\"prompt_template\", None)\n            if data:\n                agent_dict[\"data\"] = data\n            if model:\n                agent_dict[\"model\"] = model\n            if prompt_template:\n                agent_dict[\"prompt_template\"] = prompt_template\n            params_copy = params.copy()\n            params_copy.pop(\"data\", None)\n            params_copy.pop(\"model\", None)\n            params_copy.pop(\"prompt_template\", None)\n            if params_copy:\n                agent_dict[\"params\"] = params_copy\n```\n</details>\n<!-- suggestion_end -->\n",
    "line_number": 494,
    "enriched": "File: mindsdb/interfaces/storage/db.py\nCode: @@ -448,19 +448,53 @@ class Agents(Base):\n     deleted_at = Column(DateTime)\n \n     def as_dict(self) -> Dict:\n-        return {\n+        skills = []\n+        skills_extra_parameters = {}\n+        for rel in self.skills_relationships:\n+            skill = rel.skill\n+            # Skip auto-generated SQL skills\n+            if skill.params.get(\"description\", \"\").startswith(\"Auto-generated SQL skill for agent\"):\n+                continue\n+            skills.append(skill.as_dict())\n+            skills_extra_parameters[skill.name] = rel.parameters or {}\n+\n+        params = self.params.copy()\n+\n+        agent_dict = {\n             \"id\": self.id,\n             \"name\": self.name,\n             \"project_id\": self.project_id,\n-            \"model_name\": self.model_name,\n-            \"skills\": [rel.skill.as_dict() for rel in self.skills_relationships],\n-            \"skills_extra_parameters\": {rel.skill.name: (rel.parameters or {}) for rel in self.skills_relationships},\n-            \"provider\": self.provider,\n-            \"params\": self.params,\n             \"updated_at\": self.updated_at,\n             \"created_at\": self.created_at,\n         }\n \n+        if self.model_name:\n+            agent_dict[\"model_name\"] = self.model_name\n+\n+        if self.provider:\n+            agent_dict[\"provider\"] = self.provider\n+\n+        # Since skills were depreciated, they are only used with Minds\n+        # Minds expects the parameters to be provided as is without breaking them down\n+        if skills:\n+            agent_dict[\"skills\"] = skills\n+            agent_dict[\"skills_extra_parameters\"] = skills_extra_parameters\n+            agent_dict[\"params\"] = params\n+        else:\n+            data = params.pop(\"data\", {})\n+            model = params.pop(\"model\", {})\n+            prompt_template = params.pop(\"prompt_template\", None)\n+            if data:\n+                agent_dict[\"data\"] = data\n+            if model:\n+                agent_dict[\"model\"] = model\n+            if prompt_template:\n+                agent_dict[\"prompt_template\"] = prompt_template\n+            if params:\n+                agent_dict[\"params\"] = params\nComment: **correctness**: `params.pop()` in `Agents.as_dict` mutates the original `params` dict, causing loss of data for subsequent calls or ORM objects sharing the same dict.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/interfaces/storage/db.py, lines 484-494, the code uses `params.pop()` to extract 'data', 'model', and 'prompt_template' from the params dict. This mutates the original params dict, which can cause data loss or unexpected behavior if the same dict is reused elsewhere (e.g., by SQLAlchemy ORM). Refactor this block to use `params.get()` for extraction, and only remove these keys from a copy of the dict when assigning to 'params'. Ensure the original params dict is never mutated. Replace the block with the following:\n\ndata = params.get(\"data\", {})\nmodel = params.get(\"model\", {})\nprompt_template = params.get(\"prompt_template\", None)\nif data:\n    agent_dict[\"data\"] = data\nif model:\n    agent_dict[\"model\"] = model\nif prompt_template:\n    agent_dict[\"prompt_template\"] = prompt_template\nparams_copy = params.copy()\nparams_copy.pop(\"data\", None)\nparams_copy.pop(\"model\", None)\nparams_copy.pop(\"prompt_template\", None)\nif params_copy:\n    agent_dict[\"params\"] = params_copy\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n            data = params.get(\"data\", {})\n            model = params.get(\"model\", {})\n            prompt_template = params.get(\"prompt_template\", None)\n            if data:\n                agent_dict[\"data\"] = data\n            if model:\n                agent_dict[\"model\"] = model\n            if prompt_template:\n                agent_dict[\"prompt_template\"] = prompt_template\n            params_copy = params.copy()\n            params_copy.pop(\"data\", None)\n            params_copy.pop(\"model\", None)\n            params_copy.pop(\"prompt_template\", None)\n            if params_copy:\n                agent_dict[\"params\"] = params_copy\n```\n</details>\n<!-- suggestion_end -->\n",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/interfaces/storage/db.py",
    "pr_number": 11255,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2194103541,
    "comment_created_at": "2025-07-09T06:05:12Z"
  },
  {
    "code": "@@ -275,3 +277,85 @@ def delete(self, project_name: str, knowledge_base_name: str):\n \n         session_controller.kb_controller.delete(knowledge_base_name, project_name)\n         return '', HTTPStatus.NO_CONTENT\n+\n+\n+@ns_conf.route('/<project_name>/knowledge_bases/<knowledge_base_name>/completions')\n+@ns_conf.param('project_name', 'Name of the project')\n+@ns_conf.param('knowledge_base_name', 'Name of the knowledge_base')\n+class KnowledgeBaseCompletions(Resource):\n+    @ns_conf.doc('knowledge_base_completions')\n+    @api_endpoint_metrics('POST', '/knowledge_bases/knowledge_base/completions')\n+    def post(self, project_name, knowledge_base_name):\n+        \"\"\"\n+        Add support for LLM generation on the response from knowledge base\n+        \"\"\"\n+        # Check for required parameters.\n+        if 'knowledge_base' not in request.json:\n+            return http_error(\n+                HTTPStatus.BAD_REQUEST,\n+                'Missing parameter',\n+                'Must provide \"knowledge_base\" parameter in POST body'\n+            )\n+\n+        # Check for required parameters\n+        query = request.json.get('query')\n+        if query is None:\n+            return http_error(\n+                HTTPStatus.BAD_REQUEST,\n+                'Missing parameter',\n+                'Must provide \"query\" parameter in POST body'",
    "comment": "maybe it could be good to log errors ",
    "line_number": 306,
    "enriched": "File: mindsdb/api/http/namespaces/knowledge_bases.py\nCode: @@ -275,3 +277,85 @@ def delete(self, project_name: str, knowledge_base_name: str):\n \n         session_controller.kb_controller.delete(knowledge_base_name, project_name)\n         return '', HTTPStatus.NO_CONTENT\n+\n+\n+@ns_conf.route('/<project_name>/knowledge_bases/<knowledge_base_name>/completions')\n+@ns_conf.param('project_name', 'Name of the project')\n+@ns_conf.param('knowledge_base_name', 'Name of the knowledge_base')\n+class KnowledgeBaseCompletions(Resource):\n+    @ns_conf.doc('knowledge_base_completions')\n+    @api_endpoint_metrics('POST', '/knowledge_bases/knowledge_base/completions')\n+    def post(self, project_name, knowledge_base_name):\n+        \"\"\"\n+        Add support for LLM generation on the response from knowledge base\n+        \"\"\"\n+        # Check for required parameters.\n+        if 'knowledge_base' not in request.json:\n+            return http_error(\n+                HTTPStatus.BAD_REQUEST,\n+                'Missing parameter',\n+                'Must provide \"knowledge_base\" parameter in POST body'\n+            )\n+\n+        # Check for required parameters\n+        query = request.json.get('query')\n+        if query is None:\n+            return http_error(\n+                HTTPStatus.BAD_REQUEST,\n+                'Missing parameter',\n+                'Must provide \"query\" parameter in POST body'\nComment: maybe it could be good to log errors ",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/api/http/namespaces/knowledge_bases.py",
    "pr_number": 9920,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1805103543,
    "comment_created_at": "2024-10-17T16:45:47Z"
  },
  {
    "code": "@@ -369,6 +369,9 @@ def predict(self, model_name: str, data: list, pred_format: str = 'dict',\n             args['dtype_dict'] = predictor_record.dtype_dict\n             args['learn_args'] = predictor_record.learn_args\n \n+        if self.handler_class.__name__ in ('LangChainHandler',):",
    "comment": "Instead I would put this code into _get_ml_handler\r\n```\r\n    command_executor = None\r\n    if it is LangChainHandler:\r\n         sql_session = SessionController()\r\n         command_executor = ExecuteCommands(sql_session, None)\r\n    ml_handler = self.handler_class(\r\n                engine_storage=handlerStorage,\r\n                model_storage=modelStorage,\r\n                executor=command_executor,\r\n     )\r\n```\r\n",
    "line_number": 372,
    "enriched": "File: mindsdb/integrations/libs/ml_exec_base.py\nCode: @@ -369,6 +369,9 @@ def predict(self, model_name: str, data: list, pred_format: str = 'dict',\n             args['dtype_dict'] = predictor_record.dtype_dict\n             args['learn_args'] = predictor_record.learn_args\n \n+        if self.handler_class.__name__ in ('LangChainHandler',):\nComment: Instead I would put this code into _get_ml_handler\r\n```\r\n    command_executor = None\r\n    if it is LangChainHandler:\r\n         sql_session = SessionController()\r\n         command_executor = ExecuteCommands(sql_session, None)\r\n    ml_handler = self.handler_class(\r\n                engine_storage=handlerStorage,\r\n                model_storage=modelStorage,\r\n                executor=command_executor,\r\n     )\r\n```\r\n",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/libs/ml_exec_base.py",
    "pr_number": 5479,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1160717719,
    "comment_created_at": "2023-04-07T13:54:02Z"
  },
  {
    "code": "@@ -0,0 +1,309 @@\n+import json\n+from typing import Iterable, List, Optional, Sequence, Tuple, Any, Union\n+import numpy as np\n+import faiss  # faiss or faiss-gpu\n+\n+def _as_np(x: Union[np.ndarray, Iterable[Iterable[float]]]) -> np.ndarray:\n+    arr = np.ascontiguousarray(np.array(x, dtype=\"float32\"))\n+    if arr.ndim == 1:\n+        arr = arr.reshape(1, -1)\n+    return arr\n+\n+def _normalize_rows(x: np.ndarray) -> np.ndarray:\n+    norms = np.linalg.norm(x, axis=1, keepdims=True) + 1e-12\n+    return x / norms\n+\n+class FaissIndexWithFilter:\n+    def __init__(\n+        self,\n+        dim: int,\n+        metric: str = \"cosine\",      # \"cosine\" or \"l2\"\n+        backend: str = \"ivf\",        # \"ivf\", \"flat\", \"hnsw\"\n+        use_gpu: bool = False,\n+        nlist: int = 1024,\n+        nprobe: int = 32,\n+        train_threshold: int = 5000,\n+        hnsw_m: int = 32,\n+        hnsw_ef_search: int = 64,\n+        hnsw_ef_construction: int = 200,\n+    ):\n+        assert metric in {\"cosine\", \"l2\"}\n+        assert backend in {\"ivf\", \"flat\", \"hnsw\"}\n+\n+        self.dim = dim\n+        self.metric = metric\n+        self.backend = backend\n+        self.use_gpu = use_gpu\n+\n+        self.nlist = nlist\n+        self.nprobe = nprobe\n+        self.train_threshold = train_threshold\n+\n+        self.hnsw_m = hnsw_m\n+        self.hnsw_ef_search = hnsw_ef_search\n+        self.hnsw_ef_construction = hnsw_ef_construction\n+\n+        self._index: Optional[faiss.Index] = None\n+        self._is_trained = False\n+\n+        self._buf_vectors: List[np.ndarray] = []\n+        self._buf_ids: List[int] = []\n+\n+        self._next_id = 0\n+        self._meta: dict[int, Any] = {}",
    "comment": "Use `Dict[int, Any]` from typing module instead of `dict[int, Any]` for compatibility with Python versions < 3.9. Import from typing: `from typing import Dict`.",
    "line_number": 53,
    "enriched": "File: mindsdb/integrations/handlers/duckdb_faiss_handler/faiss_index.py\nCode: @@ -0,0 +1,309 @@\n+import json\n+from typing import Iterable, List, Optional, Sequence, Tuple, Any, Union\n+import numpy as np\n+import faiss  # faiss or faiss-gpu\n+\n+def _as_np(x: Union[np.ndarray, Iterable[Iterable[float]]]) -> np.ndarray:\n+    arr = np.ascontiguousarray(np.array(x, dtype=\"float32\"))\n+    if arr.ndim == 1:\n+        arr = arr.reshape(1, -1)\n+    return arr\n+\n+def _normalize_rows(x: np.ndarray) -> np.ndarray:\n+    norms = np.linalg.norm(x, axis=1, keepdims=True) + 1e-12\n+    return x / norms\n+\n+class FaissIndexWithFilter:\n+    def __init__(\n+        self,\n+        dim: int,\n+        metric: str = \"cosine\",      # \"cosine\" or \"l2\"\n+        backend: str = \"ivf\",        # \"ivf\", \"flat\", \"hnsw\"\n+        use_gpu: bool = False,\n+        nlist: int = 1024,\n+        nprobe: int = 32,\n+        train_threshold: int = 5000,\n+        hnsw_m: int = 32,\n+        hnsw_ef_search: int = 64,\n+        hnsw_ef_construction: int = 200,\n+    ):\n+        assert metric in {\"cosine\", \"l2\"}\n+        assert backend in {\"ivf\", \"flat\", \"hnsw\"}\n+\n+        self.dim = dim\n+        self.metric = metric\n+        self.backend = backend\n+        self.use_gpu = use_gpu\n+\n+        self.nlist = nlist\n+        self.nprobe = nprobe\n+        self.train_threshold = train_threshold\n+\n+        self.hnsw_m = hnsw_m\n+        self.hnsw_ef_search = hnsw_ef_search\n+        self.hnsw_ef_construction = hnsw_ef_construction\n+\n+        self._index: Optional[faiss.Index] = None\n+        self._is_trained = False\n+\n+        self._buf_vectors: List[np.ndarray] = []\n+        self._buf_ids: List[int] = []\n+\n+        self._next_id = 0\n+        self._meta: dict[int, Any] = {}\nComment: Use `Dict[int, Any]` from typing module instead of `dict[int, Any]` for compatibility with Python versions < 3.9. Import from typing: `from typing import Dict`.",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/duckdb_faiss_handler/faiss_index.py",
    "pr_number": 11750,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2433766587,
    "comment_created_at": "2025-10-15T19:54:13Z"
  },
  {
    "code": "@@ -1058,4 +1058,129 @@ def get_columns(self) -> List[str]:\n             \"following\",\n             \"created_at\",\n             \"updated_at\"\n-        ]\n\\ No newline at end of file\n+        ]\n+        \n+class GithubMilestonesTable(APITable):\n+    \"\"\"The GitHub Milestones Table implementation\"\"\"\n+\n+    def select(self, query: ast.Select) -> pd.DataFrame:\n+        \"\"\"Pulls data from the GitHub \"List repository milestones\" API\n+\n+        Parameters\n+        ----------\n+        query : ast.Select\n+           Given SQL SELECT query\n+\n+        Returns\n+        -------\n+        pd.DataFrame\n+\n+            GitHub milestones matching the query\n+\n+        Raises\n+        ------\n+        ValueError\n+            If the query contains an unsupported condition\n+        \"\"\"\n+\n+        select_statement_parser = SELECTQueryParser(\n+            query,\n+            'milestones',\n+            self.get_columns()\n+        )\n+\n+        selected_columns, where_conditions, order_by_conditions, result_limit = select_statement_parser.parse_query()\n+\n+        total_results = result_limit if result_limit else 20\n+\n+        self.handler.connect()\n+\n+        github_milestones_df = pd.DataFrame(columns=self.get_columns())\n+\n+        start = 0\n+\n+        while True:",
    "comment": "Can we re write this logic as:\r\n\r\n```\r\n    milestones = self.handler.connection.get_repo(self.handler.repository).get_milestones()\r\n    for milestone in milestones[start:start + total_results]:\r\n```",
    "line_number": 1102,
    "enriched": "File: mindsdb/integrations/handlers/github_handler/github_tables.py\nCode: @@ -1058,4 +1058,129 @@ def get_columns(self) -> List[str]:\n             \"following\",\n             \"created_at\",\n             \"updated_at\"\n-        ]\n\\ No newline at end of file\n+        ]\n+        \n+class GithubMilestonesTable(APITable):\n+    \"\"\"The GitHub Milestones Table implementation\"\"\"\n+\n+    def select(self, query: ast.Select) -> pd.DataFrame:\n+        \"\"\"Pulls data from the GitHub \"List repository milestones\" API\n+\n+        Parameters\n+        ----------\n+        query : ast.Select\n+           Given SQL SELECT query\n+\n+        Returns\n+        -------\n+        pd.DataFrame\n+\n+            GitHub milestones matching the query\n+\n+        Raises\n+        ------\n+        ValueError\n+            If the query contains an unsupported condition\n+        \"\"\"\n+\n+        select_statement_parser = SELECTQueryParser(\n+            query,\n+            'milestones',\n+            self.get_columns()\n+        )\n+\n+        selected_columns, where_conditions, order_by_conditions, result_limit = select_statement_parser.parse_query()\n+\n+        total_results = result_limit if result_limit else 20\n+\n+        self.handler.connect()\n+\n+        github_milestones_df = pd.DataFrame(columns=self.get_columns())\n+\n+        start = 0\n+\n+        while True:\nComment: Can we re write this logic as:\r\n\r\n```\r\n    milestones = self.handler.connection.get_repo(self.handler.repository).get_milestones()\r\n    for milestone in milestones[start:start + total_results]:\r\n```",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/github_handler/github_tables.py",
    "pr_number": 7730,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1374459976,
    "comment_created_at": "2023-10-27T11:50:11Z"
  },
  {
    "code": "@@ -294,16 +294,23 @@ def from_sql_retriever(cls, config: RAGPipelineModel):\n         retriever = SQLRetriever(\n             fallback_retriever=vector_store_retriever,\n             vector_store_handler=knowledge_base_table.get_vector_db(),\n-            metadata_schemas=retriever_config.metadata_schemas,\n-            examples=retriever_config.examples,\n+            min_k=retriever_config.min_k,\n+            max_filters=retriever_config.max_filters,\n+            filter_threshold=retriever_config.filter_threshold,\n+            database_schema=retriever_config.metadata_schemas,",
    "comment": "Should be \r\n\r\n`database_schema=retriever_config.database_schema`",
    "line_number": 300,
    "enriched": "File: mindsdb/integrations/utilities/rag/pipelines/rag.py\nCode: @@ -294,16 +294,23 @@ def from_sql_retriever(cls, config: RAGPipelineModel):\n         retriever = SQLRetriever(\n             fallback_retriever=vector_store_retriever,\n             vector_store_handler=knowledge_base_table.get_vector_db(),\n-            metadata_schemas=retriever_config.metadata_schemas,\n-            examples=retriever_config.examples,\n+            min_k=retriever_config.min_k,\n+            max_filters=retriever_config.max_filters,\n+            filter_threshold=retriever_config.filter_threshold,\n+            database_schema=retriever_config.metadata_schemas,\nComment: Should be \r\n\r\n`database_schema=retriever_config.database_schema`",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/utilities/rag/pipelines/rag.py",
    "pr_number": 10463,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1956654665,
    "comment_created_at": "2025-02-14T19:57:13Z"
  },
  {
    "code": "@@ -180,8 +180,34 @@ def add_agent(\n         if agent is not None:\n             raise ValueError(f\"Agent with name already exists: {name}\")\n \n+        # Get default LLM parameters from config if they exist\n+        default_llm_params = config.get(\"default_llm\", {})",
    "comment": "Hey @dusvyat,\r\nCurrently, when using Knowledge Bases, we allow users to set the default model parameters, but they have the option of [overriding them](https://github.com/mindsdb/mindsdb/blob/f12e82a4a6a883b9fad40c9e4a87bfe2f10a6f5d/mindsdb/interfaces/knowledge_base/controller.py#L58) by explicitly passing them with the CREATE statement. I think it would be best if we allowed the same for agents?",
    "line_number": 184,
    "enriched": "File: mindsdb/interfaces/agents/agents_controller.py\nCode: @@ -180,8 +180,34 @@ def add_agent(\n         if agent is not None:\n             raise ValueError(f\"Agent with name already exists: {name}\")\n \n+        # Get default LLM parameters from config if they exist\n+        default_llm_params = config.get(\"default_llm\", {})\nComment: Hey @dusvyat,\r\nCurrently, when using Knowledge Bases, we allow users to set the default model parameters, but they have the option of [overriding them](https://github.com/mindsdb/mindsdb/blob/f12e82a4a6a883b9fad40c9e4a87bfe2f10a6f5d/mindsdb/interfaces/knowledge_base/controller.py#L58) by explicitly passing them with the CREATE statement. I think it would be best if we allowed the same for agents?",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/interfaces/agents/agents_controller.py",
    "pr_number": 10997,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2126242868,
    "comment_created_at": "2025-06-04T10:18:18Z"
  },
  {
    "code": "@@ -42,9 +42,11 @@ def create(self, target: str, df: Optional[pd.DataFrame] = None, args: Optional[\n             raise Exception(\"Error: Training failed: \" + resp['status'])\n \n     def predict(self, df, args=None):\n-        args = self.model_storage.json_get('args')  # override any incoming args for now\n+        args = {**(self.model_storage.json_get('args')), **args }  # merge incoming args",
    "comment": "maybe extracting this key and sending to ray server will be enough?\r\n`pred_args = args.get('predict_params', {})`\r\nin pred_args  should be everything from `using` clause of the query:\r\n```\r\nSELECT *\r\nFROM table as t\r\nJOIN model as m\r\nusing my_var=12;\r\n``` \r\nOr something else required to be passed to ray server? ",
    "line_number": 45,
    "enriched": "File: mindsdb/integrations/handlers/ray_serve_handler/ray_serve_handler.py\nCode: @@ -42,9 +42,11 @@ def create(self, target: str, df: Optional[pd.DataFrame] = None, args: Optional[\n             raise Exception(\"Error: Training failed: \" + resp['status'])\n \n     def predict(self, df, args=None):\n-        args = self.model_storage.json_get('args')  # override any incoming args for now\n+        args = {**(self.model_storage.json_get('args')), **args }  # merge incoming args\nComment: maybe extracting this key and sending to ray server will be enough?\r\n`pred_args = args.get('predict_params', {})`\r\nin pred_args  should be everything from `using` clause of the query:\r\n```\r\nSELECT *\r\nFROM table as t\r\nJOIN model as m\r\nusing my_var=12;\r\n``` \r\nOr something else required to be passed to ray server? ",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/ray_serve_handler/ray_serve_handler.py",
    "pr_number": 10371,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1918216144,
    "comment_created_at": "2025-01-16T10:34:14Z"
  },
  {
    "code": "@@ -195,23 +215,20 @@ def connect(self):\n         if self.is_connected is True:\n             return self.connection\n \n-        # Mandatory connection parameters\n-        if not all(key in self.connection_data for key in [\"account\", \"user\", \"password\", \"database\"]):\n-            raise ValueError(\"Required parameters (account, user, password, database) must be provided.\")\n-\n-        config = {\n-            \"account\": self.connection_data.get(\"account\"),\n-            \"user\": self.connection_data.get(\"user\"),\n-            \"password\": self.connection_data.get(\"password\"),\n-            \"database\": self.connection_data.get(\"database\"),\n-            \"schema\": self.connection_data.get(\"schema\", \"PUBLIC\"),\n-        }\n-\n-        # Optional connection parameters\n-        optional_params = [\"warehouse\", \"role\"]\n-        for param in optional_params:\n-            if param in self.connection_data:\n-                config[param] = self.connection_data[param]\n+        credential_type = self.connection_data.get(\"credential_type\", None)\n+\n+        # Fallback logic for backward compatibility if credential_type is not set\n+        if credential_type is None:\n+            if \"private_key_path\" in self.connection_data and self.connection_data.get(\"private_key_path\"):\n+                credential_type = \"custom-key-pair\"\n+            else:\n+                credential_type = None  # Default to PasswordAuthType\n+\n+        auth_type = self._auth_types.get(credential_type)\n+        if not auth_type:\n+            raise ValueError(\"Invalid credential type provided.\")\n+\n+        config = auth_type.get_config(**self.connection_data)",
    "comment": "**correctness**: `credential_type` value 'custom-key-pair' is not handled in `_auth_types`, causing key pair authentication to fail if 'private_key_path' is set and 'credential_type' is not provided.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/integrations/handlers/snowflake_handler/snowflake_handler.py, lines 218-231, the fallback logic for key pair authentication sets credential_type to 'custom-key-pair', but the _auth_types mapping only recognizes 'key_pair'. This causes key pair authentication to fail if 'private_key_path' is set and 'credential_type' is not provided. Change the fallback assignment from 'custom-key-pair' to 'key_pair' so that the correct authentication type is used.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n        credential_type = self.connection_data.get(\"credential_type\", None)\n\n        # Fallback logic for backward compatibility if credential_type is not set\n        if credential_type is None:\n            if \"private_key_path\" in self.connection_data and self.connection_data.get(\"private_key_path\"):\n                credential_type = \"key_pair\"\n            else:\n                credential_type = None  # Default to PasswordAuthType\n\n        auth_type = self._auth_types.get(credential_type)\n        if not auth_type:\n            raise ValueError(\"Invalid credential type provided.\")\n\n        config = auth_type.get_config(**self.connection_data)\n```\n</details>\n<!-- suggestion_end -->\n",
    "line_number": 230,
    "enriched": "File: mindsdb/integrations/handlers/snowflake_handler/snowflake_handler.py\nCode: @@ -195,23 +215,20 @@ def connect(self):\n         if self.is_connected is True:\n             return self.connection\n \n-        # Mandatory connection parameters\n-        if not all(key in self.connection_data for key in [\"account\", \"user\", \"password\", \"database\"]):\n-            raise ValueError(\"Required parameters (account, user, password, database) must be provided.\")\n-\n-        config = {\n-            \"account\": self.connection_data.get(\"account\"),\n-            \"user\": self.connection_data.get(\"user\"),\n-            \"password\": self.connection_data.get(\"password\"),\n-            \"database\": self.connection_data.get(\"database\"),\n-            \"schema\": self.connection_data.get(\"schema\", \"PUBLIC\"),\n-        }\n-\n-        # Optional connection parameters\n-        optional_params = [\"warehouse\", \"role\"]\n-        for param in optional_params:\n-            if param in self.connection_data:\n-                config[param] = self.connection_data[param]\n+        credential_type = self.connection_data.get(\"credential_type\", None)\n+\n+        # Fallback logic for backward compatibility if credential_type is not set\n+        if credential_type is None:\n+            if \"private_key_path\" in self.connection_data and self.connection_data.get(\"private_key_path\"):\n+                credential_type = \"custom-key-pair\"\n+            else:\n+                credential_type = None  # Default to PasswordAuthType\n+\n+        auth_type = self._auth_types.get(credential_type)\n+        if not auth_type:\n+            raise ValueError(\"Invalid credential type provided.\")\n+\n+        config = auth_type.get_config(**self.connection_data)\nComment: **correctness**: `credential_type` value 'custom-key-pair' is not handled in `_auth_types`, causing key pair authentication to fail if 'private_key_path' is set and 'credential_type' is not provided.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/integrations/handlers/snowflake_handler/snowflake_handler.py, lines 218-231, the fallback logic for key pair authentication sets credential_type to 'custom-key-pair', but the _auth_types mapping only recognizes 'key_pair'. This causes key pair authentication to fail if 'private_key_path' is set and 'credential_type' is not provided. Change the fallback assignment from 'custom-key-pair' to 'key_pair' so that the correct authentication type is used.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n        credential_type = self.connection_data.get(\"credential_type\", None)\n\n        # Fallback logic for backward compatibility if credential_type is not set\n        if credential_type is None:\n            if \"private_key_path\" in self.connection_data and self.connection_data.get(\"private_key_path\"):\n                credential_type = \"key_pair\"\n            else:\n                credential_type = None  # Default to PasswordAuthType\n\n        auth_type = self._auth_types.get(credential_type)\n        if not auth_type:\n            raise ValueError(\"Invalid credential type provided.\")\n\n        config = auth_type.get_config(**self.connection_data)\n```\n</details>\n<!-- suggestion_end -->\n",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/snowflake_handler/snowflake_handler.py",
    "pr_number": 11960,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2583053306,
    "comment_created_at": "2025-12-02T23:03:40Z"
  },
  {
    "code": "@@ -52,3 +52,59 @@ def get_payments(self, **kwargs) -> List[Dict]:\n         connection = self.handler.connect()\n         payments = paypalrestsdk.Payment.all(kwargs, api=connection)\n         return [payment.to_dict() for payment in payments['payments']]\n+\n+\n+import paypalrestsdk",
    "comment": "Let's add the imports on top",
    "line_number": 57,
    "enriched": "File: mindsdb/integrations/handlers/paypal_handler/paypal_tables.py\nCode: @@ -52,3 +52,59 @@ def get_payments(self, **kwargs) -> List[Dict]:\n         connection = self.handler.connect()\n         payments = paypalrestsdk.Payment.all(kwargs, api=connection)\n         return [payment.to_dict() for payment in payments['payments']]\n+\n+\n+import paypalrestsdk\nComment: Let's add the imports on top",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/paypal_handler/paypal_tables.py",
    "pr_number": 7743,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1358158754,
    "comment_created_at": "2023-10-13T11:51:58Z"
  },
  {
    "code": "@@ -68,7 +73,7 @@ def _get_file_extension(self, file_path_or_url: str) -> str:\n         else:\n             return os.path.splitext(file_path_or_url)[1]\n \n-    def _pdf_to_markdown(self, file_content: Union[requests.Response, bytes], **kwargs) -> str:\n+    def _pdf_to_markdown(self, file_content: Union[requests.Response, BytesIO], **kwargs) -> str:",
    "comment": "_get_file_content annotated as returning str, it mismatches with BytesIO input",
    "line_number": 76,
    "enriched": "File: mindsdb/interfaces/functions/to_markdown.py\nCode: @@ -68,7 +73,7 @@ def _get_file_extension(self, file_path_or_url: str) -> str:\n         else:\n             return os.path.splitext(file_path_or_url)[1]\n \n-    def _pdf_to_markdown(self, file_content: Union[requests.Response, bytes], **kwargs) -> str:\n+    def _pdf_to_markdown(self, file_content: Union[requests.Response, BytesIO], **kwargs) -> str:\nComment: _get_file_content annotated as returning str, it mismatches with BytesIO input",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/interfaces/functions/to_markdown.py",
    "pr_number": 11228,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2184769082,
    "comment_created_at": "2025-07-04T08:51:21Z"
  },
  {
    "code": "@@ -0,0 +1,444 @@\n+import pandas as pd\n+from typing import List\n+from mindsdb.integrations.libs.api_handler import APITable\n+from mindsdb.integrations.handlers.utilities.query_utilities import SELECTQueryParser, SELECTQueryExecutor\n+from mindsdb.utilities.log import get_log\n+from mindsdb_sql.parser import ast\n+\n+logger = get_log(\"integrations.aqicn_handler\")\n+\n+\n+class AQByUserLocationTable(APITable):\n+    \"\"\"The Air Quality By User Location Table implementation\"\"\"\n+\n+    def select(self, query: ast.Select) -> pd.DataFrame:\n+        \"\"\"Pulls data from the https://aqicn.org/json-api/doc/#api-Geolocalized_Feed-GetHereFeed\" API\n+\n+        Parameters\n+        ----------\n+        query : ast.Select\n+           Given SQL SELECT query\n+\n+        Returns\n+        -------\n+        pd.DataFrame\n+            Air Quality Data\n+\n+        Raises\n+        ------\n+        ValueError\n+            If the query contains an unsupported condition\n+        \"\"\"\n+\n+        select_statement_parser = SELECTQueryParser(\n+            query,\n+            'air_quality_user_location',\n+            self.get_columns()\n+        )\n+\n+        selected_columns, where_conditions, order_by_conditions, result_limit = select_statement_parser.parse_query()\n+\n+        subset_where_conditions = []\n+        for op, arg1, arg2 in where_conditions:\n+            if arg1 in self.get_columns():\n+                subset_where_conditions.append([op, arg1, arg2])\n+\n+        df = pd.DataFrame(columns=self.get_columns())\n+\n+        response = self.handler.aqicn_client.air_quality_user_location()\n+\n+        self.check_res(res=response)\n+\n+        df = pd.json_normalize(response[\"content\"])\n+\n+        select_statement_executor = SELECTQueryExecutor(\n+            df,\n+            selected_columns,\n+            subset_where_conditions,\n+            order_by_conditions,\n+            result_limit\n+        )\n+\n+        df = select_statement_executor.execute_query()\n+\n+        return df\n+\n+    def check_res(self, res):\n+        if res[\"code\"] != 200:\n+            raise Exception(\"Error fetching results - \" + res[\"content\"])\n+\n+    def get_columns(self) -> List[str]:\n+        \"\"\"Gets all columns to be returned in pandas DataFrame responses\n+\n+        Returns\n+        -------\n+        List[str]\n+            List of columns\n+        \"\"\"\n+\n+        return [\n+            \"status\",\n+            \"data.aqi\",\n+            \"data.idx\",\n+            \"data.attributions\",\n+            \"data.city.geo\",\n+            \"data.city.name\",\n+            \"data.city.url\",\n+            \"data.city.location\",\n+            \"data.dominentpol\",\n+            \"data.iaqi.co.v\",\n+            \"data.iaqi.dew.v\",\n+            \"data.iaqi.h.v\",\n+            \"data.iaqi.no2.v\",\n+            \"data.iaqi.p.v\",\n+            \"data.iaqi.pm10.v\",\n+            \"data.iaqi.so2.v\",\n+            \"data.iaqi.t.v\",\n+            \"data.iaqi.w.v\",\n+            \"data.time.s\",\n+            \"data.time.tz\",\n+            \"data.time.v\",\n+            \"data.time.iso\",\n+            \"data.forecast.daily.o3\",\n+            \"data.forecast.daily.pm10\",\n+            \"data.forecast.daily.pm25\",\n+            \"data.debug.sync\"\n+        ]\n+\n+\n+class AQByCityTable(APITable):\n+    \"\"\"The Air Quality By City Table implementation\"\"\"\n+\n+    def select(self, query: ast.Select) -> pd.DataFrame:\n+        \"\"\"Pulls data from the https://aqicn.org/json-api/doc/#api-City_Feed-GetCityFeed\" API\n+\n+        Parameters\n+        ----------\n+        query : ast.Select\n+           Given SQL SELECT query\n+\n+        Returns\n+        -------\n+        pd.DataFrame\n+            Air Quality Data\n+\n+        Raises\n+        ------\n+        ValueError\n+            If the query contains an unsupported condition\n+        \"\"\"\n+\n+        select_statement_parser = SELECTQueryParser(\n+            query,\n+            'air_quality_city',\n+            self.get_columns()\n+        )\n+\n+        selected_columns, where_conditions, order_by_conditions, result_limit = select_statement_parser.parse_query()\n+\n+        search_params = {}\n+        subset_where_conditions = []\n+        for op, arg1, arg2 in where_conditions:\n+            if arg1 == 'city':\n+                if op == '=':\n+                    search_params[\"city\"] = arg2\n+                else:\n+                    raise NotImplementedError(\"Only '=' operator is supported for city column.\")\n+            elif arg1 in self.get_columns():\n+                subset_where_conditions.append([op, arg1, arg2])\n+\n+        filter_flag = (\"city\" in search_params)\n+\n+        if not filter_flag:\n+            raise NotImplementedError(\"city column has to be present in where clause.\")\n+\n+        df = pd.DataFrame(columns=self.get_columns())\n+\n+        response = self.handler.aqicn_client.air_quality_city(search_params[\"city\"])\n+\n+        self.check_res(res=response)\n+\n+        df = pd.json_normalize(response[\"content\"])\n+\n+        select_statement_executor = SELECTQueryExecutor(\n+            df,\n+            selected_columns,\n+            subset_where_conditions,\n+            order_by_conditions,\n+            result_limit\n+        )\n+\n+        df = select_statement_executor.execute_query()\n+\n+        return df\n+\n+    def check_res(self, res):\n+        if res[\"code\"] != 200:\n+            raise Exception(\"Error fetching results - \" + res[\"content\"])\n+\n+    def get_columns(self) -> List[str]:\n+        \"\"\"Gets all columns to be returned in pandas DataFrame responses\n+\n+        Returns\n+        -------\n+        List[str]\n+            List of columns\n+        \"\"\"\n+\n+        return [\n+            \"status\",\n+            \"data.aqi\",\n+            \"data.idx\",\n+            \"data.attributions\",\n+            \"data.city.geo\",\n+            \"data.city.name\",\n+            \"data.city.url\",\n+            \"data.city.location\",\n+            \"data.dominentpol\",\n+            \"data.iaqi.co.v\",\n+            \"data.iaqi.dew.v\",\n+            \"data.iaqi.h.v\",\n+            \"data.iaqi.no2.v\",\n+            \"data.iaqi.o3.v\",\n+            \"data.iaqi.p.v\",\n+            \"data.iaqi.pm10.v\",\n+            \"data.iaqi.pm25.v\",\n+            \"data.iaqi.so2.v\",\n+            \"data.iaqi.t.v\",\n+            \"data.iaqi.w.v\",\n+            \"data.time.s\",\n+            \"data.time.tz\",\n+            \"data.time.v\",\n+            \"data.time.iso\",\n+            \"data.forecast.daily.o3\",\n+            \"data.forecast.daily.pm10\",\n+            \"data.forecast.daily.pm25\",\n+            \"data.debug.sync\"\n+        ]\n+\n+\n+class AQByLatLngTable(APITable):\n+    \"\"\"The Air Quality By Lat Lng Table implementation\"\"\"\n+\n+    def select(self, query: ast.Select) -> pd.DataFrame:\n+        \"\"\"Pulls data from the https://aqicn.org/json-api/doc/#api-Geolocalized_Feed-GetGeolocFeed\" API\n+\n+        Parameters\n+        ----------\n+        query : ast.Select\n+           Given SQL SELECT query\n+\n+        Returns\n+        -------\n+        pd.DataFrame\n+            Air Quality Data\n+\n+        Raises\n+        ------\n+        ValueError\n+            If the query contains an unsupported condition\n+        \"\"\"\n+\n+        select_statement_parser = SELECTQueryParser(\n+            query,\n+            'air_quality_lat_lng',\n+            self.get_columns()\n+        )\n+\n+        selected_columns, where_conditions, order_by_conditions, result_limit = select_statement_parser.parse_query()\n+\n+        search_params = {}\n+        subset_where_conditions = []\n+        for op, arg1, arg2 in where_conditions:\n+            if arg1 == 'lat':\n+                if op == '=':\n+                    search_params[\"lat\"] = arg2\n+                else:\n+                    raise NotImplementedError(\"Only '=' operator is supported for lat column.\")\n+            if arg1 == 'lng':\n+                if op == '=':\n+                    search_params[\"lng\"] = arg2\n+                else:\n+                    raise NotImplementedError(\"Only '=' operator is supported for lng column.\")\n+            elif arg1 in self.get_columns():\n+                subset_where_conditions.append([op, arg1, arg2])\n+\n+        filter_flag = (\"lat\" in search_params) and (\"lng\" in search_params)\n+\n+        if not filter_flag:\n+            raise NotImplementedError(\"lat and lng columns have to be present in where clause.\")\n+\n+        df = pd.DataFrame(columns=self.get_columns())\n+\n+        response = self.handler.aqicn_client.air_quality_lat_lng(search_params[\"lat\"], search_params[\"lng\"])\n+\n+        self.check_res(res=response)\n+\n+        df = pd.json_normalize(response[\"content\"])\n+\n+        select_statement_executor = SELECTQueryExecutor(\n+            df,\n+            selected_columns,\n+            subset_where_conditions,\n+            order_by_conditions,\n+            result_limit\n+        )\n+\n+        df = select_statement_executor.execute_query()\n+\n+        return df\n+\n+    def check_res(self, res):\n+        if res[\"code\"] != 200:\n+            raise Exception(\"Error fetching results - \" + res[\"content\"])\n+\n+    def get_columns(self) -> List[str]:\n+        \"\"\"Gets all columns to be returned in pandas DataFrame responses\n+\n+        Returns\n+        -------\n+        List[str]\n+            List of columns\n+        \"\"\"\n+\n+        return [\n+            \"status\",\n+            \"data.aqi\",\n+            \"data.idx\",\n+            \"data.attributions\",\n+            \"data.city.geo\",\n+            \"data.city.name\",\n+            \"data.city.url\",\n+            \"data.city.location\",\n+            \"data.dominentpol\",\n+            \"data.iaqi.co.v\",\n+            \"data.iaqi.dew.v\",\n+            \"data.iaqi.h.v\",\n+            \"data.iaqi.no2.v\",\n+            \"data.iaqi.o3.v\",\n+            \"data.iaqi.p.v\",\n+            \"data.iaqi.pm10.v\",\n+            \"data.iaqi.pm25.v\",\n+            \"data.iaqi.so2.v\",\n+            \"data.iaqi.t.v\",\n+            \"data.iaqi.w.v\",\n+            \"data.time.s\",\n+            \"data.time.tz\",\n+            \"data.time.v\",\n+            \"data.time.iso\",\n+            \"data.forecast.daily.o3\",\n+            \"data.forecast.daily.pm10\",\n+            \"data.forecast.daily.pm25\",\n+            \"data.debug.sync\"\n+        ]\n+\n+\n+class AQByNetworkStationTable(APITable):",
    "comment": "This table does not seem to be registered with the handler. Is it still in progress? If we can use it, can you please register it add an example to the README?",
    "line_number": 336,
    "enriched": "File: mindsdb/integrations/handlers/aqicn_handler/aqicn_tables.py\nCode: @@ -0,0 +1,444 @@\n+import pandas as pd\n+from typing import List\n+from mindsdb.integrations.libs.api_handler import APITable\n+from mindsdb.integrations.handlers.utilities.query_utilities import SELECTQueryParser, SELECTQueryExecutor\n+from mindsdb.utilities.log import get_log\n+from mindsdb_sql.parser import ast\n+\n+logger = get_log(\"integrations.aqicn_handler\")\n+\n+\n+class AQByUserLocationTable(APITable):\n+    \"\"\"The Air Quality By User Location Table implementation\"\"\"\n+\n+    def select(self, query: ast.Select) -> pd.DataFrame:\n+        \"\"\"Pulls data from the https://aqicn.org/json-api/doc/#api-Geolocalized_Feed-GetHereFeed\" API\n+\n+        Parameters\n+        ----------\n+        query : ast.Select\n+           Given SQL SELECT query\n+\n+        Returns\n+        -------\n+        pd.DataFrame\n+            Air Quality Data\n+\n+        Raises\n+        ------\n+        ValueError\n+            If the query contains an unsupported condition\n+        \"\"\"\n+\n+        select_statement_parser = SELECTQueryParser(\n+            query,\n+            'air_quality_user_location',\n+            self.get_columns()\n+        )\n+\n+        selected_columns, where_conditions, order_by_conditions, result_limit = select_statement_parser.parse_query()\n+\n+        subset_where_conditions = []\n+        for op, arg1, arg2 in where_conditions:\n+            if arg1 in self.get_columns():\n+                subset_where_conditions.append([op, arg1, arg2])\n+\n+        df = pd.DataFrame(columns=self.get_columns())\n+\n+        response = self.handler.aqicn_client.air_quality_user_location()\n+\n+        self.check_res(res=response)\n+\n+        df = pd.json_normalize(response[\"content\"])\n+\n+        select_statement_executor = SELECTQueryExecutor(\n+            df,\n+            selected_columns,\n+            subset_where_conditions,\n+            order_by_conditions,\n+            result_limit\n+        )\n+\n+        df = select_statement_executor.execute_query()\n+\n+        return df\n+\n+    def check_res(self, res):\n+        if res[\"code\"] != 200:\n+            raise Exception(\"Error fetching results - \" + res[\"content\"])\n+\n+    def get_columns(self) -> List[str]:\n+        \"\"\"Gets all columns to be returned in pandas DataFrame responses\n+\n+        Returns\n+        -------\n+        List[str]\n+            List of columns\n+        \"\"\"\n+\n+        return [\n+            \"status\",\n+            \"data.aqi\",\n+            \"data.idx\",\n+            \"data.attributions\",\n+            \"data.city.geo\",\n+            \"data.city.name\",\n+            \"data.city.url\",\n+            \"data.city.location\",\n+            \"data.dominentpol\",\n+            \"data.iaqi.co.v\",\n+            \"data.iaqi.dew.v\",\n+            \"data.iaqi.h.v\",\n+            \"data.iaqi.no2.v\",\n+            \"data.iaqi.p.v\",\n+            \"data.iaqi.pm10.v\",\n+            \"data.iaqi.so2.v\",\n+            \"data.iaqi.t.v\",\n+            \"data.iaqi.w.v\",\n+            \"data.time.s\",\n+            \"data.time.tz\",\n+            \"data.time.v\",\n+            \"data.time.iso\",\n+            \"data.forecast.daily.o3\",\n+            \"data.forecast.daily.pm10\",\n+            \"data.forecast.daily.pm25\",\n+            \"data.debug.sync\"\n+        ]\n+\n+\n+class AQByCityTable(APITable):\n+    \"\"\"The Air Quality By City Table implementation\"\"\"\n+\n+    def select(self, query: ast.Select) -> pd.DataFrame:\n+        \"\"\"Pulls data from the https://aqicn.org/json-api/doc/#api-City_Feed-GetCityFeed\" API\n+\n+        Parameters\n+        ----------\n+        query : ast.Select\n+           Given SQL SELECT query\n+\n+        Returns\n+        -------\n+        pd.DataFrame\n+            Air Quality Data\n+\n+        Raises\n+        ------\n+        ValueError\n+            If the query contains an unsupported condition\n+        \"\"\"\n+\n+        select_statement_parser = SELECTQueryParser(\n+            query,\n+            'air_quality_city',\n+            self.get_columns()\n+        )\n+\n+        selected_columns, where_conditions, order_by_conditions, result_limit = select_statement_parser.parse_query()\n+\n+        search_params = {}\n+        subset_where_conditions = []\n+        for op, arg1, arg2 in where_conditions:\n+            if arg1 == 'city':\n+                if op == '=':\n+                    search_params[\"city\"] = arg2\n+                else:\n+                    raise NotImplementedError(\"Only '=' operator is supported for city column.\")\n+            elif arg1 in self.get_columns():\n+                subset_where_conditions.append([op, arg1, arg2])\n+\n+        filter_flag = (\"city\" in search_params)\n+\n+        if not filter_flag:\n+            raise NotImplementedError(\"city column has to be present in where clause.\")\n+\n+        df = pd.DataFrame(columns=self.get_columns())\n+\n+        response = self.handler.aqicn_client.air_quality_city(search_params[\"city\"])\n+\n+        self.check_res(res=response)\n+\n+        df = pd.json_normalize(response[\"content\"])\n+\n+        select_statement_executor = SELECTQueryExecutor(\n+            df,\n+            selected_columns,\n+            subset_where_conditions,\n+            order_by_conditions,\n+            result_limit\n+        )\n+\n+        df = select_statement_executor.execute_query()\n+\n+        return df\n+\n+    def check_res(self, res):\n+        if res[\"code\"] != 200:\n+            raise Exception(\"Error fetching results - \" + res[\"content\"])\n+\n+    def get_columns(self) -> List[str]:\n+        \"\"\"Gets all columns to be returned in pandas DataFrame responses\n+\n+        Returns\n+        -------\n+        List[str]\n+            List of columns\n+        \"\"\"\n+\n+        return [\n+            \"status\",\n+            \"data.aqi\",\n+            \"data.idx\",\n+            \"data.attributions\",\n+            \"data.city.geo\",\n+            \"data.city.name\",\n+            \"data.city.url\",\n+            \"data.city.location\",\n+            \"data.dominentpol\",\n+            \"data.iaqi.co.v\",\n+            \"data.iaqi.dew.v\",\n+            \"data.iaqi.h.v\",\n+            \"data.iaqi.no2.v\",\n+            \"data.iaqi.o3.v\",\n+            \"data.iaqi.p.v\",\n+            \"data.iaqi.pm10.v\",\n+            \"data.iaqi.pm25.v\",\n+            \"data.iaqi.so2.v\",\n+            \"data.iaqi.t.v\",\n+            \"data.iaqi.w.v\",\n+            \"data.time.s\",\n+            \"data.time.tz\",\n+            \"data.time.v\",\n+            \"data.time.iso\",\n+            \"data.forecast.daily.o3\",\n+            \"data.forecast.daily.pm10\",\n+            \"data.forecast.daily.pm25\",\n+            \"data.debug.sync\"\n+        ]\n+\n+\n+class AQByLatLngTable(APITable):\n+    \"\"\"The Air Quality By Lat Lng Table implementation\"\"\"\n+\n+    def select(self, query: ast.Select) -> pd.DataFrame:\n+        \"\"\"Pulls data from the https://aqicn.org/json-api/doc/#api-Geolocalized_Feed-GetGeolocFeed\" API\n+\n+        Parameters\n+        ----------\n+        query : ast.Select\n+           Given SQL SELECT query\n+\n+        Returns\n+        -------\n+        pd.DataFrame\n+            Air Quality Data\n+\n+        Raises\n+        ------\n+        ValueError\n+            If the query contains an unsupported condition\n+        \"\"\"\n+\n+        select_statement_parser = SELECTQueryParser(\n+            query,\n+            'air_quality_lat_lng',\n+            self.get_columns()\n+        )\n+\n+        selected_columns, where_conditions, order_by_conditions, result_limit = select_statement_parser.parse_query()\n+\n+        search_params = {}\n+        subset_where_conditions = []\n+        for op, arg1, arg2 in where_conditions:\n+            if arg1 == 'lat':\n+                if op == '=':\n+                    search_params[\"lat\"] = arg2\n+                else:\n+                    raise NotImplementedError(\"Only '=' operator is supported for lat column.\")\n+            if arg1 == 'lng':\n+                if op == '=':\n+                    search_params[\"lng\"] = arg2\n+                else:\n+                    raise NotImplementedError(\"Only '=' operator is supported for lng column.\")\n+            elif arg1 in self.get_columns():\n+                subset_where_conditions.append([op, arg1, arg2])\n+\n+        filter_flag = (\"lat\" in search_params) and (\"lng\" in search_params)\n+\n+        if not filter_flag:\n+            raise NotImplementedError(\"lat and lng columns have to be present in where clause.\")\n+\n+        df = pd.DataFrame(columns=self.get_columns())\n+\n+        response = self.handler.aqicn_client.air_quality_lat_lng(search_params[\"lat\"], search_params[\"lng\"])\n+\n+        self.check_res(res=response)\n+\n+        df = pd.json_normalize(response[\"content\"])\n+\n+        select_statement_executor = SELECTQueryExecutor(\n+            df,\n+            selected_columns,\n+            subset_where_conditions,\n+            order_by_conditions,\n+            result_limit\n+        )\n+\n+        df = select_statement_executor.execute_query()\n+\n+        return df\n+\n+    def check_res(self, res):\n+        if res[\"code\"] != 200:\n+            raise Exception(\"Error fetching results - \" + res[\"content\"])\n+\n+    def get_columns(self) -> List[str]:\n+        \"\"\"Gets all columns to be returned in pandas DataFrame responses\n+\n+        Returns\n+        -------\n+        List[str]\n+            List of columns\n+        \"\"\"\n+\n+        return [\n+            \"status\",\n+            \"data.aqi\",\n+            \"data.idx\",\n+            \"data.attributions\",\n+            \"data.city.geo\",\n+            \"data.city.name\",\n+            \"data.city.url\",\n+            \"data.city.location\",\n+            \"data.dominentpol\",\n+            \"data.iaqi.co.v\",\n+            \"data.iaqi.dew.v\",\n+            \"data.iaqi.h.v\",\n+            \"data.iaqi.no2.v\",\n+            \"data.iaqi.o3.v\",\n+            \"data.iaqi.p.v\",\n+            \"data.iaqi.pm10.v\",\n+            \"data.iaqi.pm25.v\",\n+            \"data.iaqi.so2.v\",\n+            \"data.iaqi.t.v\",\n+            \"data.iaqi.w.v\",\n+            \"data.time.s\",\n+            \"data.time.tz\",\n+            \"data.time.v\",\n+            \"data.time.iso\",\n+            \"data.forecast.daily.o3\",\n+            \"data.forecast.daily.pm10\",\n+            \"data.forecast.daily.pm25\",\n+            \"data.debug.sync\"\n+        ]\n+\n+\n+class AQByNetworkStationTable(APITable):\nComment: This table does not seem to be registered with the handler. Is it still in progress? If we can use it, can you please register it add an example to the README?",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/aqicn_handler/aqicn_tables.py",
    "pr_number": 8133,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1379628258,
    "comment_created_at": "2023-11-02T05:58:11Z"
  },
  {
    "code": "@@ -73,7 +74,17 @@ def construct_model_from_args(args: Dict) -> Embeddings:\n         class_name = EMBEDDING_MODELS[class_name]\n     MODEL_CLASS = get_langchain_class(class_name)\n     serialized_dict = copy.deepcopy(args)\n-    serialized_dict.pop(\"input_columns\", None)\n+\n+    # Make sure we don't pass in unnecessary arguments.\n+    if issubclass(MODEL_CLASS, BaseModel):\n+        keys_to_exclude = []\n+        for embedding_model_param in serialized_dict:\n+            if embedding_model_param not in MODEL_CLASS.model_fields:\n+                # Should not pass extra args.\n+                keys_to_exclude.append(embedding_model_param)\n+        for key in keys_to_exclude:\n+            serialized_dict.pop(key, None)",
    "comment": "```\r\nserialized_dict = {\r\n    k: v for k, v in serialized_dict.items() if k in MODEL_CLASS.model_fields\r\n}\r\n```\r\n\r\nmaybe dictionary comprehension would be better here?",
    "line_number": 86,
    "enriched": "File: mindsdb/integrations/handlers/langchain_embedding_handler/langchain_embedding_handler.py\nCode: @@ -73,7 +74,17 @@ def construct_model_from_args(args: Dict) -> Embeddings:\n         class_name = EMBEDDING_MODELS[class_name]\n     MODEL_CLASS = get_langchain_class(class_name)\n     serialized_dict = copy.deepcopy(args)\n-    serialized_dict.pop(\"input_columns\", None)\n+\n+    # Make sure we don't pass in unnecessary arguments.\n+    if issubclass(MODEL_CLASS, BaseModel):\n+        keys_to_exclude = []\n+        for embedding_model_param in serialized_dict:\n+            if embedding_model_param not in MODEL_CLASS.model_fields:\n+                # Should not pass extra args.\n+                keys_to_exclude.append(embedding_model_param)\n+        for key in keys_to_exclude:\n+            serialized_dict.pop(key, None)\nComment: ```\r\nserialized_dict = {\r\n    k: v for k, v in serialized_dict.items() if k in MODEL_CLASS.model_fields\r\n}\r\n```\r\n\r\nmaybe dictionary comprehension would be better here?",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/langchain_embedding_handler/langchain_embedding_handler.py",
    "pr_number": 10154,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1839115982,
    "comment_created_at": "2024-11-13T01:03:55Z"
  },
  {
    "code": "@@ -151,7 +151,7 @@ def store_pages_as_feather(self, dest_dir: Path, pages_files: dict):\n     def delete_file(self, name):\n         file_record = db.session.query(db.File).filter_by(company_id=ctx.company_id, name=name).first()\n         if file_record is None:\n-            return None\n+            raise Exception(f\"File '{name}' does not exists\")",
    "comment": "**correctness**: `delete_file` now raises a generic `Exception` when a file does not exist, which can cause unhandled 500 errors in API endpoints expecting a controlled error response.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/interfaces/file/file_controller.py, line 154, the code raises a generic Exception when a file does not exist. This can cause unhandled 500 errors in API endpoints. Change it to raise FileNotFoundError instead, so that higher layers can catch and return a proper 404 response. Replace:\n    raise Exception(f\"File '{name}' does not exists\")\nwith:\n    raise FileNotFoundError(f\"File '{name}' does not exist\")\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n            raise FileNotFoundError(f\"File '{name}' does not exist\")\n```\n</details>\n<!-- suggestion_end -->\n",
    "line_number": 154,
    "enriched": "File: mindsdb/interfaces/file/file_controller.py\nCode: @@ -151,7 +151,7 @@ def store_pages_as_feather(self, dest_dir: Path, pages_files: dict):\n     def delete_file(self, name):\n         file_record = db.session.query(db.File).filter_by(company_id=ctx.company_id, name=name).first()\n         if file_record is None:\n-            return None\n+            raise Exception(f\"File '{name}' does not exists\")\nComment: **correctness**: `delete_file` now raises a generic `Exception` when a file does not exist, which can cause unhandled 500 errors in API endpoints expecting a controlled error response.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/interfaces/file/file_controller.py, line 154, the code raises a generic Exception when a file does not exist. This can cause unhandled 500 errors in API endpoints. Change it to raise FileNotFoundError instead, so that higher layers can catch and return a proper 404 response. Replace:\n    raise Exception(f\"File '{name}' does not exists\")\nwith:\n    raise FileNotFoundError(f\"File '{name}' does not exist\")\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n            raise FileNotFoundError(f\"File '{name}' does not exist\")\n```\n</details>\n<!-- suggestion_end -->\n",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/interfaces/file/file_controller.py",
    "pr_number": 11485,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2305458539,
    "comment_created_at": "2025-08-27T22:38:19Z"
  },
  {
    "code": "@@ -134,61 +132,24 @@ def get_table_info(self, table_names: Optional[List[str]] = None) -> str:\n         If `sample_rows_in_table_info`, the specified number of sample rows will be\n         appended to each table description. This can increase performance as demonstrated in the paper.\n         \"\"\"\n-        try:",
    "comment": "@dusvyat, here I've switched using cache per table to:\r\n- make it simplier\r\n- get rid of using _database ",
    "line_number": 147,
    "enriched": "File: mindsdb/interfaces/skills/sql_agent.py\nCode: @@ -134,61 +132,24 @@ def get_table_info(self, table_names: Optional[List[str]] = None) -> str:\n         If `sample_rows_in_table_info`, the specified number of sample rows will be\n         appended to each table description. This can increase performance as demonstrated in the paper.\n         \"\"\"\n-        try:\nComment: @dusvyat, here I've switched using cache per table to:\r\n- make it simplier\r\n- get rid of using _database ",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/interfaces/skills/sql_agent.py",
    "pr_number": 9590,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1704122089,
    "comment_created_at": "2024-08-05T13:27:05Z"
  },
  {
    "code": "@@ -434,18 +434,17 @@ def do_clean_process_marks():\n             args=(config.cmd_args.verbose,)\n         )\n     }\n-\n-    for api_enum in api_arr:\n-        trunc_processes_struct[api_enum].need_to_run = True\n-\n-    if config['jobs']['disable'] is False:\n-        trunc_processes_struct[TrunkProcessEnum.JOBS].need_to_run = True\n-\n-    if config['tasks']['disable'] is False:\n-        trunc_processes_struct[TrunkProcessEnum.TASKS].need_to_run = True\n-\n     if config.cmd_args.ml_task_queue_consumer is True:\n         trunc_processes_struct[TrunkProcessEnum.ML_TASK_QUEUE].need_to_run = True\n+    else:",
    "comment": "@StpMax, please take a look. When I start ML_TASK_QUEUE worker it also tries to run http service. I think it is unexpected behavior",
    "line_number": 439,
    "enriched": "File: mindsdb/__main__.py\nCode: @@ -434,18 +434,17 @@ def do_clean_process_marks():\n             args=(config.cmd_args.verbose,)\n         )\n     }\n-\n-    for api_enum in api_arr:\n-        trunc_processes_struct[api_enum].need_to_run = True\n-\n-    if config['jobs']['disable'] is False:\n-        trunc_processes_struct[TrunkProcessEnum.JOBS].need_to_run = True\n-\n-    if config['tasks']['disable'] is False:\n-        trunc_processes_struct[TrunkProcessEnum.TASKS].need_to_run = True\n-\n     if config.cmd_args.ml_task_queue_consumer is True:\n         trunc_processes_struct[TrunkProcessEnum.ML_TASK_QUEUE].need_to_run = True\n+    else:\nComment: @StpMax, please take a look. When I start ML_TASK_QUEUE worker it also tries to run http service. I think it is unexpected behavior",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/__main__.py",
    "pr_number": 10326,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1897389735,
    "comment_created_at": "2024-12-25T14:35:59Z"
  },
  {
    "code": "@@ -136,21 +137,32 @@ def get_view_meta(self, query: ASTNode) -> ASTNode:\n         view_meta[\"query_ast\"] = parse_sql(view_meta[\"query\"])\n         return view_meta\n \n-    def query_view(self, query, session):\n+    def query_view(self, query: Select, session) -> pd.DataFrame:\n         view_meta = self.get_view_meta(query)\n \n         query_context_controller.set_context(\"view\", view_meta[\"id\"])\n-\n+        query_applied = False\n         try:\n-            sqlquery = SQLQuery(view_meta[\"query_ast\"], session=session)\n+            view_query = view_meta[\"query_ast\"]\n+            if isinstance(view_query, Select):",
    "comment": "@ea-rus Can the query for creating a view be something other than `Select`?",
    "line_number": 147,
    "enriched": "File: mindsdb/interfaces/database/projects.py\nCode: @@ -136,21 +137,32 @@ def get_view_meta(self, query: ASTNode) -> ASTNode:\n         view_meta[\"query_ast\"] = parse_sql(view_meta[\"query\"])\n         return view_meta\n \n-    def query_view(self, query, session):\n+    def query_view(self, query: Select, session) -> pd.DataFrame:\n         view_meta = self.get_view_meta(query)\n \n         query_context_controller.set_context(\"view\", view_meta[\"id\"])\n-\n+        query_applied = False\n         try:\n-            sqlquery = SQLQuery(view_meta[\"query_ast\"], session=session)\n+            view_query = view_meta[\"query_ast\"]\n+            if isinstance(view_query, Select):\nComment: @ea-rus Can the query for creating a view be something other than `Select`?",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/interfaces/database/projects.py",
    "pr_number": 11430,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2272765396,
    "comment_created_at": "2025-08-13T09:50:43Z"
  },
  {
    "code": "@@ -82,6 +82,54 @@ def get(self, handler_name):\n         return row\n \n \n+@ns_conf.route(\"/<handler_name>/readme\")\n+class HandlerReadme(Resource):\n+    @ns_conf.param(\"handler_name\", \"Handler name\")\n+    @api_endpoint_metrics(\"GET\", \"/handlers/handler/readme\")\n+    def get(self, handler_name):\n+        try:\n+            handler_meta = ca.integration_controller.get_handler_meta(handler_name)\n+        except Exception:\n+            return http_error(\n+                HTTPStatus.NOT_FOUND,\n+                \"Readme not found\",\n+                f\"Handler '{handler_name}' not found\",\n+            )\n+\n+        handler_folder = handler_meta.get(\"import\", {}).get(\"folder\")\n+        if handler_folder is None:\n+            return http_error(\n+                HTTPStatus.NOT_FOUND,\n+                \"Readme not found\",\n+                f\"Handler '{handler_name}' does not define a folder\",\n+            )\n+\n+        mindsdb_path = Path(importlib.util.find_spec(\"mindsdb\").origin).parent\n+        readme_path = mindsdb_path.joinpath(\"integrations/handlers\").joinpath(handler_folder).joinpath(\"README.md\")\n+        if readme_path.is_absolute() is False:\n+            readme_path = Path(os.getcwd()).joinpath(readme_path)\n+\n+        try:\n+            with open(readme_path, \"r\", encoding=\"utf-8\") as readme_file:\n+                readme_content = readme_file.read()\n+        except FileNotFoundError:\n+            return http_error(\n+                HTTPStatus.NOT_FOUND,\n+                \"Readme not found\",\n+                f\"README.md for handler '{handler_name}' not found\",\n+            )\n+        except Exception as exc:\n+            error_message = f\"Failed to read README for '{handler_name}': {exc}\"\n+            logger.warning(error_message)\n+            return http_error(\n+                HTTPStatus.INTERNAL_SERVER_ERROR,\n+                \"Readme error\",\n+                error_message,\n+            )\n+\n+        return {\"name\": handler_name, \"readme\": readme_content}",
    "comment": "**security**: `handler_name` is used directly to construct file paths for README.md without strict validation, enabling path traversal (e.g., `../../`) to read arbitrary files.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/api/http/namespaces/handlers.py, lines 89-130, the HandlerReadme.get method constructs file paths for README.md using handler_name/handler_folder without validating for path traversal. This allows attackers to supply values like '../../' to read arbitrary files. Update the code to strictly validate handler_folder (e.g., ensure it's a single directory name, no '..', '/', or '\\'), and after resolving the final path, check that it is within the allowed integrations/handlers directory. If validation fails, return an error. Apply these changes to fully mitigate path traversal risk.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n    def get(self, handler_name):\n        try:\n            handler_meta = ca.integration_controller.get_handler_meta(handler_name)\n        except Exception:\n            return http_error(\n                HTTPStatus.NOT_FOUND,\n                \"Readme not found\",\n                f\"Handler '{handler_name}' not found\",\n            )\n\n        handler_folder = handler_meta.get(\"import\", {}).get(\"folder\")\n        if handler_folder is None:\n            return http_error(\n                HTTPStatus.NOT_FOUND,\n                \"Readme not found\",\n                f\"Handler '{handler_name}' does not define a folder\",\n            )\n\n        # Prevent path traversal by ensuring handler_folder is a single directory name\n        if not handler_folder.isidentifier() and (\"..\" in handler_folder or \"/\" in handler_folder or \"\\\\\" in handler_folder):\n            return http_error(\n                HTTPStatus.BAD_REQUEST,\n                \"Invalid handler folder\",\n                f\"Handler folder '{handler_folder}' is invalid.\",\n            )\n\n        mindsdb_path = Path(importlib.util.find_spec(\"mindsdb\").origin).parent\n        base_handlers_path = mindsdb_path.joinpath(\"integrations/handlers\").resolve()\n        readme_path = base_handlers_path.joinpath(handler_folder, \"README.md\").resolve()\n        # Ensure the resolved path is within the allowed directory\n        if not str(readme_path).startswith(str(base_handlers_path)):\n            return http_error(\n                HTTPStatus.BAD_REQUEST,\n                \"Invalid handler folder\",\n                f\"Handler folder '{handler_folder}' is invalid.\",\n            )\n\n        try:\n            with open(readme_path, \"r\", encoding=\"utf-8\") as readme_file:\n                readme_content = readme_file.read()\n        except FileNotFoundError:\n            return http_error(\n                HTTPStatus.NOT_FOUND,\n                \"Readme not found\",\n                f\"README.md for handler '{handler_name}' not found\",\n            )\n        except Exception as exc:\n            error_message = f\"Failed to read README for '{handler_name}': {exc}\"\n            logger.warning(error_message)\n            return http_error(\n                HTTPStatus.INTERNAL_SERVER_ERROR,\n                \"Readme error\",\n                error_message,\n            )\n\n        return {\"name\": handler_name, \"readme\": readme_content}\n\n```\n</details>\n<!-- suggestion_end -->\n",
    "line_number": 130,
    "enriched": "File: mindsdb/api/http/namespaces/handlers.py\nCode: @@ -82,6 +82,54 @@ def get(self, handler_name):\n         return row\n \n \n+@ns_conf.route(\"/<handler_name>/readme\")\n+class HandlerReadme(Resource):\n+    @ns_conf.param(\"handler_name\", \"Handler name\")\n+    @api_endpoint_metrics(\"GET\", \"/handlers/handler/readme\")\n+    def get(self, handler_name):\n+        try:\n+            handler_meta = ca.integration_controller.get_handler_meta(handler_name)\n+        except Exception:\n+            return http_error(\n+                HTTPStatus.NOT_FOUND,\n+                \"Readme not found\",\n+                f\"Handler '{handler_name}' not found\",\n+            )\n+\n+        handler_folder = handler_meta.get(\"import\", {}).get(\"folder\")\n+        if handler_folder is None:\n+            return http_error(\n+                HTTPStatus.NOT_FOUND,\n+                \"Readme not found\",\n+                f\"Handler '{handler_name}' does not define a folder\",\n+            )\n+\n+        mindsdb_path = Path(importlib.util.find_spec(\"mindsdb\").origin).parent\n+        readme_path = mindsdb_path.joinpath(\"integrations/handlers\").joinpath(handler_folder).joinpath(\"README.md\")\n+        if readme_path.is_absolute() is False:\n+            readme_path = Path(os.getcwd()).joinpath(readme_path)\n+\n+        try:\n+            with open(readme_path, \"r\", encoding=\"utf-8\") as readme_file:\n+                readme_content = readme_file.read()\n+        except FileNotFoundError:\n+            return http_error(\n+                HTTPStatus.NOT_FOUND,\n+                \"Readme not found\",\n+                f\"README.md for handler '{handler_name}' not found\",\n+            )\n+        except Exception as exc:\n+            error_message = f\"Failed to read README for '{handler_name}': {exc}\"\n+            logger.warning(error_message)\n+            return http_error(\n+                HTTPStatus.INTERNAL_SERVER_ERROR,\n+                \"Readme error\",\n+                error_message,\n+            )\n+\n+        return {\"name\": handler_name, \"readme\": readme_content}\nComment: **security**: `handler_name` is used directly to construct file paths for README.md without strict validation, enabling path traversal (e.g., `../../`) to read arbitrary files.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/api/http/namespaces/handlers.py, lines 89-130, the HandlerReadme.get method constructs file paths for README.md using handler_name/handler_folder without validating for path traversal. This allows attackers to supply values like '../../' to read arbitrary files. Update the code to strictly validate handler_folder (e.g., ensure it's a single directory name, no '..', '/', or '\\'), and after resolving the final path, check that it is within the allowed integrations/handlers directory. If validation fails, return an error. Apply these changes to fully mitigate path traversal risk.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n    def get(self, handler_name):\n        try:\n            handler_meta = ca.integration_controller.get_handler_meta(handler_name)\n        except Exception:\n            return http_error(\n                HTTPStatus.NOT_FOUND,\n                \"Readme not found\",\n                f\"Handler '{handler_name}' not found\",\n            )\n\n        handler_folder = handler_meta.get(\"import\", {}).get(\"folder\")\n        if handler_folder is None:\n            return http_error(\n                HTTPStatus.NOT_FOUND,\n                \"Readme not found\",\n                f\"Handler '{handler_name}' does not define a folder\",\n            )\n\n        # Prevent path traversal by ensuring handler_folder is a single directory name\n        if not handler_folder.isidentifier() and (\"..\" in handler_folder or \"/\" in handler_folder or \"\\\\\" in handler_folder):\n            return http_error(\n                HTTPStatus.BAD_REQUEST,\n                \"Invalid handler folder\",\n                f\"Handler folder '{handler_folder}' is invalid.\",\n            )\n\n        mindsdb_path = Path(importlib.util.find_spec(\"mindsdb\").origin).parent\n        base_handlers_path = mindsdb_path.joinpath(\"integrations/handlers\").resolve()\n        readme_path = base_handlers_path.joinpath(handler_folder, \"README.md\").resolve()\n        # Ensure the resolved path is within the allowed directory\n        if not str(readme_path).startswith(str(base_handlers_path)):\n            return http_error(\n                HTTPStatus.BAD_REQUEST,\n                \"Invalid handler folder\",\n                f\"Handler folder '{handler_folder}' is invalid.\",\n            )\n\n        try:\n            with open(readme_path, \"r\", encoding=\"utf-8\") as readme_file:\n                readme_content = readme_file.read()\n        except FileNotFoundError:\n            return http_error(\n                HTTPStatus.NOT_FOUND,\n                \"Readme not found\",\n                f\"README.md for handler '{handler_name}' not found\",\n            )\n        except Exception as exc:\n            error_message = f\"Failed to read README for '{handler_name}': {exc}\"\n            logger.warning(error_message)\n            return http_error(\n                HTTPStatus.INTERNAL_SERVER_ERROR,\n                \"Readme error\",\n                error_message,\n            )\n\n        return {\"name\": handler_name, \"readme\": readme_content}\n\n```\n</details>\n<!-- suggestion_end -->\n",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/api/http/namespaces/handlers.py",
    "pr_number": 11895,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2532493649,
    "comment_created_at": "2025-11-17T02:47:02Z"
  },
  {
    "code": "@@ -1077,7 +1078,8 @@ def _create_integration(self, name: str, engine: str, connection_args: dict):\n             )\n             status = handler.check_connection()\n             if status.copy_storage:\n-                storage = handler.handler_storage.export_files()\n+                file_storage = handler.handler_storage.export_files()",
    "comment": "the similar export/import exists in [create database via form](https://github.com/mindsdb/mindsdb/blob/7dfd7a7823325798da6955b4787b942ddf2828d4/mindsdb/api/http/namespaces/config.py#L165) \r\nthe storage is passed to frontend (as string) and returned. \r\nif it has to work via form too, maybe better to make one string variable (with file and json storage), and then we won't modify frontend.",
    "line_number": 1081,
    "enriched": "File: mindsdb/api/executor/command_executor.py\nCode: @@ -1077,7 +1078,8 @@ def _create_integration(self, name: str, engine: str, connection_args: dict):\n             )\n             status = handler.check_connection()\n             if status.copy_storage:\n-                storage = handler.handler_storage.export_files()\n+                file_storage = handler.handler_storage.export_files()\nComment: the similar export/import exists in [create database via form](https://github.com/mindsdb/mindsdb/blob/7dfd7a7823325798da6955b4787b942ddf2828d4/mindsdb/api/http/namespaces/config.py#L165) \r\nthe storage is passed to frontend (as string) and returned. \r\nif it has to work via form too, maybe better to make one string variable (with file and json storage), and then we won't modify frontend.",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/api/executor/command_executor.py",
    "pr_number": 10470,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1963022115,
    "comment_created_at": "2025-02-20T07:53:12Z"
  },
  {
    "code": "@@ -0,0 +1,54 @@\n+import pandas as pd\n+\n+\n+def infer_column_type(column, type=None):\n+    if not pd.api.types.is_object_dtype(column):\n+        return column\n+\n+    if type is None or type == 'float':\n+        # Try to convert the entire column to integers\n+        try:\n+            return column.astype(float)\n+        except (ValueError, TypeError):\n+            pass\n+\n+    if type is None or type == 'datetime':\n+        # If already a datetime type, leave it as such\n+        if pd.api.types.is_datetime64_any_dtype(column):\n+            return column\n+\n+        # Check if the column can be converted to datetime\n+        try:\n+            return pd.to_datetime(column)\n+        except (pd.errors.ParserError, ValueError, TypeError):\n+            # If that fails, try to convert the entire column to floats\n+            pass\n+\n+    if type is None or type == 'int':\n+        try:\n+            return column.astype(int)\n+        except (ValueError, TypeError):\n+            # If both fail, leave the column as a string\n+            return column.astype(str)\n+\n+    return column\n+\n+\n+def infer_and_convert_column(df, column, sample_size=100, type=None):\n+    # Sample the column to analyze\n+    sample_data = df[column].sample(min(sample_size, len(df)))\n+\n+    # Infer the type from the sample\n+    inferred_column = infer_column_type(sample_data, type=type)\n+\n+    # Check if the inferred type is the same as the original type\n+    if inferred_column.dtype != sample_data.dtype:\n+        # If not, apply the inferred type to the entire column\n+        df[column] = infer_column_type(df[column], type=type)\n+\n+    return df\n+\n+\n+def infer_and_convert_types(df):",
    "comment": "This method is not being used anymore, perhaps drop it?",
    "line_number": 52,
    "enriched": "File: mindsdb/integrations/utilities/infer_types.py\nCode: @@ -0,0 +1,54 @@\n+import pandas as pd\n+\n+\n+def infer_column_type(column, type=None):\n+    if not pd.api.types.is_object_dtype(column):\n+        return column\n+\n+    if type is None or type == 'float':\n+        # Try to convert the entire column to integers\n+        try:\n+            return column.astype(float)\n+        except (ValueError, TypeError):\n+            pass\n+\n+    if type is None or type == 'datetime':\n+        # If already a datetime type, leave it as such\n+        if pd.api.types.is_datetime64_any_dtype(column):\n+            return column\n+\n+        # Check if the column can be converted to datetime\n+        try:\n+            return pd.to_datetime(column)\n+        except (pd.errors.ParserError, ValueError, TypeError):\n+            # If that fails, try to convert the entire column to floats\n+            pass\n+\n+    if type is None or type == 'int':\n+        try:\n+            return column.astype(int)\n+        except (ValueError, TypeError):\n+            # If both fail, leave the column as a string\n+            return column.astype(str)\n+\n+    return column\n+\n+\n+def infer_and_convert_column(df, column, sample_size=100, type=None):\n+    # Sample the column to analyze\n+    sample_data = df[column].sample(min(sample_size, len(df)))\n+\n+    # Infer the type from the sample\n+    inferred_column = infer_column_type(sample_data, type=type)\n+\n+    # Check if the inferred type is the same as the original type\n+    if inferred_column.dtype != sample_data.dtype:\n+        # If not, apply the inferred type to the entire column\n+        df[column] = infer_column_type(df[column], type=type)\n+\n+    return df\n+\n+\n+def infer_and_convert_types(df):\nComment: This method is not being used anymore, perhaps drop it?",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/utilities/infer_types.py",
    "pr_number": 7419,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1337790076,
    "comment_created_at": "2023-09-26T21:11:27Z"
  },
  {
    "code": "@@ -314,7 +314,7 @@\n         {\n           \"group\": \"Semantic Search\",\n           \"pages\": [\n-            \"sql/tutorials/rag\"\n+            \"integrations/ai-engines/rag\"",
    "comment": "This page should be listed under AI Engines (line 982).\r\n\r\nLet's remove it from this line because it is duplicated here. We can create a tutorial and list it here in Use Cases.",
    "line_number": 317,
    "enriched": "File: docs/mint.json\nCode: @@ -314,7 +314,7 @@\n         {\n           \"group\": \"Semantic Search\",\n           \"pages\": [\n-            \"sql/tutorials/rag\"\n+            \"integrations/ai-engines/rag\"\nComment: This page should be listed under AI Engines (line 982).\r\n\r\nLet's remove it from this line because it is duplicated here. We can create a tutorial and list it here in Use Cases.",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "docs/mint.json",
    "pr_number": 8951,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1530321804,
    "comment_created_at": "2024-03-19T13:00:40Z"
  },
  {
    "code": "@@ -1,26 +1,38 @@\n ---\n-title: Setup as a Docker Extension\n-sidebarTitle: Docker Extension\n+title: Docker Desktop Extension for MindsDB\n+sidebarTitle: Docker Desktop\n ---\n \n-This is work in progress but [Ajeet Singh Raina](https://github.com/ajeetraina) has built a great docker extension, should look like this\n+MindsDB provides an extension for Docker Desktop that facilitates running MindsDB on Docker Desktop.\n \n-<p align=\"center\">\n-  <img src=\"https://user-images.githubusercontent.com/34368930/230012152-c60bc8a8-3c00-48c9-961c-6d10421e7e26.png\"/>\n-</p>\n+<Tip>\n+Visit the [GitHub repository for MinsdDB Docker Desktop Extension](https://github.com/mindsdb/mindsdb-docker-extension) to learn more.\n+</Tip>\n+\n+## Prerequisites\n+\n+Before proceeding, ensure you have installed Docker Desktop, following the [official Docker Desktop documentation](https://www.docker.com/products/docker-desktop/).\n \n+## Setup\n \n+This setup of MindsDB uses one of the available Docker images, as [listed here](/setup/self-hosted/docker).",
    "comment": "The extension actually always uses `mindsdb/mindsdb:latest`. Do you think this information needs to be pointed out to them, Martyna? Since this is meant to be a way for a user (even a non-technical user) to easily start using MindsDB, maybe it is not necessary?",
    "line_number": 18,
    "enriched": "File: docs/setup/self-hosted/docker-desktop.mdx\nCode: @@ -1,26 +1,38 @@\n ---\n-title: Setup as a Docker Extension\n-sidebarTitle: Docker Extension\n+title: Docker Desktop Extension for MindsDB\n+sidebarTitle: Docker Desktop\n ---\n \n-This is work in progress but [Ajeet Singh Raina](https://github.com/ajeetraina) has built a great docker extension, should look like this\n+MindsDB provides an extension for Docker Desktop that facilitates running MindsDB on Docker Desktop.\n \n-<p align=\"center\">\n-  <img src=\"https://user-images.githubusercontent.com/34368930/230012152-c60bc8a8-3c00-48c9-961c-6d10421e7e26.png\"/>\n-</p>\n+<Tip>\n+Visit the [GitHub repository for MinsdDB Docker Desktop Extension](https://github.com/mindsdb/mindsdb-docker-extension) to learn more.\n+</Tip>\n+\n+## Prerequisites\n+\n+Before proceeding, ensure you have installed Docker Desktop, following the [official Docker Desktop documentation](https://www.docker.com/products/docker-desktop/).\n \n+## Setup\n \n+This setup of MindsDB uses one of the available Docker images, as [listed here](/setup/self-hosted/docker).\nComment: The extension actually always uses `mindsdb/mindsdb:latest`. Do you think this information needs to be pointed out to them, Martyna? Since this is meant to be a way for a user (even a non-technical user) to easily start using MindsDB, maybe it is not necessary?",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "docs/setup/self-hosted/docker-desktop.mdx",
    "pr_number": 8938,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1529742106,
    "comment_created_at": "2024-03-19T05:33:41Z"
  },
  {
    "code": "@@ -1,9 +1,28 @@\n-__title__ = 'MindsDB Email handler'\n-__package_name__ = 'mindsdb_email_handler'\n-__version__ = '0.0.1'\n+\"\"\"\n+Email handler package metadata and handler UI hints.\n+Note: Keep this file import-safe and free of heavy imports.\n+\"\"\"\n+\n+from enum import Enum\n+\n+__title__ = \"MindsDB Email handler\"\n+__package_name__ = \"mindsdb_email_handler\"\n+__version__ = \"0.0.3\"\n __description__ = \"MindsDB handler for email\"\n-__author__ = 'MindsDB Inc'\n-__github__ = 'https://github.com/mindsdb/mindsdb'\n-__pypi__ = 'https://pypi.org/project/mindsdb/'\n-__license__ = 'MIT'\n-__copyright__ = 'Copyright 2022- mindsdb'\n+__author__ = \"MindsDB Inc\"\n+__github__ = \"https://github.com/mindsdb/mindsdb\"\n+__pypi__ = \"https://pypi.org/project/mindsdb/\"\n+__license__ = \"MIT\"\n+__icon_path__ = \"icon.png\"  # fixed from icon.svg to match repository asset\n+\n+# Robust, import-safe HANDLER_TYPE assignment:\n+# - Prefer the enum value from libs.const\n+# - If unavailable, export an enum member with the same semantics (not a bare string)\n+try:\n+    from mindsdb.integrations.libs.const import HANDLER_TYPE as _HANDLER_TYPE_ENUM  # type: ignore\n+\n+    HANDLER_TYPE = getattr(_HANDLER_TYPE_ENUM, \"DATA\")\n+except Exception:\n+    # Fallback to a compatible enum member, avoiding plain strings to keep type checks stable.\n+    _FallbackHT = Enum(\"HANDLER_TYPE\", {\"DATA\": \"data\"})\n+    HANDLER_TYPE = _FallbackHT.DATA",
    "comment": "**correctness**: No significant functional bugs or runtime correctness issues are present; all changes are metadata assignments and robust enum fallback logic.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nNo action required. The file mindsdb/integrations/handlers/email_handler/__about__.py (lines 1-28) contains only metadata and robust enum fallback logic, with no functional or runtime correctness issues to address.\n```\n</details>\n<!-- ai_prompt_end -->\n\n",
    "line_number": 28,
    "enriched": "File: mindsdb/integrations/handlers/email_handler/__about__.py\nCode: @@ -1,9 +1,28 @@\n-__title__ = 'MindsDB Email handler'\n-__package_name__ = 'mindsdb_email_handler'\n-__version__ = '0.0.1'\n+\"\"\"\n+Email handler package metadata and handler UI hints.\n+Note: Keep this file import-safe and free of heavy imports.\n+\"\"\"\n+\n+from enum import Enum\n+\n+__title__ = \"MindsDB Email handler\"\n+__package_name__ = \"mindsdb_email_handler\"\n+__version__ = \"0.0.3\"\n __description__ = \"MindsDB handler for email\"\n-__author__ = 'MindsDB Inc'\n-__github__ = 'https://github.com/mindsdb/mindsdb'\n-__pypi__ = 'https://pypi.org/project/mindsdb/'\n-__license__ = 'MIT'\n-__copyright__ = 'Copyright 2022- mindsdb'\n+__author__ = \"MindsDB Inc\"\n+__github__ = \"https://github.com/mindsdb/mindsdb\"\n+__pypi__ = \"https://pypi.org/project/mindsdb/\"\n+__license__ = \"MIT\"\n+__icon_path__ = \"icon.png\"  # fixed from icon.svg to match repository asset\n+\n+# Robust, import-safe HANDLER_TYPE assignment:\n+# - Prefer the enum value from libs.const\n+# - If unavailable, export an enum member with the same semantics (not a bare string)\n+try:\n+    from mindsdb.integrations.libs.const import HANDLER_TYPE as _HANDLER_TYPE_ENUM  # type: ignore\n+\n+    HANDLER_TYPE = getattr(_HANDLER_TYPE_ENUM, \"DATA\")\n+except Exception:\n+    # Fallback to a compatible enum member, avoiding plain strings to keep type checks stable.\n+    _FallbackHT = Enum(\"HANDLER_TYPE\", {\"DATA\": \"data\"})\n+    HANDLER_TYPE = _FallbackHT.DATA\nComment: **correctness**: No significant functional bugs or runtime correctness issues are present; all changes are metadata assignments and robust enum fallback logic.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nNo action required. The file mindsdb/integrations/handlers/email_handler/__about__.py (lines 1-28) contains only metadata and robust enum fallback logic, with no functional or runtime correctness issues to address.\n```\n</details>\n<!-- ai_prompt_end -->\n\n",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/email_handler/__about__.py",
    "pr_number": 11718,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2421191178,
    "comment_created_at": "2025-10-10T16:41:51Z"
  },
  {
    "code": "@@ -0,0 +1,130 @@\n+from mindsdb.integrations.handlers.intercom_handler.intercom_tables import Articles\n+from mindsdb.integrations.libs.api_handler import APIHandler\n+from mindsdb.integrations.libs.response import HandlerStatusResponse as StatusResponse\n+from mindsdb_sql import parse_sql\n+import requests\n+import pandas as pd\n+from collections import OrderedDict\n+from mindsdb.integrations.libs.const import HANDLER_CONNECTION_ARG_TYPE as ARG_TYPE\n+import json\n+from mindsdb.utilities import log\n+\n+\n+class IntercomHandler(APIHandler):\n+    def __init__(self, name: str, **kwargs) -> None:\n+        \"\"\"initializer method\n+\n+        Args:\n+            name (str): handler name\n+        \"\"\"\n+        super().__init__(name)\n+\n+        self.connection = None\n+        self.is_connected = False\n+        self._baseUrl = 'https://api.intercom.io'\n+        args = kwargs.get('connection_data', {})\n+        if 'access_token' in args:\n+            access_token = args['access_token']\n+        self._headers = {\n+            \"Accept\": \"application/json\",\n+            \"Authorization\": f\"Bearer {access_token}\"\n+        }\n+\n+        _tables = [\n+            Articles,\n+        ]\n+\n+        for Table in _tables:",
    "comment": "Since we have only one Table let's just register without itteration",
    "line_number": 37,
    "enriched": "File: mindsdb/integrations/handlers/intercom_handler/intercom_handler.py\nCode: @@ -0,0 +1,130 @@\n+from mindsdb.integrations.handlers.intercom_handler.intercom_tables import Articles\n+from mindsdb.integrations.libs.api_handler import APIHandler\n+from mindsdb.integrations.libs.response import HandlerStatusResponse as StatusResponse\n+from mindsdb_sql import parse_sql\n+import requests\n+import pandas as pd\n+from collections import OrderedDict\n+from mindsdb.integrations.libs.const import HANDLER_CONNECTION_ARG_TYPE as ARG_TYPE\n+import json\n+from mindsdb.utilities import log\n+\n+\n+class IntercomHandler(APIHandler):\n+    def __init__(self, name: str, **kwargs) -> None:\n+        \"\"\"initializer method\n+\n+        Args:\n+            name (str): handler name\n+        \"\"\"\n+        super().__init__(name)\n+\n+        self.connection = None\n+        self.is_connected = False\n+        self._baseUrl = 'https://api.intercom.io'\n+        args = kwargs.get('connection_data', {})\n+        if 'access_token' in args:\n+            access_token = args['access_token']\n+        self._headers = {\n+            \"Accept\": \"application/json\",\n+            \"Authorization\": f\"Bearer {access_token}\"\n+        }\n+\n+        _tables = [\n+            Articles,\n+        ]\n+\n+        for Table in _tables:\nComment: Since we have only one Table let's just register without itteration",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/intercom_handler/intercom_handler.py",
    "pr_number": 8274,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1383354332,
    "comment_created_at": "2023-11-06T13:44:57Z"
  },
  {
    "code": "@@ -33,22 +33,65 @@ def clear_filename(filename: str) -> str:\n     return filename\n \n \n-def validate_urls(urls, allowed_urls):\n+def _split_url(url: str) -> tuple[str, str]:",
    "comment": "I would name it 'get_url_origin'\r\nAlso it can return f'{scheme}://{netloc}', the behavior will be the same",
    "line_number": 36,
    "enriched": "File: mindsdb/utilities/security.py\nCode: @@ -33,22 +33,65 @@ def clear_filename(filename: str) -> str:\n     return filename\n \n \n-def validate_urls(urls, allowed_urls):\n+def _split_url(url: str) -> tuple[str, str]:\nComment: I would name it 'get_url_origin'\r\nAlso it can return f'{scheme}://{netloc}', the behavior will be the same",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/utilities/security.py",
    "pr_number": 11179,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2168953566,
    "comment_created_at": "2025-06-26T12:30:24Z"
  },
  {
    "code": "@@ -0,0 +1,208 @@\n+\"\"\"\n+Constants for Salesforce handler.\n+\"\"\"\n+\n+\n+def get_soql_instructions(integration_name):\n+    return f\"\"\"This handler executes SOQL (Salesforce Object Query Language), NOT SQL! Follow these rules strictly:\n+\n+**BASIC STRUCTURE:**\n+- NO \"SELECT *\" - must explicitly list all fields\n+  SQL: SELECT * FROM Account;\n+  SOQL: SELECT Id, Name, Industry FROM Account\n+- NO table aliases - use full table names only\n+  SQL: SELECT a.Name FROM Account a;\n+  SOQL: SELECT Name FROM Account\n+- NO column aliases - field names cannot be aliased\n+  SQL: SELECT Name AS CompanyName FROM Account;\n+  SOQL: SELECT Name FROM Account\n+- NO DISTINCT keyword - not supported in SOQL\n+  SQL: SELECT DISTINCT Industry FROM Account;",
    "comment": "**correctness**: `get_soql_instructions` returns a string with `{integration_name}` placeholder, but if `integration_name` contains special characters or is not a valid identifier, the returned SOQL wrapper may be syntactically invalid, causing runtime query failures.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/integrations/handlers/salesforce_handler/constants.py, lines 6-20, the function get_soql_instructions directly interpolates integration_name into a SOQL wrapper string. If integration_name contains special characters or quotes, this can produce invalid SOQL and cause runtime query failures. Please sanitize integration_name by removing or escaping problematic characters (such as backticks, single and double quotes) before using it in the formatted string. Update the function to use a sanitized variable in the wrapper.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\ndef get_soql_instructions(integration_name):\n    safe_name = str(integration_name).replace('`', '').replace('\"', '').replace(\"'\", '')\n    return f\"\"\"This handler executes SOQL (Salesforce Object Query Language), NOT SQL! Follow these rules strictly:\n...\n    SELECT * \n      FROM {safe_name}(\n        /* your generated SOQL goes here, without a trailing semicolon */\n      )\n...\n\"\"\"\n```\n</details>\n<!-- suggestion_end -->\n",
    "line_number": 20,
    "enriched": "File: mindsdb/integrations/handlers/salesforce_handler/constants.py\nCode: @@ -0,0 +1,208 @@\n+\"\"\"\n+Constants for Salesforce handler.\n+\"\"\"\n+\n+\n+def get_soql_instructions(integration_name):\n+    return f\"\"\"This handler executes SOQL (Salesforce Object Query Language), NOT SQL! Follow these rules strictly:\n+\n+**BASIC STRUCTURE:**\n+- NO \"SELECT *\" - must explicitly list all fields\n+  SQL: SELECT * FROM Account;\n+  SOQL: SELECT Id, Name, Industry FROM Account\n+- NO table aliases - use full table names only\n+  SQL: SELECT a.Name FROM Account a;\n+  SOQL: SELECT Name FROM Account\n+- NO column aliases - field names cannot be aliased\n+  SQL: SELECT Name AS CompanyName FROM Account;\n+  SOQL: SELECT Name FROM Account\n+- NO DISTINCT keyword - not supported in SOQL\n+  SQL: SELECT DISTINCT Industry FROM Account;\nComment: **correctness**: `get_soql_instructions` returns a string with `{integration_name}` placeholder, but if `integration_name` contains special characters or is not a valid identifier, the returned SOQL wrapper may be syntactically invalid, causing runtime query failures.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/integrations/handlers/salesforce_handler/constants.py, lines 6-20, the function get_soql_instructions directly interpolates integration_name into a SOQL wrapper string. If integration_name contains special characters or quotes, this can produce invalid SOQL and cause runtime query failures. Please sanitize integration_name by removing or escaping problematic characters (such as backticks, single and double quotes) before using it in the formatted string. Update the function to use a sanitized variable in the wrapper.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\ndef get_soql_instructions(integration_name):\n    safe_name = str(integration_name).replace('`', '').replace('\"', '').replace(\"'\", '')\n    return f\"\"\"This handler executes SOQL (Salesforce Object Query Language), NOT SQL! Follow these rules strictly:\n...\n    SELECT * \n      FROM {safe_name}(\n        /* your generated SOQL goes here, without a trailing semicolon */\n      )\n...\n\"\"\"\n```\n</details>\n<!-- suggestion_end -->\n",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/salesforce_handler/constants.py",
    "pr_number": 11246,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2194993709,
    "comment_created_at": "2025-07-09T13:08:10Z"
  },
  {
    "code": "@@ -0,0 +1,49 @@\n+import os\n+import tempfile\n+import json\n+import time\n+\n+\n+_api_status_file = None\n+\n+\n+def get_api_status_file():\n+    global _api_status_file\n+    if _api_status_file is None:\n+        # Use a temporary file that can be shared across processes.\n+        temp_dir = tempfile.gettempdir()\n+        _api_status_file = os.path.join(temp_dir, \"mindsdb_api_status.json\")",
    "comment": "mindsdb server creates `mindsdb` folder in temp directory. this file can be also located there ",
    "line_number": 15,
    "enriched": "File: mindsdb/utilities/api_status.py\nCode: @@ -0,0 +1,49 @@\n+import os\n+import tempfile\n+import json\n+import time\n+\n+\n+_api_status_file = None\n+\n+\n+def get_api_status_file():\n+    global _api_status_file\n+    if _api_status_file is None:\n+        # Use a temporary file that can be shared across processes.\n+        temp_dir = tempfile.gettempdir()\n+        _api_status_file = os.path.join(temp_dir, \"mindsdb_api_status.json\")\nComment: mindsdb server creates `mindsdb` folder in temp directory. this file can be also located there ",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/utilities/api_status.py",
    "pr_number": 11492,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2310148563,
    "comment_created_at": "2025-08-29T13:20:05Z"
  },
  {
    "code": "@@ -1,9 +1,29 @@\n-__title__ = 'MindsDB Email handler'\n-__package_name__ = 'mindsdb_email_handler'\n-__version__ = '0.0.1'\n+\"\"\"\n+Email handler package metadata and handler UI hints.\n+Note: Keep this file import-safe and free of heavy imports.\n+\"\"\"\n+\n+from enum import Enum\n+\n+__title__ = \"MindsDB Email handler\"\n+__package_name__ = \"mindsdb_email_handler\"\n+__version__ = \"0.0.5\"\n __description__ = \"MindsDB handler for email\"\n-__author__ = 'MindsDB Inc'\n-__github__ = 'https://github.com/mindsdb/mindsdb'\n-__pypi__ = 'https://pypi.org/project/mindsdb/'\n-__license__ = 'MIT'\n-__copyright__ = 'Copyright 2022- mindsdb'\n+__author__ = \"MindsDB Inc\"\n+__github__ = \"https://github.com/mindsdb/mindsdb\"\n+__pypi__ = \"https://pypi.org/project/mindsdb/\"\n+__license__ = \"MIT\"\n+__icon_path__ = \"icon.png\"\n+# Restored for compatibility with tools that expect this metadata\n+__copyright__ = \"Copyright 2022- mindsdb\"\n+\n+# Robust, import-safe HANDLER_TYPE assignment:\n+# Prefer the enum-like source; if unavailable, fall back to a compatible enum member, not a bare string.\n+try:\n+    from mindsdb.integrations.libs.const import HANDLER_TYPE as _HANDLER_TYPE_ENUM  # type: ignore\n+\n+    HANDLER_TYPE = getattr(_HANDLER_TYPE_ENUM, \"DATA\")\n+except Exception:\n+    # Fallback to a compatible enum member to avoid breaking code that expects Enum semantics.\n+    _FallbackHT = Enum(\"HANDLER_TYPE\", {\"DATA\": \"data\"})\n+    HANDLER_TYPE = _FallbackHT.DATA",
    "comment": "**performance**: No significant performance issues or major maintainability problems found; file contains only metadata and import-safe handler type logic.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nNo action required. The file mindsdb/integrations/handlers/email_handler/__about__.py (lines 1-29) contains only metadata and import-safe handler type logic, with no significant performance or maintainability issues.\n```\n</details>\n<!-- ai_prompt_end -->\n\n",
    "line_number": 29,
    "enriched": "File: mindsdb/integrations/handlers/email_handler/__about__.py\nCode: @@ -1,9 +1,29 @@\n-__title__ = 'MindsDB Email handler'\n-__package_name__ = 'mindsdb_email_handler'\n-__version__ = '0.0.1'\n+\"\"\"\n+Email handler package metadata and handler UI hints.\n+Note: Keep this file import-safe and free of heavy imports.\n+\"\"\"\n+\n+from enum import Enum\n+\n+__title__ = \"MindsDB Email handler\"\n+__package_name__ = \"mindsdb_email_handler\"\n+__version__ = \"0.0.5\"\n __description__ = \"MindsDB handler for email\"\n-__author__ = 'MindsDB Inc'\n-__github__ = 'https://github.com/mindsdb/mindsdb'\n-__pypi__ = 'https://pypi.org/project/mindsdb/'\n-__license__ = 'MIT'\n-__copyright__ = 'Copyright 2022- mindsdb'\n+__author__ = \"MindsDB Inc\"\n+__github__ = \"https://github.com/mindsdb/mindsdb\"\n+__pypi__ = \"https://pypi.org/project/mindsdb/\"\n+__license__ = \"MIT\"\n+__icon_path__ = \"icon.png\"\n+# Restored for compatibility with tools that expect this metadata\n+__copyright__ = \"Copyright 2022- mindsdb\"\n+\n+# Robust, import-safe HANDLER_TYPE assignment:\n+# Prefer the enum-like source; if unavailable, fall back to a compatible enum member, not a bare string.\n+try:\n+    from mindsdb.integrations.libs.const import HANDLER_TYPE as _HANDLER_TYPE_ENUM  # type: ignore\n+\n+    HANDLER_TYPE = getattr(_HANDLER_TYPE_ENUM, \"DATA\")\n+except Exception:\n+    # Fallback to a compatible enum member to avoid breaking code that expects Enum semantics.\n+    _FallbackHT = Enum(\"HANDLER_TYPE\", {\"DATA\": \"data\"})\n+    HANDLER_TYPE = _FallbackHT.DATA\nComment: **performance**: No significant performance issues or major maintainability problems found; file contains only metadata and import-safe handler type logic.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nNo action required. The file mindsdb/integrations/handlers/email_handler/__about__.py (lines 1-29) contains only metadata and import-safe handler type logic, with no significant performance or maintainability issues.\n```\n</details>\n<!-- ai_prompt_end -->\n\n",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/email_handler/__about__.py",
    "pr_number": 11755,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2436532317,
    "comment_created_at": "2025-10-16T15:47:48Z"
  },
  {
    "code": "@@ -72,12 +72,21 @@\n \n \n def dataframe_checksum(df: pd.DataFrame):\n-    original_columns = df.columns\n-    df.columns = list(range(len(df.columns)))\n-    result = hashlib.sha256(\n-        str(df.values).encode()\n-    ).hexdigest()\n-    df.columns = original_columns\n+    \"\"\"Compute efficient checksum for DataFrame without column manipulation.\"\"\"\n+    # Create a more efficient hash using pandas built-in methods\n+    try:\n+        # Use pandas util.hash_pandas_object for better performance\n+        from pandas.util import hash_pandas_object\n+        hash_values = hash_pandas_object(df, index=True).values\n+        result = hashlib.sha256(hash_values.tobytes()).hexdigest()\n+    except ImportError:\n+        # Fallback to original method if pandas util not available\n+        original_columns = df.columns\n+        df.columns = list(range(len(df.columns)))\n+        result = hashlib.sha256(\n+            str(df.values).encode()\n+        ).hexdigest()\n+        df.columns = original_columns",
    "comment": "**correctness**: `dataframe_checksum` mutates the input DataFrame's columns in the fallback path, which can cause data corruption or unexpected behavior for callers relying on column order or names.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/utilities/cache.py, lines 84-89, the fallback path in `dataframe_checksum` mutates the input DataFrame's columns, which can corrupt the DataFrame for callers. Refactor this block to ensure the original columns are always restored, even if an exception occurs. Use a try/finally block and copy the columns to avoid side effects.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n    original_columns = df.columns.copy()\n    try:\n        df.columns = list(range(len(df.columns)))\n        result = hashlib.sha256(\n            str(df.values).encode()\n        ).hexdigest()\n    finally:\n        df.columns = original_columns\n```\n</details>\n<!-- suggestion_end -->\n",
    "line_number": 89,
    "enriched": "File: mindsdb/utilities/cache.py\nCode: @@ -72,12 +72,21 @@\n \n \n def dataframe_checksum(df: pd.DataFrame):\n-    original_columns = df.columns\n-    df.columns = list(range(len(df.columns)))\n-    result = hashlib.sha256(\n-        str(df.values).encode()\n-    ).hexdigest()\n-    df.columns = original_columns\n+    \"\"\"Compute efficient checksum for DataFrame without column manipulation.\"\"\"\n+    # Create a more efficient hash using pandas built-in methods\n+    try:\n+        # Use pandas util.hash_pandas_object for better performance\n+        from pandas.util import hash_pandas_object\n+        hash_values = hash_pandas_object(df, index=True).values\n+        result = hashlib.sha256(hash_values.tobytes()).hexdigest()\n+    except ImportError:\n+        # Fallback to original method if pandas util not available\n+        original_columns = df.columns\n+        df.columns = list(range(len(df.columns)))\n+        result = hashlib.sha256(\n+            str(df.values).encode()\n+        ).hexdigest()\n+        df.columns = original_columns\nComment: **correctness**: `dataframe_checksum` mutates the input DataFrame's columns in the fallback path, which can cause data corruption or unexpected behavior for callers relying on column order or names.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/utilities/cache.py, lines 84-89, the fallback path in `dataframe_checksum` mutates the input DataFrame's columns, which can corrupt the DataFrame for callers. Refactor this block to ensure the original columns are always restored, even if an exception occurs. Use a try/finally block and copy the columns to avoid side effects.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n    original_columns = df.columns.copy()\n    try:\n        df.columns = list(range(len(df.columns)))\n        result = hashlib.sha256(\n            str(df.values).encode()\n        ).hexdigest()\n    finally:\n        df.columns = original_columns\n```\n</details>\n<!-- suggestion_end -->\n",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/utilities/cache.py",
    "pr_number": 11595,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2362876908,
    "comment_created_at": "2025-09-19T13:21:12Z"
  },
  {
    "code": "@@ -0,0 +1,14 @@\n+---\n+title: How to Interact with MindsDB from PHP\n+sidebarTitle: MindsDB and PHP\n+---\n+\n+To get started with MindsDB, you need to either [create an account at MindsDB Cloud](/setup/cloud) or install MindsDB locally via [pip](/setup/self-hosted/pip/source) or [docker](/setup/self-hosted/docker).\n+\n+There are a few ways you can interact with MindsDB from the PHP code.\n+\n+1. You can connect to MindsDB using the [PHP Data Objects](https://www.php.net/manual/en/book.pdo.php) and execute statements directly on MindsDB with the `PDO::query` method.\n+\n+2. You can use the [REST API](/rest/overview) endpoints to interact with MindsDB directly from PHP.\n+\n+3. You can use the [MindsDB Python SDK](/sdk_python/overview) to interact with MindsDB and embed this Python code in PHP.",
    "comment": "I think this is mostly used for running python scripts, so I think it is better to remove it as an option",
    "line_number": 14,
    "enriched": "File: docs/faqs/mindsdb-with-php.mdx\nCode: @@ -0,0 +1,14 @@\n+---\n+title: How to Interact with MindsDB from PHP\n+sidebarTitle: MindsDB and PHP\n+---\n+\n+To get started with MindsDB, you need to either [create an account at MindsDB Cloud](/setup/cloud) or install MindsDB locally via [pip](/setup/self-hosted/pip/source) or [docker](/setup/self-hosted/docker).\n+\n+There are a few ways you can interact with MindsDB from the PHP code.\n+\n+1. You can connect to MindsDB using the [PHP Data Objects](https://www.php.net/manual/en/book.pdo.php) and execute statements directly on MindsDB with the `PDO::query` method.\n+\n+2. You can use the [REST API](/rest/overview) endpoints to interact with MindsDB directly from PHP.\n+\n+3. You can use the [MindsDB Python SDK](/sdk_python/overview) to interact with MindsDB and embed this Python code in PHP.\nComment: I think this is mostly used for running python scripts, so I think it is better to remove it as an option",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "docs/faqs/mindsdb-with-php.mdx",
    "pr_number": 7124,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1297071569,
    "comment_created_at": "2023-08-17T11:12:21Z"
  },
  {
    "code": "@@ -25,7 +26,7 @@ class InformationSchemaDataNode(DataNode):\n         'COLUMNS': ['TABLE_CATALOG', 'TABLE_SCHEMA', 'TABLE_NAME', 'COLUMN_NAME', 'ORDINAL_POSITION', 'COLUMN_DEFAULT', 'IS_NULLABLE', 'DATA_TYPE', 'CHARACTER_MAXIMUM_LENGTH', 'CHARACTER_OCTET_LENGTH', 'NUMERIC_PRECISION', 'NUMERIC_SCALE', 'DATETIME_PRECISION', 'CHARACTER_SET_NAME', 'COLLATION_NAME', 'COLUMN_TYPE', 'COLUMN_KEY', 'EXTRA', 'PRIVILEGES', 'COLUMN_COMMENT', 'GENERATION_EXPRESSION'],\n         'EVENTS': ['EVENT_CATALOG', 'EVENT_SCHEMA', 'EVENT_NAME', 'DEFINER', 'TIME_ZONE', 'EVENT_BODY', 'EVENT_DEFINITION', 'EVENT_TYPE', 'EXECUTE_AT', 'INTERVAL_VALUE', 'INTERVAL_FIELD', 'SQL_MODE', 'STARTS', 'ENDS', 'STATUS', 'ON_COMPLETION', 'CREATED', 'LAST_ALTERED', 'LAST_EXECUTED', 'EVENT_COMMENT', 'ORIGINATOR', 'CHARACTER_SET_CLIENT', 'COLLATION_CONNECTION', 'DATABASE_COLLATION'],\n         'ROUTINES': ['SPECIFIC_NAME', 'ROUTINE_CATALOG', 'ROUTINE_SCHEMA', 'ROUTINE_NAME', 'ROUTINE_TYPE', 'DATA_TYPE', 'CHARACTER_MAXIMUM_LENGTH', 'CHARACTER_OCTET_LENGTH', 'NUMERIC_PRECISION', 'NUMERIC_SCALE', 'DATETIME_PRECISION', 'CHARACTER_SET_NAME', 'COLLATION_NAME', 'DTD_IDENTIFIER', 'ROUTINE_BODY', 'ROUTINE_DEFINITION', 'EXTERNAL_NAME', 'EXTERNAL_LANGUAGE', 'PARAMETER_STYLE', 'IS_DETERMINISTIC', 'SQL_DATA_ACCESS', 'SQL_PATH', 'SECURITY_TYPE', 'CREATED', 'LAST_ALTERED', 'SQL_MODE', 'ROUTINE_COMMENT', 'DEFINER', 'CHARACTER_SET_CLIENT', 'COLLATION_CONNECTION', 'DATABASE_COLLATION'],\n-        'TRIGGERS': ['TRIGGER_CATALOG', 'TRIGGER_SCHEMA', 'TRIGGER_NAME', 'EVENT_MANIPULATION', 'EVENT_OBJECT_CATALOG', 'EVENT_OBJECT_SCHEMA', 'EVENT_OBJECT_TABLE', 'ACTION_ORDER', 'ACTION_CONDITION', 'ACTION_STATEMENT', 'ACTION_ORIENTATION', 'ACTION_TIMING', 'ACTION_REFERENCE_OLD_TABLE', 'ACTION_REFERENCE_NEW_TABLE', 'ACTION_REFERENCE_OLD_ROW', 'ACTION_REFERENCE_NEW_ROW', 'CREATED', 'SQL_MODE', 'DEFINER', 'CHARACTER_SET_CLIENT', 'COLLATION_CONNECTION', 'DATABASE_COLLATION'],\n+        # 'TRIGGERS': ['TRIGGER_CATALOG', 'TRIGGER_SCHEMA', 'TRIGGER_NAME', 'EVENT_MANIPULATION', 'EVENT_OBJECT_CATALOG', 'EVENT_OBJECT_SCHEMA', 'EVENT_OBJECT_TABLE', 'ACTION_ORDER', 'ACTION_CONDITION', 'ACTION_STATEMENT', 'ACTION_ORIENTATION', 'ACTION_TIMING', 'ACTION_REFERENCE_OLD_TABLE', 'ACTION_REFERENCE_NEW_TABLE', 'ACTION_REFERENCE_OLD_ROW', 'ACTION_REFERENCE_NEW_ROW', 'CREATED', 'SQL_MODE', 'DEFINER', 'CHARACTER_SET_CLIENT', 'COLLATION_CONNECTION', 'DATABASE_COLLATION'],",
    "comment": "Would prefer to just delete commented out code if it won't be used",
    "line_number": 29,
    "enriched": "File: mindsdb/api/mysql/mysql_proxy/datahub/datanodes/information_schema_datanode.py\nCode: @@ -25,7 +26,7 @@ class InformationSchemaDataNode(DataNode):\n         'COLUMNS': ['TABLE_CATALOG', 'TABLE_SCHEMA', 'TABLE_NAME', 'COLUMN_NAME', 'ORDINAL_POSITION', 'COLUMN_DEFAULT', 'IS_NULLABLE', 'DATA_TYPE', 'CHARACTER_MAXIMUM_LENGTH', 'CHARACTER_OCTET_LENGTH', 'NUMERIC_PRECISION', 'NUMERIC_SCALE', 'DATETIME_PRECISION', 'CHARACTER_SET_NAME', 'COLLATION_NAME', 'COLUMN_TYPE', 'COLUMN_KEY', 'EXTRA', 'PRIVILEGES', 'COLUMN_COMMENT', 'GENERATION_EXPRESSION'],\n         'EVENTS': ['EVENT_CATALOG', 'EVENT_SCHEMA', 'EVENT_NAME', 'DEFINER', 'TIME_ZONE', 'EVENT_BODY', 'EVENT_DEFINITION', 'EVENT_TYPE', 'EXECUTE_AT', 'INTERVAL_VALUE', 'INTERVAL_FIELD', 'SQL_MODE', 'STARTS', 'ENDS', 'STATUS', 'ON_COMPLETION', 'CREATED', 'LAST_ALTERED', 'LAST_EXECUTED', 'EVENT_COMMENT', 'ORIGINATOR', 'CHARACTER_SET_CLIENT', 'COLLATION_CONNECTION', 'DATABASE_COLLATION'],\n         'ROUTINES': ['SPECIFIC_NAME', 'ROUTINE_CATALOG', 'ROUTINE_SCHEMA', 'ROUTINE_NAME', 'ROUTINE_TYPE', 'DATA_TYPE', 'CHARACTER_MAXIMUM_LENGTH', 'CHARACTER_OCTET_LENGTH', 'NUMERIC_PRECISION', 'NUMERIC_SCALE', 'DATETIME_PRECISION', 'CHARACTER_SET_NAME', 'COLLATION_NAME', 'DTD_IDENTIFIER', 'ROUTINE_BODY', 'ROUTINE_DEFINITION', 'EXTERNAL_NAME', 'EXTERNAL_LANGUAGE', 'PARAMETER_STYLE', 'IS_DETERMINISTIC', 'SQL_DATA_ACCESS', 'SQL_PATH', 'SECURITY_TYPE', 'CREATED', 'LAST_ALTERED', 'SQL_MODE', 'ROUTINE_COMMENT', 'DEFINER', 'CHARACTER_SET_CLIENT', 'COLLATION_CONNECTION', 'DATABASE_COLLATION'],\n-        'TRIGGERS': ['TRIGGER_CATALOG', 'TRIGGER_SCHEMA', 'TRIGGER_NAME', 'EVENT_MANIPULATION', 'EVENT_OBJECT_CATALOG', 'EVENT_OBJECT_SCHEMA', 'EVENT_OBJECT_TABLE', 'ACTION_ORDER', 'ACTION_CONDITION', 'ACTION_STATEMENT', 'ACTION_ORIENTATION', 'ACTION_TIMING', 'ACTION_REFERENCE_OLD_TABLE', 'ACTION_REFERENCE_NEW_TABLE', 'ACTION_REFERENCE_OLD_ROW', 'ACTION_REFERENCE_NEW_ROW', 'CREATED', 'SQL_MODE', 'DEFINER', 'CHARACTER_SET_CLIENT', 'COLLATION_CONNECTION', 'DATABASE_COLLATION'],\n+        # 'TRIGGERS': ['TRIGGER_CATALOG', 'TRIGGER_SCHEMA', 'TRIGGER_NAME', 'EVENT_MANIPULATION', 'EVENT_OBJECT_CATALOG', 'EVENT_OBJECT_SCHEMA', 'EVENT_OBJECT_TABLE', 'ACTION_ORDER', 'ACTION_CONDITION', 'ACTION_STATEMENT', 'ACTION_ORIENTATION', 'ACTION_TIMING', 'ACTION_REFERENCE_OLD_TABLE', 'ACTION_REFERENCE_NEW_TABLE', 'ACTION_REFERENCE_OLD_ROW', 'ACTION_REFERENCE_NEW_ROW', 'CREATED', 'SQL_MODE', 'DEFINER', 'CHARACTER_SET_CLIENT', 'COLLATION_CONNECTION', 'DATABASE_COLLATION'],\nComment: Would prefer to just delete commented out code if it won't be used",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/api/mysql/mysql_proxy/datahub/datanodes/information_schema_datanode.py",
    "pr_number": 6790,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1274163237,
    "comment_created_at": "2023-07-25T22:02:55Z"
  },
  {
    "code": "@@ -270,6 +271,14 @@ def select(self, query, disable_reranking=False):\n                         hybrid_search_enabled_flag = hybrid_search_enabled_flag.lower() not in (\"false\")\n                     if item.value is False or (isinstance(item.value, str) and item.value.lower() == \"false\"):\n                         disable_reranking = True\n+                elif item.column == \"hybrid_search_alpha\":\n+                    # validate item.value is a float\n+                    if not isinstance(item.value, (float, int)):\n+                        raise ValueError(f\"Invalid hybrid_search_alpha value: {item.value}. Must be a float or int.\")\n+                    # validate hybrid search alpha is between 0 and 1\n+                    if not (0 <= item.value <= 1):",
    "comment": "it means we can only decrease distance of keyword search. \r\nis there a use-case when I can give semantic search a greater proirity (by increasing distance of keyword search)?",
    "line_number": 279,
    "enriched": "File: mindsdb/interfaces/knowledge_base/controller.py\nCode: @@ -270,6 +271,14 @@ def select(self, query, disable_reranking=False):\n                         hybrid_search_enabled_flag = hybrid_search_enabled_flag.lower() not in (\"false\")\n                     if item.value is False or (isinstance(item.value, str) and item.value.lower() == \"false\"):\n                         disable_reranking = True\n+                elif item.column == \"hybrid_search_alpha\":\n+                    # validate item.value is a float\n+                    if not isinstance(item.value, (float, int)):\n+                        raise ValueError(f\"Invalid hybrid_search_alpha value: {item.value}. Must be a float or int.\")\n+                    # validate hybrid search alpha is between 0 and 1\n+                    if not (0 <= item.value <= 1):\nComment: it means we can only decrease distance of keyword search. \r\nis there a use-case when I can give semantic search a greater proirity (by increasing distance of keyword search)?",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/interfaces/knowledge_base/controller.py",
    "pr_number": 11316,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2210847112,
    "comment_created_at": "2025-07-16T15:59:42Z"
  },
  {
    "code": "@@ -0,0 +1,86 @@\n+import os\n+from typing import Optional, Dict\n+\n+import together\n+from concurrent.futures import ThreadPoolExecutor\n+import pandas as pd\n+\n+from mindsdb.utilities.config import Config\n+from mindsdb.integrations.libs.base import BaseMLEngine\n+from mindsdb.integrations.libs.llm_utils import get_completed_prompts\n+\n+\n+class TogetherAIHandler(BaseMLEngine):\n+    \"\"\"\n+    Integration with the Together AI Inference API.\n+    \"\"\"\n+\n+    name = 'together_ai'\n+\n+    def __init__(self, *args, **kwargs):\n+        super().__init__(*args, **kwargs)\n+        self.default_model = 'togethercomputer/RedPajama-INCITE-7B-Chat'\n+        self.default_max_tokens = 128\n+\n+    def create(self, target: str, df: Optional[pd.DataFrame] = None, args: Optional[Dict] = None) -> None:\n+        if 'using' not in args:\n+            raise ValueError(\"Together AI engine requires a USING clause! Refer to its documentation for more details.\")\n+        # set api key\n+        api_key = self._get_together_ai_api_key(args['using'])\n+        together.api_key = api_key\n+\n+    def predict(self, df: Optional[pd.DataFrame] = None, args: Optional[Dict] = None) -> None:\n+        if 'predict_params' not in args and 'input_column' not in args['predict_params']:\n+            raise Exception(\"Together AI engine requires an input column! Refer to its documentation for more details.\")\n+        \n+        params = args['predict_params']\n+\n+        if 'prompt_template' in params:\n+            df['prompts'], empty_prompt_ids = get_completed_prompts(params['prompt_template'], df)\n+        else:\n+            df['prompts'] = df[params['input_column']]\n+\n+        new_column = params['output_column'] if 'output_column' in params else 'prediction'\n+\n+        with ThreadPoolExecutor(max_workers=5) as executor:\n+            df[new_column] = list(executor.map(lambda x: together.Complete.create(\n+                prompt = x,\n+                model = params['model_name'] if 'model_name' in params else self.default_model,\n+                max_tokens = params['max_tokens'] if 'max_tokens' in params else self.default_max_tokens,\n+                temperature = params['temperature'] if 'temperature' in params else 0.5,\n+                top_p = params['top_p'] if 'top_p' in params else None,\n+                top_k = params['top_k'] if 'top_k' in params else None,\n+                repetition_penalty = params['repetition_penalty'] if 'repetition_penalty' in params else None,\n+                logprobs = params['logprobs'] if 'logprobs' in params else None\n+            )['output']['choices'][0]['text'], df['prompts']))\n+\n+        return df\n+\n+    def _get_together_ai_api_key(self, args, strict=True):",
    "comment": "There is a common method for this in our utility modules (`mindsdb/integrations/utilities/handler_utils.py`). Please use that instead if possible.",
    "line_number": 59,
    "enriched": "File: mindsdb/integrations/handlers/together-ai-handler/together_ai_handler.py\nCode: @@ -0,0 +1,86 @@\n+import os\n+from typing import Optional, Dict\n+\n+import together\n+from concurrent.futures import ThreadPoolExecutor\n+import pandas as pd\n+\n+from mindsdb.utilities.config import Config\n+from mindsdb.integrations.libs.base import BaseMLEngine\n+from mindsdb.integrations.libs.llm_utils import get_completed_prompts\n+\n+\n+class TogetherAIHandler(BaseMLEngine):\n+    \"\"\"\n+    Integration with the Together AI Inference API.\n+    \"\"\"\n+\n+    name = 'together_ai'\n+\n+    def __init__(self, *args, **kwargs):\n+        super().__init__(*args, **kwargs)\n+        self.default_model = 'togethercomputer/RedPajama-INCITE-7B-Chat'\n+        self.default_max_tokens = 128\n+\n+    def create(self, target: str, df: Optional[pd.DataFrame] = None, args: Optional[Dict] = None) -> None:\n+        if 'using' not in args:\n+            raise ValueError(\"Together AI engine requires a USING clause! Refer to its documentation for more details.\")\n+        # set api key\n+        api_key = self._get_together_ai_api_key(args['using'])\n+        together.api_key = api_key\n+\n+    def predict(self, df: Optional[pd.DataFrame] = None, args: Optional[Dict] = None) -> None:\n+        if 'predict_params' not in args and 'input_column' not in args['predict_params']:\n+            raise Exception(\"Together AI engine requires an input column! Refer to its documentation for more details.\")\n+        \n+        params = args['predict_params']\n+\n+        if 'prompt_template' in params:\n+            df['prompts'], empty_prompt_ids = get_completed_prompts(params['prompt_template'], df)\n+        else:\n+            df['prompts'] = df[params['input_column']]\n+\n+        new_column = params['output_column'] if 'output_column' in params else 'prediction'\n+\n+        with ThreadPoolExecutor(max_workers=5) as executor:\n+            df[new_column] = list(executor.map(lambda x: together.Complete.create(\n+                prompt = x,\n+                model = params['model_name'] if 'model_name' in params else self.default_model,\n+                max_tokens = params['max_tokens'] if 'max_tokens' in params else self.default_max_tokens,\n+                temperature = params['temperature'] if 'temperature' in params else 0.5,\n+                top_p = params['top_p'] if 'top_p' in params else None,\n+                top_k = params['top_k'] if 'top_k' in params else None,\n+                repetition_penalty = params['repetition_penalty'] if 'repetition_penalty' in params else None,\n+                logprobs = params['logprobs'] if 'logprobs' in params else None\n+            )['output']['choices'][0]['text'], df['prompts']))\n+\n+        return df\n+\n+    def _get_together_ai_api_key(self, args, strict=True):\nComment: There is a common method for this in our utility modules (`mindsdb/integrations/utilities/handler_utils.py`). Please use that instead if possible.",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/together-ai-handler/together_ai_handler.py",
    "pr_number": 8180,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1375508521,
    "comment_created_at": "2023-10-29T20:40:14Z"
  },
  {
    "code": "@@ -1,173 +1,439 @@\n # HubSpot Handler\n \n HubSpot handler for MindsDB provides interfaces to connect to HubSpot via APIs and pull store data into MindsDB.\n-\n ---\n \n ## Table of Contents\n \n - [HubSpot Handler](#hubspot-handler)\n+  - [HubSpot handler for MindsDB provides interfaces to connect to HubSpot via APIs and pull store data into MindsDB.](#hubspot-handler-for-mindsdb-provides-interfaces-to-connect-to-hubspot-via-apis-and-pull-store-data-into-mindsdb)\n   - [Table of Contents](#table-of-contents)\n   - [About HubSpot](#about-hubspot)\n   - [HubSpot Handler Implementation](#hubspot-handler-implementation)\n-  - [HubSpot Handler Initialization](#hubspot-handler-initialization)\n+  - [Installation](#installation)\n+  - [Authentication](#authentication)\n+    - [Access Token Authentication](#access-token-authentication)\n+    - [OAuth Authentication](#oauth-authentication)\n+  - [Enterprise Features](#enterprise-features)\n+  - [Data Catalog Support](#data-catalog-support)\n   - [Implemented Features](#implemented-features)\n+    - [🏢 Companies Table](#-companies-table)\n+    - [👤 Contacts Table](#-contacts-table)\n+    - [💼 Deals Table](#-deals-table)\n+  - [Available Tables](#available-tables)\n   - [Example Usage](#example-usage)\n+    - [Basic Connection](#basic-connection)\n+    - [Data Catalog Operations](#data-catalog-operations)\n+    - [Querying Data](#querying-data)\n+    - [Data Manipulation](#data-manipulation)\n+  - [Error Handling](#error-handling)\n+  - [Security](#security)\n+  - [Performance](#performance)\n+  - [Troubleshooting](#troubleshooting)\n+    - [Common Issues](#common-issues)\n+    - [Debugging Steps](#debugging-steps)\n+    - [Performance Tuning](#performance-tuning)\n+  - [Known Limitations](#known-limitations)\n+  - [API Coverage](#api-coverage)\n \n ---\n \n ## About HubSpot\n \n-HubSpot is a CRM platform with all the software, integrations, and resources you need to connect your marketing, sales, content management, and customer service.\n-<br>\n-https://www.hubspot.com/products?hubs_content=www.hubspot.com%2F&hubs_content-cta=All%20Products%20and%20Features\n+HubSpot is a comprehensive CRM platform providing marketing, sales, content management, and customer service tools. This integration provides secure, enterprise-ready access to HubSpot's CRM data through MindsDB's unified interface.\n+\n+**Official Website:** https://www.hubspot.com/products\n+**API Documentation:** https://developers.hubspot.com/docs/api/overview\n \n ## HubSpot Handler Implementation\n \n-This handler was implemented using [hubspot-api-client\n-](https://github.com/HubSpot/hubspot-api-python), the Python library for the HubSpot API.\n+This enterprise-ready handler is implemented using the official [HubSpot API Client](https://github.com/HubSpot/hubspot-api-python) with comprehensive enhancements for:\n+\n+- **Enterprise Security**: Secure credential handling with no sensitive data in logs\n+- **Data Catalog**: Complete metadata support including table/column descriptions and statistics\n+- **Error Handling**: Comprehensive error handling with detailed logging and recovery\n+- **Performance**: Optimized API calls with intelligent caching and batching\n+- **Standards Compliance**: Full PEP8 compliance with comprehensive type hints\n+\n+## Installation\n+\n+Install the handler dependencies using pip:\n+\n+```bash\n+pip install -r requirements.txt\n+```\n+\n+**Required Dependencies:**\n+- `hubspot-api-client==12.0.0` - Official HubSpot Python client\n+- `pandas>=1.3.0` - Data manipulation and analysis\n+- `typing-extensions>=4.0.0` - Enhanced type hints support",
    "comment": "where is it use?",
    "line_number": 69,
    "enriched": "File: mindsdb/integrations/handlers/hubspot_handler/README.md\nCode: @@ -1,173 +1,439 @@\n # HubSpot Handler\n \n HubSpot handler for MindsDB provides interfaces to connect to HubSpot via APIs and pull store data into MindsDB.\n-\n ---\n \n ## Table of Contents\n \n - [HubSpot Handler](#hubspot-handler)\n+  - [HubSpot handler for MindsDB provides interfaces to connect to HubSpot via APIs and pull store data into MindsDB.](#hubspot-handler-for-mindsdb-provides-interfaces-to-connect-to-hubspot-via-apis-and-pull-store-data-into-mindsdb)\n   - [Table of Contents](#table-of-contents)\n   - [About HubSpot](#about-hubspot)\n   - [HubSpot Handler Implementation](#hubspot-handler-implementation)\n-  - [HubSpot Handler Initialization](#hubspot-handler-initialization)\n+  - [Installation](#installation)\n+  - [Authentication](#authentication)\n+    - [Access Token Authentication](#access-token-authentication)\n+    - [OAuth Authentication](#oauth-authentication)\n+  - [Enterprise Features](#enterprise-features)\n+  - [Data Catalog Support](#data-catalog-support)\n   - [Implemented Features](#implemented-features)\n+    - [🏢 Companies Table](#-companies-table)\n+    - [👤 Contacts Table](#-contacts-table)\n+    - [💼 Deals Table](#-deals-table)\n+  - [Available Tables](#available-tables)\n   - [Example Usage](#example-usage)\n+    - [Basic Connection](#basic-connection)\n+    - [Data Catalog Operations](#data-catalog-operations)\n+    - [Querying Data](#querying-data)\n+    - [Data Manipulation](#data-manipulation)\n+  - [Error Handling](#error-handling)\n+  - [Security](#security)\n+  - [Performance](#performance)\n+  - [Troubleshooting](#troubleshooting)\n+    - [Common Issues](#common-issues)\n+    - [Debugging Steps](#debugging-steps)\n+    - [Performance Tuning](#performance-tuning)\n+  - [Known Limitations](#known-limitations)\n+  - [API Coverage](#api-coverage)\n \n ---\n \n ## About HubSpot\n \n-HubSpot is a CRM platform with all the software, integrations, and resources you need to connect your marketing, sales, content management, and customer service.\n-<br>\n-https://www.hubspot.com/products?hubs_content=www.hubspot.com%2F&hubs_content-cta=All%20Products%20and%20Features\n+HubSpot is a comprehensive CRM platform providing marketing, sales, content management, and customer service tools. This integration provides secure, enterprise-ready access to HubSpot's CRM data through MindsDB's unified interface.\n+\n+**Official Website:** https://www.hubspot.com/products\n+**API Documentation:** https://developers.hubspot.com/docs/api/overview\n \n ## HubSpot Handler Implementation\n \n-This handler was implemented using [hubspot-api-client\n-](https://github.com/HubSpot/hubspot-api-python), the Python library for the HubSpot API.\n+This enterprise-ready handler is implemented using the official [HubSpot API Client](https://github.com/HubSpot/hubspot-api-python) with comprehensive enhancements for:\n+\n+- **Enterprise Security**: Secure credential handling with no sensitive data in logs\n+- **Data Catalog**: Complete metadata support including table/column descriptions and statistics\n+- **Error Handling**: Comprehensive error handling with detailed logging and recovery\n+- **Performance**: Optimized API calls with intelligent caching and batching\n+- **Standards Compliance**: Full PEP8 compliance with comprehensive type hints\n+\n+## Installation\n+\n+Install the handler dependencies using pip:\n+\n+```bash\n+pip install -r requirements.txt\n+```\n+\n+**Required Dependencies:**\n+- `hubspot-api-client==12.0.0` - Official HubSpot Python client\n+- `pandas>=1.3.0` - Data manipulation and analysis\n+- `typing-extensions>=4.0.0` - Enhanced type hints support\nComment: where is it use?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/hubspot_handler/README.md",
    "pr_number": 11831,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2498057553,
    "comment_created_at": "2025-11-06T08:50:35Z"
  },
  {
    "code": "@@ -0,0 +1,406 @@\n+import os\n+from twilio.rest import Client\n+import re\n+from datetime import datetime as datetime\n+from typing import List\n+import pandas as pd\n+\n+from mindsdb.utilities import log\n+from mindsdb.utilities.config import Config\n+\n+from mindsdb_sql.parser import ast\n+from mindsdb.integrations.utilities.date_utils import parse_local_date\n+\n+from mindsdb.integrations.libs.api_handler import APIHandler, APITable\n+\n+from mindsdb.integrations.utilities.sql_utils import extract_comparison_conditions, project_dataframe, filter_dataframe\n+\n+from mindsdb.integrations.libs.response import (\n+    HandlerStatusResponse as StatusResponse,\n+    HandlerResponse as Response,\n+    RESPONSE_TYPE\n+)\n+\n+\n+class WhatsAppMessagesTable(APITable):\n+    def select(self, query: ast.Select) -> Response:\n+        \"\"\"\n+        Retrieves messages sent/received from the database using Twilio Whatsapp API\n+        Returns\n+            Response: conversation_history\n+        \"\"\"\n+\n+        # Extract comparison conditions from the query\n+        conditions = extract_comparison_conditions(query.where)\n+        params = {}\n+        filters = []\n+\n+        # Build the filters and parameters for the query\n+        for op, arg1, arg2 in conditions:\n+            if op == 'or':\n+                raise NotImplementedError('OR is not supported')\n+\n+            if arg1 == 'sent_at' and arg2 is not None:\n+                date = parse_local_date(arg2)\n+                if op == '>':\n+                    params['date_sent_after'] = date\n+                elif op == '<':\n+                    params['date_sent_before'] = date\n+                else:\n+                    raise NotImplementedError\n+\n+                # also add to post query filter because date_sent_after=date1 will include date1\n+                filters.append([op, arg1, arg2])\n+\n+            elif arg1 == 'sid':\n+                if op == '=':\n+                    params['sid'] = arg2\n+                else:\n+                    NotImplementedError('Only  \"from_number=\" is implemented')\n+\n+            elif arg1 == 'from_number':\n+                if op == '=':\n+                    params['from_number'] = arg2\n+                else:\n+                    NotImplementedError('Only  \"from_number=\" is implemented')\n+\n+            elif arg1 == 'to_number':\n+                if op == '=':\n+                    params['to_number'] = arg2\n+                else:\n+                    NotImplementedError('Only  \"to_number=\" is implemented')\n+\n+            else:\n+                filters.append([op, arg1, arg2])\n+\n+        # Fetch messages based on the filters\n+        result = self.handler.fetch_messages(params, df=True)\n+\n+        # filter targets\n+        result = filter_dataframe(result, filters)\n+\n+        # If limit is specified\n+        if query.limit is not None:\n+            result = result[:int(query.limit.value)]\n+\n+        # project targets\n+        result = project_dataframe(result, query.targets, self.get_columns())\n+\n+        return result\n+\n+    def get_columns(self):\n+        return [\n+            'sid',\n+            'from_number',\n+            'to_number',\n+            'body',\n+            'direction',\n+            'msg_status',\n+            'sent_at',  # datetime.strptime(str(msg.date_sent), '%Y-%m-%d %H:%M:%S%z'),\n+            'account_sid',\n+            'price',\n+            'price_unit',\n+            'api_version',\n+            'uri'\n+        ]\n+\n+    def insert(self, query: ast.Insert):\n+        \"\"\"\n+        Sends a whatsapp message\n+\n+        Args:\n+            body: message body\n+            from_number: number from which to send the message\n+            to_number: number to which message will be sent\n+        \"\"\"\n+\n+        # get column names and values from the query\n+        columns = [col.name for col in query.columns]\n+\n+        ret = []\n+\n+        insert_params = [\"body\", \"from_number\", \"to_number\"]\n+        for row in query.values:\n+            params = dict(zip(columns, row))\n+\n+            # Check text length\n+            max_text_len = 1500\n+            text = params[\"body\"]\n+            words = re.split('( )', text)\n+            messages = []\n+\n+            # Check text pattern\n+            text2 = ''\n+            pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n+            for word in words:\n+                # replace the links in word to string with the length as twitter short url (23)\n+                word2 = re.sub(pattern, '-' * 23, word)\n+                if len(text2) + len(word2) > max_text_len - 3 - 7:  # 3 is for ..., 7 is for (10/11)\n+                    messages.append(text2.strip())\n+\n+                    text2 = ''\n+                text2 += word\n+\n+            # Parse last message\n+            if text2.strip() != '':\n+                messages.append(text2.strip())\n+\n+            len_messages = len(messages)\n+\n+            # Modify message based on the length\n+            for i, text in enumerate(messages):\n+                if i < len_messages - 1:\n+                    text += '...'\n+                else:\n+                    text += ' '\n+\n+                if i >= 1:\n+                    text += f'({i + 1}/{len_messages})'\n+\n+                # Pass parameters and call 'send_message'\n+                params['body'] = text\n+                params_to_send = {key: params[key] for key in insert_params if (key in params)}\n+                ret_row = self.handler.send_message(params_to_send, ret_as_dict=True)\n+\n+                # Save the results\n+                ret_row['body'] = text\n+                ret.append(ret_row)\n+\n+        return pd.DataFrame(ret)\n+\n+\n+class WhatsAppHandler(APIHandler):\n+    \"\"\"\n+    A class for handling connections and interactions with Twilio WhatsApp API.\n+    Args:\n+        account_sid(str): Accound ID of the twilio account.\n+        auth_token(str): Authentication Token obtained from the twilio account.\n+    \"\"\"\n+\n+    def __init__(self, name=None, **kwargs):\n+        \"\"\"\n+        Initializes the connection by checking all the params are provided by the user.\n+        \"\"\"\n+        super().__init__(name)\n+\n+        args = kwargs.get('connection_data', {})\n+        self.connection_args = {}\n+        handler_config = Config().get('whatsapp_handler', {})\n+        for k in ['account_sid', 'auth_token']:\n+            if k in args:\n+                self.connection_args[k] = args[k]\n+            elif f'TWILIO_{k.upper()}' in os.environ:\n+                self.connection_args[k] = os.environ[f'TWILIO_{k.upper()}']\n+            elif k in handler_config:\n+                self.connection_args[k] = handler_config[k]\n+        self.client = None\n+        self.is_connected = False\n+\n+        messages = WhatsAppMessagesTable(self)\n+        self._register_table('messages', messages)\n+\n+    def connect(self):\n+        \"\"\"\n+        Authenticate with the Twilio API using the provided `account_SID` and `auth_token`.\n+        \"\"\"\n+        if self.is_connected is True:\n+            return self.client\n+\n+        self.client = Client(",
    "comment": "Is the number required here?",
    "line_number": 209,
    "enriched": "File: mindsdb/integrations/handlers/whatsapp_handler/whatsapp_handler.py\nCode: @@ -0,0 +1,406 @@\n+import os\n+from twilio.rest import Client\n+import re\n+from datetime import datetime as datetime\n+from typing import List\n+import pandas as pd\n+\n+from mindsdb.utilities import log\n+from mindsdb.utilities.config import Config\n+\n+from mindsdb_sql.parser import ast\n+from mindsdb.integrations.utilities.date_utils import parse_local_date\n+\n+from mindsdb.integrations.libs.api_handler import APIHandler, APITable\n+\n+from mindsdb.integrations.utilities.sql_utils import extract_comparison_conditions, project_dataframe, filter_dataframe\n+\n+from mindsdb.integrations.libs.response import (\n+    HandlerStatusResponse as StatusResponse,\n+    HandlerResponse as Response,\n+    RESPONSE_TYPE\n+)\n+\n+\n+class WhatsAppMessagesTable(APITable):\n+    def select(self, query: ast.Select) -> Response:\n+        \"\"\"\n+        Retrieves messages sent/received from the database using Twilio Whatsapp API\n+        Returns\n+            Response: conversation_history\n+        \"\"\"\n+\n+        # Extract comparison conditions from the query\n+        conditions = extract_comparison_conditions(query.where)\n+        params = {}\n+        filters = []\n+\n+        # Build the filters and parameters for the query\n+        for op, arg1, arg2 in conditions:\n+            if op == 'or':\n+                raise NotImplementedError('OR is not supported')\n+\n+            if arg1 == 'sent_at' and arg2 is not None:\n+                date = parse_local_date(arg2)\n+                if op == '>':\n+                    params['date_sent_after'] = date\n+                elif op == '<':\n+                    params['date_sent_before'] = date\n+                else:\n+                    raise NotImplementedError\n+\n+                # also add to post query filter because date_sent_after=date1 will include date1\n+                filters.append([op, arg1, arg2])\n+\n+            elif arg1 == 'sid':\n+                if op == '=':\n+                    params['sid'] = arg2\n+                else:\n+                    NotImplementedError('Only  \"from_number=\" is implemented')\n+\n+            elif arg1 == 'from_number':\n+                if op == '=':\n+                    params['from_number'] = arg2\n+                else:\n+                    NotImplementedError('Only  \"from_number=\" is implemented')\n+\n+            elif arg1 == 'to_number':\n+                if op == '=':\n+                    params['to_number'] = arg2\n+                else:\n+                    NotImplementedError('Only  \"to_number=\" is implemented')\n+\n+            else:\n+                filters.append([op, arg1, arg2])\n+\n+        # Fetch messages based on the filters\n+        result = self.handler.fetch_messages(params, df=True)\n+\n+        # filter targets\n+        result = filter_dataframe(result, filters)\n+\n+        # If limit is specified\n+        if query.limit is not None:\n+            result = result[:int(query.limit.value)]\n+\n+        # project targets\n+        result = project_dataframe(result, query.targets, self.get_columns())\n+\n+        return result\n+\n+    def get_columns(self):\n+        return [\n+            'sid',\n+            'from_number',\n+            'to_number',\n+            'body',\n+            'direction',\n+            'msg_status',\n+            'sent_at',  # datetime.strptime(str(msg.date_sent), '%Y-%m-%d %H:%M:%S%z'),\n+            'account_sid',\n+            'price',\n+            'price_unit',\n+            'api_version',\n+            'uri'\n+        ]\n+\n+    def insert(self, query: ast.Insert):\n+        \"\"\"\n+        Sends a whatsapp message\n+\n+        Args:\n+            body: message body\n+            from_number: number from which to send the message\n+            to_number: number to which message will be sent\n+        \"\"\"\n+\n+        # get column names and values from the query\n+        columns = [col.name for col in query.columns]\n+\n+        ret = []\n+\n+        insert_params = [\"body\", \"from_number\", \"to_number\"]\n+        for row in query.values:\n+            params = dict(zip(columns, row))\n+\n+            # Check text length\n+            max_text_len = 1500\n+            text = params[\"body\"]\n+            words = re.split('( )', text)\n+            messages = []\n+\n+            # Check text pattern\n+            text2 = ''\n+            pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n+            for word in words:\n+                # replace the links in word to string with the length as twitter short url (23)\n+                word2 = re.sub(pattern, '-' * 23, word)\n+                if len(text2) + len(word2) > max_text_len - 3 - 7:  # 3 is for ..., 7 is for (10/11)\n+                    messages.append(text2.strip())\n+\n+                    text2 = ''\n+                text2 += word\n+\n+            # Parse last message\n+            if text2.strip() != '':\n+                messages.append(text2.strip())\n+\n+            len_messages = len(messages)\n+\n+            # Modify message based on the length\n+            for i, text in enumerate(messages):\n+                if i < len_messages - 1:\n+                    text += '...'\n+                else:\n+                    text += ' '\n+\n+                if i >= 1:\n+                    text += f'({i + 1}/{len_messages})'\n+\n+                # Pass parameters and call 'send_message'\n+                params['body'] = text\n+                params_to_send = {key: params[key] for key in insert_params if (key in params)}\n+                ret_row = self.handler.send_message(params_to_send, ret_as_dict=True)\n+\n+                # Save the results\n+                ret_row['body'] = text\n+                ret.append(ret_row)\n+\n+        return pd.DataFrame(ret)\n+\n+\n+class WhatsAppHandler(APIHandler):\n+    \"\"\"\n+    A class for handling connections and interactions with Twilio WhatsApp API.\n+    Args:\n+        account_sid(str): Accound ID of the twilio account.\n+        auth_token(str): Authentication Token obtained from the twilio account.\n+    \"\"\"\n+\n+    def __init__(self, name=None, **kwargs):\n+        \"\"\"\n+        Initializes the connection by checking all the params are provided by the user.\n+        \"\"\"\n+        super().__init__(name)\n+\n+        args = kwargs.get('connection_data', {})\n+        self.connection_args = {}\n+        handler_config = Config().get('whatsapp_handler', {})\n+        for k in ['account_sid', 'auth_token']:\n+            if k in args:\n+                self.connection_args[k] = args[k]\n+            elif f'TWILIO_{k.upper()}' in os.environ:\n+                self.connection_args[k] = os.environ[f'TWILIO_{k.upper()}']\n+            elif k in handler_config:\n+                self.connection_args[k] = handler_config[k]\n+        self.client = None\n+        self.is_connected = False\n+\n+        messages = WhatsAppMessagesTable(self)\n+        self._register_table('messages', messages)\n+\n+    def connect(self):\n+        \"\"\"\n+        Authenticate with the Twilio API using the provided `account_SID` and `auth_token`.\n+        \"\"\"\n+        if self.is_connected is True:\n+            return self.client\n+\n+        self.client = Client(\nComment: Is the number required here?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/whatsapp_handler/whatsapp_handler.py",
    "pr_number": 8393,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1415309704,
    "comment_created_at": "2023-12-05T10:23:24Z"
  },
  {
    "code": "@@ -612,16 +616,16 @@ def answer_stmt_execute(self, stmt_id, parameters):\n \n         if self.client_capabilities.DEPRECATE_EOF is False:\n             packages.append(self.packet(EofPacket, status=0x0062))\n-        else:",
    "comment": "what is changed here: why we are starting to always send?",
    "line_number": 615,
    "enriched": "File: mindsdb/api/mysql/mysql_proxy/mysql_proxy.py\nCode: @@ -612,16 +616,16 @@ def answer_stmt_execute(self, stmt_id, parameters):\n \n         if self.client_capabilities.DEPRECATE_EOF is False:\n             packages.append(self.packet(EofPacket, status=0x0062))\n-        else:\nComment: what is changed here: why we are starting to always send?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/api/mysql/mysql_proxy/mysql_proxy.py",
    "pr_number": 10205,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1860731048,
    "comment_created_at": "2024-11-27T14:10:32Z"
  },
  {
    "code": "@@ -82,9 +82,6 @@ def test_version_managing(self, data_handler):\n         assert ret['ENGINE'][0] == 'dummy_ml'\n         self.wait_predictor('proj', 'task_model')\n \n-        # check input to data handler",
    "comment": "Why checking of sql is removed from test?",
    "line_number": 85,
    "enriched": "File: tests/unit/test_project_structure.py\nCode: @@ -82,9 +82,6 @@ def test_version_managing(self, data_handler):\n         assert ret['ENGINE'][0] == 'dummy_ml'\n         self.wait_predictor('proj', 'task_model')\n \n-        # check input to data handler\nComment: Why checking of sql is removed from test?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "tests/unit/test_project_structure.py",
    "pr_number": 6605,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1238394561,
    "comment_created_at": "2023-06-22T11:27:30Z"
  },
  {
    "code": "@@ -3,59 +3,73 @@ title: Chatbot\n sidebarTitle: Chatbot\n ---\n \n-Within MindsDB, chatbots are [agents](/agents/agent) connected to some messaging interface.\n+Within MindsDB, chatbots are [agents](/mindsdb_sql/agents/agent) connected to a chat interface.\n \n-Creating a chatbot requires an [AI agent](/agents/agent) and a connection to a chat app, like [Slack](/integrations/app-integrations/slack) or [MS Teams](/integrations/app-integrations/microsoft-teams).\n-\n-<Tip>\n-Currently, the recommended chat app is Slack. MS Teams will be fully supported soon.\n-</Tip>\n+Creating a chatbot requires either an [AI agent](/mindsdb_sql/agents/agent) or an LLM, and a connection to a chat app, like [Slack](/integrations/app-integrations/slack) or [MS Teams](/integrations/app-integrations/microsoft-teams).\n \n <p align=\"center\">\n   <img src=\"/assets/chatbot_diagram.png\" />\n </p>\n \n-## How to work with chatbots\n+## Chatbot with an AI agent\n \n-A chatbot can be created, deleted, queried, and updated. Here is how you can do that using SQL API.\n+[AI Agents](/mindsdb_sql/agents/agent) are customized AI models that can answer questions over your data. You can connect your data to agents in the form of [skills](/mindsdb_sql/agents/agent#create-skills).\n \n-* Creating a chatbot:\n+Here is how to create a chatbot that integrates an AI Agent and can be connected to a chat interface to have a conversation with your data.\n \n-    ```sql\n-    CREATE CHATBOT my_chatbot\n-    USING\n-        database = 'my_slack',                    -- this must be created with CREATE DATABASE\n-        agent = 'customer_support_agent',         -- this must be created with CREATE AGENT\n-        is_running = true;                        -- default is true\n-    ```\n+```sql\n+CREATE CHATBOT my_chatbot\n+USING\n+    database = 'my_slack',   -- created with CREATE DATABASE my_slack\n+    agent = 'my_agent',   -- created with CREATE AGENT my_agent\n+    is_running = true;   -- default is true\n+```\n+\n+The parameters include the following:\n+\n+* `database` stores connection to a chat app (like [Slack](/integrations/app-integrations/slack) or [MS Teams](/integrations/app-integrations/microsoft-teams)) that should be created with the `CREATE DATABASE` statement.\n+* `agent` is an [AI agent](/mindsdb_sql/agents/agent) created with the `CREATE AGENT` command. It consists of an AI model trained with defined data sets.",
    "comment": "I think 'AI model trained with defined data sets' might not be accurate. The model would not exactly be trained on the data; it would be RAG-based.",
    "line_number": 31,
    "enriched": "File: docs/mindsdb_sql/agents/chatbot.mdx\nCode: @@ -3,59 +3,73 @@ title: Chatbot\n sidebarTitle: Chatbot\n ---\n \n-Within MindsDB, chatbots are [agents](/agents/agent) connected to some messaging interface.\n+Within MindsDB, chatbots are [agents](/mindsdb_sql/agents/agent) connected to a chat interface.\n \n-Creating a chatbot requires an [AI agent](/agents/agent) and a connection to a chat app, like [Slack](/integrations/app-integrations/slack) or [MS Teams](/integrations/app-integrations/microsoft-teams).\n-\n-<Tip>\n-Currently, the recommended chat app is Slack. MS Teams will be fully supported soon.\n-</Tip>\n+Creating a chatbot requires either an [AI agent](/mindsdb_sql/agents/agent) or an LLM, and a connection to a chat app, like [Slack](/integrations/app-integrations/slack) or [MS Teams](/integrations/app-integrations/microsoft-teams).\n \n <p align=\"center\">\n   <img src=\"/assets/chatbot_diagram.png\" />\n </p>\n \n-## How to work with chatbots\n+## Chatbot with an AI agent\n \n-A chatbot can be created, deleted, queried, and updated. Here is how you can do that using SQL API.\n+[AI Agents](/mindsdb_sql/agents/agent) are customized AI models that can answer questions over your data. You can connect your data to agents in the form of [skills](/mindsdb_sql/agents/agent#create-skills).\n \n-* Creating a chatbot:\n+Here is how to create a chatbot that integrates an AI Agent and can be connected to a chat interface to have a conversation with your data.\n \n-    ```sql\n-    CREATE CHATBOT my_chatbot\n-    USING\n-        database = 'my_slack',                    -- this must be created with CREATE DATABASE\n-        agent = 'customer_support_agent',         -- this must be created with CREATE AGENT\n-        is_running = true;                        -- default is true\n-    ```\n+```sql\n+CREATE CHATBOT my_chatbot\n+USING\n+    database = 'my_slack',   -- created with CREATE DATABASE my_slack\n+    agent = 'my_agent',   -- created with CREATE AGENT my_agent\n+    is_running = true;   -- default is true\n+```\n+\n+The parameters include the following:\n+\n+* `database` stores connection to a chat app (like [Slack](/integrations/app-integrations/slack) or [MS Teams](/integrations/app-integrations/microsoft-teams)) that should be created with the `CREATE DATABASE` statement.\n+* `agent` is an [AI agent](/mindsdb_sql/agents/agent) created with the `CREATE AGENT` command. It consists of an AI model trained with defined data sets.\nComment: I think 'AI model trained with defined data sets' might not be accurate. The model would not exactly be trained on the data; it would be RAG-based.",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "docs/mindsdb_sql/agents/chatbot.mdx",
    "pr_number": 10101,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1825455530,
    "comment_created_at": "2024-11-01T05:57:47Z"
  },
  {
    "code": "@@ -0,0 +1,118 @@\n+from http import HTTPStatus\n+from typing import List\n+\n+from flask import request\n+from flask_restx import Resource\n+from langchain_text_splitters import MarkdownHeaderTextSplitter",
    "comment": "what is dependency for it? is it installed with mindsdb?",
    "line_number": 6,
    "enriched": "File: mindsdb/api/http/namespaces/knowledge_bases.py\nCode: @@ -0,0 +1,118 @@\n+from http import HTTPStatus\n+from typing import List\n+\n+from flask import request\n+from flask_restx import Resource\n+from langchain_text_splitters import MarkdownHeaderTextSplitter\nComment: what is dependency for it? is it installed with mindsdb?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/api/http/namespaces/knowledge_bases.py",
    "pr_number": 9137,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1581848616,
    "comment_created_at": "2024-04-27T15:38:40Z"
  },
  {
    "code": "@@ -0,0 +1,56 @@\n+from linkup import LinkupClient\n+from pydantic import PrivateAttr\n+\n+class LinkupSearchTool:\n+    name: str = \"Linkup Search Tool\"\n+    description: str = \"Performs an API call to Linkup to retrieve contextual information.\"\n+    _client: LinkupClient = PrivateAttr()\n+\n+    def __init__(self, api_key: str):\n+        \"\"\"\n+        Initialize the tool with an API key.\n+        \"\"\"\n+        self._client = LinkupClient(api_key=api_key)\n+\n+    def _run(self, query: str, depth: str, output_type: str, structured_output_schema: dict = None) -> dict:\n+        \"\"\"\n+        Executes a search using the Linkup API.\n+\n+        :param query: The query to search for.\n+        :param depth: Search depth (default is \"standard\").\n+        :param output_type: Desired result type (default is \"searchResults\").\n+        :param structured_output_schema: JSON schema for structured output (only used if output_type is \"structured\").\n+        :return: A dictionary containing the results or an error message.\n+        \"\"\"\n+        try:\n+            if output_type == \"structured\" and structured_output_schema:\n+                response = self._client.search(\n+                    query=query,\n+                    depth=depth,\n+                    output_type=output_type,\n+                    structured_output_schema=structured_output_schema\n+                )\n+            else:\n+                response = self._client.search(\n+                    query=query,\n+                    depth=depth,\n+                    output_type=output_type\n+                )\n+            if output_type == \"sourcedAnswer\":\n+                return {\n+                    \"success\": True,\n+                    \"answer\": response.answer,\n+                    \"sources\": response.sources\n+                }",
    "comment": "@juliette0704 you should do something like {\"success\": True, **vars(search_response)} instead.\r\nWe want to keep integration logic to a minimum to reduce maintenance overhead.",
    "line_number": 44,
    "enriched": "File: mindsdb/integrations/handlers/linkup_handler/linkup_handler.py\nCode: @@ -0,0 +1,56 @@\n+from linkup import LinkupClient\n+from pydantic import PrivateAttr\n+\n+class LinkupSearchTool:\n+    name: str = \"Linkup Search Tool\"\n+    description: str = \"Performs an API call to Linkup to retrieve contextual information.\"\n+    _client: LinkupClient = PrivateAttr()\n+\n+    def __init__(self, api_key: str):\n+        \"\"\"\n+        Initialize the tool with an API key.\n+        \"\"\"\n+        self._client = LinkupClient(api_key=api_key)\n+\n+    def _run(self, query: str, depth: str, output_type: str, structured_output_schema: dict = None) -> dict:\n+        \"\"\"\n+        Executes a search using the Linkup API.\n+\n+        :param query: The query to search for.\n+        :param depth: Search depth (default is \"standard\").\n+        :param output_type: Desired result type (default is \"searchResults\").\n+        :param structured_output_schema: JSON schema for structured output (only used if output_type is \"structured\").\n+        :return: A dictionary containing the results or an error message.\n+        \"\"\"\n+        try:\n+            if output_type == \"structured\" and structured_output_schema:\n+                response = self._client.search(\n+                    query=query,\n+                    depth=depth,\n+                    output_type=output_type,\n+                    structured_output_schema=structured_output_schema\n+                )\n+            else:\n+                response = self._client.search(\n+                    query=query,\n+                    depth=depth,\n+                    output_type=output_type\n+                )\n+            if output_type == \"sourcedAnswer\":\n+                return {\n+                    \"success\": True,\n+                    \"answer\": response.answer,\n+                    \"sources\": response.sources\n+                }\nComment: @juliette0704 you should do something like {\"success\": True, **vars(search_response)} instead.\r\nWe want to keep integration logic to a minimum to reduce maintenance overhead.",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/linkup_handler/linkup_handler.py",
    "pr_number": 10305,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1911963906,
    "comment_created_at": "2025-01-11T10:53:43Z"
  },
  {
    "code": "@@ -50,8 +50,8 @@\n from mindsdb.utilities.ps import is_pid_listen_port, wait_func_is_true\n from mindsdb.utilities.sentry import sentry_sdk  # noqa: F401\n from mindsdb.utilities.otel import trace  # noqa: F401\n-from opentelemetry.instrumentation.flask import FlaskInstrumentor  # noqa: F401\n-from opentelemetry.instrumentation.requests import RequestsInstrumentor  # noqa: F401\n+# from opentelemetry.instrumentation.flask import FlaskInstrumentor  # noqa: F401",
    "comment": "Maybe remove this file or merge main",
    "line_number": 53,
    "enriched": "File: mindsdb/api/http/initialize.py\nCode: @@ -50,8 +50,8 @@\n from mindsdb.utilities.ps import is_pid_listen_port, wait_func_is_true\n from mindsdb.utilities.sentry import sentry_sdk  # noqa: F401\n from mindsdb.utilities.otel import trace  # noqa: F401\n-from opentelemetry.instrumentation.flask import FlaskInstrumentor  # noqa: F401\n-from opentelemetry.instrumentation.requests import RequestsInstrumentor  # noqa: F401\n+# from opentelemetry.instrumentation.flask import FlaskInstrumentor  # noqa: F401\nComment: Maybe remove this file or merge main",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/api/http/initialize.py",
    "pr_number": 10824,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2088761520,
    "comment_created_at": "2025-05-14T11:55:02Z"
  },
  {
    "code": "@@ -0,0 +1,180 @@\n+from __future__ import annotations\n+\n+import logging\n+from typing import (\n+    Any,\n+    Dict,\n+    List,\n+    Mapping,\n+    Optional,\n+)\n+\n+from langchain_core.callbacks import (\n+    CallbackManagerForLLMRun,\n+)\n+from langchain_core.language_models.chat_models import (\n+    BaseChatModel,\n+)\n+from langchain_core.messages import (\n+    AIMessage,\n+    BaseMessage,\n+    ChatMessage,\n+    FunctionMessage,\n+    HumanMessage,\n+    SystemMessage,\n+)\n+from langchain_core.outputs import (\n+    ChatGeneration,\n+    ChatResult,\n+)\n+from langchain_core.pydantic_v1 import root_validator\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+def _convert_message_to_dict(message: BaseMessage) -> dict:\n+    if isinstance(message, ChatMessage):\n+        message_dict = {\"role\": message.role, \"content\": message.content}\n+    elif isinstance(message, HumanMessage):\n+        message_dict = {\"role\": \"user\", \"content\": message.content}\n+    elif isinstance(message, AIMessage):\n+        message_dict = {\"role\": \"assistant\", \"content\": message.content}\n+        if \"function_call\" in message.additional_kwargs:\n+            message_dict[\"function_call\"] = message.additional_kwargs[\"function_call\"]\n+    elif isinstance(message, SystemMessage):\n+        message_dict = {\"role\": \"system\", \"content\": message.content}\n+    elif isinstance(message, FunctionMessage):\n+        message_dict = {\n+            \"role\": \"function\",\n+            \"content\": message.content,\n+            \"name\": message.name,\n+        }\n+    else:\n+        raise ValueError(f\"Got unknown type {message}\")\n+    if \"name\" in message.additional_kwargs:\n+        message_dict[\"name\"] = message.additional_kwargs[\"name\"]\n+    return message_dict\n+\n+\n+class ChatMindsdb(BaseChatModel):\n+    \"\"\"A chat model that uses the Mindsdb\"\"\"\n+\n+    model_name: str\n+    project_name: Optional[str] = 'mindsdb'\n+    model_info: Optional[dict] = None\n+    output_col: Optional[str] = None\n+    project_datanode: Optional[Any] = None\n+\n+    @property\n+    def _default_params(self) -> Dict[str, Any]:\n+        return {}\n+\n+    def completion(\n+        self, messages",
    "comment": "nit: missing type annotation",
    "line_number": 73,
    "enriched": "File: mindsdb/integrations/handlers/langchain_handler/mindsdb_chat_model.py\nCode: @@ -0,0 +1,180 @@\n+from __future__ import annotations\n+\n+import logging\n+from typing import (\n+    Any,\n+    Dict,\n+    List,\n+    Mapping,\n+    Optional,\n+)\n+\n+from langchain_core.callbacks import (\n+    CallbackManagerForLLMRun,\n+)\n+from langchain_core.language_models.chat_models import (\n+    BaseChatModel,\n+)\n+from langchain_core.messages import (\n+    AIMessage,\n+    BaseMessage,\n+    ChatMessage,\n+    FunctionMessage,\n+    HumanMessage,\n+    SystemMessage,\n+)\n+from langchain_core.outputs import (\n+    ChatGeneration,\n+    ChatResult,\n+)\n+from langchain_core.pydantic_v1 import root_validator\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+def _convert_message_to_dict(message: BaseMessage) -> dict:\n+    if isinstance(message, ChatMessage):\n+        message_dict = {\"role\": message.role, \"content\": message.content}\n+    elif isinstance(message, HumanMessage):\n+        message_dict = {\"role\": \"user\", \"content\": message.content}\n+    elif isinstance(message, AIMessage):\n+        message_dict = {\"role\": \"assistant\", \"content\": message.content}\n+        if \"function_call\" in message.additional_kwargs:\n+            message_dict[\"function_call\"] = message.additional_kwargs[\"function_call\"]\n+    elif isinstance(message, SystemMessage):\n+        message_dict = {\"role\": \"system\", \"content\": message.content}\n+    elif isinstance(message, FunctionMessage):\n+        message_dict = {\n+            \"role\": \"function\",\n+            \"content\": message.content,\n+            \"name\": message.name,\n+        }\n+    else:\n+        raise ValueError(f\"Got unknown type {message}\")\n+    if \"name\" in message.additional_kwargs:\n+        message_dict[\"name\"] = message.additional_kwargs[\"name\"]\n+    return message_dict\n+\n+\n+class ChatMindsdb(BaseChatModel):\n+    \"\"\"A chat model that uses the Mindsdb\"\"\"\n+\n+    model_name: str\n+    project_name: Optional[str] = 'mindsdb'\n+    model_info: Optional[dict] = None\n+    output_col: Optional[str] = None\n+    project_datanode: Optional[Any] = None\n+\n+    @property\n+    def _default_params(self) -> Dict[str, Any]:\n+        return {}\n+\n+    def completion(\n+        self, messages\nComment: nit: missing type annotation",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/langchain_handler/mindsdb_chat_model.py",
    "pr_number": 9046,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1566550657,
    "comment_created_at": "2024-04-15T23:45:37Z"
  },
  {
    "code": "@@ -0,0 +1,65 @@\n+import os\n+from llama_hub.github_repo import GithubRepositoryReader\n+\n+\n+def _get_github_token(args, connection_args):",
    "comment": "Is it possible to implement this using the `get_api_key()` method from `mindsdb.integrations.utilities.handler_utils`?",
    "line_number": 5,
    "enriched": "File: mindsdb/integrations/handlers/llama_index_handler/github_loader_helper.py\nCode: @@ -0,0 +1,65 @@\n+import os\n+from llama_hub.github_repo import GithubRepositoryReader\n+\n+\n+def _get_github_token(args, connection_args):\nComment: Is it possible to implement this using the `get_api_key()` method from `mindsdb.integrations.utilities.handler_utils`?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/llama_index_handler/github_loader_helper.py",
    "pr_number": 8562,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1440113419,
    "comment_created_at": "2024-01-03T06:34:26Z"
  },
  {
    "code": "@@ -1,2 +1,2 @@\n-pycaret\n-pycaret[models]\n+pycaret[models]==3.3.2\n+scikit-learn==1.2.2",
    "comment": "Do we need scikit-learn? I think this is handled by `pycaret` https://github.com/pycaret/pycaret/blob/master/requirements.txt#L10",
    "line_number": 2,
    "enriched": "File: mindsdb/integrations/handlers/pycaret_handler/requirements.txt\nCode: @@ -1,2 +1,2 @@\n-pycaret\n-pycaret[models]\n+pycaret[models]==3.3.2\n+scikit-learn==1.2.2\nComment: Do we need scikit-learn? I think this is handled by `pycaret` https://github.com/pycaret/pycaret/blob/master/requirements.txt#L10",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/pycaret_handler/requirements.txt",
    "pr_number": 9900,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1804604379,
    "comment_created_at": "2024-10-17T11:35:37Z"
  },
  {
    "code": "@@ -0,0 +1,52 @@\n+### Examples of usage\n+\n+**Create model for chatbot (conversational mode):**\n+\n+Openai model\n+```sql\n+CREATE MODEL airline_model\n+\tPREDICT answer USING\n+\tengine = \"langchain_engine\",\n+\tinput_column = \"question\",\n+\topenai_api_key = \"<open api key>\",\n+\tmode = \"conversational\",\n+\tuser_column = \"question\",\n+\tassistant_column = \"answer\",\n+\tmodel_name = \"gpt-3.5-turbo\",\n+\tverbose=True,\n+\tprompt_template=\"Answer the user input in a helpful way\";\n+```\n+\n+Anyscale provider (defined in 'provider' param)\n+```sql\n+CREATE MODEL airline_model\n+\tPREDICT answer \n+\tUSING\n+\tengine = \"langchain_engine\",\n+\tprovider='anyscale',\n+\tmodel_name = \"mistralai/Mistral-7B-Instruct-v0.1\",\n+\tanyscale_api_key = '<anyscale api key>',",
    "comment": "@ea-rus \r\nThis one should be `anyscale_endpoints_api_key`.",
    "line_number": 28,
    "enriched": "File: mindsdb/integrations/handlers/langchain_handler/README.md\nCode: @@ -0,0 +1,52 @@\n+### Examples of usage\n+\n+**Create model for chatbot (conversational mode):**\n+\n+Openai model\n+```sql\n+CREATE MODEL airline_model\n+\tPREDICT answer USING\n+\tengine = \"langchain_engine\",\n+\tinput_column = \"question\",\n+\topenai_api_key = \"<open api key>\",\n+\tmode = \"conversational\",\n+\tuser_column = \"question\",\n+\tassistant_column = \"answer\",\n+\tmodel_name = \"gpt-3.5-turbo\",\n+\tverbose=True,\n+\tprompt_template=\"Answer the user input in a helpful way\";\n+```\n+\n+Anyscale provider (defined in 'provider' param)\n+```sql\n+CREATE MODEL airline_model\n+\tPREDICT answer \n+\tUSING\n+\tengine = \"langchain_engine\",\n+\tprovider='anyscale',\n+\tmodel_name = \"mistralai/Mistral-7B-Instruct-v0.1\",\n+\tanyscale_api_key = '<anyscale api key>',\nComment: @ea-rus \r\nThis one should be `anyscale_endpoints_api_key`.",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/langchain_handler/README.md",
    "pr_number": 8765,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1494636539,
    "comment_created_at": "2024-02-19T14:33:01Z"
  },
  {
    "code": "@@ -16,119 +16,98 @@ class ConfluencePagesTable(APITable):\n     \"\"\"Confluence Pages Table implementation\"\"\"\n \n     def select(self, query: ast.Select) -> pd.DataFrame:\n-        \"\"\"Pulls data from the Confluence \"get_all_pages_from_space\" API endpoint\n-        Parameters\n-        ----------\n-        query : ast.Select\n-           Given SQL SELECT query\n-        Returns\n-        -------\n-        pd.DataFrame\n-            confluence \"get_all_pages_from_space\" matching the query\n-        Raises\n-        ------\n-        ValueError\n-            If the query contains an unsupported condition\n-        \"\"\"\n+        \"\"\"Pulls data from the Confluence \"get_all_pages_from_space\" API endpoint\"\"\"\n         conditions = extract_comparison_conditions(query.where)\n \n         if query.limit:\n             total_results = query.limit.value\n         else:\n             total_results = 50\n \n-        pages_kwargs = {}\n-        order_by_conditions = {}\n-\n-        if query.order_by and len(query.order_by) > 0:\n-            order_by_conditions[\"columns\"] = []\n-            order_by_conditions[\"ascending\"] = []\n-\n-            for an_order in query.order_by:\n-                if an_order.field.parts[0] != \"\":\n-                    next\n-                if an_order.field.parts[1] in self.get_columns():\n-                    order_by_conditions[\"columns\"].append(an_order.field.parts[1])\n-\n-                    if an_order.direction == \"ASC\":\n-                        order_by_conditions[\"ascending\"].append(True)\n-                    else:\n-                        order_by_conditions[\"ascending\"].append(False)\n-                else:\n-                    raise ValueError(\n-                        f\"Order by unknown column {an_order.field.parts[1]}\"\n-                    )\n+        space_name = None\n+        page_id = None\n \n         for a_where in conditions:\n             if a_where[1] == \"space\":\n                 space_name = a_where[2]\n                 if a_where[0] != \"=\":\n                     raise ValueError(\"Unsupported where operation for space\")\n-                pages_kwargs[\"space\"] = space_name\n-            elif a_where[1] not in [\"id\", \"space\"]:\n-                raise ValueError(f\"Unsupported where argument {a_where[1]}\")\n-\n-        confluence_pages_records = self.handler.connect().get_all_pages_from_space(\n-            space_name, start=0, limit=total_results, expand=\"body.storage\"\n-        )\n-        confluence_pages_df = pd.json_normalize(confluence_pages_records)\n-\n-        for a_where in conditions:\n-            if a_where[1] == \"id\":\n-                id = a_where[2]\n+            elif a_where[1] == \"id\":\n+                page_id = a_where[2]\n                 if a_where[0] != \"=\":\n                     raise ValueError(\"Unsupported where operation for id\")\n-                confluence_pages_df = confluence_pages_df[\n-                    confluence_pages_df.id == id\n-                ]\n-                \n+            elif a_where[1] not in [\"id\", \"space\"]:\n+                raise ValueError(f\"Unsupported where argument {a_where[1]}\")\n \n-        def extract_space(input_string):\n-            parts = input_string.split('/')\n-            return parts[-1]\n+        if space_name is None:\n+            raise ValueError(\"Space name must be provided in the WHERE clause\")\n \n-        confluence_pages_df[\"space\"] = confluence_pages_df[\"_expandable.space\"].apply(\n-            extract_space\n-        )\n+        if page_id is not None:\n+            try:\n+                page = self.handler.connect().get_page_by_id(page_id, expand='body.storage,space')\n+                confluence_pages_records = [page] if page['space']['key'] == space_name else []\n+            except Exception as e:\n+                logger.error(f\"Error fetching page with ID {page_id}: {str(e)}\")\n+                confluence_pages_records = []\n+        else:\n+            confluence_pages_records = self.handler.connect().get_all_pages_from_space(\n+                space_name, start=0, limit=total_results, expand=\"body.storage,space\"\n+            )\n \n-        confluence_pages_df.columns = confluence_pages_df.columns.str.replace(\n-            \"body.storage.value\", \"body\"\n-        )\n+        if not confluence_pages_records:\n+            return pd.DataFrame(columns=self.get_columns())\n \n-        confluence_pages_df = confluence_pages_df[self.get_columns()]\n+        confluence_pages_df = pd.json_normalize(confluence_pages_records)\n \n-        if \"space\" in pages_kwargs:\n-            confluence_pages_df = confluence_pages_df[\n-                confluence_pages_df.space == pages_kwargs[\"space\"]\n-            ]\n+        \n+        confluence_pages_df.columns = confluence_pages_df.columns.str.replace(\"body.storage.value\", \"body\")\n+        confluence_pages_df['space'] = confluence_pages_df['space.key']\n+        \n+        available_columns = set(confluence_pages_df.columns).intersection(set(self.get_columns()))\n+        confluence_pages_df = confluence_pages_df[list(available_columns)]\n \n         selected_columns = []\n         for target in query.targets:\n             if isinstance(target, ast.Star):\n-                selected_columns = self.get_columns()\n+                selected_columns = list(available_columns)\n                 break\n             elif isinstance(target, ast.Identifier):\n-                selected_columns.append(target.parts[-1])\n+                col = target.parts[-1]\n+                if col in available_columns:\n+                    selected_columns.append(col)\n+                else:\n+                    raise ValueError(f\"Unknown column: {col}\")\n             else:\n                 raise ValueError(f\"Unknown query target {type(target)}\")\n \n-        if len(confluence_pages_df) == 0:\n-            confluence_pages_df = pd.DataFrame([], columns=selected_columns)\n-        else:\n-            confluence_pages_df.columns = self.get_columns()\n-            for col in set(confluence_pages_df.columns).difference(\n-                set(selected_columns)\n-            ):\n-                confluence_pages_df = confluence_pages_df.drop(col, axis=1)\n+        confluence_pages_df = confluence_pages_df[selected_columns]\n \n-            if len(order_by_conditions.get(\"columns\", [])) > 0:\n+        if query.order_by and len(query.order_by) > 0:\n+            sort_columns = []\n+            sort_ascending = []\n+            for an_order in query.order_by:\n+                if isinstance(an_order.field, ast.Identifier):\n+                    column = an_order.field.parts[-1]\n+                    if column in selected_columns:\n+                        sort_columns.append(column)\n+                        sort_ascending.append(an_order.direction == \"ASC\")\n+                    else:\n+                        raise ValueError(f\"Order by unknown column {column}\")\n+                else:\n+                    raise ValueError(f\"Unsupported order by clause: {an_order}\")\n+            \n+            if sort_columns:\n                 confluence_pages_df = confluence_pages_df.sort_values(\n-                    by=order_by_conditions[\"columns\"],\n-                    ascending=order_by_conditions[\"ascending\"],\n+                    by=sort_columns,\n+                    ascending=sort_ascending\n                 )\n \n-        return confluence_pages_df\n+        if query.limit:\n+            confluence_pages_df = confluence_pages_df.head(query.limit.value)",
    "comment": "There is a limit being applied when calling the Confluence API. Is there a need to apply this again?",
    "line_number": 106,
    "enriched": "File: mindsdb/integrations/handlers/confluence_handler/confluence_table.py\nCode: @@ -16,119 +16,98 @@ class ConfluencePagesTable(APITable):\n     \"\"\"Confluence Pages Table implementation\"\"\"\n \n     def select(self, query: ast.Select) -> pd.DataFrame:\n-        \"\"\"Pulls data from the Confluence \"get_all_pages_from_space\" API endpoint\n-        Parameters\n-        ----------\n-        query : ast.Select\n-           Given SQL SELECT query\n-        Returns\n-        -------\n-        pd.DataFrame\n-            confluence \"get_all_pages_from_space\" matching the query\n-        Raises\n-        ------\n-        ValueError\n-            If the query contains an unsupported condition\n-        \"\"\"\n+        \"\"\"Pulls data from the Confluence \"get_all_pages_from_space\" API endpoint\"\"\"\n         conditions = extract_comparison_conditions(query.where)\n \n         if query.limit:\n             total_results = query.limit.value\n         else:\n             total_results = 50\n \n-        pages_kwargs = {}\n-        order_by_conditions = {}\n-\n-        if query.order_by and len(query.order_by) > 0:\n-            order_by_conditions[\"columns\"] = []\n-            order_by_conditions[\"ascending\"] = []\n-\n-            for an_order in query.order_by:\n-                if an_order.field.parts[0] != \"\":\n-                    next\n-                if an_order.field.parts[1] in self.get_columns():\n-                    order_by_conditions[\"columns\"].append(an_order.field.parts[1])\n-\n-                    if an_order.direction == \"ASC\":\n-                        order_by_conditions[\"ascending\"].append(True)\n-                    else:\n-                        order_by_conditions[\"ascending\"].append(False)\n-                else:\n-                    raise ValueError(\n-                        f\"Order by unknown column {an_order.field.parts[1]}\"\n-                    )\n+        space_name = None\n+        page_id = None\n \n         for a_where in conditions:\n             if a_where[1] == \"space\":\n                 space_name = a_where[2]\n                 if a_where[0] != \"=\":\n                     raise ValueError(\"Unsupported where operation for space\")\n-                pages_kwargs[\"space\"] = space_name\n-            elif a_where[1] not in [\"id\", \"space\"]:\n-                raise ValueError(f\"Unsupported where argument {a_where[1]}\")\n-\n-        confluence_pages_records = self.handler.connect().get_all_pages_from_space(\n-            space_name, start=0, limit=total_results, expand=\"body.storage\"\n-        )\n-        confluence_pages_df = pd.json_normalize(confluence_pages_records)\n-\n-        for a_where in conditions:\n-            if a_where[1] == \"id\":\n-                id = a_where[2]\n+            elif a_where[1] == \"id\":\n+                page_id = a_where[2]\n                 if a_where[0] != \"=\":\n                     raise ValueError(\"Unsupported where operation for id\")\n-                confluence_pages_df = confluence_pages_df[\n-                    confluence_pages_df.id == id\n-                ]\n-                \n+            elif a_where[1] not in [\"id\", \"space\"]:\n+                raise ValueError(f\"Unsupported where argument {a_where[1]}\")\n \n-        def extract_space(input_string):\n-            parts = input_string.split('/')\n-            return parts[-1]\n+        if space_name is None:\n+            raise ValueError(\"Space name must be provided in the WHERE clause\")\n \n-        confluence_pages_df[\"space\"] = confluence_pages_df[\"_expandable.space\"].apply(\n-            extract_space\n-        )\n+        if page_id is not None:\n+            try:\n+                page = self.handler.connect().get_page_by_id(page_id, expand='body.storage,space')\n+                confluence_pages_records = [page] if page['space']['key'] == space_name else []\n+            except Exception as e:\n+                logger.error(f\"Error fetching page with ID {page_id}: {str(e)}\")\n+                confluence_pages_records = []\n+        else:\n+            confluence_pages_records = self.handler.connect().get_all_pages_from_space(\n+                space_name, start=0, limit=total_results, expand=\"body.storage,space\"\n+            )\n \n-        confluence_pages_df.columns = confluence_pages_df.columns.str.replace(\n-            \"body.storage.value\", \"body\"\n-        )\n+        if not confluence_pages_records:\n+            return pd.DataFrame(columns=self.get_columns())\n \n-        confluence_pages_df = confluence_pages_df[self.get_columns()]\n+        confluence_pages_df = pd.json_normalize(confluence_pages_records)\n \n-        if \"space\" in pages_kwargs:\n-            confluence_pages_df = confluence_pages_df[\n-                confluence_pages_df.space == pages_kwargs[\"space\"]\n-            ]\n+        \n+        confluence_pages_df.columns = confluence_pages_df.columns.str.replace(\"body.storage.value\", \"body\")\n+        confluence_pages_df['space'] = confluence_pages_df['space.key']\n+        \n+        available_columns = set(confluence_pages_df.columns).intersection(set(self.get_columns()))\n+        confluence_pages_df = confluence_pages_df[list(available_columns)]\n \n         selected_columns = []\n         for target in query.targets:\n             if isinstance(target, ast.Star):\n-                selected_columns = self.get_columns()\n+                selected_columns = list(available_columns)\n                 break\n             elif isinstance(target, ast.Identifier):\n-                selected_columns.append(target.parts[-1])\n+                col = target.parts[-1]\n+                if col in available_columns:\n+                    selected_columns.append(col)\n+                else:\n+                    raise ValueError(f\"Unknown column: {col}\")\n             else:\n                 raise ValueError(f\"Unknown query target {type(target)}\")\n \n-        if len(confluence_pages_df) == 0:\n-            confluence_pages_df = pd.DataFrame([], columns=selected_columns)\n-        else:\n-            confluence_pages_df.columns = self.get_columns()\n-            for col in set(confluence_pages_df.columns).difference(\n-                set(selected_columns)\n-            ):\n-                confluence_pages_df = confluence_pages_df.drop(col, axis=1)\n+        confluence_pages_df = confluence_pages_df[selected_columns]\n \n-            if len(order_by_conditions.get(\"columns\", [])) > 0:\n+        if query.order_by and len(query.order_by) > 0:\n+            sort_columns = []\n+            sort_ascending = []\n+            for an_order in query.order_by:\n+                if isinstance(an_order.field, ast.Identifier):\n+                    column = an_order.field.parts[-1]\n+                    if column in selected_columns:\n+                        sort_columns.append(column)\n+                        sort_ascending.append(an_order.direction == \"ASC\")\n+                    else:\n+                        raise ValueError(f\"Order by unknown column {column}\")\n+                else:\n+                    raise ValueError(f\"Unsupported order by clause: {an_order}\")\n+            \n+            if sort_columns:\n                 confluence_pages_df = confluence_pages_df.sort_values(\n-                    by=order_by_conditions[\"columns\"],\n-                    ascending=order_by_conditions[\"ascending\"],\n+                    by=sort_columns,\n+                    ascending=sort_ascending\n                 )\n \n-        return confluence_pages_df\n+        if query.limit:\n+            confluence_pages_df = confluence_pages_df.head(query.limit.value)\nComment: There is a limit being applied when calling the Confluence API. Is there a need to apply this again?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/confluence_handler/confluence_table.py",
    "pr_number": 9924,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1810878453,
    "comment_created_at": "2024-10-22T14:47:32Z"
  },
  {
    "code": "@@ -60,7 +60,8 @@ def __init__(self, name: str, connection_data: Optional[dict], **kwargs):\n     def connect(self) -> Connection:\n         if self.is_connected is True:\n             return self.connection\n-\n+        d = None  # default suitable for Linux\n+        oracledb.init_oracle_client(lib_dir=d)",
    "comment": "@tanbirali Can you please provide more info on how this provides the thick mode support? Is this working ",
    "line_number": 64,
    "enriched": "File: mindsdb/integrations/handlers/oracle_handler/oracle_handler.py\nCode: @@ -60,7 +60,8 @@ def __init__(self, name: str, connection_data: Optional[dict], **kwargs):\n     def connect(self) -> Connection:\n         if self.is_connected is True:\n             return self.connection\n-\n+        d = None  # default suitable for Linux\n+        oracledb.init_oracle_client(lib_dir=d)\nComment: @tanbirali Can you please provide more info on how this provides the thick mode support? Is this working ",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/oracle_handler/oracle_handler.py",
    "pr_number": 7760,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1374468361,
    "comment_created_at": "2023-10-27T11:58:02Z"
  },
  {
    "code": "@@ -0,0 +1,58 @@\n+# PostgreSQL Integration\n+\n+This documentation describes the integration of MindsDB with PostgreSQL, a powerful, open-source, object-relational database system. The integration allows for advanced SQL functionalities, extending PostgreSQL's capabilities with MindsDB's features.\n+\n+## Getting Started\n+\n+### Prerequisites\n+\n+   1. Ensure that MindsDB and PostgreSQL are installed on your system or you have access to cloud options.\n+   2. If running locally install the dependencies as `pip install [postgres]`.",
    "comment": "This should be changed to `pip install mindsdb[postgres]`?\r\n",
    "line_number": 10,
    "enriched": "File: mindsdb/integrations/handlers/postgres_handler/README.md\nCode: @@ -0,0 +1,58 @@\n+# PostgreSQL Integration\n+\n+This documentation describes the integration of MindsDB with PostgreSQL, a powerful, open-source, object-relational database system. The integration allows for advanced SQL functionalities, extending PostgreSQL's capabilities with MindsDB's features.\n+\n+## Getting Started\n+\n+### Prerequisites\n+\n+   1. Ensure that MindsDB and PostgreSQL are installed on your system or you have access to cloud options.\n+   2. If running locally install the dependencies as `pip install [postgres]`.\nComment: This should be changed to `pip install mindsdb[postgres]`?\r\n",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/postgres_handler/README.md",
    "pr_number": 8680,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1467279962,
    "comment_created_at": "2024-01-26T06:50:36Z"
  },
  {
    "code": "@@ -130,13 +130,16 @@ def adapt_query(node, is_table, **kwargs):\n                 node.parts = [node.parts[-1]]\n                 return node\n         if isinstance(node, Function):\n+            # These functions also exist in DuckDB, therefore, the results need to be replaced.\n+            fnc_results = {\n+                \"database\": session.database if session else None,\n+                \"version\": \"8.0.17\",",
    "comment": " What is it for? Was some test broken without it?",
    "line_number": 136,
    "enriched": "File: mindsdb/api/executor/utilities/sql.py\nCode: @@ -130,13 +130,16 @@ def adapt_query(node, is_table, **kwargs):\n                 node.parts = [node.parts[-1]]\n                 return node\n         if isinstance(node, Function):\n+            # These functions also exist in DuckDB, therefore, the results need to be replaced.\n+            fnc_results = {\n+                \"database\": session.database if session else None,\n+                \"version\": \"8.0.17\",\nComment:  What is it for? Was some test broken without it?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/api/executor/utilities/sql.py",
    "pr_number": 10699,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2052074523,
    "comment_created_at": "2025-04-21T07:28:57Z"
  },
  {
    "code": "@@ -0,0 +1,69 @@\n+from typing import List\n+\n+from langchain_community.agent_toolkits.sql.toolkit import SQLDatabaseToolkit\n+from langchain_community.tools import ListSQLDatabaseTool, InfoSQLDatabaseTool, QuerySQLDataBaseTool, \\\n+    QuerySQLCheckerTool\n+from langchain_core.tools import BaseTool\n+\n+from mindsdb.interfaces.skills.custom.text2sql.mindsdb_sql_tool import MindsDBSQLParserTool\n+\n+\n+class MindsDBSQLToolkit(SQLDatabaseToolkit):\n+\n+    def get_tools(self) -> List[BaseTool]:\n+        # Return the tools that this toolkit provides as well as MindDB's SQL validator tool\n+\n+        \"\"\"Get the tools in the toolkit.\"\"\"\n+        list_sql_database_tool = ListSQLDatabaseTool(db=self.db)\n+        info_sql_database_tool_description = (\n+            \"Input to this tool is a comma-separated list of tables, output is the \"\n+            \"schema and sample rows for those tables. \"\n+            \"Be sure that the tables actually exist by calling \"\n+            f\"{list_sql_database_tool.name} first! \"\n+            \"Example Input: table1, table2, table3\"\n+        )\n+        info_sql_database_tool = InfoSQLDatabaseTool(\n+            db=self.db, description=info_sql_database_tool_description\n+        )\n+        query_sql_database_tool_description = (\n+            \"Input to this tool is a detailed and correct SQL query, output is a \"\n+            \"result from the database. If the query is not correct, an error message \"\n+            \"will be returned. If an error is returned, rewrite the query, check the \"\n+            \"query, and try again. If you encounter an issue with Unknown column \"\n+            f\"'xxxx' in 'field list', use {info_sql_database_tool.name} \"\n+            \"to query the correct table fields.\"\n+        )\n+        query_sql_database_tool = QuerySQLDataBaseTool(\n+            db=self.db, description=query_sql_database_tool_description\n+        )\n+\n+        mindsdb_sql_parser_tool_description = (\n+            \"Use this tool to ensure that a SQL query passes the MindsDB SQL parser.\"",
    "comment": "Do the llm need to know that it is MindsDB server or is better to rewrite 'to ensure that a SQL query passes parser of the database server.'\r\nI don't know the answer, just wondering",
    "line_number": 41,
    "enriched": "File: mindsdb/interfaces/skills/custom/text2sql/mindsdb_sql_toolkit.py\nCode: @@ -0,0 +1,69 @@\n+from typing import List\n+\n+from langchain_community.agent_toolkits.sql.toolkit import SQLDatabaseToolkit\n+from langchain_community.tools import ListSQLDatabaseTool, InfoSQLDatabaseTool, QuerySQLDataBaseTool, \\\n+    QuerySQLCheckerTool\n+from langchain_core.tools import BaseTool\n+\n+from mindsdb.interfaces.skills.custom.text2sql.mindsdb_sql_tool import MindsDBSQLParserTool\n+\n+\n+class MindsDBSQLToolkit(SQLDatabaseToolkit):\n+\n+    def get_tools(self) -> List[BaseTool]:\n+        # Return the tools that this toolkit provides as well as MindDB's SQL validator tool\n+\n+        \"\"\"Get the tools in the toolkit.\"\"\"\n+        list_sql_database_tool = ListSQLDatabaseTool(db=self.db)\n+        info_sql_database_tool_description = (\n+            \"Input to this tool is a comma-separated list of tables, output is the \"\n+            \"schema and sample rows for those tables. \"\n+            \"Be sure that the tables actually exist by calling \"\n+            f\"{list_sql_database_tool.name} first! \"\n+            \"Example Input: table1, table2, table3\"\n+        )\n+        info_sql_database_tool = InfoSQLDatabaseTool(\n+            db=self.db, description=info_sql_database_tool_description\n+        )\n+        query_sql_database_tool_description = (\n+            \"Input to this tool is a detailed and correct SQL query, output is a \"\n+            \"result from the database. If the query is not correct, an error message \"\n+            \"will be returned. If an error is returned, rewrite the query, check the \"\n+            \"query, and try again. If you encounter an issue with Unknown column \"\n+            f\"'xxxx' in 'field list', use {info_sql_database_tool.name} \"\n+            \"to query the correct table fields.\"\n+        )\n+        query_sql_database_tool = QuerySQLDataBaseTool(\n+            db=self.db, description=query_sql_database_tool_description\n+        )\n+\n+        mindsdb_sql_parser_tool_description = (\n+            \"Use this tool to ensure that a SQL query passes the MindsDB SQL parser.\"\nComment: Do the llm need to know that it is MindsDB server or is better to rewrite 'to ensure that a SQL query passes parser of the database server.'\r\nI don't know the answer, just wondering",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/interfaces/skills/custom/text2sql/mindsdb_sql_toolkit.py",
    "pr_number": 9411,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1660885131,
    "comment_created_at": "2024-07-01T11:14:42Z"
  },
  {
    "code": "@@ -0,0 +1,50 @@\n+from mindsdb.integrations.libs.api_handler import APIHandler, APITable, FuncParser\n+\n+from mindsdb.integrations.libs.response import (\n+    HandlerStatusResponse as StatusResponse,\n+    HandlerResponse as Response,\n+    RESPONSE_TYPE\n+)\n+\n+\n+class TwitterHandler(APIHandler):",
    "comment": "why is this here?",
    "line_number": 10,
    "enriched": "File: mindsdb/integrations/handlers/spotify_handler/spotify_handler.py\nCode: @@ -0,0 +1,50 @@\n+from mindsdb.integrations.libs.api_handler import APIHandler, APITable, FuncParser\n+\n+from mindsdb.integrations.libs.response import (\n+    HandlerStatusResponse as StatusResponse,\n+    HandlerResponse as Response,\n+    RESPONSE_TYPE\n+)\n+\n+\n+class TwitterHandler(APIHandler):\nComment: why is this here?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/spotify_handler/spotify_handler.py",
    "pr_number": 5612,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1165182788,
    "comment_created_at": "2023-04-13T08:26:14Z"
  },
  {
    "code": "@@ -405,7 +405,7 @@ def length(self):\n \n \n class SQLQuery():\n-    def __init__(self, sql, session, execute=True):\n+    def __init__(self, sql, session, execute=True, prepare=False):",
    "comment": "Does this change is required? It seems not used\r\n",
    "line_number": 408,
    "enriched": "File: mindsdb/api/mysql/mysql_proxy/classes/sql_query.py\nCode: @@ -405,7 +405,7 @@ def length(self):\n \n \n class SQLQuery():\n-    def __init__(self, sql, session, execute=True):\n+    def __init__(self, sql, session, execute=True, prepare=False):\nComment: Does this change is required? It seems not used\r\n",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/api/mysql/mysql_proxy/classes/sql_query.py",
    "pr_number": 5649,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1172563513,
    "comment_created_at": "2023-04-20T13:08:39Z"
  },
  {
    "code": "@@ -0,0 +1,9 @@\n+{",
    "comment": "This should probably be removed.",
    "line_number": 1,
    "enriched": "File: config.json\nCode: @@ -0,0 +1,9 @@\n+{\nComment: This should probably be removed.",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "config.json",
    "pr_number": 9012,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1546196953,
    "comment_created_at": "2024-04-01T10:31:58Z"
  },
  {
    "code": "@@ -27,11 +27,11 @@ WHERE name = 'home_rentals';\n On execution, we get:",
    "comment": "About this type of queries\r\n`SELECT * FROM mindsdb.models WHERE name = 'home_rentals';`\r\nwe can replace to:\r\n- `describe model home_rentals`\r\n- or `show models where name='home_rentals`",
    "line_number": 27,
    "enriched": "File: docs/mindsdb_sql/sql/api/manage-models-versions.mdx\nCode: @@ -27,11 +27,11 @@ WHERE name = 'home_rentals';\n On execution, we get:\nComment: About this type of queries\r\n`SELECT * FROM mindsdb.models WHERE name = 'home_rentals';`\r\nwe can replace to:\r\n- `describe model home_rentals`\r\n- or `show models where name='home_rentals`",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "docs/mindsdb_sql/sql/api/manage-models-versions.mdx",
    "pr_number": 9322,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1633417272,
    "comment_created_at": "2024-06-10T15:08:26Z"
  },
  {
    "code": "@@ -8,47 +8,49 @@\n from langchain.schema import SystemMessage\n from langchain.agents import AgentType\n from langchain.llms import OpenAI\n-from langchain.chat_models import ChatOpenAI  # GPT-4 fails to follow the output langchain requires, avoid using for now\n+from langchain.chat_models import ChatAnthropic, ChatOpenAI  # GPT-4 fails to follow the output langchain requires, avoid using for now\n from langchain.agents import initialize_agent, load_tools, Tool, create_sql_agent\n from langchain.prompts import PromptTemplate\n from langchain.utilities import GoogleSerperAPIWrapper\n from langchain.agents.agent_toolkits import SQLDatabaseToolkit\n from langchain.chains.conversation.memory import ConversationSummaryBufferMemory\n \n-from mindsdb.integrations.handlers.openai_handler.openai_handler import OpenAIHandler, CHAT_MODELS\n+from mindsdb.integrations.handlers.openai_handler.openai_handler import CHAT_MODELS as OPEN_AI_CHAT_MODELS\n from mindsdb.integrations.handlers.langchain_handler.mindsdb_database_agent import MindsDBSQL\n+from mindsdb.integrations.libs.base import BaseMLEngine\n+from mindsdb.integrations.utilities.handler_utils import get_api_key\n from mindsdb_sql import parse_sql, Insert\n \n \n-_DEFAULT_MODEL = 'gpt-3.5-turbo'  # TODO: enable other LLM backends (AI21, Anthropic, etc.)\n+_DEFAULT_MODEL = 'gpt-3.5-turbo'\n _DEFAULT_MAX_TOKENS = 2048  # requires more than vanilla OpenAI due to ongoing summarization and 3rd party input\n _DEFAULT_AGENT_MODEL = 'zero-shot-react-description'\n _DEFAULT_AGENT_TOOLS = ['python_repl', 'wikipedia']  # these require no additional arguments\n+_ANTHROPIC_CHAT_MODELS = {'claude-2', 'claude-instant-1'}\n+_PARSING_ERROR_PREFIX = 'Could not parse LLM output: `'\n \n-\n-class LangChainHandler(OpenAIHandler):\n+class LangChainHandler(BaseMLEngine):\n     \"\"\"\n     This is a MindsDB integration for the LangChain library, which provides a unified interface for interacting with\n     various large language models (LLMs).\n \n-    Currently, this integration supports exposing OpenAI's LLMs with normal text completion support. They are then\n+    Currently, this integration supports exposing OpenAI and Anthrophic's LLMs with normal text completion support. They are then\n     wrapped in a zero shot react description agent that offers a few third party tools out of the box, with support\n     for additional ones if an API key is provided. Ongoing memory is also provided.\n \n     Full tool support list:\n         - wikipedia\n         - python_repl\n         - serper.dev search\n-\n-    This integration inherits from the OpenAI engine, so it shares a lot of the requirements, features (e.g. prompt\n-    templating) and limitations.\n     \"\"\"\n     name = 'langchain'\n \n     def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n         self.generative = True\n         self.stops = []\n+        self.default_mode = 'default'  # can also be 'conversational' or 'conversational-full'\n+        self.supported_modes = ['default', 'conversational', 'conversational-full', 'image']",
    "comment": "Are Anthropic models multi-modal? If not, then `image` is probably not a supported mode in LangChain's case.",
    "line_number": 53,
    "enriched": "File: mindsdb/integrations/handlers/langchain_handler/langchain_handler.py\nCode: @@ -8,47 +8,49 @@\n from langchain.schema import SystemMessage\n from langchain.agents import AgentType\n from langchain.llms import OpenAI\n-from langchain.chat_models import ChatOpenAI  # GPT-4 fails to follow the output langchain requires, avoid using for now\n+from langchain.chat_models import ChatAnthropic, ChatOpenAI  # GPT-4 fails to follow the output langchain requires, avoid using for now\n from langchain.agents import initialize_agent, load_tools, Tool, create_sql_agent\n from langchain.prompts import PromptTemplate\n from langchain.utilities import GoogleSerperAPIWrapper\n from langchain.agents.agent_toolkits import SQLDatabaseToolkit\n from langchain.chains.conversation.memory import ConversationSummaryBufferMemory\n \n-from mindsdb.integrations.handlers.openai_handler.openai_handler import OpenAIHandler, CHAT_MODELS\n+from mindsdb.integrations.handlers.openai_handler.openai_handler import CHAT_MODELS as OPEN_AI_CHAT_MODELS\n from mindsdb.integrations.handlers.langchain_handler.mindsdb_database_agent import MindsDBSQL\n+from mindsdb.integrations.libs.base import BaseMLEngine\n+from mindsdb.integrations.utilities.handler_utils import get_api_key\n from mindsdb_sql import parse_sql, Insert\n \n \n-_DEFAULT_MODEL = 'gpt-3.5-turbo'  # TODO: enable other LLM backends (AI21, Anthropic, etc.)\n+_DEFAULT_MODEL = 'gpt-3.5-turbo'\n _DEFAULT_MAX_TOKENS = 2048  # requires more than vanilla OpenAI due to ongoing summarization and 3rd party input\n _DEFAULT_AGENT_MODEL = 'zero-shot-react-description'\n _DEFAULT_AGENT_TOOLS = ['python_repl', 'wikipedia']  # these require no additional arguments\n+_ANTHROPIC_CHAT_MODELS = {'claude-2', 'claude-instant-1'}\n+_PARSING_ERROR_PREFIX = 'Could not parse LLM output: `'\n \n-\n-class LangChainHandler(OpenAIHandler):\n+class LangChainHandler(BaseMLEngine):\n     \"\"\"\n     This is a MindsDB integration for the LangChain library, which provides a unified interface for interacting with\n     various large language models (LLMs).\n \n-    Currently, this integration supports exposing OpenAI's LLMs with normal text completion support. They are then\n+    Currently, this integration supports exposing OpenAI and Anthrophic's LLMs with normal text completion support. They are then\n     wrapped in a zero shot react description agent that offers a few third party tools out of the box, with support\n     for additional ones if an API key is provided. Ongoing memory is also provided.\n \n     Full tool support list:\n         - wikipedia\n         - python_repl\n         - serper.dev search\n-\n-    This integration inherits from the OpenAI engine, so it shares a lot of the requirements, features (e.g. prompt\n-    templating) and limitations.\n     \"\"\"\n     name = 'langchain'\n \n     def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n         self.generative = True\n         self.stops = []\n+        self.default_mode = 'default'  # can also be 'conversational' or 'conversational-full'\n+        self.supported_modes = ['default', 'conversational', 'conversational-full', 'image']\nComment: Are Anthropic models multi-modal? If not, then `image` is probably not a supported mode in LangChain's case.",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/langchain_handler/langchain_handler.py",
    "pr_number": 6912,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1271543736,
    "comment_created_at": "2023-07-23T18:58:30Z"
  },
  {
    "code": "@@ -35,60 +35,75 @@ Required connection parameters include the following:\n Microsoft Entra ID was previously known as Azure Active Directory (Azure AD).\n </Note>\n \n-## Usage\n-\n-This integration can only be used to create chatbots for Microsoft Teams via the [`CREATE CHATBOT`](/agents/chatbot) syntax. Currently, *it cannot be used as a data source for other workloads*.\n+### How to set up Microsoft Teams app",
    "comment": "I wonder if it is correct to put it this way. You will also be setting up the chat-bot in MindsDB during this process. ",
    "line_number": 38,
    "enriched": "File: docs/integrations/app-integrations/microsoft-teams.mdx\nCode: @@ -35,60 +35,75 @@ Required connection parameters include the following:\n Microsoft Entra ID was previously known as Azure Active Directory (Azure AD).\n </Note>\n \n-## Usage\n-\n-This integration can only be used to create chatbots for Microsoft Teams via the [`CREATE CHATBOT`](/agents/chatbot) syntax. Currently, *it cannot be used as a data source for other workloads*.\n+### How to set up Microsoft Teams app\nComment: I wonder if it is correct to put it this way. You will also be setting up the chat-bot in MindsDB during this process. ",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "docs/integrations/app-integrations/microsoft-teams.mdx",
    "pr_number": 10102,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1825442189,
    "comment_created_at": "2024-11-01T05:30:46Z"
  },
  {
    "code": "@@ -506,6 +516,74 @@ def update_agent(\n \n         return existing_agent\n \n+    def _run_data_catalog_loader_for_skill(\n+        self,\n+        skill: Union[str, db.Skills],\n+        project_name: str,\n+        tables: List[str] = None,\n+    ):\n+        \"\"\"\n+        Runs Data Catalog loader for a skill if enabled in the config.\n+        This is used to load metadata for SQL skills when they are added or updated.\n+        \"\"\"\n+        if not config.get(\"data_catalog\", {}).get(\"enabled\", False):\n+            return\n+\n+        skill = skill if isinstance(skill, db.Skills) else self.skills_controller.get_skill(skill, project_name)\n+        if skill.type == \"sql\":\n+            if \"database\" in skill.params:\n+                valid_table_names = skill.params.get(\"tables\") if skill.params.get(\"tables\") else tables\n+                data_catalog_loader = DataCatalogLoader(\n+                    database_name=skill.params[\"database\"], table_names=valid_table_names\n+                )\n+                data_catalog_loader.load_metadata()\n+            else:\n+                raise ValueError(\n+                    \"Data Catalog loading is enabled, but the provided parameters for the new skills are insufficient to load metadata. \"\n+                )\n+\n+    def _run_data_catalog_loader_for_table_entries(\n+        self,\n+        table_entries: List[str],\n+        project_name: str,\n+        skill: Union[str, db.Skills] = None,\n+    ):\n+        \"\"\"\n+        Runs Data Catalog loader for a list of table entries if enabled in the config.\n+        This is used to load metadata for SQL skills when they are added or updated.\n+        \"\"\"\n+        if not config.get(\"data_catalog\", {}).get(\"enabled\", False):\n+            return\n+\n+        skill = skill if isinstance(skill, db.Skills) else self.skills_controller.get_skill(skill, project_name)\n+        if not skill or skill.type == \"sql\":\n+            database_table_map = {}\n+            for table_entry in table_entries:\n+                parts = table_entry.split(\".\", 1)\n+\n+                # Ensure the table name is in 'database.table' format.\n+                if len(parts) != 2:\n+                    raise ValueError(f\"Invalid table name format: {table_entry}. Expected 'database.table' format.\")",
    "comment": "Maybe we make this to silently fail so we continue loading the catalog for other tables?",
    "line_number": 566,
    "enriched": "File: mindsdb/interfaces/agents/agents_controller.py\nCode: @@ -506,6 +516,74 @@ def update_agent(\n \n         return existing_agent\n \n+    def _run_data_catalog_loader_for_skill(\n+        self,\n+        skill: Union[str, db.Skills],\n+        project_name: str,\n+        tables: List[str] = None,\n+    ):\n+        \"\"\"\n+        Runs Data Catalog loader for a skill if enabled in the config.\n+        This is used to load metadata for SQL skills when they are added or updated.\n+        \"\"\"\n+        if not config.get(\"data_catalog\", {}).get(\"enabled\", False):\n+            return\n+\n+        skill = skill if isinstance(skill, db.Skills) else self.skills_controller.get_skill(skill, project_name)\n+        if skill.type == \"sql\":\n+            if \"database\" in skill.params:\n+                valid_table_names = skill.params.get(\"tables\") if skill.params.get(\"tables\") else tables\n+                data_catalog_loader = DataCatalogLoader(\n+                    database_name=skill.params[\"database\"], table_names=valid_table_names\n+                )\n+                data_catalog_loader.load_metadata()\n+            else:\n+                raise ValueError(\n+                    \"Data Catalog loading is enabled, but the provided parameters for the new skills are insufficient to load metadata. \"\n+                )\n+\n+    def _run_data_catalog_loader_for_table_entries(\n+        self,\n+        table_entries: List[str],\n+        project_name: str,\n+        skill: Union[str, db.Skills] = None,\n+    ):\n+        \"\"\"\n+        Runs Data Catalog loader for a list of table entries if enabled in the config.\n+        This is used to load metadata for SQL skills when they are added or updated.\n+        \"\"\"\n+        if not config.get(\"data_catalog\", {}).get(\"enabled\", False):\n+            return\n+\n+        skill = skill if isinstance(skill, db.Skills) else self.skills_controller.get_skill(skill, project_name)\n+        if not skill or skill.type == \"sql\":\n+            database_table_map = {}\n+            for table_entry in table_entries:\n+                parts = table_entry.split(\".\", 1)\n+\n+                # Ensure the table name is in 'database.table' format.\n+                if len(parts) != 2:\n+                    raise ValueError(f\"Invalid table name format: {table_entry}. Expected 'database.table' format.\")\nComment: Maybe we make this to silently fail so we continue loading the catalog for other tables?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/interfaces/agents/agents_controller.py",
    "pr_number": 11329,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2219202586,
    "comment_created_at": "2025-07-21T13:22:26Z"
  },
  {
    "code": "@@ -34,6 +34,10 @@ def find_to_ast(query, database):\n             filter=filter,\n         )\n         table_select.parentheses = True\n+        table_select.alias = Identifier(query['filter']['collection'])\n+        table_select.alias.parts = [table_select.alias.parts[-1]]\n+        if 'limit' in query:",
    "comment": "I think this looks better: \r\n```\r\n       table_select.alias = Identifier(\r\n          parts=query['filter']['collection'][-1:]\r\n        )\r\n```\r\n",
    "line_number": 39,
    "enriched": "File: mindsdb/api/mongo/responders/find.py\nCode: @@ -34,6 +34,10 @@ def find_to_ast(query, database):\n             filter=filter,\n         )\n         table_select.parentheses = True\n+        table_select.alias = Identifier(query['filter']['collection'])\n+        table_select.alias.parts = [table_select.alias.parts[-1]]\n+        if 'limit' in query:\nComment: I think this looks better: \r\n```\r\n       table_select.alias = Identifier(\r\n          parts=query['filter']['collection'][-1:]\r\n        )\r\n```\r\n",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/api/mongo/responders/find.py",
    "pr_number": 7215,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1311690911,
    "comment_created_at": "2023-08-31T14:05:34Z"
  },
  {
    "code": "@@ -100,63 +117,57 @@ def select_query(self, query: Select) -> pd.DataFrame:\n                 targets.append(target)\n         query.targets = targets\n         logger.debug(f\"Modified query targets: {targets}\")\n-\n         # Get response from vector db\n         db_handler = self.get_vector_db()\n         logger.debug(f\"Using vector db handler: {type(db_handler)}\")\n-\n         df = db_handler.dispatch_select(query)\n-\n-        if df is not None:\n-\n-            logger.debug(f\"Query returned {len(df)} rows\")\n-            logger.debug(f\"Columns in response: {df.columns.tolist()}\")\n-            # Log a sample of IDs to help diagnose issues\n-            if not df.empty:\n-                logger.debug(f\"Sample of IDs in response: {df['id'].head().tolist()}\")\n-        else:\n+        if df is None or df.empty:\n             logger.warning(\"Query returned no data\")\n-\n+            # Return empty DataFrame with appropriate columns\n+            columns = [TableField.ID.value, TableField.CONTENT.value, TableField.METADATA.value]\n+            if relevance_requested:\n+                columns.append(relevance_column)\n+            return pd.DataFrame(columns=columns)\n+        logger.debug(f\"Query returned {len(df)} rows\")\n+        logger.debug(f\"Columns in response: {df.columns.tolist()}\")\n+        # Check if we have a rerank_model configured in KB params\n         rerank_model = self._kb.params.get(\"rerank_model\")\n-        if rerank_model and df is not None and not df.empty:\n+        if rerank_model and query_text:\n+            # Use reranker for relevance score\n             try:\n-                logger.info(f\"Using reranker model: {rerank_model}\")\n+                logger.info(f\"Using reranker model {rerank_model} for relevance calculation\")\n                 reranker = LLMReranker(model=rerank_model)\n-                # convert response from a dataframe to a list of strings\n-                content_column = df[TableField.CONTENT.value]\n-                # convert to list\n-                documents = content_column.tolist()\n-                # Extract query text from WHERE clause if it exists\n-                query_text = \"\"\n-                if query.where:\n-                    def extract_content(node, **kwargs):\n-                        nonlocal query_text\n-                        is_binary_op = isinstance(node, BinaryOperation)\n-                        is_identifier = isinstance(node.args[0], Identifier)\n-                        is_content = node.args[0].parts[-1].lower() == 'content'\n-                        is_constant = isinstance(node.args[1], Constant)\n-                        if is_binary_op and is_identifier and is_content and is_constant:\n-                            query_text = node.args[1].value\n-                    query_traversal(query.where, extract_content)\n-                    logger.debug(f\"Extracted query text: {query_text}\")\n-                # Get scores from reranker\n+                # Get documents to rerank\n+                documents = df[TableField.CONTENT.value].tolist()\n+                # Use the get_scores method with disable_events=True\n                 scores = reranker.get_scores(query_text, documents)\n-                # Add scores as a new column for filtering\n+                # Add scores as the relevance column\n+                df[relevance_column] = scores\n+                # Sort by relevance\n+                df = df.sort_values(by=relevance_column, ascending=False)\n+                # Filter by threshold\n                 scores_array = np.array(scores)\n-                # Add temporary column for sorting\n-                df['_relevance_score'] = scores\n-                # Filter by score threshold using numpy array for element-wise comparison\n                 df = df[scores_array > reranker.filtering_threshold]\n-                # Sort by relevance (higher score = more relevant)\n-                df = df.sort_values(by='_relevance_score', ascending=False)\n-                # Remove temporary column\n-                # df = df.drop(columns=['_relevance_score'])\n-                # Apply original limit if it exists\n-                if query.limit and len(df) > query.limit.value:\n-                    df = df.iloc[:query.limit.value]\n                 logger.debug(f\"Applied reranking with model {rerank_model}\")\n             except Exception as e:",
    "comment": "what kind of error we expect here? \r\nI would leave only expected exceptions. If user has error because openai balance is empty, should he know about it (and see this error)? ",
    "line_number": 152,
    "enriched": "File: mindsdb/interfaces/knowledge_base/controller.py\nCode: @@ -100,63 +117,57 @@ def select_query(self, query: Select) -> pd.DataFrame:\n                 targets.append(target)\n         query.targets = targets\n         logger.debug(f\"Modified query targets: {targets}\")\n-\n         # Get response from vector db\n         db_handler = self.get_vector_db()\n         logger.debug(f\"Using vector db handler: {type(db_handler)}\")\n-\n         df = db_handler.dispatch_select(query)\n-\n-        if df is not None:\n-\n-            logger.debug(f\"Query returned {len(df)} rows\")\n-            logger.debug(f\"Columns in response: {df.columns.tolist()}\")\n-            # Log a sample of IDs to help diagnose issues\n-            if not df.empty:\n-                logger.debug(f\"Sample of IDs in response: {df['id'].head().tolist()}\")\n-        else:\n+        if df is None or df.empty:\n             logger.warning(\"Query returned no data\")\n-\n+            # Return empty DataFrame with appropriate columns\n+            columns = [TableField.ID.value, TableField.CONTENT.value, TableField.METADATA.value]\n+            if relevance_requested:\n+                columns.append(relevance_column)\n+            return pd.DataFrame(columns=columns)\n+        logger.debug(f\"Query returned {len(df)} rows\")\n+        logger.debug(f\"Columns in response: {df.columns.tolist()}\")\n+        # Check if we have a rerank_model configured in KB params\n         rerank_model = self._kb.params.get(\"rerank_model\")\n-        if rerank_model and df is not None and not df.empty:\n+        if rerank_model and query_text:\n+            # Use reranker for relevance score\n             try:\n-                logger.info(f\"Using reranker model: {rerank_model}\")\n+                logger.info(f\"Using reranker model {rerank_model} for relevance calculation\")\n                 reranker = LLMReranker(model=rerank_model)\n-                # convert response from a dataframe to a list of strings\n-                content_column = df[TableField.CONTENT.value]\n-                # convert to list\n-                documents = content_column.tolist()\n-                # Extract query text from WHERE clause if it exists\n-                query_text = \"\"\n-                if query.where:\n-                    def extract_content(node, **kwargs):\n-                        nonlocal query_text\n-                        is_binary_op = isinstance(node, BinaryOperation)\n-                        is_identifier = isinstance(node.args[0], Identifier)\n-                        is_content = node.args[0].parts[-1].lower() == 'content'\n-                        is_constant = isinstance(node.args[1], Constant)\n-                        if is_binary_op and is_identifier and is_content and is_constant:\n-                            query_text = node.args[1].value\n-                    query_traversal(query.where, extract_content)\n-                    logger.debug(f\"Extracted query text: {query_text}\")\n-                # Get scores from reranker\n+                # Get documents to rerank\n+                documents = df[TableField.CONTENT.value].tolist()\n+                # Use the get_scores method with disable_events=True\n                 scores = reranker.get_scores(query_text, documents)\n-                # Add scores as a new column for filtering\n+                # Add scores as the relevance column\n+                df[relevance_column] = scores\n+                # Sort by relevance\n+                df = df.sort_values(by=relevance_column, ascending=False)\n+                # Filter by threshold\n                 scores_array = np.array(scores)\n-                # Add temporary column for sorting\n-                df['_relevance_score'] = scores\n-                # Filter by score threshold using numpy array for element-wise comparison\n                 df = df[scores_array > reranker.filtering_threshold]\n-                # Sort by relevance (higher score = more relevant)\n-                df = df.sort_values(by='_relevance_score', ascending=False)\n-                # Remove temporary column\n-                # df = df.drop(columns=['_relevance_score'])\n-                # Apply original limit if it exists\n-                if query.limit and len(df) > query.limit.value:\n-                    df = df.iloc[:query.limit.value]\n                 logger.debug(f\"Applied reranking with model {rerank_model}\")\n             except Exception as e:\nComment: what kind of error we expect here? \r\nI would leave only expected exceptions. If user has error because openai balance is empty, should he know about it (and see this error)? ",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/interfaces/knowledge_base/controller.py",
    "pr_number": 10627,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2018508352,
    "comment_created_at": "2025-03-28T11:38:22Z"
  },
  {
    "code": "@@ -1,38 +1,9 @@\n import os\n-import time\n-import pandas as pd\n \n-from mindsdb_sql import parse_sql\n from ..executor_test_base import BaseExecutorTest\n \n \n-class BaseMLTest(BaseExecutorTest):",
    "comment": "This is unused so it's ok to remove",
    "line_number": 9,
    "enriched": "File: tests/unit/ml_handlers/base_ml_test.py\nCode: @@ -1,38 +1,9 @@\n import os\n-import time\n-import pandas as pd\n \n-from mindsdb_sql import parse_sql\n from ..executor_test_base import BaseExecutorTest\n \n \n-class BaseMLTest(BaseExecutorTest):\nComment: This is unused so it's ok to remove",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "tests/unit/ml_handlers/base_ml_test.py",
    "pr_number": 8844,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1506044241,
    "comment_created_at": "2024-02-28T14:22:37Z"
  },
  {
    "code": "@@ -112,6 +116,15 @@ def post(self):\n             traceback=error_traceback,\n         )\n \n+        end_time = time.time()\n+        log_msg = f\"SQL processed in {(end_time - start_time):.2f}s ({end_time:.2f}-{start_time:.2f}), result is {query_response['type']}\"\n+        if query_response[\"type\"] is SQL_RESPONSE_TYPE.TABLE:\n+            log_msg += f\" ({len(query_response['data'])} rows), \"\n+        elif query_response[\"type\"] is SQL_RESPONSE_TYPE.ERROR:\n+            log_msg += f\" ({query_response['error_message']}), \"\n+        log_msg += f\"used handlers {ctx.used_handlers}\"\n+        logger.info(log_msg)",
    "comment": "maybe use debug level? `logger.debug`",
    "line_number": 126,
    "enriched": "File: mindsdb/api/http/namespaces/sql.py\nCode: @@ -112,6 +116,15 @@ def post(self):\n             traceback=error_traceback,\n         )\n \n+        end_time = time.time()\n+        log_msg = f\"SQL processed in {(end_time - start_time):.2f}s ({end_time:.2f}-{start_time:.2f}), result is {query_response['type']}\"\n+        if query_response[\"type\"] is SQL_RESPONSE_TYPE.TABLE:\n+            log_msg += f\" ({len(query_response['data'])} rows), \"\n+        elif query_response[\"type\"] is SQL_RESPONSE_TYPE.ERROR:\n+            log_msg += f\" ({query_response['error_message']}), \"\n+        log_msg += f\"used handlers {ctx.used_handlers}\"\n+        logger.info(log_msg)\nComment: maybe use debug level? `logger.debug`",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/api/http/namespaces/sql.py",
    "pr_number": 11730,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2432170067,
    "comment_created_at": "2025-10-15T11:14:19Z"
  },
  {
    "code": "@@ -251,11 +251,13 @@ def get_table_info(self, table_names: Optional[List[str]] = None) -> str:\n         tables_info = []\n         for table in all_tables:\n             key = f\"{ctx.company_id}_{table}_info\"\n-            table_info = self._cache.get(key) if self._cache else None\n+            # Sanitize the key to avoid table (file) names with backticks and slashes.\n+            sanitized_key = re.sub(r'[^\\w\\-.]', '_', key)",
    "comment": "As I understand the real problem is located [here](https://github.com/mindsdb/mindsdb/blob/4ea0d3f57fff448a9ce12155dd146c2b26027a1d/mindsdb/utilities/cache.py#L157) - file cache wrongly makes file name, it mustn't create subderictories if key name has `/`\r\nMaybe do this update more globally - in fcache module? ",
    "line_number": 255,
    "enriched": "File: mindsdb/interfaces/skills/sql_agent.py\nCode: @@ -251,11 +251,13 @@ def get_table_info(self, table_names: Optional[List[str]] = None) -> str:\n         tables_info = []\n         for table in all_tables:\n             key = f\"{ctx.company_id}_{table}_info\"\n-            table_info = self._cache.get(key) if self._cache else None\n+            # Sanitize the key to avoid table (file) names with backticks and slashes.\n+            sanitized_key = re.sub(r'[^\\w\\-.]', '_', key)\nComment: As I understand the real problem is located [here](https://github.com/mindsdb/mindsdb/blob/4ea0d3f57fff448a9ce12155dd146c2b26027a1d/mindsdb/utilities/cache.py#L157) - file cache wrongly makes file name, it mustn't create subderictories if key name has `/`\r\nMaybe do this update more globally - in fcache module? ",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/interfaces/skills/sql_agent.py",
    "pr_number": 10644,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2030991347,
    "comment_created_at": "2025-04-07T10:54:52Z"
  },
  {
    "code": "@@ -111,37 +119,33 @@ def truncate_msgs_for_token_limit(messages, model_name, max_tokens, truncate='fi\n \n \n def count_tokens(messages, encoder, model_name='gpt-3.5-turbo-0301'):\n-    \"\"\" Original token count implementation can be found in the OpenAI cookbook. \"\"\"\n-    if \"gpt-3.5-turbo\" in model_name:  # note: future models may deviate from this (only 0301 really complies)\n+    \"\"\"Original token count implementation can be found in the OpenAI cookbook.\"\"\"\n+    if (\n+        \"gpt-3.5-turbo\" in model_name\n+    ):  # note: future models may deviate from this (only 0301 really complies)\n         num_tokens = 0\n         for message in messages:\n-            num_tokens += 4  # every message follows <im_start>{role/name}\\n{content}<im_end>\\n\n+            num_tokens += (\n+                4  # every message follows <im_start>{role/name}\\n{content}<im_end>\\n\n+            )\n             for key, value in message.items():\n                 num_tokens += len(encoder.encode(value))\n                 if key == \"name\":  # if there's a name, the role is omitted\n                     num_tokens += -1  # role is always required and always 1 token\n         num_tokens += 2  # every reply is primed with <im_start>assistant\n         return num_tokens\n     else:\n-        raise NotImplementedError(f\"\"\"_count_tokens() is not presently implemented for model {model_name}.\"\"\")\n+        raise NotImplementedError(\n+            f\"\"\"_count_tokens() is not presently implemented for model {model_name}.\"\"\"\n+        )\n \n \n-def get_available_models(api_key: str, all_models: List, finetune_suffix: Optional[str] = None) -> List[str]:\n+def get_available_models(api_key: str) -> List[str]:",
    "comment": "change here",
    "line_number": 143,
    "enriched": "File: mindsdb/integrations/handlers/openai_handler/helpers.py\nCode: @@ -111,37 +119,33 @@ def truncate_msgs_for_token_limit(messages, model_name, max_tokens, truncate='fi\n \n \n def count_tokens(messages, encoder, model_name='gpt-3.5-turbo-0301'):\n-    \"\"\" Original token count implementation can be found in the OpenAI cookbook. \"\"\"\n-    if \"gpt-3.5-turbo\" in model_name:  # note: future models may deviate from this (only 0301 really complies)\n+    \"\"\"Original token count implementation can be found in the OpenAI cookbook.\"\"\"\n+    if (\n+        \"gpt-3.5-turbo\" in model_name\n+    ):  # note: future models may deviate from this (only 0301 really complies)\n         num_tokens = 0\n         for message in messages:\n-            num_tokens += 4  # every message follows <im_start>{role/name}\\n{content}<im_end>\\n\n+            num_tokens += (\n+                4  # every message follows <im_start>{role/name}\\n{content}<im_end>\\n\n+            )\n             for key, value in message.items():\n                 num_tokens += len(encoder.encode(value))\n                 if key == \"name\":  # if there's a name, the role is omitted\n                     num_tokens += -1  # role is always required and always 1 token\n         num_tokens += 2  # every reply is primed with <im_start>assistant\n         return num_tokens\n     else:\n-        raise NotImplementedError(f\"\"\"_count_tokens() is not presently implemented for model {model_name}.\"\"\")\n+        raise NotImplementedError(\n+            f\"\"\"_count_tokens() is not presently implemented for model {model_name}.\"\"\"\n+        )\n \n \n-def get_available_models(api_key: str, all_models: List, finetune_suffix: Optional[str] = None) -> List[str]:\n+def get_available_models(api_key: str) -> List[str]:\nComment: change here",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/openai_handler/helpers.py",
    "pr_number": 8289,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1384747212,
    "comment_created_at": "2023-11-07T11:02:58Z"
  },
  {
    "code": "@@ -119,13 +119,19 @@ def _resolve_table_names(self, table_names: List[str], all_tables: List[Identifi\n             # by path\n             tables_idx[tuple(table.parts)] = table\n \n+        # Some LLMs (e.g. gpt-4o) may include backticks or quotes when invoking tools.\n+        strip_chars = ' \"\\'\\n\\r'\n+        # Remove backticks only if the actual table names don't have them.\n+        # Querying data sources like MS OneDrive and Amazon S3 requires tables (files) to be wrapped in backticks.\n+        if not any('`' in table_name.get_string() for table_name in all_tables):\n+            strip_chars = strip_chars + '`'\n+\n         tables = []\n         for table_name in table_names:\n             if not table_name.strip():\n                 continue\n \n-            # Some LLMs (e.g. gpt-4o) may include backticks or quotes when invoking tools.\n-            table_name = table_name.strip(' `\"\\'\\n\\r')\n+            table_name = table_name.strip(strip_chars)",
    "comment": "Identifier have to be able to digest backticks and use them when it is required:\r\n```python\r\nIdentifier('df1.`iris.csv`').parts\r\n>>> ['df1', 'iris.csv']\r\n\r\nIdentifier('df1.`iris`').to_string()\r\n>>>'df1.iris'\r\n\r\nIdentifier('df1.`iris.csv`').to_string()\r\n>>>'df1.`iris.csv`'\r\n```\r\n\r\nMaybe we can left this line like this? \r\n`table_name = table_name.strip(' \"\\'\\n\\r')`",
    "line_number": 134,
    "enriched": "File: mindsdb/interfaces/skills/sql_agent.py\nCode: @@ -119,13 +119,19 @@ def _resolve_table_names(self, table_names: List[str], all_tables: List[Identifi\n             # by path\n             tables_idx[tuple(table.parts)] = table\n \n+        # Some LLMs (e.g. gpt-4o) may include backticks or quotes when invoking tools.\n+        strip_chars = ' \"\\'\\n\\r'\n+        # Remove backticks only if the actual table names don't have them.\n+        # Querying data sources like MS OneDrive and Amazon S3 requires tables (files) to be wrapped in backticks.\n+        if not any('`' in table_name.get_string() for table_name in all_tables):\n+            strip_chars = strip_chars + '`'\n+\n         tables = []\n         for table_name in table_names:\n             if not table_name.strip():\n                 continue\n \n-            # Some LLMs (e.g. gpt-4o) may include backticks or quotes when invoking tools.\n-            table_name = table_name.strip(' `\"\\'\\n\\r')\n+            table_name = table_name.strip(strip_chars)\nComment: Identifier have to be able to digest backticks and use them when it is required:\r\n```python\r\nIdentifier('df1.`iris.csv`').parts\r\n>>> ['df1', 'iris.csv']\r\n\r\nIdentifier('df1.`iris`').to_string()\r\n>>>'df1.iris'\r\n\r\nIdentifier('df1.`iris.csv`').to_string()\r\n>>>'df1.`iris.csv`'\r\n```\r\n\r\nMaybe we can left this line like this? \r\n`table_name = table_name.strip(' \"\\'\\n\\r')`",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/interfaces/skills/sql_agent.py",
    "pr_number": 10311,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1897940477,
    "comment_created_at": "2024-12-26T13:59:50Z"
  },
  {
    "code": "@@ -3,145 +3,173 @@ title: Agent\n sidebarTitle: Agent\n ---\n \n-With MindsDB, you can create and deploy AI agents that comprise AI models and customizable skills such as knowledge bases and text-to-SQL.\n+With MindsDB, you can create and deploy AI agents that comprise AI models and customizable skills such as [knowledge bases](/mindsdb_sql/knowledge-bases) and text-to-SQL.\n \n <p align=\"center\">\n   <img src=\"/assets/agent_diagram.png\" />\n </p>\n \n AI agents use a conversational model (like OpenAI) from LangChain utilizing [tools as skills](https://python.langchain.com/docs/modules/agents/tools/) to respond to user input. Users can customize AI agents with their own prompts to fit their use cases.\n \n-A [chatbot](/agents/chatbot) can be thought of as an agent connected to some messaging interface.\n+A [chatbot](/agents/chatbot) can be thought of as an agent connected to a messaging interface, such as Slack.\n \n-## How to work with AI agents\n+## `CREATE AGENT` Syntax\n \n-### Create skills\n+Here is the syntax for creating agents.\n \n-Start by setting up the skills. Here is how you can create and manage skills using SQL API.\n+```sql\n+CREATE AGENT my_agent\n+USING \n+    skills = ['text_to_sql_skill'],\n+    provider = 'openai',\n+    model = 'gpt-4',\n+    prompt_template = 'Answer the user input in a helpful way using tools',\n+    verbose = True,\n+    max_tokens = 100;\n+```\n+\n+<Note>\n+Users can define a default model in the [MindsDB configuration under `default_llm`](/setup/custom-config) to be used with agents.",
    "comment": "Should we mention this? If users use docker they will need to build new image for this to take effect? Maybe mention that we use this default model",
    "line_number": 32,
    "enriched": "File: docs/mindsdb_sql/agents/agent.mdx\nCode: @@ -3,145 +3,173 @@ title: Agent\n sidebarTitle: Agent\n ---\n \n-With MindsDB, you can create and deploy AI agents that comprise AI models and customizable skills such as knowledge bases and text-to-SQL.\n+With MindsDB, you can create and deploy AI agents that comprise AI models and customizable skills such as [knowledge bases](/mindsdb_sql/knowledge-bases) and text-to-SQL.\n \n <p align=\"center\">\n   <img src=\"/assets/agent_diagram.png\" />\n </p>\n \n AI agents use a conversational model (like OpenAI) from LangChain utilizing [tools as skills](https://python.langchain.com/docs/modules/agents/tools/) to respond to user input. Users can customize AI agents with their own prompts to fit their use cases.\n \n-A [chatbot](/agents/chatbot) can be thought of as an agent connected to some messaging interface.\n+A [chatbot](/agents/chatbot) can be thought of as an agent connected to a messaging interface, such as Slack.\n \n-## How to work with AI agents\n+## `CREATE AGENT` Syntax\n \n-### Create skills\n+Here is the syntax for creating agents.\n \n-Start by setting up the skills. Here is how you can create and manage skills using SQL API.\n+```sql\n+CREATE AGENT my_agent\n+USING \n+    skills = ['text_to_sql_skill'],\n+    provider = 'openai',\n+    model = 'gpt-4',\n+    prompt_template = 'Answer the user input in a helpful way using tools',\n+    verbose = True,\n+    max_tokens = 100;\n+```\n+\n+<Note>\n+Users can define a default model in the [MindsDB configuration under `default_llm`](/setup/custom-config) to be used with agents.\nComment: Should we mention this? If users use docker they will need to build new image for this to take effect? Maybe mention that we use this default model",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "docs/mindsdb_sql/agents/agent.mdx",
    "pr_number": 10816,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2090885661,
    "comment_created_at": "2025-05-15T10:47:41Z"
  },
  {
    "code": "@@ -148,3 +145,59 @@ def call_youtube_comments_api(self,video_id):\n \n \n         return all_youtube_comments_df\n+\n+\n+class YoutubeChannelTable(APITable):\n+\n+    \"\"\"Youtube Channel Info  by channel id Table implementation\"\"\"\n+\n+    def select(self, query: ast.Select) -> pd.DataFrame:\n+        select_statement_parser = SELECTQueryParser(\n+            query,\n+            'channel',\n+            self.get_columns()\n+        )\n+\n+        selected_columns, where_conditions, order_by_conditions, result_limit = select_statement_parser.parse_query()\n+\n+        channel_id = None\n+        for op, arg1, arg2 in where_conditions:\n+            if arg1 == 'channel_id':\n+                if op == '=':\n+                    channel_id = arg2\n+                    break\n+                else:\n+                    raise NotImplementedError(\"Only '=' operator is supported for channel_id column.\")\n+\n+        if not channel_id:",
    "comment": "Same question as before with your other PR, is it always mandatory to include the channel ID?",
    "line_number": 172,
    "enriched": "File: mindsdb/integrations/handlers/youtube_handler/youtube_tables.py\nCode: @@ -148,3 +145,59 @@ def call_youtube_comments_api(self,video_id):\n \n \n         return all_youtube_comments_df\n+\n+\n+class YoutubeChannelTable(APITable):\n+\n+    \"\"\"Youtube Channel Info  by channel id Table implementation\"\"\"\n+\n+    def select(self, query: ast.Select) -> pd.DataFrame:\n+        select_statement_parser = SELECTQueryParser(\n+            query,\n+            'channel',\n+            self.get_columns()\n+        )\n+\n+        selected_columns, where_conditions, order_by_conditions, result_limit = select_statement_parser.parse_query()\n+\n+        channel_id = None\n+        for op, arg1, arg2 in where_conditions:\n+            if arg1 == 'channel_id':\n+                if op == '=':\n+                    channel_id = arg2\n+                    break\n+                else:\n+                    raise NotImplementedError(\"Only '=' operator is supported for channel_id column.\")\n+\n+        if not channel_id:\nComment: Same question as before with your other PR, is it always mandatory to include the channel ID?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/youtube_handler/youtube_tables.py",
    "pr_number": 7585,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1348712587,
    "comment_created_at": "2023-10-06T13:18:53Z"
  },
  {
    "code": "@@ -0,0 +1,10 @@\n+mindsdb>=22.6.2.1",
    "comment": "why mindsdb dependency here? handlers don't use mindsdb, mindsdb uses handlers",
    "line_number": 1,
    "enriched": "File: mindsdb/integrations/handlers/huggingface_handler/requirements_cpu.txt\nCode: @@ -0,0 +1,10 @@\n+mindsdb>=22.6.2.1\nComment: why mindsdb dependency here? handlers don't use mindsdb, mindsdb uses handlers",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/huggingface_handler/requirements_cpu.txt",
    "pr_number": 6781,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1269331238,
    "comment_created_at": "2023-07-20T11:32:11Z"
  },
  {
    "code": "@@ -13,10 +13,10 @@ This handler is implemented using the `fdb` library, the Python driver for Fireb\n \n The required arguments to establish a connection are as follows:\n \n-* `host` is the host name or IP address of the Firebird server.\n-* `database` is the port to use when connecting with the Firebird server.\n-* `user` is the username to authenticate the user with the Firebird server.\n-* `password` is the password to authenticate the user with the Firebird server.\n+- `host` is the host name or IP address of the Firebird server.",
    "comment": "You changed `*` into `-`.\r\n\r\nPlease change it back to `*` for all four points.",
    "line_number": 16,
    "enriched": "File: docs/data-integrations/firebird.mdx\nCode: @@ -13,10 +13,10 @@ This handler is implemented using the `fdb` library, the Python driver for Fireb\n \n The required arguments to establish a connection are as follows:\n \n-* `host` is the host name or IP address of the Firebird server.\n-* `database` is the port to use when connecting with the Firebird server.\n-* `user` is the username to authenticate the user with the Firebird server.\n-* `password` is the password to authenticate the user with the Firebird server.\n+- `host` is the host name or IP address of the Firebird server.\nComment: You changed `*` into `-`.\r\n\r\nPlease change it back to `*` for all four points.",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "docs/data-integrations/firebird.mdx",
    "pr_number": 5857,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1179452485,
    "comment_created_at": "2023-04-27T16:53:06Z"
  },
  {
    "code": "@@ -0,0 +1,151 @@\n+# OpenStreetMap Handler\n+\n+OpenStreetMap handler for MindsDB provides interfaces to connect to OpenStreetMap via APIs and pull map data into MindsDB.\n+\n+---\n+\n+## Table of Contents\n+\n+- [OpenStreetMap Handler](#openstreetmap-handler)\n+  - [Table of Contents](#table-of-contents)\n+  - [About OpenStreetMap](#about-openstreetmap)\n+  - [OpenStreetMap Handler Implementation](#openstreetmap-handler-implementation)\n+  - [OpenStreetMap Handler Initialization](#openstreetmap-handler-initialization)\n+  - [Implemented Features](#implemented-features)\n+  - [TODO](#todo)\n+  - [Example Usage](#example-usage)\n+    \n+---\n+\n+## About OpenStreetMap\n+\n+OpenStreetMap is a map of the world, created by people like you and free to use under an open license.\n+<br>\n+https://www.openstreetmap.org/about\n+\n+## OpenStreetMap Handler Implementation\n+\n+This handler was implemented using [OSMPythonTools](https://wiki.openstreetmap.org/wiki/Overpass_API), the Overpass API wrapper for Python.\n+\n+## OpenStreetMap Handler Initialization\n+\n+The OpenStreetMap handler is initialized with the following parameters:\n+\n+- `area`: a required area to query for map data.\n+- `timeout`: a required timeout to use for the query.\n+\n+## Implemented Features\n+\n+- [x] OpenStreetMap Nodes Table for a given Area\n+  - [x] Support SELECT\n+    - [x] Support LIMIT\n+    - [x] Support WHERE\n+    - [x] Support ORDER BY\n+    - [x] Support column selection\n+- [x] OpenStreetMap Ways Table for a given Area\n+    - [x] Support SELECT\n+        - [x] Support LIMIT\n+        - [x] Support WHERE\n+        - [x] Support ORDER BY\n+        - [x] Support column selection\n+- [x] OpenStreetMap Relations Table for a given Area\n+    - [x] Support SELECT\n+        - [x] Support LIMIT\n+        - [x] Support WHERE\n+        - [x] Support ORDER BY\n+        - [x] Support column selection\n+\n+## TODO\n+\n+- [ ] Support for more OpenStreetMap tables\n+- [ ] Support for more OpenStreetMap queries\n+\n+## Example Usage\n+\n+~~~~sql\n+CREATE DATABASE openstreetmap_datasource\n+WITH\n+engine='openstreetmap',\n+parameters={\n+    \"area\": \"New York City\",\n+    \"timeout\": 1000\n+};\n+~~~~\n+\n+```sql \n+SELECT * FROM nodes LIMIT 10;",
    "comment": "Hey @singh1203,\r\nI believe the tables used for the example SELECT statements that you have provided here would need to be accompanied by the DATABASE that you have created. For instance, it should be ,\r\n`SELECT * FROM openstreetmap_datasource.nodes LIMIT 10;`",
    "line_number": 76,
    "enriched": "File: mindsdb/integrations/handlers/openstreetmap_handler/README.md\nCode: @@ -0,0 +1,151 @@\n+# OpenStreetMap Handler\n+\n+OpenStreetMap handler for MindsDB provides interfaces to connect to OpenStreetMap via APIs and pull map data into MindsDB.\n+\n+---\n+\n+## Table of Contents\n+\n+- [OpenStreetMap Handler](#openstreetmap-handler)\n+  - [Table of Contents](#table-of-contents)\n+  - [About OpenStreetMap](#about-openstreetmap)\n+  - [OpenStreetMap Handler Implementation](#openstreetmap-handler-implementation)\n+  - [OpenStreetMap Handler Initialization](#openstreetmap-handler-initialization)\n+  - [Implemented Features](#implemented-features)\n+  - [TODO](#todo)\n+  - [Example Usage](#example-usage)\n+    \n+---\n+\n+## About OpenStreetMap\n+\n+OpenStreetMap is a map of the world, created by people like you and free to use under an open license.\n+<br>\n+https://www.openstreetmap.org/about\n+\n+## OpenStreetMap Handler Implementation\n+\n+This handler was implemented using [OSMPythonTools](https://wiki.openstreetmap.org/wiki/Overpass_API), the Overpass API wrapper for Python.\n+\n+## OpenStreetMap Handler Initialization\n+\n+The OpenStreetMap handler is initialized with the following parameters:\n+\n+- `area`: a required area to query for map data.\n+- `timeout`: a required timeout to use for the query.\n+\n+## Implemented Features\n+\n+- [x] OpenStreetMap Nodes Table for a given Area\n+  - [x] Support SELECT\n+    - [x] Support LIMIT\n+    - [x] Support WHERE\n+    - [x] Support ORDER BY\n+    - [x] Support column selection\n+- [x] OpenStreetMap Ways Table for a given Area\n+    - [x] Support SELECT\n+        - [x] Support LIMIT\n+        - [x] Support WHERE\n+        - [x] Support ORDER BY\n+        - [x] Support column selection\n+- [x] OpenStreetMap Relations Table for a given Area\n+    - [x] Support SELECT\n+        - [x] Support LIMIT\n+        - [x] Support WHERE\n+        - [x] Support ORDER BY\n+        - [x] Support column selection\n+\n+## TODO\n+\n+- [ ] Support for more OpenStreetMap tables\n+- [ ] Support for more OpenStreetMap queries\n+\n+## Example Usage\n+\n+~~~~sql\n+CREATE DATABASE openstreetmap_datasource\n+WITH\n+engine='openstreetmap',\n+parameters={\n+    \"area\": \"New York City\",\n+    \"timeout\": 1000\n+};\n+~~~~\n+\n+```sql \n+SELECT * FROM nodes LIMIT 10;\nComment: Hey @singh1203,\r\nI believe the tables used for the example SELECT statements that you have provided here would need to be accompanied by the DATABASE that you have created. For instance, it should be ,\r\n`SELECT * FROM openstreetmap_datasource.nodes LIMIT 10;`",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/openstreetmap_handler/README.md",
    "pr_number": 6740,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1265684244,
    "comment_created_at": "2023-07-17T17:29:19Z"
  },
  {
    "code": "@@ -71,40 +75,70 @@ def calc_next_date(schedule_str, base_date: dt.datetime):\n     return next_date\n \n \n+def parse_job_date(date_str: str) -> dt.datetime:\n+    \"\"\"\n+    Convert string used as job data to datetime object\n+    :param date_str:\n+    :return:\n+    \"\"\"\n+\n+    if date_str.upper() == 'NOW':\n+        return dt.datetime.now()\n+\n+    date_formats = ['%Y-%m-%d %H:%M:%S', '%Y-%m-%d']\n+    date = None\n+    for date_format in date_formats:\n+        try:\n+            date = dt.datetime.strptime(date_str, date_format)\n+        except ValueError:\n+            pass",
    "comment": "Should we do any logging here?",
    "line_number": 94,
    "enriched": "File: mindsdb/interfaces/jobs/jobs_controller.py\nCode: @@ -71,40 +75,70 @@ def calc_next_date(schedule_str, base_date: dt.datetime):\n     return next_date\n \n \n+def parse_job_date(date_str: str) -> dt.datetime:\n+    \"\"\"\n+    Convert string used as job data to datetime object\n+    :param date_str:\n+    :return:\n+    \"\"\"\n+\n+    if date_str.upper() == 'NOW':\n+        return dt.datetime.now()\n+\n+    date_formats = ['%Y-%m-%d %H:%M:%S', '%Y-%m-%d']\n+    date = None\n+    for date_format in date_formats:\n+        try:\n+            date = dt.datetime.strptime(date_str, date_format)\n+        except ValueError:\n+            pass\nComment: Should we do any logging here?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/interfaces/jobs/jobs_controller.py",
    "pr_number": 9142,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1583474861,
    "comment_created_at": "2024-04-29T17:42:56Z"
  },
  {
    "code": "@@ -0,0 +1,148 @@\n+---\n+title: Naming Standards for MindsDB Objects\n+sidebarTitle: MindsDB Objects\n+icon: \"puzzle-piece-simple\"\n+---\n+\n+MindsDB allows you to create and manage a variety of entities within its ecosystem. All MindsDB objects follow the same naming conventions to ensure consistency and compatibility across the platform.\n+\n+## MindsDB Entities",
    "comment": "should we also mention  `create skill` and `create model`?",
    "line_number": 9,
    "enriched": "File: docs/mindsdb-objects.mdx\nCode: @@ -0,0 +1,148 @@\n+---\n+title: Naming Standards for MindsDB Objects\n+sidebarTitle: MindsDB Objects\n+icon: \"puzzle-piece-simple\"\n+---\n+\n+MindsDB allows you to create and manage a variety of entities within its ecosystem. All MindsDB objects follow the same naming conventions to ensure consistency and compatibility across the platform.\n+\n+## MindsDB Entities\nComment: should we also mention  `create skill` and `create model`?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "docs/mindsdb-objects.mdx",
    "pr_number": 11519,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2324565111,
    "comment_created_at": "2025-09-05T09:19:43Z"
  },
  {
    "code": "@@ -451,26 +453,40 @@ def prepare_env(self, modules, engine_id, engine_version: int):\n                 env_folder_name = f'{env_folder_name}_{engine_version}'\n             self.env_path = base_path / env_folder_name\n \n+            is_cloud = Config().get('cloud', False)\n+            env_install_path = (\n+                Path(tempfile.mkdtemp()) / env_folder_name if is_cloud\n+                else self.env_path\n+            )\n+            pip_cmd = env_install_path / 'bin' / 'pip'\n+\n             self.python_path = self.env_path / 'bin' / 'python'\n \n             if self.env_path.exists():\n                 # already exists. it means requirements are already installed\n                 return\n \n             # create\n-            virtualenv.cli_run(['-p', sys.executable, str(self.env_path)])\n-            logger.info(f\"Created new environment: {self.env_path}\")\n+            virtualenv.cli_run(['-p', sys.executable, str(env_install_path)])\n+            logger.info(f\"Created new environment: {env_install_path}\")\n \n             if len(modules) > 0:\n-                self.install_modules(modules)\n+                self.install_modules(modules, pip_cmd=pip_cmd)\n         except Exception:\n             # DANGER !!! VENV MUST BE CREATED\n             logger.info(\"Can't create virtual environment. venv module should be installed\")\n \n             self.python_path = Path(sys.executable)\n \n             # try to install modules everytime\n-            self.install_modules(modules)\n+            self.install_modules(modules, pip_cmd=pip_cmd)\n+\n+        # fastest way to copy files if destination is NFS",
    "comment": "do we need to do it only for cloud? ",
    "line_number": 484,
    "enriched": "File: mindsdb/integrations/handlers/byom_handler/byom_handler.py\nCode: @@ -451,26 +453,40 @@ def prepare_env(self, modules, engine_id, engine_version: int):\n                 env_folder_name = f'{env_folder_name}_{engine_version}'\n             self.env_path = base_path / env_folder_name\n \n+            is_cloud = Config().get('cloud', False)\n+            env_install_path = (\n+                Path(tempfile.mkdtemp()) / env_folder_name if is_cloud\n+                else self.env_path\n+            )\n+            pip_cmd = env_install_path / 'bin' / 'pip'\n+\n             self.python_path = self.env_path / 'bin' / 'python'\n \n             if self.env_path.exists():\n                 # already exists. it means requirements are already installed\n                 return\n \n             # create\n-            virtualenv.cli_run(['-p', sys.executable, str(self.env_path)])\n-            logger.info(f\"Created new environment: {self.env_path}\")\n+            virtualenv.cli_run(['-p', sys.executable, str(env_install_path)])\n+            logger.info(f\"Created new environment: {env_install_path}\")\n \n             if len(modules) > 0:\n-                self.install_modules(modules)\n+                self.install_modules(modules, pip_cmd=pip_cmd)\n         except Exception:\n             # DANGER !!! VENV MUST BE CREATED\n             logger.info(\"Can't create virtual environment. venv module should be installed\")\n \n             self.python_path = Path(sys.executable)\n \n             # try to install modules everytime\n-            self.install_modules(modules)\n+            self.install_modules(modules, pip_cmd=pip_cmd)\n+\n+        # fastest way to copy files if destination is NFS\nComment: do we need to do it only for cloud? ",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/byom_handler/byom_handler.py",
    "pr_number": 8482,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1420064841,
    "comment_created_at": "2023-12-08T07:57:01Z"
  },
  {
    "code": "@@ -51,6 +51,7 @@ pgvector==0.3.6 # Required for knowledge bases\n prometheus-client==0.20.0\n transformers >= 4.42.4\n sentry-sdk[flask] == 2.14.0\n+openai<2.0.0,>=1.58.1",
    "comment": "We will have to move the `openai` requirement for all handlers here not just Langchain handler",
    "line_number": 54,
    "enriched": "File: requirements/requirements.txt\nCode: @@ -51,6 +51,7 @@ pgvector==0.3.6 # Required for knowledge bases\n prometheus-client==0.20.0\n transformers >= 4.42.4\n sentry-sdk[flask] == 2.14.0\n+openai<2.0.0,>=1.58.1\nComment: We will have to move the `openai` requirement for all handlers here not just Langchain handler",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "requirements/requirements.txt",
    "pr_number": 10490,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1962064687,
    "comment_created_at": "2025-02-19T17:06:52Z"
  },
  {
    "code": "@@ -0,0 +1,31 @@\n+from mindsdb.integrations.libs.const import HANDLER_TYPE\n+\n+from .__about__ import __description__ as description\n+from .__about__ import __version__ as version\n+\n+try:\n+    from .iceberg_handler import IcebergHandler as Handler\n+    from .iceberg_handler import connection_args, connection_args_example\n+\n+    import_error = None\n+except Exception as e:\n+    Handler = None\n+    import_error = e\n+\n+title = 'Apache Iceberg'\n+name = 'iceberg'\n+type = HANDLER_TYPE.DATA\n+icon_path = 'icon.svg'",
    "comment": "The icon you have added is .png",
    "line_number": 18,
    "enriched": "File: mindsdb/integrations/handlers/iceberg_handler/__init__.py\nCode: @@ -0,0 +1,31 @@\n+from mindsdb.integrations.libs.const import HANDLER_TYPE\n+\n+from .__about__ import __description__ as description\n+from .__about__ import __version__ as version\n+\n+try:\n+    from .iceberg_handler import IcebergHandler as Handler\n+    from .iceberg_handler import connection_args, connection_args_example\n+\n+    import_error = None\n+except Exception as e:\n+    Handler = None\n+    import_error = e\n+\n+title = 'Apache Iceberg'\n+name = 'iceberg'\n+type = HANDLER_TYPE.DATA\n+icon_path = 'icon.svg'\nComment: The icon you have added is .png",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/iceberg_handler/__init__.py",
    "pr_number": 8082,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1374557360,
    "comment_created_at": "2023-10-27T13:12:11Z"
  },
  {
    "code": "@@ -29,38 +34,375 @@ def get_storage():\n     return storageFactory(0)\n \n \n+def _is_request_valid() -> bool:\n+    \"\"\"check if request body contains all (and only) required fields\n+\n+    Returns:\n+        bool: True if all required data in the request\n+    \"\"\"\n+    try:\n+        data = request.json\n+    except Exception:\n+        return False\n+    if (\n+        isinstance(data, dict) is False\n+        or len(data.keys()) == 0\n+        or len(set(data.keys()) - {'index', 'name', 'content'}) != 0\n+    ):\n+        return False\n+    return True\n+\n+\n+class TabsController:",
    "comment": "Maybe move it from routes folder, next to other controllers?",
    "line_number": 56,
    "enriched": "File: mindsdb/api/http/namespaces/tab.py\nCode: @@ -29,38 +34,375 @@ def get_storage():\n     return storageFactory(0)\n \n \n+def _is_request_valid() -> bool:\n+    \"\"\"check if request body contains all (and only) required fields\n+\n+    Returns:\n+        bool: True if all required data in the request\n+    \"\"\"\n+    try:\n+        data = request.json\n+    except Exception:\n+        return False\n+    if (\n+        isinstance(data, dict) is False\n+        or len(data.keys()) == 0\n+        or len(set(data.keys()) - {'index', 'name', 'content'}) != 0\n+    ):\n+        return False\n+    return True\n+\n+\n+class TabsController:\nComment: Maybe move it from routes folder, next to other controllers?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/api/http/namespaces/tab.py",
    "pr_number": 8898,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1524376306,
    "comment_created_at": "2024-03-14T07:47:36Z"
  },
  {
    "code": "@@ -0,0 +1,87 @@\n+import github\n+\n+from mindsdb.integrations.handlers.github_handler.github_tables import GithubIssuesTable\n+from mindsdb.integrations.libs.api_handler import APIHandler\n+from mindsdb.integrations.libs.response import (\n+    HandlerStatusResponse as StatusResponse,\n+)\n+\n+from mindsdb.utilities.log import get_log\n+from mindsdb_sql import parse_sql\n+\n+\n+logger = get_log(\"integrations.github_handler\")\n+\n+\n+class GithubHandler(APIHandler):\n+    \"\"\"The GitHub handler implementation\"\"\"\n+\n+    def __init__(self, name: str, **kwargs):\n+        \"\"\"Initialize the GitHub handler.\n+\n+        Parameters\n+        ----------\n+        name : str\n+            name of a handler instance\n+        \"\"\"\n+        super().__init__(name)\n+\n+        connection_data = kwargs.get(\"connection_data\", {})\n+        self.connection_data = connection_data\n+        self.repository = connection_data[\"repository\"]\n+        self.kwargs = kwargs\n+\n+        self.connection = None\n+        self.is_connected = False\n+\n+        github_issues_data = GithubIssuesTable(self)\n+        self._register_table(\"issues\", github_issues_data)\n+\n+    def connect(self) -> StatusResponse:\n+        \"\"\"Set up the connection required by the handler.\n+\n+        Returns\n+        -------\n+        StatusResponse\n+            connection object\n+        \"\"\"\n+\n+        connection_kwargs = {}\n+\n+        if self.connection_data.get(\"api_key\", None):\n+            connection_kwargs[\"login_or_token\"] = self.connection_data[\"api_key\"]\n+\n+        if self.connection_data.get(\"github_url\", None):\n+            connection_kwargs[\"base_url\"] = self.connection_data[\"github_url\"]\n+\n+        self.connection = github.Github(**connection_kwargs)\n+        self.is_connected = True\n+\n+        return self.connection\n+\n+    def check_connection(self) -> StatusResponse:\n+        \"\"\"Check connection to the handler.\n+\n+        Returns\n+        -------\n+        StatusResponse\n+            Status confirmation\n+        \"\"\"\n+\n+        return StatusResponse(True)",
    "comment": "Why not to check connection here?",
    "line_number": 71,
    "enriched": "File: mindsdb/integrations/handlers/github_handler/github_handler.py\nCode: @@ -0,0 +1,87 @@\n+import github\n+\n+from mindsdb.integrations.handlers.github_handler.github_tables import GithubIssuesTable\n+from mindsdb.integrations.libs.api_handler import APIHandler\n+from mindsdb.integrations.libs.response import (\n+    HandlerStatusResponse as StatusResponse,\n+)\n+\n+from mindsdb.utilities.log import get_log\n+from mindsdb_sql import parse_sql\n+\n+\n+logger = get_log(\"integrations.github_handler\")\n+\n+\n+class GithubHandler(APIHandler):\n+    \"\"\"The GitHub handler implementation\"\"\"\n+\n+    def __init__(self, name: str, **kwargs):\n+        \"\"\"Initialize the GitHub handler.\n+\n+        Parameters\n+        ----------\n+        name : str\n+            name of a handler instance\n+        \"\"\"\n+        super().__init__(name)\n+\n+        connection_data = kwargs.get(\"connection_data\", {})\n+        self.connection_data = connection_data\n+        self.repository = connection_data[\"repository\"]\n+        self.kwargs = kwargs\n+\n+        self.connection = None\n+        self.is_connected = False\n+\n+        github_issues_data = GithubIssuesTable(self)\n+        self._register_table(\"issues\", github_issues_data)\n+\n+    def connect(self) -> StatusResponse:\n+        \"\"\"Set up the connection required by the handler.\n+\n+        Returns\n+        -------\n+        StatusResponse\n+            connection object\n+        \"\"\"\n+\n+        connection_kwargs = {}\n+\n+        if self.connection_data.get(\"api_key\", None):\n+            connection_kwargs[\"login_or_token\"] = self.connection_data[\"api_key\"]\n+\n+        if self.connection_data.get(\"github_url\", None):\n+            connection_kwargs[\"base_url\"] = self.connection_data[\"github_url\"]\n+\n+        self.connection = github.Github(**connection_kwargs)\n+        self.is_connected = True\n+\n+        return self.connection\n+\n+    def check_connection(self) -> StatusResponse:\n+        \"\"\"Check connection to the handler.\n+\n+        Returns\n+        -------\n+        StatusResponse\n+            Status confirmation\n+        \"\"\"\n+\n+        return StatusResponse(True)\nComment: Why not to check connection here?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/github_handler/github_handler.py",
    "pr_number": 5474,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1161536953,
    "comment_created_at": "2023-04-10T08:22:50Z"
  },
  {
    "code": "@@ -82,8 +82,9 @@ def predict(self, df: pd.DataFrame, args: Optional[Dict] = None) -> pd.DataFrame\n \n     def finetune(self, df: Optional[pd.DataFrame] = None, args: Optional[Dict] = None) -> None:\n         with self._anyscale_base_api(args):\n-            self._set_models(args.get('using', {}))\n-            super().finetune(df, args)\n+            using_args = args.get('using', {})\n+            self._set_models(using_args)\n+            super().finetune(df, using_args)",
    "comment": "Curious as to why this was done? It introduced a bug where `target` may not be present. Shouldn't we still send `args` proper to `finetune()` instead?\r\n\r\nEDIT: presumably there was another bug that this fixed, but we need to get a solution where both work fine. Would love to get more details.",
    "line_number": 87,
    "enriched": "File: mindsdb/integrations/handlers/anyscale_endpoints_handler/anyscale_endpoints_handler.py\nCode: @@ -82,8 +82,9 @@ def predict(self, df: pd.DataFrame, args: Optional[Dict] = None) -> pd.DataFrame\n \n     def finetune(self, df: Optional[pd.DataFrame] = None, args: Optional[Dict] = None) -> None:\n         with self._anyscale_base_api(args):\n-            self._set_models(args.get('using', {}))\n-            super().finetune(df, args)\n+            using_args = args.get('using', {})\n+            self._set_models(using_args)\n+            super().finetune(df, using_args)\nComment: Curious as to why this was done? It introduced a bug where `target` may not be present. Shouldn't we still send `args` proper to `finetune()` instead?\r\n\r\nEDIT: presumably there was another bug that this fixed, but we need to get a solution where both work fine. Would love to get more details.",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/anyscale_endpoints_handler/anyscale_endpoints_handler.py",
    "pr_number": 8713,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1476783082,
    "comment_created_at": "2024-02-02T21:55:48Z"
  },
  {
    "code": "@@ -7,3 +7,4 @@ setuptools\n wheel\n deptry==0.12.0\n twine\n+importlib_metadata==7.2.1 #fix twine bug",
    "comment": "```suggestion\r\nimportlib_metadata==7.2.1 #fix twine bug\r\n\r\n```",
    "line_number": 10,
    "enriched": "File: requirements/requirements-dev.txt\nCode: @@ -7,3 +7,4 @@ setuptools\n wheel\n deptry==0.12.0\n twine\n+importlib_metadata==7.2.1 #fix twine bug\nComment: ```suggestion\r\nimportlib_metadata==7.2.1 #fix twine bug\r\n\r\n```",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "requirements/requirements-dev.txt",
    "pr_number": 9400,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1653582811,
    "comment_created_at": "2024-06-25T21:15:11Z"
  },
  {
    "code": "@@ -701,6 +723,41 @@ def _convert_metadata_value(self, value):\n         # Convert everything else to string\n         return str(value)\n \n+    def _filter_out_threshold_condition(self, where_clause):",
    "comment": "@ala12326571 Can you explain why this is done this way? There's only a single re-reranking threshold condition, right?",
    "line_number": 726,
    "enriched": "File: mindsdb/interfaces/knowledge_base/controller.py\nCode: @@ -701,6 +723,41 @@ def _convert_metadata_value(self, value):\n         # Convert everything else to string\n         return str(value)\n \n+    def _filter_out_threshold_condition(self, where_clause):\nComment: @ala12326571 Can you explain why this is done this way? There's only a single re-reranking threshold condition, right?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/interfaces/knowledge_base/controller.py",
    "pr_number": 10653,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2029225948,
    "comment_created_at": "2025-04-04T17:53:06Z"
  },
  {
    "code": "@@ -828,8 +828,10 @@ def _check_ft_status(model_id):\n         ft_stats = _check_ft_status(ft_result.id)\n \n         if ft_stats.status != 'succeeded':\n+            err_message = ft_stats.events[-1].message if hasattr(ft_stats, 'events') else 'could not retrieve!'",
    "comment": "Guards against time outs, or interrupted internet connection when checking.",
    "line_number": 831,
    "enriched": "File: mindsdb/integrations/handlers/openai_handler/openai_handler.py\nCode: @@ -828,8 +828,10 @@ def _check_ft_status(model_id):\n         ft_stats = _check_ft_status(ft_result.id)\n \n         if ft_stats.status != 'succeeded':\n+            err_message = ft_stats.events[-1].message if hasattr(ft_stats, 'events') else 'could not retrieve!'\nComment: Guards against time outs, or interrupted internet connection when checking.",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/openai_handler/openai_handler.py",
    "pr_number": 8710,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1470441392,
    "comment_created_at": "2024-01-30T01:00:57Z"
  },
  {
    "code": "@@ -0,0 +1,174 @@\n+import os\n+import json\n+import openai\n+import contextlib\n+from typing import Optional, Dict\n+\n+import pandas as pd\n+\n+from mindsdb.integrations.handlers.openai_handler.openai_handler import OpenAIHandler\n+from mindsdb.integrations.handlers.openai_handler.constants import OPENAI_API_BASE\n+\n+\n+CHAT_MODELS = (",
    "comment": "Ideally this will be retrieved dynamically from Anyscale.\r\n\r\nEDIT: can't do this through their API yet.",
    "line_number": 13,
    "enriched": "File: mindsdb/integrations/handlers/anyscale_endpoints_handler/anyscale_endpoints_handler.py\nCode: @@ -0,0 +1,174 @@\n+import os\n+import json\n+import openai\n+import contextlib\n+from typing import Optional, Dict\n+\n+import pandas as pd\n+\n+from mindsdb.integrations.handlers.openai_handler.openai_handler import OpenAIHandler\n+from mindsdb.integrations.handlers.openai_handler.constants import OPENAI_API_BASE\n+\n+\n+CHAT_MODELS = (\nComment: Ideally this will be retrieved dynamically from Anyscale.\r\n\r\nEDIT: can't do this through their API yet.",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/anyscale_endpoints_handler/anyscale_endpoints_handler.py",
    "pr_number": 7711,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1362388005,
    "comment_created_at": "2023-10-17T16:07:38Z"
  },
  {
    "code": "@@ -51,6 +51,7 @@ def get_requirements_from_file(path):\n         \"dataprep_ml\",\n         \"opentelemetry\",\n         \"langfuse\",\n+        \"langchain_aws\",",
    "comment": "Can you add a comment above for why langchain_aws is here please?",
    "line_number": 54,
    "enriched": "File: tests/scripts/check_requirements.py\nCode: @@ -51,6 +51,7 @@ def get_requirements_from_file(path):\n         \"dataprep_ml\",\n         \"opentelemetry\",\n         \"langfuse\",\n+        \"langchain_aws\",\nComment: Can you add a comment above for why langchain_aws is here please?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "tests/scripts/check_requirements.py",
    "pr_number": 11494,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2312666114,
    "comment_created_at": "2025-08-31T23:07:36Z"
  },
  {
    "code": "@@ -94,19 +94,31 @@ def set(self, handler: DatabaseHandler):\n                 pass\n             self._start_clean()\n \n-    def get(self, name: str) -> Optional[DatabaseHandler]:\n+    def get(self, name: str, thread_safe: bool = False) -> Optional[DatabaseHandler]:",
    "comment": "I don't really like to add additional arguments to functions. Better keep interface simple. What if add 'thread_safe' arg to the handler class itself?\r\nTherefore 'set' may look like:\r\n```python\r\ndef set(self, handler: DatabaseHandler):\r\n    ...\r\n    key = (handler.name, ctx.company_id, 0 if getattr(handler, 'thread_safe', False) else thread_id)\r\n    ...\r\n```\r\nand 'get' like:\r\n```python\r\ndef get(self, name: str) -> Optional[DatabaseHandler]:\r\n    ...\r\n    key = (name, ctx.company_id, thread_id)\r\n    if key not in cache:\r\n        key = (name, ctx.company_id, 0)\r\n    if (\r\n        key not in self.handlers\r\n        or self.handlers[key]['expired_at'] < time()\r\n    ):\r\n        return None\r\n    ...\r\n```\r\nwhat do you think?",
    "line_number": 97,
    "enriched": "File: mindsdb/interfaces/database/integrations.py\nCode: @@ -94,19 +94,31 @@ def set(self, handler: DatabaseHandler):\n                 pass\n             self._start_clean()\n \n-    def get(self, name: str) -> Optional[DatabaseHandler]:\n+    def get(self, name: str, thread_safe: bool = False) -> Optional[DatabaseHandler]:\nComment: I don't really like to add additional arguments to functions. Better keep interface simple. What if add 'thread_safe' arg to the handler class itself?\r\nTherefore 'set' may look like:\r\n```python\r\ndef set(self, handler: DatabaseHandler):\r\n    ...\r\n    key = (handler.name, ctx.company_id, 0 if getattr(handler, 'thread_safe', False) else thread_id)\r\n    ...\r\n```\r\nand 'get' like:\r\n```python\r\ndef get(self, name: str) -> Optional[DatabaseHandler]:\r\n    ...\r\n    key = (name, ctx.company_id, thread_id)\r\n    if key not in cache:\r\n        key = (name, ctx.company_id, 0)\r\n    if (\r\n        key not in self.handlers\r\n        or self.handlers[key]['expired_at'] < time()\r\n    ):\r\n        return None\r\n    ...\r\n```\r\nwhat do you think?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/interfaces/database/integrations.py",
    "pr_number": 9495,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1684049821,
    "comment_created_at": "2024-07-19T08:36:43Z"
  },
  {
    "code": "@@ -0,0 +1,12 @@\n+---\n+title: What IPs need to be whitelisted?\n+sidebarTitle: Whitelisting IPs\n+---\n+\n+Here are the IPs that need to be whitelisted to allow communication with MindsDB:",
    "comment": "Maybe we can mention that this are the MindsDB Cloud Public IP addresses ",
    "line_number": 6,
    "enriched": "File: docs/faqs/whitelist-ips.mdx\nCode: @@ -0,0 +1,12 @@\n+---\n+title: What IPs need to be whitelisted?\n+sidebarTitle: Whitelisting IPs\n+---\n+\n+Here are the IPs that need to be whitelisted to allow communication with MindsDB:\nComment: Maybe we can mention that this are the MindsDB Cloud Public IP addresses ",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "docs/faqs/whitelist-ips.mdx",
    "pr_number": 6281,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1205453498,
    "comment_created_at": "2023-05-25T12:35:33Z"
  },
  {
    "code": "@@ -44,16 +47,25 @@ def connect(self):\n \n         Raises Expection if ping check fails\n         \"\"\"\n+        # if self.is_connected is True:",
    "comment": "Can we remove the comments?",
    "line_number": 50,
    "enriched": "File: mindsdb/integrations/handlers/influxdb_handler/influxdb_handler.py\nCode: @@ -44,16 +47,25 @@ def connect(self):\n \n         Raises Expection if ping check fails\n         \"\"\"\n+        # if self.is_connected is True:\nComment: Can we remove the comments?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/influxdb_handler/influxdb_handler.py",
    "pr_number": 8093,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1380016141,
    "comment_created_at": "2023-11-02T12:18:27Z"
  },
  {
    "code": "@@ -0,0 +1,308 @@\n+from collections import namedtuple\n+from enum import Enum\n+from typing import Dict, List, Optional, Tuple, Union\n+\n+import lightfm\n+import numpy as np\n+import pandas as pd\n+import scipy as sp\n+from pydantic import BaseModel\n+\n+# possibly redundant",
    "comment": "Presumably leftover? I'd be inclined to remove this if so.",
    "line_number": 11,
    "enriched": "File: mindsdb/integrations/handlers/lightfm_handler/helpers.py\nCode: @@ -0,0 +1,308 @@\n+from collections import namedtuple\n+from enum import Enum\n+from typing import Dict, List, Optional, Tuple, Union\n+\n+import lightfm\n+import numpy as np\n+import pandas as pd\n+import scipy as sp\n+from pydantic import BaseModel\n+\n+# possibly redundant\nComment: Presumably leftover? I'd be inclined to remove this if so.",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/lightfm_handler/helpers.py",
    "pr_number": 6037,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1239011586,
    "comment_created_at": "2023-06-22T20:41:50Z"
  },
  {
    "code": "@@ -3,50 +3,94 @@ title: Microsoft SQL Server\n sidebarTitle: Microsoft SQL Server\n ---\n \n-This is the implementation of the Microsoft SQL Server data handler for MindsDB.\n-\n-[Microsoft SQL Server](https://www.microsoft.com/en-us/sql-server) is a relational database management system developed by Microsoft. As a database server, it stores and retrieves data as requested by other software applications, which may run either on the same computer or on another computer across a network.\n+This documentation describes the integration of MindsDB with Microsoft SQL Server, a relational database management system developed by Microsoft.\n+The integration allows for advanced SQL functionalities, extending PostgreSQL's capabilities with MindsDB's features.",
    "comment": "Change PostgreSQL's with Microsoft SQL Server",
    "line_number": 7,
    "enriched": "File: docs/integrations/data-integrations/microsoft-sql-server.mdx\nCode: @@ -3,50 +3,94 @@ title: Microsoft SQL Server\n sidebarTitle: Microsoft SQL Server\n ---\n \n-This is the implementation of the Microsoft SQL Server data handler for MindsDB.\n-\n-[Microsoft SQL Server](https://www.microsoft.com/en-us/sql-server) is a relational database management system developed by Microsoft. As a database server, it stores and retrieves data as requested by other software applications, which may run either on the same computer or on another computer across a network.\n+This documentation describes the integration of MindsDB with Microsoft SQL Server, a relational database management system developed by Microsoft.\n+The integration allows for advanced SQL functionalities, extending PostgreSQL's capabilities with MindsDB's features.\nComment: Change PostgreSQL's with Microsoft SQL Server",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "docs/integrations/data-integrations/microsoft-sql-server.mdx",
    "pr_number": 9083,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1572259254,
    "comment_created_at": "2024-04-19T11:56:39Z"
  },
  {
    "code": "@@ -119,7 +119,25 @@ jobs:\n           # Company independent\n           echo -e \"\\n===============test company independent===============\\n\"\n           pytest -vx tests/integration_tests/flows/test_company_independent.py\n-\n+        fi",
    "comment": "Let's remove this file from the PR",
    "line_number": 122,
    "enriched": "File: .github/workflows/mindsdb.yml\nCode: @@ -119,7 +119,25 @@ jobs:\n           # Company independent\n           echo -e \"\\n===============test company independent===============\\n\"\n           pytest -vx tests/integration_tests/flows/test_company_independent.py\n-\n+        fi\nComment: Let's remove this file from the PR",
    "subcategory": "question",
    "category": "discussion",
    "file_path": ".github/workflows/mindsdb.yml",
    "pr_number": 8062,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1374544127,
    "comment_created_at": "2023-10-27T13:00:27Z"
  },
  {
    "code": "@@ -116,48 +90,132 @@ def select(self, query: ast.Select) -> pd.DataFrame:\n                             params['start_' + arg1] = date.strftime('%Y-%m-%d')\n                             date = date + pd.Timedelta(interval)\n                             params['end_' + arg1] = date.strftime('%Y-%m-%d')\n-                elif arg1 in params_metadata['properties'] or not strict_filter:\n+\n+                elif arg1 in params_metadata['fields'] or not strict_filter:\n                     if op == '=':\n                         params[arg1] = arg2\n                         columns_to_add[arg1] = arg2\n-                filters.append([op, arg1, arg2])\n-\n-            if not all(mandatory_args.values()):\n-                string = 'You must specify the following arguments in the WHERE statement:'\n-                for key in mandatory_args:\n-                    if not mandatory_args[key]:\n-                        string += \"\\n--(required)---\\n* {key}:\\n{help}\\n \".format(key=key, help=dict_to_yaml(params_metadata['properties'][key]))\n-                for key in params_metadata[\"properties\"]:\n-                    if key not in mandatory_args:\n-                        string += \"\\n--(optional)---\\n* {key}:\\n{help}\\n \".format(key=key, help=dict_to_yaml(params_metadata['properties'][key]))\n-                raise NotImplementedError(string)\n-\n-            result = obb_function(**params).to_df()\n \n-            # Check if index is a datetime, if it is we want that as a column\n-            if isinstance(result.index, pd.DatetimeIndex):\n-                result.reset_index(inplace=True)\n-\n-            if query.limit is not None:\n-                result = result.head(query.limit.value)\n-\n-            for key in columns_to_add:\n-                result[key] = params[key]\n-            # filter targets\n-            result = filter_dataframe(result, filters)\n-\n-            columns = self.get_columns()\n+                filters.append([op, arg1, arg2])\n \n-            columns += [col for col in result.columns if col not in columns]\n+            if not all(mandatory_args_set.values()):\n+                missing_args = \", \".join([k for k, v in mandatory_args_set.items() if v is False])\n+                text = f\"You must specify the following arguments in the WHERE statement: {missing_args}\\n\"\n+\n+                # Create docstring for the current function\n+                text += \"\\nDocstring:\"\n+                for param in params_metadata['fields']:\n+                    field = params_metadata['fields'][param]\n+                    if getattr(field.annotation, '__origin__', None) is Union:\n+                        annotation = f\"Union[{', '.join(arg.__name__ for arg in field.annotation.__args__)}]\"\n+                    else:\n+                        annotation = field.annotation.__name__\n+                    text += f\"\\n  * {param}{'' if field.is_required() else ' (optional)'}: {annotation}\\n{field.description}\"\n+\n+                text += f\"\\n\\nFor more information check {func_docs}\"\n+\n+                raise NotImplementedError(text)\n+\n+            try:\n+                obbject = obb_function(**params)\n+\n+                # Extract data in dataframe format\n+                result = obbject.to_df()\n+\n+                if result is None:\n+                    raise Exception(f\"For more information check {func_docs}.\")\n+\n+                # Check if index is a datetime, if it is we want that as a column\n+                if isinstance(result.index, pd.DatetimeIndex):\n+                    result.reset_index(inplace=True)\n+\n+                if query.limit is not None:\n+                    result = result.head(query.limit.value)\n+\n+                    if result is None:\n+                        raise Exception(f\"For more information check {func_docs}.\")\n+\n+                for key in columns_to_add:\n+                    result[key] = params[key]\n+\n+                # filter targets\n+                result = filter_dataframe(result, filters)\n+\n+                if result is None:\n+                    raise Exception(f\"For more information check {func_docs}.\")\n+\n+                columns = self.get_columns()\n+\n+                columns += [col for col in result.columns if col not in columns]\n+\n+                # project targets\n+                result = project_dataframe(result, query.targets, columns)\n+                # test this\n+                if query.order_by:\n+                    result = sort_dataframe(result, query.order_by)\n+\n+                return result\n+            \n+            except AttributeError as e:\n+                print(str(e))",
    "comment": "Can you remove print?",
    "line_number": 160,
    "enriched": "File: mindsdb/integrations/handlers/openbb_handler/openbb_tables.py\nCode: @@ -116,48 +90,132 @@ def select(self, query: ast.Select) -> pd.DataFrame:\n                             params['start_' + arg1] = date.strftime('%Y-%m-%d')\n                             date = date + pd.Timedelta(interval)\n                             params['end_' + arg1] = date.strftime('%Y-%m-%d')\n-                elif arg1 in params_metadata['properties'] or not strict_filter:\n+\n+                elif arg1 in params_metadata['fields'] or not strict_filter:\n                     if op == '=':\n                         params[arg1] = arg2\n                         columns_to_add[arg1] = arg2\n-                filters.append([op, arg1, arg2])\n-\n-            if not all(mandatory_args.values()):\n-                string = 'You must specify the following arguments in the WHERE statement:'\n-                for key in mandatory_args:\n-                    if not mandatory_args[key]:\n-                        string += \"\\n--(required)---\\n* {key}:\\n{help}\\n \".format(key=key, help=dict_to_yaml(params_metadata['properties'][key]))\n-                for key in params_metadata[\"properties\"]:\n-                    if key not in mandatory_args:\n-                        string += \"\\n--(optional)---\\n* {key}:\\n{help}\\n \".format(key=key, help=dict_to_yaml(params_metadata['properties'][key]))\n-                raise NotImplementedError(string)\n-\n-            result = obb_function(**params).to_df()\n \n-            # Check if index is a datetime, if it is we want that as a column\n-            if isinstance(result.index, pd.DatetimeIndex):\n-                result.reset_index(inplace=True)\n-\n-            if query.limit is not None:\n-                result = result.head(query.limit.value)\n-\n-            for key in columns_to_add:\n-                result[key] = params[key]\n-            # filter targets\n-            result = filter_dataframe(result, filters)\n-\n-            columns = self.get_columns()\n+                filters.append([op, arg1, arg2])\n \n-            columns += [col for col in result.columns if col not in columns]\n+            if not all(mandatory_args_set.values()):\n+                missing_args = \", \".join([k for k, v in mandatory_args_set.items() if v is False])\n+                text = f\"You must specify the following arguments in the WHERE statement: {missing_args}\\n\"\n+\n+                # Create docstring for the current function\n+                text += \"\\nDocstring:\"\n+                for param in params_metadata['fields']:\n+                    field = params_metadata['fields'][param]\n+                    if getattr(field.annotation, '__origin__', None) is Union:\n+                        annotation = f\"Union[{', '.join(arg.__name__ for arg in field.annotation.__args__)}]\"\n+                    else:\n+                        annotation = field.annotation.__name__\n+                    text += f\"\\n  * {param}{'' if field.is_required() else ' (optional)'}: {annotation}\\n{field.description}\"\n+\n+                text += f\"\\n\\nFor more information check {func_docs}\"\n+\n+                raise NotImplementedError(text)\n+\n+            try:\n+                obbject = obb_function(**params)\n+\n+                # Extract data in dataframe format\n+                result = obbject.to_df()\n+\n+                if result is None:\n+                    raise Exception(f\"For more information check {func_docs}.\")\n+\n+                # Check if index is a datetime, if it is we want that as a column\n+                if isinstance(result.index, pd.DatetimeIndex):\n+                    result.reset_index(inplace=True)\n+\n+                if query.limit is not None:\n+                    result = result.head(query.limit.value)\n+\n+                    if result is None:\n+                        raise Exception(f\"For more information check {func_docs}.\")\n+\n+                for key in columns_to_add:\n+                    result[key] = params[key]\n+\n+                # filter targets\n+                result = filter_dataframe(result, filters)\n+\n+                if result is None:\n+                    raise Exception(f\"For more information check {func_docs}.\")\n+\n+                columns = self.get_columns()\n+\n+                columns += [col for col in result.columns if col not in columns]\n+\n+                # project targets\n+                result = project_dataframe(result, query.targets, columns)\n+                # test this\n+                if query.order_by:\n+                    result = sort_dataframe(result, query.order_by)\n+\n+                return result\n+            \n+            except AttributeError as e:\n+                print(str(e))\nComment: Can you remove print?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/openbb_handler/openbb_tables.py",
    "pr_number": 8409,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1414037124,
    "comment_created_at": "2023-12-04T15:06:24Z"
  },
  {
    "code": "@@ -0,0 +1,86 @@\n+# Welcome to the MindsDB Manual QA Testing for DocumentDB Handler",
    "comment": "Let's exclude Manual QA file, we are removing this files.",
    "line_number": 1,
    "enriched": "File: mindsdb/integrations/handlers/documentdb_handler/Manual_QA.md\nCode: @@ -0,0 +1,86 @@\n+# Welcome to the MindsDB Manual QA Testing for DocumentDB Handler\nComment: Let's exclude Manual QA file, we are removing this files.",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/documentdb_handler/Manual_QA.md",
    "pr_number": 7607,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1350077482,
    "comment_created_at": "2023-10-09T09:43:49Z"
  },
  {
    "code": "@@ -0,0 +1 @@\n+# KDB Handler",
    "comment": "Let's make sure we have the initial README structure.",
    "line_number": 1,
    "enriched": "File: mindsdb/integrations/handlers/kdb_handler/README.md\nCode: @@ -0,0 +1 @@\n+# KDB Handler\nComment: Let's make sure we have the initial README structure.",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/kdb_handler/README.md",
    "pr_number": 8783,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1494688058,
    "comment_created_at": "2024-02-19T15:07:27Z"
  },
  {
    "code": "@@ -137,6 +139,12 @@ def process_result_value(self, value, dialect):  # select\n         return json.loads(value) if value is not None else None\n \n \n+# Hack to make mind-castle use mindsdb's \"Json\" type decorator as a backend\n+# We need to switch this column to be a postgres json column in future\n+class SecretDataJson(SecretData):\n+    impl = Json",
    "comment": "How much extra work would be to change the column now?\nIs there any tests we can add to validate this new behavior?",
    "line_number": 145,
    "enriched": "File: mindsdb/interfaces/storage/db.py\nCode: @@ -137,6 +139,12 @@ def process_result_value(self, value, dialect):  # select\n         return json.loads(value) if value is not None else None\n \n \n+# Hack to make mind-castle use mindsdb's \"Json\" type decorator as a backend\n+# We need to switch this column to be a postgres json column in future\n+class SecretDataJson(SecretData):\n+    impl = Json\nComment: How much extra work would be to change the column now?\nIs there any tests we can add to validate this new behavior?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/interfaces/storage/db.py",
    "pr_number": 11808,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2492221719,
    "comment_created_at": "2025-11-04T22:33:58Z"
  },
  {
    "code": "@@ -0,0 +1,127 @@\n+-- ============================================\n+-- Knowledge Base Evaluation Suite\n+-- Crypto Protocol Auditor - web3_kb\n+-- ============================================\n+\n+-- Step 1: Verify KB exists and get metadata\n+-- ===========================================\n+SELECT \n+    'KB Metadata Check' as test_name,\n+    COUNT(*) as total_documents,\n+    'PASS' as status\n+FROM web3_kb;\n+\n+-- Step 2: Test Basic Search Functionality\n+-- =========================================\n+SELECT \n+    'Test 1: Proof of Work' as query,\n+    COUNT(*) as results_found,\n+    ROUND(AVG(relevance), 3) as avg_relevance\n+FROM web3_kb",
    "comment": "**correctness**: `relevance` column is used in multiple queries (e.g., AVG, MIN, MAX, ORDER BY), but if `web3_kb` does not always have a `relevance` column (e.g., if it's not a view or table with that field), these queries will fail at runtime.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nCheck the schema of the `web3_kb` table or view in 'mindsdb hacktoberfest/use-cases/crypto-protocol-auditor/KB_EVALUATION.sql' (lines 19-20 and similar). If `relevance` is not a column, replace all references to `relevance` with the correct column name or remove those aggregations. Ensure all queries referencing `relevance` will not fail at runtime.\n```\n</details>\n<!-- ai_prompt_end -->\n\n",
    "line_number": 20,
    "enriched": "File: mindsdb hacktoberfest/use-cases/crypto-protocol-auditor/KB_EVALUATION.sql\nCode: @@ -0,0 +1,127 @@\n+-- ============================================\n+-- Knowledge Base Evaluation Suite\n+-- Crypto Protocol Auditor - web3_kb\n+-- ============================================\n+\n+-- Step 1: Verify KB exists and get metadata\n+-- ===========================================\n+SELECT \n+    'KB Metadata Check' as test_name,\n+    COUNT(*) as total_documents,\n+    'PASS' as status\n+FROM web3_kb;\n+\n+-- Step 2: Test Basic Search Functionality\n+-- =========================================\n+SELECT \n+    'Test 1: Proof of Work' as query,\n+    COUNT(*) as results_found,\n+    ROUND(AVG(relevance), 3) as avg_relevance\n+FROM web3_kb\nComment: **correctness**: `relevance` column is used in multiple queries (e.g., AVG, MIN, MAX, ORDER BY), but if `web3_kb` does not always have a `relevance` column (e.g., if it's not a view or table with that field), these queries will fail at runtime.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nCheck the schema of the `web3_kb` table or view in 'mindsdb hacktoberfest/use-cases/crypto-protocol-auditor/KB_EVALUATION.sql' (lines 19-20 and similar). If `relevance` is not a column, replace all references to `relevance` with the correct column name or remove those aggregations. Ensure all queries referencing `relevance` will not fail at runtime.\n```\n</details>\n<!-- ai_prompt_end -->\n\n",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb hacktoberfest/use-cases/crypto-protocol-auditor/KB_EVALUATION.sql",
    "pr_number": 11843,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2483974225,
    "comment_created_at": "2025-11-01T22:14:36Z"
  },
  {
    "code": "@@ -1,137 +1,348 @@\n ---\n-title: AI Agents with LLMs and Skills\n-sidebarTitle: Agents\n+title: How to Use Agents\n+sidebarTitle: AI Agents\n ---\n \n-## Description\n+Agents enable conversation with data, including structured and unstructured data connected to MindsDB.\n \n-With MindsDB, you can create and deploy AI agents that comprise AI models and customizable skills such as knowledge bases and text-to-SQL.\n+## Deploy Agents\n \n-<p align=\"center\">\n-  <img src=\"/assets/agent_diagram.png\" />\n-</p>\n+Here is the syntax for creating an agent:\n \n-AI agents comprise of skills, such as text2sql and knowledge_base, and a conversational model.\n-\n-* Skills provide data resources to an agent, enabling it to answer questions about available data. Learn more about [skills here](/sdks/python/agents_skills). Learn more about [knowledge bases here](/sdks/python/agents_knowledge_bases).\n-\n-* A conversational model (like OpenAI) from LangChain utilizes [tools as skills](https://python.langchain.com/docs/how_to/#tools) to respond to user input. Users can customize these models with their own prompts to fit their use cases.\n+```python\n+# prerequisite: get an existing project\n+project = server.get_project()",
    "comment": "There isn't really a need to do this, because the `Server` object returned when `mindsdb_sdk.connect()` is called is a `Project` in itself.",
    "line_number": 14,
    "enriched": "File: docs/sdks/python/agents.mdx\nCode: @@ -1,137 +1,348 @@\n ---\n-title: AI Agents with LLMs and Skills\n-sidebarTitle: Agents\n+title: How to Use Agents\n+sidebarTitle: AI Agents\n ---\n \n-## Description\n+Agents enable conversation with data, including structured and unstructured data connected to MindsDB.\n \n-With MindsDB, you can create and deploy AI agents that comprise AI models and customizable skills such as knowledge bases and text-to-SQL.\n+## Deploy Agents\n \n-<p align=\"center\">\n-  <img src=\"/assets/agent_diagram.png\" />\n-</p>\n+Here is the syntax for creating an agent:\n \n-AI agents comprise of skills, such as text2sql and knowledge_base, and a conversational model.\n-\n-* Skills provide data resources to an agent, enabling it to answer questions about available data. Learn more about [skills here](/sdks/python/agents_skills). Learn more about [knowledge bases here](/sdks/python/agents_knowledge_bases).\n-\n-* A conversational model (like OpenAI) from LangChain utilizes [tools as skills](https://python.langchain.com/docs/how_to/#tools) to respond to user input. Users can customize these models with their own prompts to fit their use cases.\n+```python\n+# prerequisite: get an existing project\n+project = server.get_project()\nComment: There isn't really a need to do this, because the `Server` object returned when `mindsdb_sdk.connect()` is called is a `Project` in itself.",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "docs/sdks/python/agents.mdx",
    "pr_number": 11415,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2266035144,
    "comment_created_at": "2025-08-11T08:41:38Z"
  },
  {
    "code": "@@ -109,12 +109,12 @@ target \"images\" {\n       {\n         # If you make any changes here, make them to cloud-cpu as well\n         name = \"cloud\"\n-        extras = \".[lightwood,huggingface,statsforecast-extra,neuralforecast-extra,timegpt,mssql,youtube,gmail,pgvector,writer,rag,github,snowflake,clickhouse,bigquery,elasticsearch,s3,dynamodb,databricks,oracle,teradata,hive,one_drive,opentelemetry,langfuse,jira,salesforce] darts datasetsforecast\"\n+        extras = \".[lightwood,huggingface,statsforecast-extra,neuralforecast-extra,timegpt,mssql,youtube,gmail,pgvector,writer,rag,github,snowflake,clickhouse,bigquery,elasticsearch,s3,dynamodb,databricks,oracle,teradata,hive,one_drive,opentelemetry,langfuse,jira,salesforce] darts datasetsforecast transformers\"",
    "comment": "Why move the transformers dep here? ",
    "line_number": 112,
    "enriched": "File: docker/docker-bake.hcl\nCode: @@ -109,12 +109,12 @@ target \"images\" {\n       {\n         # If you make any changes here, make them to cloud-cpu as well\n         name = \"cloud\"\n-        extras = \".[lightwood,huggingface,statsforecast-extra,neuralforecast-extra,timegpt,mssql,youtube,gmail,pgvector,writer,rag,github,snowflake,clickhouse,bigquery,elasticsearch,s3,dynamodb,databricks,oracle,teradata,hive,one_drive,opentelemetry,langfuse,jira,salesforce] darts datasetsforecast\"\n+        extras = \".[lightwood,huggingface,statsforecast-extra,neuralforecast-extra,timegpt,mssql,youtube,gmail,pgvector,writer,rag,github,snowflake,clickhouse,bigquery,elasticsearch,s3,dynamodb,databricks,oracle,teradata,hive,one_drive,opentelemetry,langfuse,jira,salesforce] darts datasetsforecast transformers\"\nComment: Why move the transformers dep here? ",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "docker/docker-bake.hcl",
    "pr_number": 11155,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2162850870,
    "comment_created_at": "2025-06-24T02:39:34Z"
  },
  {
    "code": "@@ -694,12 +694,11 @@ def answer_describe_predictor(self, statement):\n         # try full name\n         attribute = None\n         model_info = self._get_model_info(statement.value)\n+        parts = statement.value.parts.copy()\n+        attribute = parts.pop(-1)\n+        model_info = self._get_model_info(Identifier(parts=parts)) if not model_info  else model_info",
    "comment": "here two _get_model_info are called in sequence. \r\nThe right way is: to call once,  and call once again with different name only if model wasn't found.\r\n(As it was before)\r\nTry this PR #6686 (or update from staging, because it was merged long ago) - it should help you",
    "line_number": 699,
    "enriched": "File: mindsdb/api/mysql/mysql_proxy/executor/executor_commands.py\nCode: @@ -694,12 +694,11 @@ def answer_describe_predictor(self, statement):\n         # try full name\n         attribute = None\n         model_info = self._get_model_info(statement.value)\n+        parts = statement.value.parts.copy()\n+        attribute = parts.pop(-1)\n+        model_info = self._get_model_info(Identifier(parts=parts)) if not model_info  else model_info\nComment: here two _get_model_info are called in sequence. \r\nThe right way is: to call once,  and call once again with different name only if model wasn't found.\r\n(As it was before)\r\nTry this PR #6686 (or update from staging, because it was merged long ago) - it should help you",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/api/mysql/mysql_proxy/executor/executor_commands.py",
    "pr_number": 6614,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1279089488,
    "comment_created_at": "2023-07-31T10:14:50Z"
  },
  {
    "code": "@@ -0,0 +1,112 @@\n+name: Test on Deploy\n+\n+on:\n+  workflow_call:\n+    secrets:\n+      OPENAI_API_KEY:\n+        required: true\n+\n+defaults:\n+  run:\n+    shell: bash\n+\n+\n+jobs:\n+  # Creates a matrix of environments to test against using matrix_includes.json\n+  matrix_prep:\n+    runs-on: ubuntu-latest\n+    outputs:\n+      matrix: ${{ steps.set-matrix.outputs.matrix }}\n+    steps:\n+      - uses: actions/checkout@v3.5.3\n+      - id: set-matrix\n+        uses: JoshuaTheMiller/conditional-build-matrix@v1.0.1\n+        with:\n+          filter: '[?runOnBranch==`${{ github.ref }}` || runOnBranch==`always`]'\n+\n+  # Run our integration tests\n+  test:\n+    needs: [matrix_prep]\n+    strategy:\n+      matrix: ${{fromJson(needs.matrix_prep.outputs.matrix)}}\n+    name: Run integration tests\n+    runs-on: ${{ matrix.runs_on }}\n+    if: github.ref_type == 'branch'\n+    steps:\n+    - uses: actions/checkout@v3.5.3\n+    - name: Set up Python\n+      uses: actions/setup-python@v4.7.0\n+      with:\n+        python-version: ${{ matrix.python-version }}\n+        cache: pip\n+        cache-dependency-path: '**/requirements*.txt'\n+    - name: Install dependencies\n+      run: |\n+        pip install .\n+        pip install -r requirements/requirements-test.txt\n+        pip install .[lightwood]  # TODO: for now some tests rely on lightwood and huggingface\n+        pip install .[huggingface]\n+        pip freeze  \n+    - name: Run integration api and flow tests\n+      run: |\n+\n+        if [ \"$RUNNER_OS\" == \"Linux\" ]; then\n+\n+          mkdir prometheus_metrics\n+\n+          pytest -vx tests/integration_tests/flows/test_ml_task_queue.py\n+\n+          # MySQL API\n+          echo -e \"\\n===============test MySQL API===============\\n\"\n+          pytest -vx tests/integration_tests/flows/test_mysql_api.py\n+          \n+          # MySQL binary API\n+          echo -e \"\\n===============test MySQL binary API===============\\n\"\n+          pytest -vx -k 'not TestMySqlApi' tests/integration_tests/flows/test_mysql_bin_api.py\n+\n+          # echo -e \"\\n===============TS predictions===============\\n\"\n+          pytest -svx tests/integration_tests/flows/test_ts_predictions.py\n+\n+          # HTTP\n+          echo -e \"\\n===============test HTTP===============\\n\"\n+          pytest -vx tests/integration_tests/flows/test_http.py\n+\n+          # Company independent\n+          echo -e \"\\n===============test company independent===============\\n\"\n+          pytest -vx tests/integration_tests/flows/test_company_independent.py\n+        fi\n+      env:\n+        PROMETHEUS_MULTIPROC_DIR: ./prometheus_metrics\n+        CHECK_FOR_UPDATES: False\n+\n+    - name: Run Learning Hub Tests\n+      run: |\n+          # Currently requires a DB to be running. New testing system is being set up\n+          # echo -e \"\\n===============MySQL Handler===============\\n\"\n+          # pytest -svx mindsdb/integrations/handlers/mysql_handler/\n+\n+          # Currently requires a DB to be running. New testing system is being set up\n+          # echo -e \"\\n===============PostgreSQL Handler===============\\n\"\n+          # pytest -svx mindsdb/integrations/handlers/postgres_handler/\n+\n+          echo -e \"\\n===============MariaDB Handler===============\\n\"\n+          pytest -svx tests/handler_tests/test_mariadb_handler.py\n+\n+          echo -e \"\\n===============File Handler===============\\n\"\n+          pytest -svx mindsdb/integrations/handlers/file_handler/\n+\n+          echo -e \"\\n===============Home Rental Home Prices===============\\n\"\n+          pytest -vx tests/integration_tests/flows/test_home_rental_prices_tutorial.py\n+\n+          echo -e \"\\n===============Forecast Quaterly House Sales===============\\n\"\n+          pytest -vx tests/integration_tests/flows/test_forecast_quaterly_house_tutorial.py\n+\n+          echo -e \"\\n===============Predict Text Sentiment Wih Huggingface===============\\n\"\n+          pytest -vx tests/integration_tests/flows/test_predict_text_sentiment_huggingface_tutorial.py\n+\n+          echo -e \"\\n===============Predict Text Sentiment Wih OpenAI===============\\n\"\n+          pytest -vx tests/integration_tests/flows/test_predict_text_sentiment_openai_tutorial.py",
    "comment": "Nitpick, and feel free to ignore it. \r\nI prefer having the list of tests in a makefile, which I can run before submitting the commit.\r\n\r\nOther than that, LGTM.",
    "line_number": 108,
    "enriched": "File: .github/workflows/test_on_deploy.yml\nCode: @@ -0,0 +1,112 @@\n+name: Test on Deploy\n+\n+on:\n+  workflow_call:\n+    secrets:\n+      OPENAI_API_KEY:\n+        required: true\n+\n+defaults:\n+  run:\n+    shell: bash\n+\n+\n+jobs:\n+  # Creates a matrix of environments to test against using matrix_includes.json\n+  matrix_prep:\n+    runs-on: ubuntu-latest\n+    outputs:\n+      matrix: ${{ steps.set-matrix.outputs.matrix }}\n+    steps:\n+      - uses: actions/checkout@v3.5.3\n+      - id: set-matrix\n+        uses: JoshuaTheMiller/conditional-build-matrix@v1.0.1\n+        with:\n+          filter: '[?runOnBranch==`${{ github.ref }}` || runOnBranch==`always`]'\n+\n+  # Run our integration tests\n+  test:\n+    needs: [matrix_prep]\n+    strategy:\n+      matrix: ${{fromJson(needs.matrix_prep.outputs.matrix)}}\n+    name: Run integration tests\n+    runs-on: ${{ matrix.runs_on }}\n+    if: github.ref_type == 'branch'\n+    steps:\n+    - uses: actions/checkout@v3.5.3\n+    - name: Set up Python\n+      uses: actions/setup-python@v4.7.0\n+      with:\n+        python-version: ${{ matrix.python-version }}\n+        cache: pip\n+        cache-dependency-path: '**/requirements*.txt'\n+    - name: Install dependencies\n+      run: |\n+        pip install .\n+        pip install -r requirements/requirements-test.txt\n+        pip install .[lightwood]  # TODO: for now some tests rely on lightwood and huggingface\n+        pip install .[huggingface]\n+        pip freeze  \n+    - name: Run integration api and flow tests\n+      run: |\n+\n+        if [ \"$RUNNER_OS\" == \"Linux\" ]; then\n+\n+          mkdir prometheus_metrics\n+\n+          pytest -vx tests/integration_tests/flows/test_ml_task_queue.py\n+\n+          # MySQL API\n+          echo -e \"\\n===============test MySQL API===============\\n\"\n+          pytest -vx tests/integration_tests/flows/test_mysql_api.py\n+          \n+          # MySQL binary API\n+          echo -e \"\\n===============test MySQL binary API===============\\n\"\n+          pytest -vx -k 'not TestMySqlApi' tests/integration_tests/flows/test_mysql_bin_api.py\n+\n+          # echo -e \"\\n===============TS predictions===============\\n\"\n+          pytest -svx tests/integration_tests/flows/test_ts_predictions.py\n+\n+          # HTTP\n+          echo -e \"\\n===============test HTTP===============\\n\"\n+          pytest -vx tests/integration_tests/flows/test_http.py\n+\n+          # Company independent\n+          echo -e \"\\n===============test company independent===============\\n\"\n+          pytest -vx tests/integration_tests/flows/test_company_independent.py\n+        fi\n+      env:\n+        PROMETHEUS_MULTIPROC_DIR: ./prometheus_metrics\n+        CHECK_FOR_UPDATES: False\n+\n+    - name: Run Learning Hub Tests\n+      run: |\n+          # Currently requires a DB to be running. New testing system is being set up\n+          # echo -e \"\\n===============MySQL Handler===============\\n\"\n+          # pytest -svx mindsdb/integrations/handlers/mysql_handler/\n+\n+          # Currently requires a DB to be running. New testing system is being set up\n+          # echo -e \"\\n===============PostgreSQL Handler===============\\n\"\n+          # pytest -svx mindsdb/integrations/handlers/postgres_handler/\n+\n+          echo -e \"\\n===============MariaDB Handler===============\\n\"\n+          pytest -svx tests/handler_tests/test_mariadb_handler.py\n+\n+          echo -e \"\\n===============File Handler===============\\n\"\n+          pytest -svx mindsdb/integrations/handlers/file_handler/\n+\n+          echo -e \"\\n===============Home Rental Home Prices===============\\n\"\n+          pytest -vx tests/integration_tests/flows/test_home_rental_prices_tutorial.py\n+\n+          echo -e \"\\n===============Forecast Quaterly House Sales===============\\n\"\n+          pytest -vx tests/integration_tests/flows/test_forecast_quaterly_house_tutorial.py\n+\n+          echo -e \"\\n===============Predict Text Sentiment Wih Huggingface===============\\n\"\n+          pytest -vx tests/integration_tests/flows/test_predict_text_sentiment_huggingface_tutorial.py\n+\n+          echo -e \"\\n===============Predict Text Sentiment Wih OpenAI===============\\n\"\n+          pytest -vx tests/integration_tests/flows/test_predict_text_sentiment_openai_tutorial.py\nComment: Nitpick, and feel free to ignore it. \r\nI prefer having the list of tests in a makefile, which I can run before submitting the commit.\r\n\r\nOther than that, LGTM.",
    "subcategory": "question",
    "category": "discussion",
    "file_path": ".github/workflows/test_on_deploy.yml",
    "pr_number": 8902,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1531292091,
    "comment_created_at": "2024-03-20T00:02:20Z"
  },
  {
    "code": "@@ -0,0 +1,33 @@\n+---\n+title: The from_env() Function\n+sidebarTitle: The from_env() Function\n+---\n+\n+MindsDB provides the `from_env()` function that lets users pull values from the environment variables into MindsDB.\n+\n+## Usage\n+\n+Here is how to use the `from_env()` function.\n+\n+* Save the environment variable into a variable using the `from_env()` function (or saving the value directly).\n+\n+```sql\n+SET @my_env_var  = from_env(\"MY_ENV_VAR\")",
    "comment": "due to security conserns (possible access to artitrary env variables), only variables with \r\nMDB_ prefix  is possible to use",
    "line_number": 15,
    "enriched": "File: docs/mindsdb_sql/functions/from_env.mdx\nCode: @@ -0,0 +1,33 @@\n+---\n+title: The from_env() Function\n+sidebarTitle: The from_env() Function\n+---\n+\n+MindsDB provides the `from_env()` function that lets users pull values from the environment variables into MindsDB.\n+\n+## Usage\n+\n+Here is how to use the `from_env()` function.\n+\n+* Save the environment variable into a variable using the `from_env()` function (or saving the value directly).\n+\n+```sql\n+SET @my_env_var  = from_env(\"MY_ENV_VAR\")\nComment: due to security conserns (possible access to artitrary env variables), only variables with \r\nMDB_ prefix  is possible to use",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "docs/mindsdb_sql/functions/from_env.mdx",
    "pr_number": 10945,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2120925071,
    "comment_created_at": "2025-06-02T11:54:12Z"
  },
  {
    "code": "@@ -108,6 +108,12 @@ This section contains instructions on how to connect data sources to MindsDB.\n \n </CardGroup>\n \n+### Vector Databases\n+\n+<CardGroup cols={4}>\n+    <Card title=\"Databricks\" icon=\"link\" href=\"/integrations/vector-db-integrations/chromadb.mdx\"></Card>",
    "comment": "should title be 'Databricks'?",
    "line_number": 114,
    "enriched": "File: docs/integrations/data-sources-overview.mdx\nCode: @@ -108,6 +108,12 @@ This section contains instructions on how to connect data sources to MindsDB.\n \n </CardGroup>\n \n+### Vector Databases\n+\n+<CardGroup cols={4}>\n+    <Card title=\"Databricks\" icon=\"link\" href=\"/integrations/vector-db-integrations/chromadb.mdx\"></Card>\nComment: should title be 'Databricks'?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "docs/integrations/data-sources-overview.mdx",
    "pr_number": 8281,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1383761379,
    "comment_created_at": "2023-11-06T18:10:05Z"
  },
  {
    "code": "@@ -19,13 +19,14 @@\n \n class LLMReranker(BaseDocumentCompressor):\n     filtering_threshold: float = 0.0  # Default threshold for filtering\n+    provider: str = 'openai'\n     model: str = DEFAULT_RERANKING_MODEL  # Model to use for reranking\n     temperature: float = 0.0  # Temperature for the model\n-    openai_api_key: Optional[str] = None\n+    api_key: Optional[str] = None",
    "comment": "@tmichaeldb, I didn't find that this name is used as input to LLMReranker\r\nbut could miss something ",
    "line_number": 25,
    "enriched": "File: mindsdb/integrations/utilities/rag/rerankers/reranker_compressor.py\nCode: @@ -19,13 +19,14 @@\n \n class LLMReranker(BaseDocumentCompressor):\n     filtering_threshold: float = 0.0  # Default threshold for filtering\n+    provider: str = 'openai'\n     model: str = DEFAULT_RERANKING_MODEL  # Model to use for reranking\n     temperature: float = 0.0  # Temperature for the model\n-    openai_api_key: Optional[str] = None\n+    api_key: Optional[str] = None\nComment: @tmichaeldb, I didn't find that this name is used as input to LLMReranker\r\nbut could miss something ",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/utilities/rag/rerankers/reranker_compressor.py",
    "pr_number": 10702,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2039810282,
    "comment_created_at": "2025-04-11T15:40:44Z"
  },
  {
    "code": "@@ -9,14 +9,25 @@ class ListFilesTable(APIResource):\n     def list(self,\n              targets: List[str] = None,\n              conditions: List[FilterCondition] = None,\n+             limit: int = None,\n              *args, **kwargs) -> pd.DataFrame:\n \n-        tables = self.handler._get_tables()\n+        buckets = None\n+        for condition in conditions:\n+            if condition.column == 'bucket':\n+                if condition.op == FilterOperator.IN:",
    "comment": "`FilterOperator` has been used, but not imported.",
    "line_number": 18,
    "enriched": "File: mindsdb/integrations/handlers/gcs_handler/gcs_tables.py\nCode: @@ -9,14 +9,25 @@ class ListFilesTable(APIResource):\n     def list(self,\n              targets: List[str] = None,\n              conditions: List[FilterCondition] = None,\n+             limit: int = None,\n              *args, **kwargs) -> pd.DataFrame:\n \n-        tables = self.handler._get_tables()\n+        buckets = None\n+        for condition in conditions:\n+            if condition.column == 'bucket':\n+                if condition.op == FilterOperator.IN:\nComment: `FilterOperator` has been used, but not imported.",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/gcs_handler/gcs_tables.py",
    "pr_number": 10005,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1814923362,
    "comment_created_at": "2024-10-24T12:51:32Z"
  },
  {
    "code": "@@ -394,14 +399,21 @@ def _process_websocket_message(client: SocketModeClient, request: SocketModeRequ\n                 return\n \n             payload_event = request.payload['event']\n-            if payload_event['type'] not in ('message', 'app_mention'):\n+\n+            if payload_event['type'] == 'message' and payload_event['channel_type'] != 'im':",
    "comment": "[Here](https://github.com/mindsdb/mindsdb/pull/9402/files#diff-a28ff1351772fd317f3255e86983b784fb17ebb20c90f5cd4b63637627c5e931L378) I removed `['channel_type'] != 'im'` because someone wanted to get answers in channels...",
    "line_number": 403,
    "enriched": "File: mindsdb/integrations/handlers/slack_handler/slack_handler.py\nCode: @@ -394,14 +399,21 @@ def _process_websocket_message(client: SocketModeClient, request: SocketModeRequ\n                 return\n \n             payload_event = request.payload['event']\n-            if payload_event['type'] not in ('message', 'app_mention'):\n+\n+            if payload_event['type'] == 'message' and payload_event['channel_type'] != 'im':\nComment: [Here](https://github.com/mindsdb/mindsdb/pull/9402/files#diff-a28ff1351772fd317f3255e86983b784fb17ebb20c90f5cd4b63637627c5e931L378) I removed `['channel_type'] != 'im'` because someone wanted to get answers in channels...",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/slack_handler/slack_handler.py",
    "pr_number": 9732,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1764954254,
    "comment_created_at": "2024-09-18T12:20:13Z"
  },
  {
    "code": "@@ -273,6 +273,8 @@ def post(self, project_name, agent_name):\n             )\n \n         # Add OpenAI API key to agent params if not already present.\n+        if not existing_agent.params:",
    "comment": "it does not matter, but maybe `existing_agent.params is None`? because `not {}` is true",
    "line_number": 276,
    "enriched": "File: mindsdb/api/http/namespaces/agents.py\nCode: @@ -273,6 +273,8 @@ def post(self, project_name, agent_name):\n             )\n \n         # Add OpenAI API key to agent params if not already present.\n+        if not existing_agent.params:\nComment: it does not matter, but maybe `existing_agent.params is None`? because `not {}` is true",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/api/http/namespaces/agents.py",
    "pr_number": 9420,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1658867363,
    "comment_created_at": "2024-06-28T14:45:25Z"
  },
  {
    "code": "@@ -125,7 +133,145 @@ def __init__(self, name: str):\n         super().__init__(name)\n \n \n-class BaseMLEngine:\n+class ArgProbeMixin:\n+    \"\"\"\n+    A mixin class that provides probing of arguments that\n+    are needed by a handler during creation and prediction time\n+    by running the static analysis on the source code of the handler.\n+    \"\"\"\n+\n+    class ArgProbeVisitor(ast.NodeVisitor):\n+        def __init__(self):\n+            self.arg_keys = []\n+            self.var_names_to_track = {\"args\"}\n+\n+        def visit_Assign(self, node):",
    "comment": "Not sure we actually do this anywhere at the moment, but would a new method be required in the `ArgProbeVisitor` for things like `arg_name = my_args.get(key, value)`? Or would `visit_Assign()` still capture it?",
    "line_number": 148,
    "enriched": "File: mindsdb/integrations/libs/base.py\nCode: @@ -125,7 +133,145 @@ def __init__(self, name: str):\n         super().__init__(name)\n \n \n-class BaseMLEngine:\n+class ArgProbeMixin:\n+    \"\"\"\n+    A mixin class that provides probing of arguments that\n+    are needed by a handler during creation and prediction time\n+    by running the static analysis on the source code of the handler.\n+    \"\"\"\n+\n+    class ArgProbeVisitor(ast.NodeVisitor):\n+        def __init__(self):\n+            self.arg_keys = []\n+            self.var_names_to_track = {\"args\"}\n+\n+        def visit_Assign(self, node):\nComment: Not sure we actually do this anywhere at the moment, but would a new method be required in the `ArgProbeVisitor` for things like `arg_name = my_args.get(key, value)`? Or would `visit_Assign()` still capture it?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/libs/base.py",
    "pr_number": 7087,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1296359700,
    "comment_created_at": "2023-08-16T19:49:45Z"
  },
  {
    "code": "@@ -58,3 +60,16 @@ def infer_frequency(df, time_column, default=DEFAULT_FREQUENCY):\n     except TypeError:\n         inferred_freq = default\n     return inferred_freq if inferred_freq is not None else default\n+\n+\n+def get_best_model_from_results_df(nixtla_results_df, error_metric=mean_absolute_error):\n+    \"\"\"Gets the best model based, on lowest error, from a results df\n+    with a column for each nixtla model.\n+    \"\"\"\n+    best_model, current_error = None, np.inf\n+    for result_column in nixtla_results_df.columns[3:]:\n+        model_error = error_metric(nixtla_results_df[result_column], nixtla_results_df[\"y\"])\n+        if model_error < current_error:\n+            best_model = result_column\n+            current_error = model_error\n+    return best_model",
    "comment": "Maybe worth reporting the `model_error`s here for `DESCRIBE`? Probably out of scope for this PR though.",
    "line_number": 75,
    "enriched": "File: mindsdb/integrations/utilities/time_series_utils.py\nCode: @@ -58,3 +60,16 @@ def infer_frequency(df, time_column, default=DEFAULT_FREQUENCY):\n     except TypeError:\n         inferred_freq = default\n     return inferred_freq if inferred_freq is not None else default\n+\n+\n+def get_best_model_from_results_df(nixtla_results_df, error_metric=mean_absolute_error):\n+    \"\"\"Gets the best model based, on lowest error, from a results df\n+    with a column for each nixtla model.\n+    \"\"\"\n+    best_model, current_error = None, np.inf\n+    for result_column in nixtla_results_df.columns[3:]:\n+        model_error = error_metric(nixtla_results_df[result_column], nixtla_results_df[\"y\"])\n+        if model_error < current_error:\n+            best_model = result_column\n+            current_error = model_error\n+    return best_model\nComment: Maybe worth reporting the `model_error`s here for `DESCRIBE`? Probably out of scope for this PR though.",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/utilities/time_series_utils.py",
    "pr_number": 5012,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1142417076,
    "comment_created_at": "2023-03-20T16:45:33Z"
  },
  {
    "code": "@@ -174,6 +174,7 @@ def create_validation(target: Text, args: Dict = None, **kwargs: Any) -> None:\n                 \"max_tokens\",\n                 \"temperature\",\n                 \"openai_api_key\",\n+                \"api_key\",",
    "comment": "is this line required? ",
    "line_number": 177,
    "enriched": "File: mindsdb/integrations/handlers/openai_handler/openai_handler.py\nCode: @@ -174,6 +174,7 @@ def create_validation(target: Text, args: Dict = None, **kwargs: Any) -> None:\n                 \"max_tokens\",\n                 \"temperature\",\n                 \"openai_api_key\",\n+                \"api_key\",\nComment: is this line required? ",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/openai_handler/openai_handler.py",
    "pr_number": 11312,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2209989418,
    "comment_created_at": "2025-07-16T10:53:42Z"
  },
  {
    "code": "@@ -0,0 +1,47 @@\n+### Briefly describe what ML framework does this handler integrate to MindsDB, and how? \n+This handler integrates with MindsDB using FLAML, an open-source machine learning framework developed by Microsoft for research purposes. FLAML is a powerful Python library that automates the process of selecting models, hyperparameters, and other choices in machine learning applications. By utilizing FLAML, this handler can take advantage of its efficient and effective automation capabilities. Although there is more work to be done in implementing FLAML, it is a useful tool for automating machine learning tasks. Microsoft is currently developing new features for automated machine learning, which are in beta stage and are planned to be integrated into MindsDB in the near future.\n+\n+### Why is this integration useful? What does the ideal predictive use case for this integration look like? When would you definitely not use this integration? \n+This integration is useful for automating machine learning tasks and improving predictive accuracy. An ideal use case is when there is a large dataset with many variables to consider. However, it may not be suitable for situations where there is limited data or when the model needs to be highly customized.\n+\n+### Are models created with this integration fast and scalable, in general?\n+Yes, models created with the MindsDB-FLAML integration are generally fast and scalable. FLAML is designed to efficiently automate the machine learning process, including the selection of models and hyperparameters, resulting in faster training times and better performance. \n+\n+### What are the recommended system specifications for models created with this framework?\n+N/A\n+\n+### To what degree can users control the underlying framework by passing parameters via the USING syntax?\n+While there are many parameters available for users to control the underlying framework through the USING syntax, not all of them may be immediately useful. The parameters you have listed are some of the most commonly used and relevant for users to consider",
    "comment": "> The parameters you have listed\r\n\r\nThis reads weirdly, please fix. ",
    "line_number": 14,
    "enriched": "File: mindsdb/integrations/handlers/flaml_handler/README.md\nCode: @@ -0,0 +1,47 @@\n+### Briefly describe what ML framework does this handler integrate to MindsDB, and how? \n+This handler integrates with MindsDB using FLAML, an open-source machine learning framework developed by Microsoft for research purposes. FLAML is a powerful Python library that automates the process of selecting models, hyperparameters, and other choices in machine learning applications. By utilizing FLAML, this handler can take advantage of its efficient and effective automation capabilities. Although there is more work to be done in implementing FLAML, it is a useful tool for automating machine learning tasks. Microsoft is currently developing new features for automated machine learning, which are in beta stage and are planned to be integrated into MindsDB in the near future.\n+\n+### Why is this integration useful? What does the ideal predictive use case for this integration look like? When would you definitely not use this integration? \n+This integration is useful for automating machine learning tasks and improving predictive accuracy. An ideal use case is when there is a large dataset with many variables to consider. However, it may not be suitable for situations where there is limited data or when the model needs to be highly customized.\n+\n+### Are models created with this integration fast and scalable, in general?\n+Yes, models created with the MindsDB-FLAML integration are generally fast and scalable. FLAML is designed to efficiently automate the machine learning process, including the selection of models and hyperparameters, resulting in faster training times and better performance. \n+\n+### What are the recommended system specifications for models created with this framework?\n+N/A\n+\n+### To what degree can users control the underlying framework by passing parameters via the USING syntax?\n+While there are many parameters available for users to control the underlying framework through the USING syntax, not all of them may be immediately useful. The parameters you have listed are some of the most commonly used and relevant for users to consider\nComment: > The parameters you have listed\r\n\r\nThis reads weirdly, please fix. ",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/flaml_handler/README.md",
    "pr_number": 6035,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1209141548,
    "comment_created_at": "2023-05-29T09:51:16Z"
  },
  {
    "code": "@@ -0,0 +1,47 @@\n+---\n+title: Integrations README \n+sidebarTitle: Writing Integrations README\n+---\n+\n+The README file is a crucial document that guides users in understanding, using, and contributing to the MindsDb integration. It serves as the first point of contact for anyone interacting with the integration, hence the need for it to be comprehensive, clear, and user-friendly.\n+",
    "comment": "LGTM!\r\n\r\nDo we have a README that follows these guidelines? We could link it here as an example.",
    "line_number": 7,
    "enriched": "File: docs/contribute/integrations-readme.mdx\nCode: @@ -0,0 +1,47 @@\n+---\n+title: Integrations README \n+sidebarTitle: Writing Integrations README\n+---\n+\n+The README file is a crucial document that guides users in understanding, using, and contributing to the MindsDb integration. It serves as the first point of contact for anyone interacting with the integration, hence the need for it to be comprehensive, clear, and user-friendly.\n+\nComment: LGTM!\r\n\r\nDo we have a README that follows these guidelines? We could link it here as an example.",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "docs/contribute/integrations-readme.mdx",
    "pr_number": 8651,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1457478937,
    "comment_created_at": "2024-01-18T13:57:14Z"
  },
  {
    "code": "@@ -173,10 +173,7 @@ def get_current_context(self) -> str:\n         \"\"\"\n         returns current context name\n         \"\"\"\n-        try:",
    "comment": "get_current_context could be called before (or without) set_context",
    "line_number": 176,
    "enriched": "File: mindsdb/interfaces/query_context/context_controller.py\nCode: @@ -173,10 +173,7 @@ def get_current_context(self) -> str:\n         \"\"\"\n         returns current context name\n         \"\"\"\n-        try:\nComment: get_current_context could be called before (or without) set_context",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/interfaces/query_context/context_controller.py",
    "pr_number": 8453,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1422238884,
    "comment_created_at": "2023-12-11T10:09:49Z"
  },
  {
    "code": "@@ -0,0 +1,13 @@\n+langchain\n+pygpt4all==1.1.0\n+chromadb\n+urllib3==1.26.6\n+numpy\n+pandas\n+torch>=1.13.0,<1.14.0",
    "comment": "Why 1.13, specifically? Ideally we could use >= 2.0 here, otherwise there will be a conflict with Lightwood",
    "line_number": 7,
    "enriched": "File: mindsdb/integrations/handlers/writer_handler/requirements.txt\nCode: @@ -0,0 +1,13 @@\n+langchain\n+pygpt4all==1.1.0\n+chromadb\n+urllib3==1.26.6\n+numpy\n+pandas\n+torch>=1.13.0,<1.14.0\nComment: Why 1.13, specifically? Ideally we could use >= 2.0 here, otherwise there will be a conflict with Lightwood",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/writer_handler/requirements.txt",
    "pr_number": 6347,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1210861457,
    "comment_created_at": "2023-05-30T21:54:43Z"
  },
  {
    "code": "@@ -220,6 +273,18 @@ def predict(self, df, model_state, args):\n         pred_df = self._run_command(params)\n         return pd_decode(pred_df)\n \n+    def finetune(self, df, model_state, args):\n+        params = {\n+            'method': 'finetune',\n+            'model_state': model_state,\n+            'df': pd_encode(df),\n+            'code': self.code,\n+            'args': args,\n+        }\n+\n+        model_state = self._run_command(params)  # TODO: we are not actually returning anything, maybe in order for this to work we need to return self, but that is a hack, the proper way is different",
    "comment": "This has been solved, remove leftover comment",
    "line_number": 285,
    "enriched": "File: mindsdb/integrations/handlers/byom_handler/byom_handler.py\nCode: @@ -220,6 +273,18 @@ def predict(self, df, model_state, args):\n         pred_df = self._run_command(params)\n         return pd_decode(pred_df)\n \n+    def finetune(self, df, model_state, args):\n+        params = {\n+            'method': 'finetune',\n+            'model_state': model_state,\n+            'df': pd_encode(df),\n+            'code': self.code,\n+            'args': args,\n+        }\n+\n+        model_state = self._run_command(params)  # TODO: we are not actually returning anything, maybe in order for this to work we need to return self, but that is a hack, the proper way is different\nComment: This has been solved, remove leftover comment",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/byom_handler/byom_handler.py",
    "pr_number": 7141,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1298975749,
    "comment_created_at": "2023-08-19T01:02:28Z"
  },
  {
    "code": "@@ -549,35 +537,45 @@ to collect a royalty for further conveying from those to whom you convey\n the Program, the only way you could satisfy both those terms and this\n License would be to refrain entirely from conveying the Program.\n \n-  13. Use with the GNU Affero General Public License.\n+  13. Remote Network Interaction; Use with the GNU General Public License.\n+\n+  Notwithstanding any other provision of this License, if you modify the\n+Program, your modified version must prominently offer all users\n+interacting with it remotely through a computer network (if your version\n+supports such interaction) an opportunity to receive the Corresponding\n+Source of your version by providing access to the Corresponding Source\n+from a network server at no charge, through some standard or customary\n+means of facilitating copying of software.  This Corresponding Source\n+shall include the Corresponding Source for any work covered by version 3\n+of the GNU General Public License that is incorporated pursuant to the\n+following paragraph.\n \n   Notwithstanding any other provision of this License, you have\n permission to link or combine any covered work with a work licensed\n-under version 3 of the GNU Affero General Public License into a single\n+under version 3 of the GNU General Public License into a single",
    "comment": "Should we have 'Affero' here?",
    "line_number": 555,
    "enriched": "File: LICENSE\nCode: @@ -549,35 +537,45 @@ to collect a royalty for further conveying from those to whom you convey\n the Program, the only way you could satisfy both those terms and this\n License would be to refrain entirely from conveying the Program.\n \n-  13. Use with the GNU Affero General Public License.\n+  13. Remote Network Interaction; Use with the GNU General Public License.\n+\n+  Notwithstanding any other provision of this License, if you modify the\n+Program, your modified version must prominently offer all users\n+interacting with it remotely through a computer network (if your version\n+supports such interaction) an opportunity to receive the Corresponding\n+Source of your version by providing access to the Corresponding Source\n+from a network server at no charge, through some standard or customary\n+means of facilitating copying of software.  This Corresponding Source\n+shall include the Corresponding Source for any work covered by version 3\n+of the GNU General Public License that is incorporated pursuant to the\n+following paragraph.\n \n   Notwithstanding any other provision of this License, you have\n permission to link or combine any covered work with a work licensed\n-under version 3 of the GNU Affero General Public License into a single\n+under version 3 of the GNU General Public License into a single\nComment: Should we have 'Affero' here?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "LICENSE",
    "pr_number": 7924,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1372203307,
    "comment_created_at": "2023-10-25T19:01:58Z"
  },
  {
    "code": "@@ -198,6 +202,35 @@ The `type` parameter specifies the type of caching mechanism to use for storing\n * `local` (default): Stores prediction results in the `cache` folder (as defined in the `paths` configuration). This is useful for repeated queries where the result doesn't change.\n * `redis`: Stores prediction results in a Redis instance. This option requires the `connection` parameter, which specifies the Redis connection string.\n \n+The `connection` parameter is required only if the `type` parameter is set to `redis`. It stores the Redis connection string.\n+\n+```bash\n+    \"logging\": {\n+        \"handlers\": {\n+            \"console\": {\n+                \"enabled\": true,\n+                \"level\": \"INFO\" # optional (alternatively, it can be defined in the MINDSDB_CONSOLE_LOG_LEVEL environment variable)\n+            },\n+            \"file\": {\n+                \"enabled\": False,\n+                \"level\": \"INFO\", # optional (alternatively, it can be defined in the MINDSDB_FILE_LOG_LEVEL environment variable)\n+                \"filename\": \"app.log\",\n+                \"maxBytes\": 524288, # 0.5 Mb\n+                \"backupCount\": 3\n+            }\n+        }\n+    },\n+```\n+\n+The above parameters are implemented based on [Python's Dictionary Schema](https://docs.python.org/3/library/logging.config.html#logging-config-dictschema).",
    "comment": "hm, may be better `Python's logging Dictionary Schema`, or `Logging's Dictionary Schema`",
    "line_number": 225,
    "enriched": "File: docs/setup/custom-config.mdx\nCode: @@ -198,6 +202,35 @@ The `type` parameter specifies the type of caching mechanism to use for storing\n * `local` (default): Stores prediction results in the `cache` folder (as defined in the `paths` configuration). This is useful for repeated queries where the result doesn't change.\n * `redis`: Stores prediction results in a Redis instance. This option requires the `connection` parameter, which specifies the Redis connection string.\n \n+The `connection` parameter is required only if the `type` parameter is set to `redis`. It stores the Redis connection string.\n+\n+```bash\n+    \"logging\": {\n+        \"handlers\": {\n+            \"console\": {\n+                \"enabled\": true,\n+                \"level\": \"INFO\" # optional (alternatively, it can be defined in the MINDSDB_CONSOLE_LOG_LEVEL environment variable)\n+            },\n+            \"file\": {\n+                \"enabled\": False,\n+                \"level\": \"INFO\", # optional (alternatively, it can be defined in the MINDSDB_FILE_LOG_LEVEL environment variable)\n+                \"filename\": \"app.log\",\n+                \"maxBytes\": 524288, # 0.5 Mb\n+                \"backupCount\": 3\n+            }\n+        }\n+    },\n+```\n+\n+The above parameters are implemented based on [Python's Dictionary Schema](https://docs.python.org/3/library/logging.config.html#logging-config-dictschema).\nComment: hm, may be better `Python's logging Dictionary Schema`, or `Logging's Dictionary Schema`",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "docs/setup/custom-config.mdx",
    "pr_number": 10340,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1910399310,
    "comment_created_at": "2025-01-10T13:46:36Z"
  },
  {
    "code": "@@ -0,0 +1,183 @@\n+from enum import Enum\n+from typing import List, Union\n+\n+from langchain_community.vectorstores.chroma import Chroma\n+from langchain_community.vectorstores.pgvector import PGVector\n+from langchain_core.documents import Document\n+from langchain_core.embeddings import Embeddings\n+from langchain_core.language_models import BaseChatModel\n+from langchain_core.vectorstores import VectorStore\n+from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n+from langchain_core.stores import BaseStore\n+from langchain.text_splitter import TextSplitter\n+from pydantic import BaseModel, root_validator\n+\n+# Multi retriever specific\n+DEFAULT_ID_KEY = \"doc_id\"\n+DEFAULT_MAX_CONCURRENCY = 5\n+\n+DEFAULT_CARDINALITY_THRESHOLD = 40\n+DEFAULT_CHUNK_SIZE = 1000\n+DEFAULT_CHUNK_OVERLAP = 50\n+DEFAULT_POOL_RECYCLE = 3600\n+DEFAULT_LLM_MODEL = \"gpt-3.5-turbo\"\n+DEFAULT_CONTENT_COLUMN_NAME = \"body\"\n+DEFAULT_DATASET_DESCRIPTION = \"email inbox\"\n+DEFAULT_TEST_TABLE_NAME = \"test_email\"\n+DEFAULT_LLM = ChatOpenAI(model_name=DEFAULT_LLM_MODEL, temperature=0)\n+DEFAULT_EMBEDDINGS = OpenAIEmbeddings()\n+DEFAULT_VECTOR_STORE = Chroma\n+DEFAULT_AUTO_META_PROMPT_TEMPLATE = \"\"\"\n+Below is a json representation of a table with information about {description}.\n+Return a JSON list with an entry for each column. Each entry should have\n+{{\"name\": \"column name\", \"description\": \"column description\", \"type\": \"column data type\"}}\n+\\n\\n{dataframe}\\n\\nJSON:\\n\n+\"\"\"\n+DEFAULT_RAG_PROMPT_TEMPLATE = '''You are an assistant for\n+question-answering tasks. Use the following pieces of retrieved context\n+to answer the question. If you don't know the answer, just say that you\n+don't know. Use two sentences maximum and keep the answer concise.\n+Question: {question}\n+Context: {context}\n+Answer:'''\n+\n+DEFAULT_QA_GENERATION_PROMPT_TEMPLATE = '''You are an assistant for\n+generating sample questions and answers from the given document and metadata. Given\n+a document and its metadata as context, generate a question and answer from that document and its metadata.\n+\n+The document will be a string. The metadata will be a JSON string. You need\n+to parse the JSON to understand it.\n+\n+Generate a question that requires BOTH the document and metadata to answer, if possible.\n+Otherwise, generate a question that requires ONLY the document to answer.\n+\n+Return a JSON dictionary with the question and answer like this:\n+{{ \"question\": <the full generated question>, \"answer\": <the full generated answer> }}\n+\n+Make sure the JSON string is valid before returning it. You must return the question and answer\n+in the specified JSON format no matter what.\n+\n+Document: {document}\n+Metadata: {metadata}\n+Answer:'''\n+\n+DEFAULT_TEXT_2_PGVECTOR_PROMPT_TEMPLATE = \"\"\"You are a Postgres expert. Given an input question, first create a syntactically correct Postgres query to run, then look at the results of the query and return the answer to the input question.\n+Unless the user specifies in the question a specific number of examples to obtain, query for at most 5 results using the LIMIT clause as per Postgres. You can order the results to return the most informative data in the database.\n+Never query for all columns from a table. You must query only the columns that are needed to answer the question. Wrap each column name in double quotes (\") to denote them as delimited identifiers.\n+Pay attention to use only the column names you can see in the tables below. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.\n+Pay attention to use date('now') function to get the current date, if the question involves \"today\".\n+\n+You can use an extra extension which allows you to run semantic similarity using <-> operator on tables containing columns named \"embeddings\".\n+<-> operator can ONLY be used on embeddings columns.\n+The embeddings value for a given row typically represents the semantic meaning of that row.\n+The vector represents an embedding representation of the question, given below.\n+Do NOT fill in the vector values directly, but rather specify a `[search_word]` placeholder, which should contain the word that would be embedded for filtering.\n+For example, if the user asks for songs about 'the feeling of loneliness' the query could be:\n+'SELECT \"[whatever_table_name]\".\"SongName\" FROM \"[whatever_table_name]\" ORDER BY \"embeddings\" <-> '[loneliness]' LIMIT 5'\n+\n+Use the following format:\n+\n+Question: <Question here>\n+SQLQuery: <SQL Query to run>\n+SQLResult: <Result of the SQLQuery>\n+Answer: <Final answer here>\n+\n+Only use the following tables:\n+\n+{schema}\n+\"\"\"\n+\n+DEFAULT_SQL_RESULT_PROMPT_TEMPLATE = \"\"\"Based on the table schema below, question, sql query, and sql response, write a natural language response:\n+{schema}\n+\n+Question: {question}\n+SQL Query: {query}\n+SQL Response: {response}\"\"\"\n+\n+DEFAULT_SQL_RETRIEVAL_PROMPT_TEMPLATE = {\n+    \"sql_query\": DEFAULT_TEXT_2_PGVECTOR_PROMPT_TEMPLATE,\n+    \"sql_result\": DEFAULT_SQL_RESULT_PROMPT_TEMPLATE\n+}\n+\n+\n+class MultiVectorRetrieverMode(Enum):\n+    \"\"\"\n+    Enum for MultiVectorRetriever types.\n+    \"\"\"\n+    SPLIT = \"split\"\n+    SUMMARIZE = \"summarize\"\n+    BOTH = \"both\"\n+\n+\n+class VectorStoreType(Enum):\n+    CHROMA = 'chroma'\n+    PGVECTOR = 'pgvector'\n+\n+\n+vector_store_map = {\n+    VectorStoreType.CHROMA: Chroma,\n+    VectorStoreType.PGVECTOR: PGVector\n+}\n+\n+\n+class RetrieverType(Enum):\n+    VECTOR_STORE = 'vector_store'\n+    AUTO = 'auto'\n+    SQL = 'sql'\n+    MULTI = 'multi'\n+\n+\n+class VectorStoreConfig(BaseModel):\n+    vector_store_type: VectorStoreType = VectorStoreType.CHROMA\n+    persist_directory: str = None\n+    collection_name: str = None\n+    connection_string: str = None\n+\n+    class Config:\n+        arbitrary_types_allowed = True\n+        extra = \"forbid\"\n+\n+\n+class RAGPipelineModel(BaseModel):",
    "comment": "@ea-rus this is config for rag ",
    "line_number": 141,
    "enriched": "File: mindsdb/integrations/utilities/rag/settings.py\nCode: @@ -0,0 +1,183 @@\n+from enum import Enum\n+from typing import List, Union\n+\n+from langchain_community.vectorstores.chroma import Chroma\n+from langchain_community.vectorstores.pgvector import PGVector\n+from langchain_core.documents import Document\n+from langchain_core.embeddings import Embeddings\n+from langchain_core.language_models import BaseChatModel\n+from langchain_core.vectorstores import VectorStore\n+from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n+from langchain_core.stores import BaseStore\n+from langchain.text_splitter import TextSplitter\n+from pydantic import BaseModel, root_validator\n+\n+# Multi retriever specific\n+DEFAULT_ID_KEY = \"doc_id\"\n+DEFAULT_MAX_CONCURRENCY = 5\n+\n+DEFAULT_CARDINALITY_THRESHOLD = 40\n+DEFAULT_CHUNK_SIZE = 1000\n+DEFAULT_CHUNK_OVERLAP = 50\n+DEFAULT_POOL_RECYCLE = 3600\n+DEFAULT_LLM_MODEL = \"gpt-3.5-turbo\"\n+DEFAULT_CONTENT_COLUMN_NAME = \"body\"\n+DEFAULT_DATASET_DESCRIPTION = \"email inbox\"\n+DEFAULT_TEST_TABLE_NAME = \"test_email\"\n+DEFAULT_LLM = ChatOpenAI(model_name=DEFAULT_LLM_MODEL, temperature=0)\n+DEFAULT_EMBEDDINGS = OpenAIEmbeddings()\n+DEFAULT_VECTOR_STORE = Chroma\n+DEFAULT_AUTO_META_PROMPT_TEMPLATE = \"\"\"\n+Below is a json representation of a table with information about {description}.\n+Return a JSON list with an entry for each column. Each entry should have\n+{{\"name\": \"column name\", \"description\": \"column description\", \"type\": \"column data type\"}}\n+\\n\\n{dataframe}\\n\\nJSON:\\n\n+\"\"\"\n+DEFAULT_RAG_PROMPT_TEMPLATE = '''You are an assistant for\n+question-answering tasks. Use the following pieces of retrieved context\n+to answer the question. If you don't know the answer, just say that you\n+don't know. Use two sentences maximum and keep the answer concise.\n+Question: {question}\n+Context: {context}\n+Answer:'''\n+\n+DEFAULT_QA_GENERATION_PROMPT_TEMPLATE = '''You are an assistant for\n+generating sample questions and answers from the given document and metadata. Given\n+a document and its metadata as context, generate a question and answer from that document and its metadata.\n+\n+The document will be a string. The metadata will be a JSON string. You need\n+to parse the JSON to understand it.\n+\n+Generate a question that requires BOTH the document and metadata to answer, if possible.\n+Otherwise, generate a question that requires ONLY the document to answer.\n+\n+Return a JSON dictionary with the question and answer like this:\n+{{ \"question\": <the full generated question>, \"answer\": <the full generated answer> }}\n+\n+Make sure the JSON string is valid before returning it. You must return the question and answer\n+in the specified JSON format no matter what.\n+\n+Document: {document}\n+Metadata: {metadata}\n+Answer:'''\n+\n+DEFAULT_TEXT_2_PGVECTOR_PROMPT_TEMPLATE = \"\"\"You are a Postgres expert. Given an input question, first create a syntactically correct Postgres query to run, then look at the results of the query and return the answer to the input question.\n+Unless the user specifies in the question a specific number of examples to obtain, query for at most 5 results using the LIMIT clause as per Postgres. You can order the results to return the most informative data in the database.\n+Never query for all columns from a table. You must query only the columns that are needed to answer the question. Wrap each column name in double quotes (\") to denote them as delimited identifiers.\n+Pay attention to use only the column names you can see in the tables below. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.\n+Pay attention to use date('now') function to get the current date, if the question involves \"today\".\n+\n+You can use an extra extension which allows you to run semantic similarity using <-> operator on tables containing columns named \"embeddings\".\n+<-> operator can ONLY be used on embeddings columns.\n+The embeddings value for a given row typically represents the semantic meaning of that row.\n+The vector represents an embedding representation of the question, given below.\n+Do NOT fill in the vector values directly, but rather specify a `[search_word]` placeholder, which should contain the word that would be embedded for filtering.\n+For example, if the user asks for songs about 'the feeling of loneliness' the query could be:\n+'SELECT \"[whatever_table_name]\".\"SongName\" FROM \"[whatever_table_name]\" ORDER BY \"embeddings\" <-> '[loneliness]' LIMIT 5'\n+\n+Use the following format:\n+\n+Question: <Question here>\n+SQLQuery: <SQL Query to run>\n+SQLResult: <Result of the SQLQuery>\n+Answer: <Final answer here>\n+\n+Only use the following tables:\n+\n+{schema}\n+\"\"\"\n+\n+DEFAULT_SQL_RESULT_PROMPT_TEMPLATE = \"\"\"Based on the table schema below, question, sql query, and sql response, write a natural language response:\n+{schema}\n+\n+Question: {question}\n+SQL Query: {query}\n+SQL Response: {response}\"\"\"\n+\n+DEFAULT_SQL_RETRIEVAL_PROMPT_TEMPLATE = {\n+    \"sql_query\": DEFAULT_TEXT_2_PGVECTOR_PROMPT_TEMPLATE,\n+    \"sql_result\": DEFAULT_SQL_RESULT_PROMPT_TEMPLATE\n+}\n+\n+\n+class MultiVectorRetrieverMode(Enum):\n+    \"\"\"\n+    Enum for MultiVectorRetriever types.\n+    \"\"\"\n+    SPLIT = \"split\"\n+    SUMMARIZE = \"summarize\"\n+    BOTH = \"both\"\n+\n+\n+class VectorStoreType(Enum):\n+    CHROMA = 'chroma'\n+    PGVECTOR = 'pgvector'\n+\n+\n+vector_store_map = {\n+    VectorStoreType.CHROMA: Chroma,\n+    VectorStoreType.PGVECTOR: PGVector\n+}\n+\n+\n+class RetrieverType(Enum):\n+    VECTOR_STORE = 'vector_store'\n+    AUTO = 'auto'\n+    SQL = 'sql'\n+    MULTI = 'multi'\n+\n+\n+class VectorStoreConfig(BaseModel):\n+    vector_store_type: VectorStoreType = VectorStoreType.CHROMA\n+    persist_directory: str = None\n+    collection_name: str = None\n+    connection_string: str = None\n+\n+    class Config:\n+        arbitrary_types_allowed = True\n+        extra = \"forbid\"\n+\n+\n+class RAGPipelineModel(BaseModel):\nComment: @ea-rus this is config for rag ",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/utilities/rag/settings.py",
    "pr_number": 8996,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1548176761,
    "comment_created_at": "2024-04-02T16:12:51Z"
  },
  {
    "code": "@@ -29,10 +29,23 @@ def create(self, target: str, df: Optional[pd.DataFrame] = None, args: Optional[\n         # TODO: use join_learn_process to notify users when ray has finished the training process\n         args = args['using']  # ignore the rest of the problem definition\n         args['target'] = target\n+\n+        # Store the args in model storage for later use\n         self.model_storage.json_set('args', args)\n+\n+        # Prepare the payload for training, including additional parameters\n+        payload = {\n+            'df': df.to_json(orient='records'),\n+            'target': target\n+        }\n+\n+        # Include additional parameters if they exist\n+        if 'learn' in args:\n+            payload['learn'] = args['learn']",
    "comment": "User's query  to create model will be:\r\n```sql\r\n CREATE MODEL rayserve_model\r\n    FROM example_db (SELECT * FROM home_rentals)\r\n    PREDICT location\r\n    USING\r\n    engine='ray_serve',\r\n    train_url='http://127.0.0.1:8000/my_model/train',\r\n    predict_url='http://127.0.0.1:8000/my_model/predict',\r\n    custom_arg1=1,\r\n    custom_arg1='asdf';\r\n```\r\nAll arguments are located in args parameter. What to do: cut standard arguments (train_url, predict_url) and send the rest as args parameter to ray server",
    "line_number": 44,
    "enriched": "File: mindsdb/integrations/handlers/ray_serve_handler/ray_serve_handler.py\nCode: @@ -29,10 +29,23 @@ def create(self, target: str, df: Optional[pd.DataFrame] = None, args: Optional[\n         # TODO: use join_learn_process to notify users when ray has finished the training process\n         args = args['using']  # ignore the rest of the problem definition\n         args['target'] = target\n+\n+        # Store the args in model storage for later use\n         self.model_storage.json_set('args', args)\n+\n+        # Prepare the payload for training, including additional parameters\n+        payload = {\n+            'df': df.to_json(orient='records'),\n+            'target': target\n+        }\n+\n+        # Include additional parameters if they exist\n+        if 'learn' in args:\n+            payload['learn'] = args['learn']\nComment: User's query  to create model will be:\r\n```sql\r\n CREATE MODEL rayserve_model\r\n    FROM example_db (SELECT * FROM home_rentals)\r\n    PREDICT location\r\n    USING\r\n    engine='ray_serve',\r\n    train_url='http://127.0.0.1:8000/my_model/train',\r\n    predict_url='http://127.0.0.1:8000/my_model/predict',\r\n    custom_arg1=1,\r\n    custom_arg1='asdf';\r\n```\r\nAll arguments are located in args parameter. What to do: cut standard arguments (train_url, predict_url) and send the rest as args parameter to ray server",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/ray_serve_handler/ray_serve_handler.py",
    "pr_number": 9832,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1830587843,
    "comment_created_at": "2024-11-06T08:35:51Z"
  },
  {
    "code": "@@ -125,8 +123,7 @@ def native_query(self, query: str) -> Response:\n             response = Response(RESPONSE_TYPE.ERROR, error_message=str(e))\n \n         cursor.close()\n-        if need_to_close is True:",
    "comment": "Why has this call to `self.disconnect()` been removed?",
    "line_number": 128,
    "enriched": "File: mindsdb/integrations/handlers/duckdb_handler/duckdb_handler.py\nCode: @@ -125,8 +123,7 @@ def native_query(self, query: str) -> Response:\n             response = Response(RESPONSE_TYPE.ERROR, error_message=str(e))\n \n         cursor.close()\n-        if need_to_close is True:\nComment: Why has this call to `self.disconnect()` been removed?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/duckdb_handler/duckdb_handler.py",
    "pr_number": 10055,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1824099130,
    "comment_created_at": "2024-10-31T08:55:01Z"
  },
  {
    "code": "@@ -1,9 +1,9 @@\n-__title__ = 'MindsDB Airtable handler'\n-__package_name__ = 'mindsdb_airtable_handler'\n-__version__ = '0.0.1'\n+__title__ = \"MindsDB Airtable handler\"\n+__package_name__ = \"mindsdb_airtable_handler\"\n+__version__ = \"0.0.1\"\n __description__ = \"MindsDB handler for Airtable\"\n-__author__ = 'Minura Punchihewa'\n-__github__ = 'https://github.com/mindsdb/mindsdb'\n-__pypi__ = 'https://pypi.org/project/mindsdb/'\n-__license__ = 'MIT'\n-__copyright__ = 'Copyright 2022- mindsdb'\n+__author__ = \"Minura Punchihewa\"",
    "comment": "Can we change this to MindsDB :)",
    "line_number": 5,
    "enriched": "File: mindsdb/integrations/handlers/airtable_handler/__about__.py\nCode: @@ -1,9 +1,9 @@\n-__title__ = 'MindsDB Airtable handler'\n-__package_name__ = 'mindsdb_airtable_handler'\n-__version__ = '0.0.1'\n+__title__ = \"MindsDB Airtable handler\"\n+__package_name__ = \"mindsdb_airtable_handler\"\n+__version__ = \"0.0.1\"\n __description__ = \"MindsDB handler for Airtable\"\n-__author__ = 'Minura Punchihewa'\n-__github__ = 'https://github.com/mindsdb/mindsdb'\n-__pypi__ = 'https://pypi.org/project/mindsdb/'\n-__license__ = 'MIT'\n-__copyright__ = 'Copyright 2022- mindsdb'\n+__author__ = \"Minura Punchihewa\"\nComment: Can we change this to MindsDB :)",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/airtable_handler/__about__.py",
    "pr_number": 10108,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1878078338,
    "comment_created_at": "2024-12-10T13:17:40Z"
  },
  {
    "code": "@@ -65,6 +66,7 @@ def decode(file_obj: IOBase) -> StringIO:\n class FormatDetector:\n \n     supported_formats = ['parquet', 'csv', 'xlsx', 'pdf', 'json', 'txt']\n+    multipage_formats = ['xlsx']",
    "comment": "How do multi-page PDFs work here? I see that read_pdf() is extracting data from all pages. We return the data in all files as a single chunk, is it?",
    "line_number": 69,
    "enriched": "File: mindsdb/integrations/utilities/files/file_reader.py\nCode: @@ -65,6 +66,7 @@ def decode(file_obj: IOBase) -> StringIO:\n class FormatDetector:\n \n     supported_formats = ['parquet', 'csv', 'xlsx', 'pdf', 'json', 'txt']\n+    multipage_formats = ['xlsx']\nComment: How do multi-page PDFs work here? I see that read_pdf() is extracting data from all pages. We return the data in all files as a single chunk, is it?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/utilities/files/file_reader.py",
    "pr_number": 10448,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1950404421,
    "comment_created_at": "2025-02-11T08:16:11Z"
  },
  {
    "code": "@@ -1147,13 +1154,9 @@ def answer_drop_view(self, statement):\n     @mark_process('learn')\n     def answer_create_predictor(self, statement):\n         integration_name = self.session.database\n-        if len(statement.name.parts) > 1:",
    "comment": "this piece of code allows to create model in other project. for example active project is mindnsdb but we can call 'create mode proj1.mindsdb'",
    "line_number": 1191,
    "enriched": "File: mindsdb/api/mysql/mysql_proxy/executor/executor_commands.py\nCode: @@ -1147,13 +1154,9 @@ def answer_drop_view(self, statement):\n     @mark_process('learn')\n     def answer_create_predictor(self, statement):\n         integration_name = self.session.database\n-        if len(statement.name.parts) > 1:\nComment: this piece of code allows to create model in other project. for example active project is mindnsdb but we can call 'create mode proj1.mindsdb'",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/api/mysql/mysql_proxy/executor/executor_commands.py",
    "pr_number": 7205,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1310824260,
    "comment_created_at": "2023-08-30T21:05:36Z"
  },
  {
    "code": "@@ -3,63 +3,155 @@ title: Knowledge Base\n sidebarTitle: Knowledge Bases\n ---\n \n-A knowledge base is an advanced system designed to go beyond traditional data storage. Its components include embedding and reranking models and a vector store.\n+A knowledge base is an advanced system that organizes information based on semantic meaning rather than simple keyword matching. It integrates embedding models, reranking models, and vector stores to enable context-aware data retrieval.\n \n-<p align=\"center\">\n-  <img src=\"https://docs.google.com/drawings/d/e/2PACX-1vTVmUqhoFXLF3ncL0NWwgdPjI7Hj19f-5xU8ED31ntYvlsTM3poM9zZKwcrEwzvxJrOhl2raIFKWlsp/pub?w=1342&h=681\" />\n-</p>\n+By performing semantic reasoning across multiple data points, a knowledge base delivers deeper insights and more accurate responses, making it a powerful tool for intelligent data access.\n \n-It intelligently organizes information in semantically meaningful ways, allowing for context-aware retrieval based on meaning rather than just keywords. With the ability to perform semantic reasoning across multiple pieces of information, it delivers deeper insights and more accurate responses.\n-\n-## Create a Knowledge Base\n+## `CREATE KNOWLEDGE_BASE` Syntax\n \n Here is the syntax for creating a knowledge base:\n \n+```sql\n+CREATE KNOWLEDGE_BASE my_kb\n+USING\n+    embedding_model = {\n+       \"provider\": \"openai\",\n+       \"model_name\" : \"text-embedding-3-large\",\n+       \"api_key\": \"sk-...\"\n+    },\n+    reranking_model = {\n+       \"provider\": \"openai\",\n+       \"model_name\": \"gpt-4o\",\n+       \"api_key\": \"sk-...\"\n+    },\n+    storage = my_vector_store.storage_table,\n+    metadata_columns = ['date', 'creator', ...],\n+    content_columns = ['review', 'content', ...],\n+    id_column = 'id';\n+```\n+\n+Upon execution, it registers `my_kb` and associates the specified models and storage.\n+\n+`my_kb` is a unique identifier of the knowledge base within MindsDB.\n+\n <Note>\n-MindsDB stores objects, such as models or knowledge bases, inside projects. Learn more about [projects here](/mindsdb_sql/sql/create/project).\n+As MindsDB stores objects, such as models or knowledge bases, inside [projects](/mindsdb_sql/sql/create/project), you can create a knowledge base inside a custom project.\n+\n+```sql\n+CREATE PROJECT my_project;\n+\n+CREATE KNOWLEDGE_BASE my_project.my_kb\n+USING\n+    ...\n+```\n </Note>\n \n+### `embedding_model`\n+\n+The embedding model is a required component of the knowledge base. It stores specifications of the embedding model to be used.\n+\n+Users can define the embedding model choosing one of the following options.\n+\n+**Option 1.** Use the `embedding_model` parameter to define the specification.",
    "comment": "Maybe mention `embedding_model` object",
    "line_number": 55,
    "enriched": "File: docs/mindsdb_sql/knowledge-bases.mdx\nCode: @@ -3,63 +3,155 @@ title: Knowledge Base\n sidebarTitle: Knowledge Bases\n ---\n \n-A knowledge base is an advanced system designed to go beyond traditional data storage. Its components include embedding and reranking models and a vector store.\n+A knowledge base is an advanced system that organizes information based on semantic meaning rather than simple keyword matching. It integrates embedding models, reranking models, and vector stores to enable context-aware data retrieval.\n \n-<p align=\"center\">\n-  <img src=\"https://docs.google.com/drawings/d/e/2PACX-1vTVmUqhoFXLF3ncL0NWwgdPjI7Hj19f-5xU8ED31ntYvlsTM3poM9zZKwcrEwzvxJrOhl2raIFKWlsp/pub?w=1342&h=681\" />\n-</p>\n+By performing semantic reasoning across multiple data points, a knowledge base delivers deeper insights and more accurate responses, making it a powerful tool for intelligent data access.\n \n-It intelligently organizes information in semantically meaningful ways, allowing for context-aware retrieval based on meaning rather than just keywords. With the ability to perform semantic reasoning across multiple pieces of information, it delivers deeper insights and more accurate responses.\n-\n-## Create a Knowledge Base\n+## `CREATE KNOWLEDGE_BASE` Syntax\n \n Here is the syntax for creating a knowledge base:\n \n+```sql\n+CREATE KNOWLEDGE_BASE my_kb\n+USING\n+    embedding_model = {\n+       \"provider\": \"openai\",\n+       \"model_name\" : \"text-embedding-3-large\",\n+       \"api_key\": \"sk-...\"\n+    },\n+    reranking_model = {\n+       \"provider\": \"openai\",\n+       \"model_name\": \"gpt-4o\",\n+       \"api_key\": \"sk-...\"\n+    },\n+    storage = my_vector_store.storage_table,\n+    metadata_columns = ['date', 'creator', ...],\n+    content_columns = ['review', 'content', ...],\n+    id_column = 'id';\n+```\n+\n+Upon execution, it registers `my_kb` and associates the specified models and storage.\n+\n+`my_kb` is a unique identifier of the knowledge base within MindsDB.\n+\n <Note>\n-MindsDB stores objects, such as models or knowledge bases, inside projects. Learn more about [projects here](/mindsdb_sql/sql/create/project).\n+As MindsDB stores objects, such as models or knowledge bases, inside [projects](/mindsdb_sql/sql/create/project), you can create a knowledge base inside a custom project.\n+\n+```sql\n+CREATE PROJECT my_project;\n+\n+CREATE KNOWLEDGE_BASE my_project.my_kb\n+USING\n+    ...\n+```\n </Note>\n \n+### `embedding_model`\n+\n+The embedding model is a required component of the knowledge base. It stores specifications of the embedding model to be used.\n+\n+Users can define the embedding model choosing one of the following options.\n+\n+**Option 1.** Use the `embedding_model` parameter to define the specification.\nComment: Maybe mention `embedding_model` object",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "docs/mindsdb_sql/knowledge-bases.mdx",
    "pr_number": 10767,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2066085806,
    "comment_created_at": "2025-04-29T11:08:00Z"
  },
  {
    "code": "@@ -216,23 +216,25 @@ def update_chatbot(\n         if existing_chatbot is None:\n             raise Exception(f'Chat bot not found: {chatbot_name}')\n \n+        existing_chatbot_rec = db.ChatBots.query.get(existing_chatbot['id'])",
    "comment": "NIT: what type of exception does it throw if id is invalid",
    "line_number": 219,
    "enriched": "File: mindsdb/interfaces/chatbot/chatbot_controller.py\nCode: @@ -216,23 +216,25 @@ def update_chatbot(\n         if existing_chatbot is None:\n             raise Exception(f'Chat bot not found: {chatbot_name}')\n \n+        existing_chatbot_rec = db.ChatBots.query.get(existing_chatbot['id'])\nComment: NIT: what type of exception does it throw if id is invalid",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/interfaces/chatbot/chatbot_controller.py",
    "pr_number": 7317,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1323956076,
    "comment_created_at": "2023-09-13T04:30:40Z"
  },
  {
    "code": "@@ -0,0 +1,37 @@\n+---\n+title: MindsDB SQL Syntax\n+sidebarTitle: MindsDB SQL Syntax\n+---\n+\n+Generally speaking, MindsDB SQL attempts to follow the syntax conventions of MySQL and PostgreSQL. The following\n+sections describe some common conventions of MindsDB SQL.\n+\n+\n+## Single/Double Quotes & Backticks\n+\n+Identifiers (databases, tables, and column names) with special characters or reserved words must use the backtick\n+\"`\":\n+\n+```sql\n+SELECT * FROM `select` WHERE `select`.id > 100;\n+\n+SELECT * FROM `select-DATABASE` WHERE `select-DATABASE`.id > 100;\n+```\n+\n+String values are represented by single and double quotes:\n+\n+```sql\n+SELECT * FROM table_name WHERE table_name.column_name = 'string';\n+SELECT * FROM table_name WHERE table_name.column_name = \"string\";\n+```\n+\n+## Parentheses\n+\n+SQL statements can be nested with parentheses:\n+\n+```sql\n+SELECT * FROM (SELECT * FROM table_name WHERE table_name.column_name = 'string') ;\n+\n+```\n+\n+[@akhilcoder](https://www.youtube.com/@akhilcoder)",
    "comment": "Let's remove the reference to this yt channel.",
    "line_number": 37,
    "enriched": "File: docs/mindsdb_sql/syntax.mdx\nCode: @@ -0,0 +1,37 @@\n+---\n+title: MindsDB SQL Syntax\n+sidebarTitle: MindsDB SQL Syntax\n+---\n+\n+Generally speaking, MindsDB SQL attempts to follow the syntax conventions of MySQL and PostgreSQL. The following\n+sections describe some common conventions of MindsDB SQL.\n+\n+\n+## Single/Double Quotes & Backticks\n+\n+Identifiers (databases, tables, and column names) with special characters or reserved words must use the backtick\n+\"`\":\n+\n+```sql\n+SELECT * FROM `select` WHERE `select`.id > 100;\n+\n+SELECT * FROM `select-DATABASE` WHERE `select-DATABASE`.id > 100;\n+```\n+\n+String values are represented by single and double quotes:\n+\n+```sql\n+SELECT * FROM table_name WHERE table_name.column_name = 'string';\n+SELECT * FROM table_name WHERE table_name.column_name = \"string\";\n+```\n+\n+## Parentheses\n+\n+SQL statements can be nested with parentheses:\n+\n+```sql\n+SELECT * FROM (SELECT * FROM table_name WHERE table_name.column_name = 'string') ;\n+\n+```\n+\n+[@akhilcoder](https://www.youtube.com/@akhilcoder)\nComment: Let's remove the reference to this yt channel.",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "docs/mindsdb_sql/syntax.mdx",
    "pr_number": 8676,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1466541452,
    "comment_created_at": "2024-01-25T15:27:03Z"
  },
  {
    "code": "@@ -1 +1,4 @@\n-statsforecast>=1.4.0, <2.0\n\\ No newline at end of file\n+statsforecast>=1.5.0, <2.0\n+neuralforecast>=1.4.0, <2.0\n+hierarchicalforecast<1.0\n+datasetsforecast<1.0",
    "comment": "Is this actually needed? Can't find any references in the rest of the PR ",
    "line_number": 4,
    "enriched": "File: mindsdb/integrations/handlers/statsforecast_handler/requirements.txt\nCode: @@ -1 +1,4 @@\n-statsforecast>=1.4.0, <2.0\n\\ No newline at end of file\n+statsforecast>=1.5.0, <2.0\n+neuralforecast>=1.4.0, <2.0\n+hierarchicalforecast<1.0\n+datasetsforecast<1.0\nComment: Is this actually needed? Can't find any references in the rest of the PR ",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/statsforecast_handler/requirements.txt",
    "pr_number": 5189,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1152562075,
    "comment_created_at": "2023-03-29T22:51:09Z"
  },
  {
    "code": "@@ -13,31 +13,35 @@ variable \"IMAGE\" {\n variable \"VERSION\" {\n   default = \"unknown\"\n }\n-variable \"PLATFORM\" {\n-  default = \"linux/amd64\"\n+variable \"PLATFORMS\" {\n+  default = \"linux/amd64,linux/arm64\"\n+}\n+variable PLATFORM_LIST {\n+  default = split(\",\", PLATFORMS)\n }\n variable \"BRANCH\" {\n   default = \"stable\"\n }\n-\n-function \"get_platform_tag\" {\n-  params = []\n-  result = replace(\"${equal(PLATFORM, \"\") ? \"\" : \"-\"}${PLATFORM}\", \"linux/\", \"\")\n+variable \"ECR_REPO\" {\n+  default = \"454861456664.dkr.ecr.us-east-2.amazonaws.com\"",
    "comment": "Is it ok to publish this info in this repository?",
    "line_number": 26,
    "enriched": "File: docker/docker-bake.hcl\nCode: @@ -13,31 +13,35 @@ variable \"IMAGE\" {\n variable \"VERSION\" {\n   default = \"unknown\"\n }\n-variable \"PLATFORM\" {\n-  default = \"linux/amd64\"\n+variable \"PLATFORMS\" {\n+  default = \"linux/amd64,linux/arm64\"\n+}\n+variable PLATFORM_LIST {\n+  default = split(\",\", PLATFORMS)\n }\n variable \"BRANCH\" {\n   default = \"stable\"\n }\n-\n-function \"get_platform_tag\" {\n-  params = []\n-  result = replace(\"${equal(PLATFORM, \"\") ? \"\" : \"-\"}${PLATFORM}\", \"linux/\", \"\")\n+variable \"ECR_REPO\" {\n+  default = \"454861456664.dkr.ecr.us-east-2.amazonaws.com\"\nComment: Is it ok to publish this info in this repository?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "docker/docker-bake.hcl",
    "pr_number": 8999,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1552683281,
    "comment_created_at": "2024-04-05T02:15:20Z"
  },
  {
    "code": "@@ -63,6 +67,52 @@ def predict(self, model_name: str, data, version=None, params=None):\n         ml_handler = self.integration_controller.get_ml_handler(model_metadata['engine_name'])\n         return ml_handler.predict(model_name, data, project_name=self.project.name, version=version, params=params)\n \n+    def _query_agent(self, query: ast.ASTNode, existing_agent: db.Agents, session):",
    "comment": "maybe move this function to agent controller? it is about how agent works",
    "line_number": 70,
    "enriched": "File: mindsdb/api/executor/datahub/datanodes/project_datanode.py\nCode: @@ -63,6 +67,52 @@ def predict(self, model_name: str, data, version=None, params=None):\n         ml_handler = self.integration_controller.get_ml_handler(model_metadata['engine_name'])\n         return ml_handler.predict(model_name, data, project_name=self.project.name, version=version, params=params)\n \n+    def _query_agent(self, query: ast.ASTNode, existing_agent: db.Agents, session):\nComment: maybe move this function to agent controller? it is about how agent works",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/api/executor/datahub/datanodes/project_datanode.py",
    "pr_number": 8856,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1512289112,
    "comment_created_at": "2024-03-05T07:57:12Z"
  },
  {
    "code": "@@ -96,14 +97,14 @@ def connect_with_jdbc(self):\n         Returns:\n             connection\n         \"\"\"\n-        jar_location = self.connection_args.get('jdbcJarLocation')\n+        jar_location = self.connection_args.get('jar_location')",
    "comment": "Why is this renamed? Shouldn't be `jdbcJarLocation` as stated in `connection_args` ?",
    "line_number": 100,
    "enriched": "File: mindsdb/integrations/handlers/altibase_handler/altibase_handler.py\nCode: @@ -96,14 +97,14 @@ def connect_with_jdbc(self):\n         Returns:\n             connection\n         \"\"\"\n-        jar_location = self.connection_args.get('jdbcJarLocation')\n+        jar_location = self.connection_args.get('jar_location')\nComment: Why is this renamed? Shouldn't be `jdbcJarLocation` as stated in `connection_args` ?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/altibase_handler/altibase_handler.py",
    "pr_number": 10143,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1838125540,
    "comment_created_at": "2024-11-12T13:43:40Z"
  },
  {
    "code": "@@ -0,0 +1,9 @@\n+install_precommit:",
    "comment": "Could we make this `install_dev`, and have it install requirements_dev.txt? (and add precommit to that requirements file).\r\nWe are trying to remove all of the `pip install` commands we have scattered around",
    "line_number": 1,
    "enriched": "File: Makefile\nCode: @@ -0,0 +1,9 @@\n+install_precommit:\nComment: Could we make this `install_dev`, and have it install requirements_dev.txt? (and add precommit to that requirements file).\r\nWe are trying to remove all of the `pip install` commands we have scattered around",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "Makefile",
    "pr_number": 8056,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1376591763,
    "comment_created_at": "2023-10-30T17:36:05Z"
  },
  {
    "code": "@@ -175,10 +175,12 @@ The `autoupdate` parameter defines whether MindsDB automatically checks for and\n             \"max_restart_count\": 1,\n             \"max_restart_interval_seconds\": 60\n         },\n-        \"mongodb\": {",
    "comment": "why mongodb is removed?",
    "line_number": 178,
    "enriched": "File: docs/setup/custom-config.mdx\nCode: @@ -175,10 +175,12 @@ The `autoupdate` parameter defines whether MindsDB automatically checks for and\n             \"max_restart_count\": 1,\n             \"max_restart_interval_seconds\": 60\n         },\n-        \"mongodb\": {\nComment: why mongodb is removed?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "docs/setup/custom-config.mdx",
    "pr_number": 11293,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2207848606,
    "comment_created_at": "2025-07-15T15:31:04Z"
  },
  {
    "code": "@@ -70,8 +72,35 @@ def on_chain_end(self, outputs: Dict[str, Any], **kwargs: Any) -> Any:\n         chain_span.end()\n \n     def on_chain_error(\n-        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any\n-    ) -> Any:\n-        '''Run when chain errors.'''\n+                self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any\n+        ) -> Any:\n+            \"\"\"Run when chain errors.\"\"\"\n+            # Do nothing for now.\n+            pass\n+\n+    def on_agent_action(self, action, **kwargs: Any) -> Any:\n+        \"\"\"Run on agent action.\"\"\"",
    "comment": "Keeping track of actions _and_ tool usage as spans could be confusing when looking at traces. Since tools are \"actions\" we will have duplicate spans for all traces that use tools.\r\n\r\nIf tools are actions (but actions are _not_ necessarily tools), it may be worth it to keep `on_agent_action` and update `on_tool_start/end` to update trace metadata only (but not create new Langfuse spans).\r\n\r\nWDYT?",
    "line_number": 82,
    "enriched": "File: mindsdb/integrations/handlers/langchain_handler/langfuse_callback_handler.py\nCode: @@ -70,8 +72,35 @@ def on_chain_end(self, outputs: Dict[str, Any], **kwargs: Any) -> Any:\n         chain_span.end()\n \n     def on_chain_error(\n-        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any\n-    ) -> Any:\n-        '''Run when chain errors.'''\n+                self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any\n+        ) -> Any:\n+            \"\"\"Run when chain errors.\"\"\"\n+            # Do nothing for now.\n+            pass\n+\n+    def on_agent_action(self, action, **kwargs: Any) -> Any:\n+        \"\"\"Run on agent action.\"\"\"\nComment: Keeping track of actions _and_ tool usage as spans could be confusing when looking at traces. Since tools are \"actions\" we will have duplicate spans for all traces that use tools.\r\n\r\nIf tools are actions (but actions are _not_ necessarily tools), it may be worth it to keep `on_agent_action` and update `on_tool_start/end` to update trace metadata only (but not create new Langfuse spans).\r\n\r\nWDYT?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/langchain_handler/langfuse_callback_handler.py",
    "pr_number": 9496,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1681327825,
    "comment_created_at": "2024-07-17T16:09:12Z"
  },
  {
    "code": "@@ -101,10 +138,37 @@ def query(self, query: ASTNode) -> Response:\n             )\n             result_df = query_df(df, query)\n             return Response(RESPONSE_TYPE.TABLE, data_frame=result_df)\n+\n+        elif type(query) is Insert:\n+            table_name = query.table.parts[-1]\n+            file_path = self.file_controller.get_file_path(table_name)\n+\n+            # Load the existing data from the file\n+            df, _ = self._handle_source(\n+                file_path,\n+                self.clean_rows,\n+                self.custom_parser,\n+                self.chunk_size,\n+                self.chunk_overlap,\n+            )\n+\n+            # Create a new dataframe with the values from the query\n+            new_df = pd.DataFrame(query.values, columns=[col.name for col in query.columns])\n+\n+            # Concatenate the new dataframe with the existing one\n+            df = pd.concat([df, new_df], ignore_index=True)\n+\n+            # Write the concatenated data to the file based on its format\n+            format = Path(file_path).suffix.strip(\".\").lower()\n+            write_method = getattr(df, f\"to_{format}\")\n+            write_method(file_path, index=False)",
    "comment": "It seems to me it won't work in cloud because only local file is updated \r\n@StpMax, how is better to update file?",
    "line_number": 164,
    "enriched": "File: mindsdb/integrations/handlers/file_handler/file_handler.py\nCode: @@ -101,10 +138,37 @@ def query(self, query: ASTNode) -> Response:\n             )\n             result_df = query_df(df, query)\n             return Response(RESPONSE_TYPE.TABLE, data_frame=result_df)\n+\n+        elif type(query) is Insert:\n+            table_name = query.table.parts[-1]\n+            file_path = self.file_controller.get_file_path(table_name)\n+\n+            # Load the existing data from the file\n+            df, _ = self._handle_source(\n+                file_path,\n+                self.clean_rows,\n+                self.custom_parser,\n+                self.chunk_size,\n+                self.chunk_overlap,\n+            )\n+\n+            # Create a new dataframe with the values from the query\n+            new_df = pd.DataFrame(query.values, columns=[col.name for col in query.columns])\n+\n+            # Concatenate the new dataframe with the existing one\n+            df = pd.concat([df, new_df], ignore_index=True)\n+\n+            # Write the concatenated data to the file based on its format\n+            format = Path(file_path).suffix.strip(\".\").lower()\n+            write_method = getattr(df, f\"to_{format}\")\n+            write_method(file_path, index=False)\nComment: It seems to me it won't work in cloud because only local file is updated \r\n@StpMax, how is better to update file?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/file_handler/file_handler.py",
    "pr_number": 9745,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1768915534,
    "comment_created_at": "2024-09-20T16:48:42Z"
  },
  {
    "code": "@@ -58,6 +58,7 @@ def embeddings_to_vectordb(self):\n             embedding=embeddings_model,\n             persist_directory=self.persist_directory,\n             client_settings=self.chroma_settings,\n+            collection_name=self.args.get(\"collection_name\", \"langchain\"),",
    "comment": "forgot to add this here ",
    "line_number": 61,
    "enriched": "File: mindsdb/integrations/handlers/writer_handler/ingest.py\nCode: @@ -58,6 +58,7 @@ def embeddings_to_vectordb(self):\n             embedding=embeddings_model,\n             persist_directory=self.persist_directory,\n             client_settings=self.chroma_settings,\n+            collection_name=self.args.get(\"collection_name\", \"langchain\"),\nComment: forgot to add this here ",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/writer_handler/ingest.py",
    "pr_number": 6650,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1238296920,
    "comment_created_at": "2023-06-22T09:58:36Z"
  },
  {
    "code": "@@ -0,0 +1,98 @@\n+import requests\r\n+from typing import Dict, Any\r\n+from collections import OrderedDict\r\n+import pandas as pd\r\n+from mindsdb.integrations.libs.api_handler import APIHandler\r\n+from mindsdb.integrations.libs.response import (\r\n+    HandlerStatusResponse as StatusResponse,\r\n+    HandlerResponse as Response,\r\n+    RESPONSE_TYPE\r\n+)\r\n+from mindsdb.integrations.libs.response import RESPONSE_TYPE\r\n+from mindsdb.integrations.libs.const import HANDLER_CONNECTION_ARG_TYPE as ARG_TYPE\r\n+\r\n+class DuckDuckGoHandler(APIHandler):\r\n+    def __init__(self, name: str = None, **kwargs):\r\n+        super().__init__(name)\r\n+        self.api_key = None\r\n+        self.is_connected = False\r\n+\r\n+        args = kwargs.get('connection_data', {})\r\n+        if 'api_key' in args:\r\n+            self.api_key = args['api_key']\r\n+\r\n+    def connect(self) -> bool:\r",
    "comment": "How this works? You will need the actually connection here",
    "line_number": 24,
    "enriched": "File: mindsdb/integrations/handlers/duckduckgozeroclick_handler/duckduckgozeroclick_handler.py\nCode: @@ -0,0 +1,98 @@\n+import requests\r\n+from typing import Dict, Any\r\n+from collections import OrderedDict\r\n+import pandas as pd\r\n+from mindsdb.integrations.libs.api_handler import APIHandler\r\n+from mindsdb.integrations.libs.response import (\r\n+    HandlerStatusResponse as StatusResponse,\r\n+    HandlerResponse as Response,\r\n+    RESPONSE_TYPE\r\n+)\r\n+from mindsdb.integrations.libs.response import RESPONSE_TYPE\r\n+from mindsdb.integrations.libs.const import HANDLER_CONNECTION_ARG_TYPE as ARG_TYPE\r\n+\r\n+class DuckDuckGoHandler(APIHandler):\r\n+    def __init__(self, name: str = None, **kwargs):\r\n+        super().__init__(name)\r\n+        self.api_key = None\r\n+        self.is_connected = False\r\n+\r\n+        args = kwargs.get('connection_data', {})\r\n+        if 'api_key' in args:\r\n+            self.api_key = args['api_key']\r\n+\r\n+    def connect(self) -> bool:\r\nComment: How this works? You will need the actually connection here",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/duckduckgozeroclick_handler/duckduckgozeroclick_handler.py",
    "pr_number": 8903,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1519744011,
    "comment_created_at": "2024-03-11T13:44:05Z"
  },
  {
    "code": "@@ -5,28 +5,28 @@ sidebarTitle: ScyllaDB\n \n This is the implementation of the ScyllaDB data handler for MindsDB.\n \n-[ScyllaDB](https://www.scylladb.com/) is an open-source distributed NoSQL wide-column data store. It was designed to be compatible with Apache Cassandra while achieving significantly higher throughputs and lower latencies.\n+[ScyllaDB](https://www.scylladb.com/) is an open-source distributed NoSQL wide-column data store. It was purposefully designed to offer compatibility with Apache Cassandra while outperforming it with higher throughputs and reduced latencies. For a comprehensive understanding of ScyllaDB, visit ScyllaDB's official website.\n \n-## Implementation\n-\n-This handler is implemented using the `scylla-driver` Python library.\n+### Integration Implementation",
    "comment": "So we keep this title in README, but here, we use Implementation and Usage titles.\r\n\r\nLet's change `Integration Implementation` back into `Implementation`.",
    "line_number": 10,
    "enriched": "File: docs/integrations/data-integrations/scylladb.mdx\nCode: @@ -5,28 +5,28 @@ sidebarTitle: ScyllaDB\n \n This is the implementation of the ScyllaDB data handler for MindsDB.\n \n-[ScyllaDB](https://www.scylladb.com/) is an open-source distributed NoSQL wide-column data store. It was designed to be compatible with Apache Cassandra while achieving significantly higher throughputs and lower latencies.\n+[ScyllaDB](https://www.scylladb.com/) is an open-source distributed NoSQL wide-column data store. It was purposefully designed to offer compatibility with Apache Cassandra while outperforming it with higher throughputs and reduced latencies. For a comprehensive understanding of ScyllaDB, visit ScyllaDB's official website.\n \n-## Implementation\n-\n-This handler is implemented using the `scylla-driver` Python library.\n+### Integration Implementation\nComment: So we keep this title in README, but here, we use Implementation and Usage titles.\r\n\r\nLet's change `Integration Implementation` back into `Implementation`.",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "docs/integrations/data-integrations/scylladb.mdx",
    "pr_number": 7171,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1310173178,
    "comment_created_at": "2023-08-30T12:11:30Z"
  },
  {
    "code": "@@ -331,45 +364,96 @@ def select(self, query: ast.Select) -> pd.DataFrame:\n                 else:\n                     raise NotImplementedError(\"Only '=' operator is supported for channel_id column.\")\n \n-        if not video_id and not channel_id:\n-            raise ValueError(\"Either video_id or channel_id has to be present in where clause.\")\n+            elif arg1 == \"query\":\n+                if op == \"=\":\n+                    search_query = arg2\n+                else:\n+                    raise NotImplementedError(\"Only '=' operator is supported for query column.\")\n+\n+        if not video_id and not channel_id and not search_query:\n+            raise ValueError(\"At least one of video_id, channel_id, or query must be present in the WHERE clause.\")\n \n         if video_id:\n             video_df = self.get_videos_by_video_ids([video_id])\n+        elif channel_id:",
    "comment": "Hey @PriyanshuPz,\r\nI think we need to handle situations where a user provides both a `channel_id` and a `search_query`. I think it is reasonable to think that they might want to search for videos within a given channel. According to what we have here now, only the `channel_id` will be taken into account.\r\n\r\nFurther, what exactly is this search based on? Is it the description of the videos?",
    "line_number": 378,
    "enriched": "File: mindsdb/integrations/handlers/youtube_handler/youtube_tables.py\nCode: @@ -331,45 +364,96 @@ def select(self, query: ast.Select) -> pd.DataFrame:\n                 else:\n                     raise NotImplementedError(\"Only '=' operator is supported for channel_id column.\")\n \n-        if not video_id and not channel_id:\n-            raise ValueError(\"Either video_id or channel_id has to be present in where clause.\")\n+            elif arg1 == \"query\":\n+                if op == \"=\":\n+                    search_query = arg2\n+                else:\n+                    raise NotImplementedError(\"Only '=' operator is supported for query column.\")\n+\n+        if not video_id and not channel_id and not search_query:\n+            raise ValueError(\"At least one of video_id, channel_id, or query must be present in the WHERE clause.\")\n \n         if video_id:\n             video_df = self.get_videos_by_video_ids([video_id])\n+        elif channel_id:\nComment: Hey @PriyanshuPz,\r\nI think we need to handle situations where a user provides both a `channel_id` and a `search_query`. I think it is reasonable to think that they might want to search for videos within a given channel. According to what we have here now, only the `channel_id` will be taken into account.\r\n\r\nFurther, what exactly is this search based on? Is it the description of the videos?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/youtube_handler/youtube_tables.py",
    "pr_number": 11195,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2199611553,
    "comment_created_at": "2025-07-11T05:11:40Z"
  },
  {
    "code": "@@ -674,6 +674,30 @@ def get_agent_llm_params(self, agent_params: dict):\n \n         return combined_model_params\n \n+    def _convert_messages_format(self, messages: list[Dict[str, str]]) -> list[Dict[str, str]]:",
    "comment": "The type hint `list[Dict[str, str]]` is too restrictive. The function handles messages that may contain fields other than strings (like metadata objects), and the docstring mentions different message formats. Consider using `List[Dict[str, Any]]` for more flexibility.\n```suggestion\n    def _convert_messages_format(self, messages: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n```",
    "line_number": 677,
    "enriched": "File: mindsdb/interfaces/agents/agents_controller.py\nCode: @@ -674,6 +674,30 @@ def get_agent_llm_params(self, agent_params: dict):\n \n         return combined_model_params\n \n+    def _convert_messages_format(self, messages: list[Dict[str, str]]) -> list[Dict[str, str]]:\nComment: The type hint `list[Dict[str, str]]` is too restrictive. The function handles messages that may contain fields other than strings (like metadata objects), and the docstring mentions different message formats. Consider using `List[Dict[str, Any]]` for more flexibility.\n```suggestion\n    def _convert_messages_format(self, messages: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n```",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/interfaces/agents/agents_controller.py",
    "pr_number": 11467,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2290833486,
    "comment_created_at": "2025-08-21T12:02:15Z"
  },
  {
    "code": "@@ -1,11 +1,11 @@\n ---\n-title: ADJUST Statement\n-sidebarTitle: ADJUST\n+title: FINETUNE Statement\n+sidebarTitle: FINETUNE\n ---\n \n ## Description\n \n-The `ADJUST` statement lets you retrain a model with additional training data.\n+The `FINETUNE` statement lets you retrain a model with additional training data.\n \n Imagine you have a model that was trained with a certain dataset. Now there is more training data available and you wish to retrain this model with a new dataset. The `ADJUST` statement lets you partially retrain the model, so it takes less time and resources than the [`RETRAIN`](/sql/api/retrain) statement. In the machine learning literature, this is also referred to as *fine-tuning* a model.",
    "comment": "You forgot to update this one:\r\n\r\n> The `ADJUST` statement lets you partially retrain the model...",
    "line_number": 10,
    "enriched": "File: docs/sql/api/adjust.mdx\nCode: @@ -1,11 +1,11 @@\n ---\n-title: ADJUST Statement\n-sidebarTitle: ADJUST\n+title: FINETUNE Statement\n+sidebarTitle: FINETUNE\n ---\n \n ## Description\n \n-The `ADJUST` statement lets you retrain a model with additional training data.\n+The `FINETUNE` statement lets you retrain a model with additional training data.\n \n Imagine you have a model that was trained with a certain dataset. Now there is more training data available and you wish to retrain this model with a new dataset. The `ADJUST` statement lets you partially retrain the model, so it takes less time and resources than the [`RETRAIN`](/sql/api/retrain) statement. In the machine learning literature, this is also referred to as *fine-tuning* a model.\nComment: You forgot to update this one:\r\n\r\n> The `ADJUST` statement lets you partially retrain the model...",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "docs/sql/api/adjust.mdx",
    "pr_number": 5677,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1169009463,
    "comment_created_at": "2023-04-17T16:42:20Z"
  },
  {
    "code": "@@ -0,0 +1,69 @@\n+# Welcome to the MindsDB Manual QA Testing for Replicate Handler\r\n+\r\n+> **Please submit your PR in the following format after the underline below `Results` section. Don't forget to add an underline after adding your changes i.e., at the end of your `Results` section.**\r\n+\r\n+## Testing Replicate Handler \r\n+\r\n+**1. CREATE MODEL**\r\n+\r\n+```sql\r\n+CREATE MODEL stability\r\n+PREDICT url\r\n+USING\r\n+    engine = 'replicate',\r\n+    model_name= 'ai-forever/kandinsky-2',\r\n+    version ='601eea49d49003e6ea75a11527209c4f510a93e2112c969d548fbb45b9c4f19f',\r\n+    api_key = 'r8_BpOZN..............................';\r\n+```\r\n+\r\n+**2. DESCRIBE PREDICTOR to see available parameters**\r\n+\r\n+```sql\r\n+DESCRIBE PREDICTOR mindsdb.aiforever.features;\r",
    "comment": "I'm not sure I follow this, shouldn't it be mindsdb.stability?",
    "line_number": 22,
    "enriched": "File: mindsdb/integrations/handlers/replicate_handler/Manual_QA.md\nCode: @@ -0,0 +1,69 @@\n+# Welcome to the MindsDB Manual QA Testing for Replicate Handler\r\n+\r\n+> **Please submit your PR in the following format after the underline below `Results` section. Don't forget to add an underline after adding your changes i.e., at the end of your `Results` section.**\r\n+\r\n+## Testing Replicate Handler \r\n+\r\n+**1. CREATE MODEL**\r\n+\r\n+```sql\r\n+CREATE MODEL stability\r\n+PREDICT url\r\n+USING\r\n+    engine = 'replicate',\r\n+    model_name= 'ai-forever/kandinsky-2',\r\n+    version ='601eea49d49003e6ea75a11527209c4f510a93e2112c969d548fbb45b9c4f19f',\r\n+    api_key = 'r8_BpOZN..............................';\r\n+```\r\n+\r\n+**2. DESCRIBE PREDICTOR to see available parameters**\r\n+\r\n+```sql\r\n+DESCRIBE PREDICTOR mindsdb.aiforever.features;\r\nComment: I'm not sure I follow this, shouldn't it be mindsdb.stability?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/replicate_handler/Manual_QA.md",
    "pr_number": 6795,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1271540613,
    "comment_created_at": "2023-07-23T18:30:45Z"
  },
  {
    "code": "@@ -306,6 +336,7 @@ def dict_to_dataframe(dict_of_dicts, columns_to_ignore=None, index_name=None):\n \n if __name__ == \"__main__\":",
    "comment": "Do we actually need this? Maybe better to remove it",
    "line_number": 337,
    "enriched": "File: mindsdb/integrations/handlers/web_handler/urlcrawl_helpers.py\nCode: @@ -306,6 +336,7 @@ def dict_to_dataframe(dict_of_dicts, columns_to_ignore=None, index_name=None):\n \n if __name__ == \"__main__\":\nComment: Do we actually need this? Maybe better to remove it",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/web_handler/urlcrawl_helpers.py",
    "pr_number": 8188,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1377611312,
    "comment_created_at": "2023-10-31T13:44:54Z"
  },
  {
    "code": "@@ -0,0 +1,13 @@\n+{\n+  \"type\": \"service_account\",\n+  \"project_id\": \"mindsdb-401709\",\n+  \"private_key_id\": \"57a068c37366ed96237dd808c1a97ca29aabafc5\",\n+  \"private_key\": \"-----BEGIN PRIVATE KEY-----\\nMIIEvAIBADANBgkqhkiG9w0BAQEFAASCBKYwggSiAgEAAoIBAQDlLynMvKuT3Ky4\\nuA/el0VQPXrjJnfpdN5sxY3OBvryhrCvkVREmfLMEZk8SQMJxcnQaBalahzmlxIU\\nmxBHKzjmYhUSAmdWQAdIVaZiKR7HSF4YFXLQ+ztJsKJ47YuMYQ/EP+fL33viUG4s\\nTpCkRBgnUxHOw3aoMo9CzNJDMuck4SnqHbY4OHwHOHliMVa1/nBPXKnx/P9Q1pX0\\neMfgAyw8Ui4De4rD8ys1Rl7u5sCr8SBE9yUDtNMTUZt8mfAgEfGmvvd/VIGfyOtC\\nrcMPhx927VoopSw2XKWDv4HoBEjwvSE5H2v1qv4fl6X9oOCar0d0BfvZn+WlW1OV\\n4LITM7U1AgMBAAECgf8Alqf9RAIKLf7O4eOyj3JXTnRSLBeir2hNM6XYUWaP24Sc\\nUO2WnVIs06VOH9Hr10OCwZZLKuxseCaqcXFSk6ppca5y4uUw2d3z/Wx3OQMHfJiB\\nbPNbcA6/TWoq2XhR7AFM2tsevQS8zY93N9jHvRSS6EYJn5wZQwHWsCxG0Tov2ip1\\ndir2bTwXmzj5Au27oEyYgnxpFvRcYEHjvg81fST/A9IlY6KuaAQreJVRruX1PhzX\\n4PAdte0Mr0oMotiI0YJT2K01SYaiHuLFxJQbx5/fiwDE7qr2g9rtGKPJr2U0kNFD\\naB5DFUrKdTVZTsmd37KCCEXZuwFaWpzBAYp0GNkCgYEA9wZqomgqGEX+2JzdOGu+\\nssK/DDJmYZaI8Q3Gwu9F6fGQiI5UuyHhxd48R1qNhfe9nHNqWqCG4t/M+UZmA9xP\\nOqHcfoBJgIP/tEQkPlMyrmeTAdE2zi1GCn5O9rxWcpViOyzXh6don/9FfOAg0QXM\\nVCiZPg3t4mH5el66oJrDOlkCgYEA7YLPFMX66wN+BuiDRXQRrCurRI1jYmj5p4ze\\nEiNdUvdg5DcD7FJxaU4/+oMAhTh4MLrPkoav6bK+PVjHx/LbGcDBrB49mkrdjovY\\n5e9u4t0ez58Ey7JhkozWq6YQibdNsttMl5aCl/M65x+NMz7HMGh+99GirgQuBKtd\\nMp7Pfj0CgYEA0lTaUvKSdRyUQ+g8zJ+hPiBn/4CigBFnBKkQRkbppqqSwFNAfh35\\nWTtC5BaMTjeFcUMsmYH5mA+rPczdbAgYRfov/l5JN24tOVaT7aV1j96M4406Nw+B\\nhcnKfngau3HKtEwYMF8wzJrUleFYz+E5+6ad5KKDnLCDddhW3A0qkfECgYBTZpya\\n2c57WfWWLW1zJQfTol34q4FbnCsWZ8PPMLXjnVRbrunkwSpr5PtYV0oGfcbOrbqX\\n5E2/TXG6DwPQNXo0d9hKPTA6jtFv7bGTRFl2tljMZTWE5bWXhXcYPS8PfNt+07z4\\nefSPsOxdHloxxXiiif8TvHxYiMU0BIysatHHrQKBgQDY+3vxm+evV7uSxVIQais4\\nDlCf0Mvs/UdWIJA1GQVFNUS+NyDhZGbnKjb9NLyR2GnWh9QxqJ3xFCx5GUvw92vg\\nlD1OfIZicJhaaJHvsFJS08As413dEsSKeb7MqzJwFndcWFtv9Hhia7Ca6BC7qmE7\\nz3kn7xaPDNN0BjawXFEAZA==\\n-----END PRIVATE KEY-----\\n\",",
    "comment": "I assume we're OK to publish this key?",
    "line_number": 5,
    "enriched": "File: tests/unit/ml_handlers/data/vertex_service_key.json\nCode: @@ -0,0 +1,13 @@\n+{\n+  \"type\": \"service_account\",\n+  \"project_id\": \"mindsdb-401709\",\n+  \"private_key_id\": \"57a068c37366ed96237dd808c1a97ca29aabafc5\",\n+  \"private_key\": \"-----BEGIN PRIVATE KEY-----\\nMIIEvAIBADANBgkqhkiG9w0BAQEFAASCBKYwggSiAgEAAoIBAQDlLynMvKuT3Ky4\\nuA/el0VQPXrjJnfpdN5sxY3OBvryhrCvkVREmfLMEZk8SQMJxcnQaBalahzmlxIU\\nmxBHKzjmYhUSAmdWQAdIVaZiKR7HSF4YFXLQ+ztJsKJ47YuMYQ/EP+fL33viUG4s\\nTpCkRBgnUxHOw3aoMo9CzNJDMuck4SnqHbY4OHwHOHliMVa1/nBPXKnx/P9Q1pX0\\neMfgAyw8Ui4De4rD8ys1Rl7u5sCr8SBE9yUDtNMTUZt8mfAgEfGmvvd/VIGfyOtC\\nrcMPhx927VoopSw2XKWDv4HoBEjwvSE5H2v1qv4fl6X9oOCar0d0BfvZn+WlW1OV\\n4LITM7U1AgMBAAECgf8Alqf9RAIKLf7O4eOyj3JXTnRSLBeir2hNM6XYUWaP24Sc\\nUO2WnVIs06VOH9Hr10OCwZZLKuxseCaqcXFSk6ppca5y4uUw2d3z/Wx3OQMHfJiB\\nbPNbcA6/TWoq2XhR7AFM2tsevQS8zY93N9jHvRSS6EYJn5wZQwHWsCxG0Tov2ip1\\ndir2bTwXmzj5Au27oEyYgnxpFvRcYEHjvg81fST/A9IlY6KuaAQreJVRruX1PhzX\\n4PAdte0Mr0oMotiI0YJT2K01SYaiHuLFxJQbx5/fiwDE7qr2g9rtGKPJr2U0kNFD\\naB5DFUrKdTVZTsmd37KCCEXZuwFaWpzBAYp0GNkCgYEA9wZqomgqGEX+2JzdOGu+\\nssK/DDJmYZaI8Q3Gwu9F6fGQiI5UuyHhxd48R1qNhfe9nHNqWqCG4t/M+UZmA9xP\\nOqHcfoBJgIP/tEQkPlMyrmeTAdE2zi1GCn5O9rxWcpViOyzXh6don/9FfOAg0QXM\\nVCiZPg3t4mH5el66oJrDOlkCgYEA7YLPFMX66wN+BuiDRXQRrCurRI1jYmj5p4ze\\nEiNdUvdg5DcD7FJxaU4/+oMAhTh4MLrPkoav6bK+PVjHx/LbGcDBrB49mkrdjovY\\n5e9u4t0ez58Ey7JhkozWq6YQibdNsttMl5aCl/M65x+NMz7HMGh+99GirgQuBKtd\\nMp7Pfj0CgYEA0lTaUvKSdRyUQ+g8zJ+hPiBn/4CigBFnBKkQRkbppqqSwFNAfh35\\nWTtC5BaMTjeFcUMsmYH5mA+rPczdbAgYRfov/l5JN24tOVaT7aV1j96M4406Nw+B\\nhcnKfngau3HKtEwYMF8wzJrUleFYz+E5+6ad5KKDnLCDddhW3A0qkfECgYBTZpya\\n2c57WfWWLW1zJQfTol34q4FbnCsWZ8PPMLXjnVRbrunkwSpr5PtYV0oGfcbOrbqX\\n5E2/TXG6DwPQNXo0d9hKPTA6jtFv7bGTRFl2tljMZTWE5bWXhXcYPS8PfNt+07z4\\nefSPsOxdHloxxXiiif8TvHxYiMU0BIysatHHrQKBgQDY+3vxm+evV7uSxVIQais4\\nDlCf0Mvs/UdWIJA1GQVFNUS+NyDhZGbnKjb9NLyR2GnWh9QxqJ3xFCx5GUvw92vg\\nlD1OfIZicJhaaJHvsFJS08As413dEsSKeb7MqzJwFndcWFtv9Hhia7Ca6BC7qmE7\\nz3kn7xaPDNN0BjawXFEAZA==\\n-----END PRIVATE KEY-----\\n\",\nComment: I assume we're OK to publish this key?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "tests/unit/ml_handlers/data/vertex_service_key.json",
    "pr_number": 7950,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1375104217,
    "comment_created_at": "2023-10-27T22:48:01Z"
  },
  {
    "code": "@@ -101,29 +108,43 @@ def _handle_source(file_path, clean_rows=True, custom_parser=None):\n             header, file_data = custom_parser(data, fmt)\n             df = pd.DataFrame(file_data, columns=header)\n \n-        elif fmt == 'parquet':\n+        elif fmt == \"parquet\":\n             df = pd.read_parquet(data)\n \n-        elif fmt == 'csv':\n+        elif fmt == \"csv\":\n             df = pd.read_csv(data, sep=dialect.delimiter, index_col=False)\n \n-        elif fmt in ['xlsx', 'xls']:\n+        elif fmt in [\"xlsx\", \"xls\"]:\n             data.seek(0)\n             df = pd.read_excel(data)\n \n-        elif fmt == 'json':\n+        elif fmt == \"json\":\n             data.seek(0)\n             json_doc = json.loads(data.read())\n             df = pd.json_normalize(json_doc, max_level=0)\n \n+        elif fmt == \"txt\":\n+            from langchain.document_loaders import TextLoader\n+\n+            loader = TextLoader(file_path, encoding=\"utf8\")\n+            docs = loader.load()\n+            df = pd.DataFrame([{\"text\": doc.page_content} for doc in docs])\n+\n+        elif fmt == \"pdf\":\n+            from langchain.document_loaders import PDFMinerLoader\n+\n+            loader = PDFMinerLoader(file_path)\n+            docs = loader.load()\n+            df = pd.DataFrame([{\"text\": doc.page_content} for doc in docs])\n+",
    "comment": "additions here",
    "line_number": 139,
    "enriched": "File: mindsdb/integrations/handlers/file_handler/file_handler.py\nCode: @@ -101,29 +108,43 @@ def _handle_source(file_path, clean_rows=True, custom_parser=None):\n             header, file_data = custom_parser(data, fmt)\n             df = pd.DataFrame(file_data, columns=header)\n \n-        elif fmt == 'parquet':\n+        elif fmt == \"parquet\":\n             df = pd.read_parquet(data)\n \n-        elif fmt == 'csv':\n+        elif fmt == \"csv\":\n             df = pd.read_csv(data, sep=dialect.delimiter, index_col=False)\n \n-        elif fmt in ['xlsx', 'xls']:\n+        elif fmt in [\"xlsx\", \"xls\"]:\n             data.seek(0)\n             df = pd.read_excel(data)\n \n-        elif fmt == 'json':\n+        elif fmt == \"json\":\n             data.seek(0)\n             json_doc = json.loads(data.read())\n             df = pd.json_normalize(json_doc, max_level=0)\n \n+        elif fmt == \"txt\":\n+            from langchain.document_loaders import TextLoader\n+\n+            loader = TextLoader(file_path, encoding=\"utf8\")\n+            docs = loader.load()\n+            df = pd.DataFrame([{\"text\": doc.page_content} for doc in docs])\n+\n+        elif fmt == \"pdf\":\n+            from langchain.document_loaders import PDFMinerLoader\n+\n+            loader = PDFMinerLoader(file_path)\n+            docs = loader.load()\n+            df = pd.DataFrame([{\"text\": doc.page_content} for doc in docs])\n+\nComment: additions here",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/file_handler/file_handler.py",
    "pr_number": 7379,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1338193546,
    "comment_created_at": "2023-09-27T07:52:16Z"
  },
  {
    "code": "@@ -58,6 +58,7 @@ opentelemetry-exporter-otlp==1.27.0\n opentelemetry-instrumentation-requests==0.48b0\n opentelemetry-instrumentation-flask==0.48b0\n opentelemetry-distro==0.48b0\n+yaml",
    "comment": "We may pin this version to avoid the CI issues, also is this pyyaml ?",
    "line_number": 61,
    "enriched": "File: requirements/requirements.txt\nCode: @@ -58,6 +58,7 @@ opentelemetry-exporter-otlp==1.27.0\n opentelemetry-instrumentation-requests==0.48b0\n opentelemetry-instrumentation-flask==0.48b0\n opentelemetry-distro==0.48b0\n+yaml\nComment: We may pin this version to avoid the CI issues, also is this pyyaml ?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "requirements/requirements.txt",
    "pr_number": 10532,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1979624026,
    "comment_created_at": "2025-03-04T14:54:06Z"
  },
  {
    "code": "@@ -1,4 +1,4 @@\n from .select_query_utilities import SELECTQueryParser, SELECTQueryExecutor\n-from .insert_query_utilities import INSERTQueryParser\n from .update_query_utilities import UPDATEQueryParser, UPDATEQueryExecutor\n-from .delete_query_utilities import DELETEQueryParser, DELETEQueryExecutor\n+#from .delete_query_utilities import DELETEQueryParser, DELETEQueryExecutor",
    "comment": "Why have these imports been commented out?",
    "line_number": 3,
    "enriched": "File: mindsdb/integrations/handlers/utilities/query_utilities/__init__.py\nCode: @@ -1,4 +1,4 @@\n from .select_query_utilities import SELECTQueryParser, SELECTQueryExecutor\n-from .insert_query_utilities import INSERTQueryParser\n from .update_query_utilities import UPDATEQueryParser, UPDATEQueryExecutor\n-from .delete_query_utilities import DELETEQueryParser, DELETEQueryExecutor\n+#from .delete_query_utilities import DELETEQueryParser, DELETEQueryExecutor\nComment: Why have these imports been commented out?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/utilities/query_utilities/__init__.py",
    "pr_number": 7578,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1354872531,
    "comment_created_at": "2023-10-11T12:15:19Z"
  },
  {
    "code": "@@ -0,0 +1,8 @@\n+{\n+    \"name\": \"MindsDB\",\n+    \"build\": {\n+      \"dockerfile\": \"../docker/mindsdb.Dockerfile\"\n+    },\n+    \"appPort\": [47334],",
    "comment": "We will need to export 47335 too",
    "line_number": 6,
    "enriched": "File: .devcontainer/devcontainer.json\nCode: @@ -0,0 +1,8 @@\n+{\n+    \"name\": \"MindsDB\",\n+    \"build\": {\n+      \"dockerfile\": \"../docker/mindsdb.Dockerfile\"\n+    },\n+    \"appPort\": [47334],\nComment: We will need to export 47335 too",
    "subcategory": "question",
    "category": "discussion",
    "file_path": ".devcontainer/devcontainer.json",
    "pr_number": 7373,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1332802334,
    "comment_created_at": "2023-09-21T09:57:52Z"
  },
  {
    "code": "@@ -0,0 +1,26 @@\n+import os\n+import pytest\n+\n+from mindsdb.api.mysql.mysql_proxy.libs.constants.response_type import RESPONSE_TYPE\n+from integration_tests.flows.http_test_helpers import HTTPHelperMixin\n+from integration_tests.flows.conftest import *  # noqa: F403,F401\n+\n+# used by (required for) mindsdb_app fixture in conftest\n+API_LIST = [\n+    \"http\",\n+]\n+\n+OPEN_AI_API_KEY = os.environ.get(\"OPEN_AI_API_KEY\")\n+\n+\n+@pytest.mark.usefixtures(\"mindsdb_app\")\n+class TestOpenAIHandler(HTTPHelperMixin):",
    "comment": "Should this file (with test) be added to CI in this PR or later?",
    "line_number": 17,
    "enriched": "File: tests/handler_tests/test_openai_handler.py\nCode: @@ -0,0 +1,26 @@\n+import os\n+import pytest\n+\n+from mindsdb.api.mysql.mysql_proxy.libs.constants.response_type import RESPONSE_TYPE\n+from integration_tests.flows.http_test_helpers import HTTPHelperMixin\n+from integration_tests.flows.conftest import *  # noqa: F403,F401\n+\n+# used by (required for) mindsdb_app fixture in conftest\n+API_LIST = [\n+    \"http\",\n+]\n+\n+OPEN_AI_API_KEY = os.environ.get(\"OPEN_AI_API_KEY\")\n+\n+\n+@pytest.mark.usefixtures(\"mindsdb_app\")\n+class TestOpenAIHandler(HTTPHelperMixin):\nComment: Should this file (with test) be added to CI in this PR or later?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "tests/handler_tests/test_openai_handler.py",
    "pr_number": 5242,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1151842270,
    "comment_created_at": "2023-03-29T12:13:43Z"
  },
  {
    "code": "@@ -145,21 +158,46 @@ def answer_to_user(self, chat_id):\n             # the last message is from bot\n             return\n \n+        df = df.rename(columns={\n+            text_col: 'text',\n+            username_col: 'username',\n+            time_col: 'sent_at'\n+        })\n+        model_output = self.apply_model(df, chat_id)\n+\n+        # send answer to user\n+        ast_query = Insert(\n+            table=Identifier(t_params['name']),\n+            columns=[t_params['chat_id_col'], t_params['text_col']],\n+            values=[\n+                [chat_id, model_output],\n+            ]\n+        )\n+\n+        self.db_handler.query(ast_query)\n+\n+    def _chat_history_to_conversation(self, df, chat_id):\n+\n+        bot_username = self.params['bot_username']\n+\n         question_col = self.params['model']['user_column']\n         answer_col = self.params['model']['bot_column']\n \n+        history_from = self.chat_memory[chat_id].get('history_from')",
    "comment": "Just to double check my interpretation: this is retrieving all messages that have not been processed by the memory, yes?",
    "line_number": 186,
    "enriched": "File: mindsdb/interfaces/chatbot/chatbot_task.py\nCode: @@ -145,21 +158,46 @@ def answer_to_user(self, chat_id):\n             # the last message is from bot\n             return\n \n+        df = df.rename(columns={\n+            text_col: 'text',\n+            username_col: 'username',\n+            time_col: 'sent_at'\n+        })\n+        model_output = self.apply_model(df, chat_id)\n+\n+        # send answer to user\n+        ast_query = Insert(\n+            table=Identifier(t_params['name']),\n+            columns=[t_params['chat_id_col'], t_params['text_col']],\n+            values=[\n+                [chat_id, model_output],\n+            ]\n+        )\n+\n+        self.db_handler.query(ast_query)\n+\n+    def _chat_history_to_conversation(self, df, chat_id):\n+\n+        bot_username = self.params['bot_username']\n+\n         question_col = self.params['model']['user_column']\n         answer_col = self.params['model']['bot_column']\n \n+        history_from = self.chat_memory[chat_id].get('history_from')\nComment: Just to double check my interpretation: this is retrieving all messages that have not been processed by the memory, yes?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/interfaces/chatbot/chatbot_task.py",
    "pr_number": 6642,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1244349665,
    "comment_created_at": "2023-06-27T21:01:12Z"
  },
  {
    "code": "@@ -9,6 +10,12 @@\n _DEFAULT_TOP_K_SIMILARITY_SEARCH = 5\n \n \n+class SkillType(enum.Enum):\n+    TEXT2SQL = 'text2sql'\n+    KNOWLEDGE_BASE = 'knowledge_base'\n+    RETRIEVAL = 'retrieval'",
    "comment": "Any particular reason we're making a new skill? Seems unnecessary since we already have the `knowledge_base` skill, which is essentially retrieval.",
    "line_number": 16,
    "enriched": "File: mindsdb/interfaces/skills/skill_tool.py\nCode: @@ -9,6 +10,12 @@\n _DEFAULT_TOP_K_SIMILARITY_SEARCH = 5\n \n \n+class SkillType(enum.Enum):\n+    TEXT2SQL = 'text2sql'\n+    KNOWLEDGE_BASE = 'knowledge_base'\n+    RETRIEVAL = 'retrieval'\nComment: Any particular reason we're making a new skill? Seems unnecessary since we already have the `knowledge_base` skill, which is essentially retrieval.",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/interfaces/skills/skill_tool.py",
    "pr_number": 9027,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1556685419,
    "comment_created_at": "2024-04-09T01:06:11Z"
  },
  {
    "code": "@@ -9,8 +10,6 @@\n from mindsdb.integrations.utilities.rag.settings import RAGPipelineModel, DEFAULT_AUTO_META_PROMPT_TEMPLATE\n from mindsdb.integrations.utilities.rag.vector_store import VectorStoreOperator\n \n-from mindsdb.interfaces.agents.safe_output_parser import SafeOutputParser",
    "comment": "This was the main issue behind the problem with streaming, streaming agent executor in langchain expects message to be a string not a dictionary.",
    "line_number": 12,
    "enriched": "File: mindsdb/integrations/utilities/rag/pipelines/rag.py\nCode: @@ -9,8 +10,6 @@\n from mindsdb.integrations.utilities.rag.settings import RAGPipelineModel, DEFAULT_AUTO_META_PROMPT_TEMPLATE\n from mindsdb.integrations.utilities.rag.vector_store import VectorStoreOperator\n \n-from mindsdb.interfaces.agents.safe_output_parser import SafeOutputParser\nComment: This was the main issue behind the problem with streaming, streaming agent executor in langchain expects message to be a string not a dictionary.",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/utilities/rag/pipelines/rag.py",
    "pr_number": 9576,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1699967768,
    "comment_created_at": "2024-08-01T11:12:09Z"
  },
  {
    "code": "@@ -56,7 +56,7 @@ def get_requirements_from_file(path):\n         # A2A API internal imports\n         \"common\", \"task_manager\", \"agent\"\n     ],\n-    \"DEP002\": [\"psycopg2-binary\", \"lark\", \"transformers\", \"langchain-experimental\", \"lxml\", \"openpyxl\", \"onnxruntime\", \"pydantic_core\"]\n+    \"DEP002\": [\"psycopg2-binary\", \"lark\", \"transformers\", \"langchain-experimental\", \"lxml\", \"openpyxl\", \"onnxruntime\", \"pydantic_core\", \"writer-sdk\"]",
    "comment": "Is this being added because writer-sdk is imported under a different name, or because it's an optional dep?\r\nCan we add a comment above explaining why it's there",
    "line_number": 59,
    "enriched": "File: tests/scripts/check_requirements.py\nCode: @@ -56,7 +56,7 @@ def get_requirements_from_file(path):\n         # A2A API internal imports\n         \"common\", \"task_manager\", \"agent\"\n     ],\n-    \"DEP002\": [\"psycopg2-binary\", \"lark\", \"transformers\", \"langchain-experimental\", \"lxml\", \"openpyxl\", \"onnxruntime\", \"pydantic_core\"]\n+    \"DEP002\": [\"psycopg2-binary\", \"lark\", \"transformers\", \"langchain-experimental\", \"lxml\", \"openpyxl\", \"onnxruntime\", \"pydantic_core\", \"writer-sdk\"]\nComment: Is this being added because writer-sdk is imported under a different name, or because it's an optional dep?\r\nCan we add a comment above explaining why it's there",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "tests/scripts/check_requirements.py",
    "pr_number": 10915,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2110226639,
    "comment_created_at": "2025-05-27T20:53:04Z"
  },
  {
    "code": "@@ -5,11 +5,11 @@ sidebarTitle: Amazon Redshift\n \n This is the implementation of the Redshift data handler for MindsDB.\n \n-[Amazon Redshift](https://docs.aws.amazon.com/redshift/latest/mgmt/welcome.html) is a fully managed, petabyte-scale data warehouse service in the cloud. You can start with just a few hundred gigabytes of data and scale to a petabyte or more. This enables you to use your data to acquire new insights for your business and customers.\n+[Amazon Redshift](https://docs.aws.amazon.com/redshift/latest/mgmt/welcome.html) is a fully managed, petabyte-scale data warehouse service in the cloud. You can start with just a few hundred gigabytes of data and scale to a petabyte or more enabling you to use your data to acquire new insights for your business and customers.",
    "comment": "There should be a comma here: `...to a petabyte or more, enabling you to use...` to make it correct.",
    "line_number": 8,
    "enriched": "File: docs/integrations/data-integrations/amazon-redshift.mdx\nCode: @@ -5,11 +5,11 @@ sidebarTitle: Amazon Redshift\n \n This is the implementation of the Redshift data handler for MindsDB.\n \n-[Amazon Redshift](https://docs.aws.amazon.com/redshift/latest/mgmt/welcome.html) is a fully managed, petabyte-scale data warehouse service in the cloud. You can start with just a few hundred gigabytes of data and scale to a petabyte or more. This enables you to use your data to acquire new insights for your business and customers.\n+[Amazon Redshift](https://docs.aws.amazon.com/redshift/latest/mgmt/welcome.html) is a fully managed, petabyte-scale data warehouse service in the cloud. You can start with just a few hundred gigabytes of data and scale to a petabyte or more enabling you to use your data to acquire new insights for your business and customers.\nComment: There should be a comma here: `...to a petabyte or more, enabling you to use...` to make it correct.",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "docs/integrations/data-integrations/amazon-redshift.mdx",
    "pr_number": 8503,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1423996322,
    "comment_created_at": "2023-12-12T13:30:17Z"
  },
  {
    "code": "@@ -60,31 +61,59 @@ tbc\n -- Create RAG engine NB you can provide api keys here or at create\n -- You only need to provide key of llm you intend to use\n \n-create ML_Engine rag_handler_test1232144 from rag\n+create ML_Engine rag from rag\n using\n openai_api_key=\"openai-api-key\",\n writer_org_id=\"writer-org\",\n writer_api_key=\"writer-api-key\";\n \n \n+--using DB table as input\n \n--- Create a RAG model - OpenAI API and FAISS vectorDB with embeddings\n-CREATE MODEL rag_handler_openai_test\n-FROM mysql_demo_db (select * from demo_fda_context)\n+CREATE MODEL rag_handler_db_test\n+FROM mysql_demo_db (select * from demo_fda_context limit 2)\n PREDICT answer\n USING\n-   engine=\"rag_handler\",\n-   top_k=4,\n+   engine=\"rag\",\n    llm_type=\"openai\",\n-   vector_store_folder_name='rag_handler_openai_test',\n-   embeddings_model_name=\"BAAI/bge-base-en\",\n-   prompt_template='Use the following pieces of context to answer the question at the end. If you do not know the answer, just say that you do not know, do not try to make up an answer.\n-Context: {context}\n-Question: {question}\n-Helpful Answer:';\n-\n--- Ask a question on your data using OpenAI LLM API\n+   vector_store_folder_name='test_db';\n+\n+select * from information_schema.models where name =\"rag_handler_db_test\" ;\n+\n+\n SELECT *\n-FROM rag_handler_openai_test\n+FROM rag_handler_db_test\n WHERE question='what product is best for treating a cold?';\n+\n+\n+--using url as input\n+\n+CREATE MODEL rag_handler_url_test\n+predict answer\n+USING\n+   engine=\"rag\",\n+   llm_type=\"openai\",\n+   url='https://docs.mindsdb.com/what-is-mindsdb',\n+   vector_store_folder_name='test_url';\n+\n+\n+SELECT *\n+FROM rag_handler_url_test\n+WHERE question='what ML use cases does mindsdb support?';\n+\n+\n+--using .txt or .pdf as input (first upload file using UI)\n+\n+CREATE MODEL rag_handler_file_test\n+predict answer\n+from files (select * from uploaded_file;)",
    "comment": "Wouldn't this semicolon induce a parsing error?",
    "line_number": 109,
    "enriched": "File: mindsdb/integrations/handlers/rag_handler/README.md\nCode: @@ -60,31 +61,59 @@ tbc\n -- Create RAG engine NB you can provide api keys here or at create\n -- You only need to provide key of llm you intend to use\n \n-create ML_Engine rag_handler_test1232144 from rag\n+create ML_Engine rag from rag\n using\n openai_api_key=\"openai-api-key\",\n writer_org_id=\"writer-org\",\n writer_api_key=\"writer-api-key\";\n \n \n+--using DB table as input\n \n--- Create a RAG model - OpenAI API and FAISS vectorDB with embeddings\n-CREATE MODEL rag_handler_openai_test\n-FROM mysql_demo_db (select * from demo_fda_context)\n+CREATE MODEL rag_handler_db_test\n+FROM mysql_demo_db (select * from demo_fda_context limit 2)\n PREDICT answer\n USING\n-   engine=\"rag_handler\",\n-   top_k=4,\n+   engine=\"rag\",\n    llm_type=\"openai\",\n-   vector_store_folder_name='rag_handler_openai_test',\n-   embeddings_model_name=\"BAAI/bge-base-en\",\n-   prompt_template='Use the following pieces of context to answer the question at the end. If you do not know the answer, just say that you do not know, do not try to make up an answer.\n-Context: {context}\n-Question: {question}\n-Helpful Answer:';\n-\n--- Ask a question on your data using OpenAI LLM API\n+   vector_store_folder_name='test_db';\n+\n+select * from information_schema.models where name =\"rag_handler_db_test\" ;\n+\n+\n SELECT *\n-FROM rag_handler_openai_test\n+FROM rag_handler_db_test\n WHERE question='what product is best for treating a cold?';\n+\n+\n+--using url as input\n+\n+CREATE MODEL rag_handler_url_test\n+predict answer\n+USING\n+   engine=\"rag\",\n+   llm_type=\"openai\",\n+   url='https://docs.mindsdb.com/what-is-mindsdb',\n+   vector_store_folder_name='test_url';\n+\n+\n+SELECT *\n+FROM rag_handler_url_test\n+WHERE question='what ML use cases does mindsdb support?';\n+\n+\n+--using .txt or .pdf as input (first upload file using UI)\n+\n+CREATE MODEL rag_handler_file_test\n+predict answer\n+from files (select * from uploaded_file;)\nComment: Wouldn't this semicolon induce a parsing error?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/rag_handler/README.md",
    "pr_number": 8283,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1384497593,
    "comment_created_at": "2023-11-07T07:43:04Z"
  },
  {
    "code": "@@ -0,0 +1,105 @@\n+import pandas as pd\n+import requests\n+from mindsdb.integrations.libs.api_handler import APIHandler\n+from mindsdb.integrations.handlers.kraken_handler.kraken_tables import KrakenTradesTable\n+from collections import OrderedDict\n+\n+from mindsdb.integrations.libs.const import HANDLER_CONNECTION_ARG_TYPE as ARG_TYPE\n+\n+from mindsdb.integrations.libs.response import HandlerStatusResponse as StatusResponse\n+\n+\n+class KrakenHandler(APIHandler):\n+    \"\"\"A class for handling connections and interactions with the Kraken API.\n+    Attributes:\n+        api_key (str): API Public key\n+        api_secret (str): API Private key\n+    \"\"\"\n+    def __init__(self, name: str = None, **kwargs):\n+        \"\"\"Registers all API tables and prepares the handler for an API connection.\n+        Args:\n+            name: (str): The handler name to use\n+        \"\"\"\n+        super().__init__(name)\n+        self.api_key = None\n+        self.api_secret = None\n+\n+        args = kwargs.get(\"connection_data\", {})\n+        if \"api_key\" in args:\n+            self.api_key = args[\"api_key\"]\n+        if \"api_secret\" in args:\n+            self.api_secret = args[\"api_secret\"]\n+        self.client = None\n+        self.is_connected = False\n+\n+        trade_history = KrakenTradesTable(self)\n+        self._register_table(\"kraken_trade_history\", trade_history)\n+\n+    def connect(self):\n+        \"\"\"Creates a new Kraken API client if needed and sets it as the client to use for requests.\n+        Returns newly created Kraken API client, or current client if already set.\n+        \"\"\"\n+        self.is_connected = True\n+        return self.client\n+\n+    def check_connection(self) -> StatusResponse:\n+        \"\"\"Checks connection to Kraken API by sending a ping request.\n+        Returns StatusResponse indicating whether or not the handler is connected.\n+        \"\"\"\n+        response = StatusResponse(True)",
    "comment": "This does not seem to call the Kraken API at all. Is there some endpoint that we can all to ensure that the connection is successful?",
    "line_number": 49,
    "enriched": "File: mindsdb/integrations/handlers/kraken_handler/kraken_handler.py\nCode: @@ -0,0 +1,105 @@\n+import pandas as pd\n+import requests\n+from mindsdb.integrations.libs.api_handler import APIHandler\n+from mindsdb.integrations.handlers.kraken_handler.kraken_tables import KrakenTradesTable\n+from collections import OrderedDict\n+\n+from mindsdb.integrations.libs.const import HANDLER_CONNECTION_ARG_TYPE as ARG_TYPE\n+\n+from mindsdb.integrations.libs.response import HandlerStatusResponse as StatusResponse\n+\n+\n+class KrakenHandler(APIHandler):\n+    \"\"\"A class for handling connections and interactions with the Kraken API.\n+    Attributes:\n+        api_key (str): API Public key\n+        api_secret (str): API Private key\n+    \"\"\"\n+    def __init__(self, name: str = None, **kwargs):\n+        \"\"\"Registers all API tables and prepares the handler for an API connection.\n+        Args:\n+            name: (str): The handler name to use\n+        \"\"\"\n+        super().__init__(name)\n+        self.api_key = None\n+        self.api_secret = None\n+\n+        args = kwargs.get(\"connection_data\", {})\n+        if \"api_key\" in args:\n+            self.api_key = args[\"api_key\"]\n+        if \"api_secret\" in args:\n+            self.api_secret = args[\"api_secret\"]\n+        self.client = None\n+        self.is_connected = False\n+\n+        trade_history = KrakenTradesTable(self)\n+        self._register_table(\"kraken_trade_history\", trade_history)\n+\n+    def connect(self):\n+        \"\"\"Creates a new Kraken API client if needed and sets it as the client to use for requests.\n+        Returns newly created Kraken API client, or current client if already set.\n+        \"\"\"\n+        self.is_connected = True\n+        return self.client\n+\n+    def check_connection(self) -> StatusResponse:\n+        \"\"\"Checks connection to Kraken API by sending a ping request.\n+        Returns StatusResponse indicating whether or not the handler is connected.\n+        \"\"\"\n+        response = StatusResponse(True)\nComment: This does not seem to call the Kraken API at all. Is there some endpoint that we can all to ensure that the connection is successful?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/kraken_handler/kraken_handler.py",
    "pr_number": 8178,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1379715793,
    "comment_created_at": "2023-11-02T07:59:14Z"
  },
  {
    "code": "@@ -29,6 +30,15 @@\n IS_PY36 = sys.version_info[1] <= 6\n \n \n+def delete_model_storage(model_id, ctx_dump):\n+    try:\n+        ctx.load(ctx_dump)\n+        modelStorage = ModelStorage(model_id)\n+        modelStorage.delete()",
    "comment": "if we are having a huge amount of versions, maybe worth implementing deleting json values ModelStorage.delete (using 'resource_id in ...' filter) ?\r\n",
    "line_number": 37,
    "enriched": "File: mindsdb/interfaces/model/model_controller.py\nCode: @@ -29,6 +30,15 @@\n IS_PY36 = sys.version_info[1] <= 6\n \n \n+def delete_model_storage(model_id, ctx_dump):\n+    try:\n+        ctx.load(ctx_dump)\n+        modelStorage = ModelStorage(model_id)\n+        modelStorage.delete()\nComment: if we are having a huge amount of versions, maybe worth implementing deleting json values ModelStorage.delete (using 'resource_id in ...' filter) ?\r\n",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/interfaces/model/model_controller.py",
    "pr_number": 6860,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1265409835,
    "comment_created_at": "2023-07-17T14:00:42Z"
  },
  {
    "code": "@@ -108,3 +110,31 @@ def native_query(self, query: str) -> StatusResponse:\n         \"\"\"\n         ast = parse_sql(query, dialect=\"mindsdb\")\n         return self.query(ast)\n+\n+\n+connection_args = OrderedDict(\n+    repository={\n+        \"type\": ARG_TYPE.STR,\n+        \"description\": \" GitHub repository name.\",\n+        \"required\": True,\n+        \"label\": \"Repository\",\n+    },\n+    api_key={\n+        \"type\": ARG_TYPE.PWD,\n+        \"description\": \"Optional GitHub API key to use for authentication.\",\n+        \"required\": False,\n+        \"label\": \"Api key\",\n+    },\n+    github_url={\n+        \"type\": ARG_TYPE.STR,\n+        \"description\": \"Optional GitHub URL to connect to a GitHub Enterprise instance.\",\n+        \"required\": False,\n+        \"label\": \"Github url\",\n+    },\n+)\n+\n+connection_args_example = OrderedDict(\n+    repository=\"mindsdb/mindsdb\", \n+    api_key=\"ghp_z91InCQZWZAMlddOzFCX7xHJrf9Fai35HT7\", ",
    "comment": "Is this randomly generated key or an actual value?",
    "line_number": 138,
    "enriched": "File: mindsdb/integrations/handlers/github_handler/github_handler.py\nCode: @@ -108,3 +110,31 @@ def native_query(self, query: str) -> StatusResponse:\n         \"\"\"\n         ast = parse_sql(query, dialect=\"mindsdb\")\n         return self.query(ast)\n+\n+\n+connection_args = OrderedDict(\n+    repository={\n+        \"type\": ARG_TYPE.STR,\n+        \"description\": \" GitHub repository name.\",\n+        \"required\": True,\n+        \"label\": \"Repository\",\n+    },\n+    api_key={\n+        \"type\": ARG_TYPE.PWD,\n+        \"description\": \"Optional GitHub API key to use for authentication.\",\n+        \"required\": False,\n+        \"label\": \"Api key\",\n+    },\n+    github_url={\n+        \"type\": ARG_TYPE.STR,\n+        \"description\": \"Optional GitHub URL to connect to a GitHub Enterprise instance.\",\n+        \"required\": False,\n+        \"label\": \"Github url\",\n+    },\n+)\n+\n+connection_args_example = OrderedDict(\n+    repository=\"mindsdb/mindsdb\", \n+    api_key=\"ghp_z91InCQZWZAMlddOzFCX7xHJrf9Fai35HT7\", \nComment: Is this randomly generated key or an actual value?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/github_handler/github_handler.py",
    "pr_number": 7051,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1296944584,
    "comment_created_at": "2023-08-17T09:36:45Z"
  },
  {
    "code": "@@ -0,0 +1,118 @@\n+from typing import List\n+\n+import requests\n+\n+\n+class ConfluenceAPIClient:\n+    def __init__(self, url: str, username: str, password: str):\n+        self.url = url\n+        self.username = username\n+        self.password = password\n+        self.session = requests.Session()\n+        self.session.auth = (self.username, self.password)\n+        self.session.headers.update({\"Accept\": \"application/json\"})\n+\n+    def get_spaces(\n+        self,\n+        ids: List[int] = None,\n+        keys: List[str] = None,\n+        type: str = None,",
    "comment": " `type` may be confused with ta built-in names?",
    "line_number": 19,
    "enriched": "File: mindsdb/integrations/handlers/confluence_handler/confluence_api_client.py\nCode: @@ -0,0 +1,118 @@\n+from typing import List\n+\n+import requests\n+\n+\n+class ConfluenceAPIClient:\n+    def __init__(self, url: str, username: str, password: str):\n+        self.url = url\n+        self.username = username\n+        self.password = password\n+        self.session = requests.Session()\n+        self.session.auth = (self.username, self.password)\n+        self.session.headers.update({\"Accept\": \"application/json\"})\n+\n+    def get_spaces(\n+        self,\n+        ids: List[int] = None,\n+        keys: List[str] = None,\n+        type: str = None,\nComment:  `type` may be confused with ta built-in names?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/confluence_handler/confluence_api_client.py",
    "pr_number": 10598,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2012365874,
    "comment_created_at": "2025-03-25T15:24:56Z"
  },
  {
    "code": "@@ -6,35 +6,105 @@ Google Generative AI is a library that provides access to powerful language mode\n \n *Note:* Ensure you have the necessary API key for accessing the Google gen AI library. You can get your API key at https://makersuite.google.com/. \n \n+>> This Handler requires python>=3.9 to work\n+>> (Default model_name is **gemini-pro** )\n+\n # Example Usage\n \n-Create a ML Engine with the new `google_gemini` engine.\n+#### Create Gemini ML Engine\n+```sql\n+CREATE ML_ENGINE g\n+FROM gemini\n+USING\n+    api_key = 'AI-i5-c001';\n+```\n \n+\n+#### Create Gemini Pro Model (Prompt-Template)\n ```sql\n-CREATE ML_ENGINE Gemini_ML_Engine\n-FROM google_gemini\n+CREATE MODEL gem_p\n+PREDICT answer\n USING\n-  api_key = 'cloud_api_key';\n+    engine = 'g',\n+    prompt_template = 'Product Description: {{description}}. Question: {{question}}. Answer:',\n+    model_name = 'gemini-pro';\n+```\n+\n+```sql\n+SELECT answer\n+FROM gem_p\n+WHERE description = \"\n+What is Rabbit R1?\n+The Rabbit R1 is a pocket-sized AI device that promises a simpler and more intuitive way to interact with technology. Instead of being app-driven, the device relies on an AI model called LAMB (large action model) to understand your instructions and complete tasks autonomously.\n+The device has a bright orange body, and is small and lightweight with a touchscreen, scroll wheel, and a talk button. There is also a rotating camera that functions as eyes of the device.\n+\n+The Rabbit R1 runs on its own operating system, called the Rabbit OS, that eliminates the need for app stores and downloads, requiring only natural language voice input to navigate. The initial version supports integration with the likes of Uber, Spotify, and Amazon, with the AI able to train and learn using other apps in the future.\n+\"\n+AND question = 'Given me bullet pointed features of product ?';",
    "comment": "Let's rephrase this question as `\"What are some key feature bullet points of this product?\"`",
    "line_number": 43,
    "enriched": "File: mindsdb/integrations/handlers/google_gemini_handler/README.md\nCode: @@ -6,35 +6,105 @@ Google Generative AI is a library that provides access to powerful language mode\n \n *Note:* Ensure you have the necessary API key for accessing the Google gen AI library. You can get your API key at https://makersuite.google.com/. \n \n+>> This Handler requires python>=3.9 to work\n+>> (Default model_name is **gemini-pro** )\n+\n # Example Usage\n \n-Create a ML Engine with the new `google_gemini` engine.\n+#### Create Gemini ML Engine\n+```sql\n+CREATE ML_ENGINE g\n+FROM gemini\n+USING\n+    api_key = 'AI-i5-c001';\n+```\n \n+\n+#### Create Gemini Pro Model (Prompt-Template)\n ```sql\n-CREATE ML_ENGINE Gemini_ML_Engine\n-FROM google_gemini\n+CREATE MODEL gem_p\n+PREDICT answer\n USING\n-  api_key = 'cloud_api_key';\n+    engine = 'g',\n+    prompt_template = 'Product Description: {{description}}. Question: {{question}}. Answer:',\n+    model_name = 'gemini-pro';\n+```\n+\n+```sql\n+SELECT answer\n+FROM gem_p\n+WHERE description = \"\n+What is Rabbit R1?\n+The Rabbit R1 is a pocket-sized AI device that promises a simpler and more intuitive way to interact with technology. Instead of being app-driven, the device relies on an AI model called LAMB (large action model) to understand your instructions and complete tasks autonomously.\n+The device has a bright orange body, and is small and lightweight with a touchscreen, scroll wheel, and a talk button. There is also a rotating camera that functions as eyes of the device.\n+\n+The Rabbit R1 runs on its own operating system, called the Rabbit OS, that eliminates the need for app stores and downloads, requiring only natural language voice input to navigate. The initial version supports integration with the likes of Uber, Spotify, and Amazon, with the AI able to train and learn using other apps in the future.\n+\"\n+AND question = 'Given me bullet pointed features of product ?';\nComment: Let's rephrase this question as `\"What are some key feature bullet points of this product?\"`",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/google_gemini_handler/README.md",
    "pr_number": 8701,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1478322790,
    "comment_created_at": "2024-02-05T14:14:28Z"
  },
  {
    "code": "@@ -417,6 +418,12 @@ def _get_handler_meta(self, module):\n     def _load_handler_modules(self):\n         mindsdb_path = Path(importlib.util.find_spec('mindsdb').origin).parent\n         handlers_path = mindsdb_path.joinpath('integrations/handlers')\n+",
    "comment": "when I remove this code test_openai.py  also works for me",
    "line_number": 421,
    "enriched": "File: mindsdb/interfaces/database/integrations.py\nCode: @@ -417,6 +418,12 @@ def _get_handler_meta(self, module):\n     def _load_handler_modules(self):\n         mindsdb_path = Path(importlib.util.find_spec('mindsdb').origin).parent\n         handlers_path = mindsdb_path.joinpath('integrations/handlers')\n+\nComment: when I remove this code test_openai.py  also works for me",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/interfaces/database/integrations.py",
    "pr_number": 5454,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1172531160,
    "comment_created_at": "2023-04-20T12:41:20Z"
  },
  {
    "code": "@@ -0,0 +1,156 @@\n+from typing import Dict, List\n+\n+from mindsdb.interfaces.storage import db\n+from mindsdb.interfaces.database.projects import ProjectController\n+\n+\n+class SkillsController:\n+    '''Handles CRUD operations at the database level for Skills'''\n+\n+    def __init__(self, project_controller: ProjectController = None):\n+        if project_controller is None:\n+            project_controller = ProjectController()\n+        self.project_controller = project_controller\n+\n+    def get_skill(self, skill_name: str, project_name: str = 'mindsdb') -> db.Skills:\n+        '''\n+        Gets a skill by name. Skills are expected to have unique names.\n+\n+        Parameters:\n+            skill_name (str): The name of the skill\n+            project_name (str): The name of the containing project\n+\n+        Returns:\n+            skill (db.Skills): The database skill object\n+\n+        Raises:\n+            ValueError: If `project_name` does not exist\n+        '''\n+\n+        project = self.project_controller.get(name=project_name)\n+        return db.Skills.query.filter(",
    "comment": "Do we have a unique constraint on `skill name` + `project id`?\r\nI think we are using this combination as a surrogate PK here.",
    "line_number": 31,
    "enriched": "File: mindsdb/interfaces/skills/skills_controller.py\nCode: @@ -0,0 +1,156 @@\n+from typing import Dict, List\n+\n+from mindsdb.interfaces.storage import db\n+from mindsdb.interfaces.database.projects import ProjectController\n+\n+\n+class SkillsController:\n+    '''Handles CRUD operations at the database level for Skills'''\n+\n+    def __init__(self, project_controller: ProjectController = None):\n+        if project_controller is None:\n+            project_controller = ProjectController()\n+        self.project_controller = project_controller\n+\n+    def get_skill(self, skill_name: str, project_name: str = 'mindsdb') -> db.Skills:\n+        '''\n+        Gets a skill by name. Skills are expected to have unique names.\n+\n+        Parameters:\n+            skill_name (str): The name of the skill\n+            project_name (str): The name of the containing project\n+\n+        Returns:\n+            skill (db.Skills): The database skill object\n+\n+        Raises:\n+            ValueError: If `project_name` does not exist\n+        '''\n+\n+        project = self.project_controller.get(name=project_name)\n+        return db.Skills.query.filter(\nComment: Do we have a unique constraint on `skill name` + `project id`?\r\nI think we are using this combination as a surrogate PK here.",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/interfaces/skills/skills_controller.py",
    "pr_number": 7279,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1319213193,
    "comment_created_at": "2023-09-07T23:46:21Z"
  },
  {
    "code": "@@ -132,10 +149,18 @@ def get_columns(self, dataset, table_name) -> Response:\n         'type': ARG_TYPE.STR,\n         'description': 'The BigQuery project id.'\n     },\n+    dataset={",
    "comment": "Can we just include this in the README also as required param",
    "line_number": 152,
    "enriched": "File: mindsdb/integrations/handlers/bigquery_handler/bigquery_handler.py\nCode: @@ -132,10 +149,18 @@ def get_columns(self, dataset, table_name) -> Response:\n         'type': ARG_TYPE.STR,\n         'description': 'The BigQuery project id.'\n     },\n+    dataset={\nComment: Can we just include this in the README also as required param",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/bigquery_handler/bigquery_handler.py",
    "pr_number": 5727,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1171420880,
    "comment_created_at": "2023-04-19T14:21:17Z"
  },
  {
    "code": "@@ -54,11 +55,25 @@ def build_retrieval_tool(tool: dict, pred_args: dict, skill: db.Skills):\n \n     rag_params = _get_rag_params(tools_config, kb_params)\n \n-    if 'vector_store_config' not in rag_params:\n-        rag_params['vector_store_config'] = {}\n-        logger.warning(f'No collection_name specified for the retrieval tool, '\n-                       f\"using default collection_name: '{DEFAULT_COLLECTION_NAME}'\"\n-                       f'\\nWarning: If this collection does not exist, no data will be retrieved')\n+    # use knowledge base table embedding model by default, even if already set\n+    if kb_table is not None:",
    "comment": "we always use kb for embeddings model if provided",
    "line_number": 59,
    "enriched": "File: mindsdb/interfaces/skills/retrieval_tool.py\nCode: @@ -54,11 +55,25 @@ def build_retrieval_tool(tool: dict, pred_args: dict, skill: db.Skills):\n \n     rag_params = _get_rag_params(tools_config, kb_params)\n \n-    if 'vector_store_config' not in rag_params:\n-        rag_params['vector_store_config'] = {}\n-        logger.warning(f'No collection_name specified for the retrieval tool, '\n-                       f\"using default collection_name: '{DEFAULT_COLLECTION_NAME}'\"\n-                       f'\\nWarning: If this collection does not exist, no data will be retrieved')\n+    # use knowledge base table embedding model by default, even if already set\n+    if kb_table is not None:\nComment: we always use kb for embeddings model if provided",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/interfaces/skills/retrieval_tool.py",
    "pr_number": 10258,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1876285168,
    "comment_created_at": "2024-12-09T16:23:20Z"
  },
  {
    "code": "@@ -25,49 +25,98 @@ CREATE DATABASE my_web\n WITH ENGINE = 'web';\n ```\n <Tip>\n-The above query creates a database called `my_web`. This database by default has a table called `crawler` that we can use to crawl data from a given url/urls.\n+The above query creates a database called `my_web`. This database by default has a table called `crawler` that stores data from a given URL or multiple URLs.\n </Tip>\n \n ## Usage\n \n-<Note>\n-Specifying a `LIMIT` clause is required. To crawl all pages on a site, consider setting the limit to a high value, such as 10,000, which exceeds the expected number of pages. Be aware that setting a higher limit may result in longer response times.\n-</Note>\n+### Parameters\n \n-### Get Websites Content\n+#### Crawl Depth\n \n-The following usage examples demonstrate how to retrieve content from `docs.mindsdb.com`:\n+The `crawl_depth` parameter defines how deep the crawler should navigate through linked pages:\n+\n+- `crawl_depth = 0`: Crawls only the specified page.\n+- `crawl_depth = 1`: Crawls the specified page and all linked pages on it.\n+- Higher values continue the pattern.\n+\n+#### Page Limits\n+\n+There are multiple ways to limit the number of pages returned:\n+\n+- `LIMIT`: Defines the maximum number of pages returned globally.\n+- `per_url_limit`: Limits the number of pages returned for each specific URL, if more than one URL is provided.",
    "comment": "@martyna-mindsdb Should we maybe mention the fact that the first is the `LIMIT` clause when issuing the query and `per_url_limit` is a condition?",
    "line_number": 48,
    "enriched": "File: docs/integrations/app-integrations/web-crawler.mdx\nCode: @@ -25,49 +25,98 @@ CREATE DATABASE my_web\n WITH ENGINE = 'web';\n ```\n <Tip>\n-The above query creates a database called `my_web`. This database by default has a table called `crawler` that we can use to crawl data from a given url/urls.\n+The above query creates a database called `my_web`. This database by default has a table called `crawler` that stores data from a given URL or multiple URLs.\n </Tip>\n \n ## Usage\n \n-<Note>\n-Specifying a `LIMIT` clause is required. To crawl all pages on a site, consider setting the limit to a high value, such as 10,000, which exceeds the expected number of pages. Be aware that setting a higher limit may result in longer response times.\n-</Note>\n+### Parameters\n \n-### Get Websites Content\n+#### Crawl Depth\n \n-The following usage examples demonstrate how to retrieve content from `docs.mindsdb.com`:\n+The `crawl_depth` parameter defines how deep the crawler should navigate through linked pages:\n+\n+- `crawl_depth = 0`: Crawls only the specified page.\n+- `crawl_depth = 1`: Crawls the specified page and all linked pages on it.\n+- Higher values continue the pattern.\n+\n+#### Page Limits\n+\n+There are multiple ways to limit the number of pages returned:\n+\n+- `LIMIT`: Defines the maximum number of pages returned globally.\n+- `per_url_limit`: Limits the number of pages returned for each specific URL, if more than one URL is provided.\nComment: @martyna-mindsdb Should we maybe mention the fact that the first is the `LIMIT` clause when issuing the query and `per_url_limit` is a condition?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "docs/integrations/app-integrations/web-crawler.mdx",
    "pr_number": 10619,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2017012264,
    "comment_created_at": "2025-03-27T15:59:59Z"
  },
  {
    "code": "@@ -84,14 +84,25 @@ def import_string(code, module_name='model'):\n \n def find_model_class(module):\n     # find the first class that contents predict and train methods\n-    for _, klass in inspect.getmembers(module, inspect.isclass):\n+    cls_list = []\n+    for _, cls in inspect.getmembers(module, inspect.isclass):\n+        if inspect.getmodule(cls) is not None:\n+            # is imported class\n+            continue\n+\n         funcs = [\n             name\n-            for name, _ in inspect.getmembers(klass, inspect.isfunction)\n+            for name, _ in inspect.getmembers(cls, inspect.isfunction)\n         ]\n         if 'predict' in funcs and 'train' in funcs:\n-            return klass\n-    raise RuntimeError('Unable to find model class (has to have `train` and `predict` methods)')\n+            # found\n+            return cls\n+        cls_list.append(cls)\n+    if len(cls_list) == 1:\n+        # only one class in file\n+        return cls_list[0]\n+\n+    raise RuntimeError('Unable to find model class (it has to have `train` and `predict` methods)')",
    "comment": "We need to update this string I think",
    "line_number": 105,
    "enriched": "File: mindsdb/integrations/handlers/byom_handler/proc_wrapper.py\nCode: @@ -84,14 +84,25 @@ def import_string(code, module_name='model'):\n \n def find_model_class(module):\n     # find the first class that contents predict and train methods\n-    for _, klass in inspect.getmembers(module, inspect.isclass):\n+    cls_list = []\n+    for _, cls in inspect.getmembers(module, inspect.isclass):\n+        if inspect.getmodule(cls) is not None:\n+            # is imported class\n+            continue\n+\n         funcs = [\n             name\n-            for name, _ in inspect.getmembers(klass, inspect.isfunction)\n+            for name, _ in inspect.getmembers(cls, inspect.isfunction)\n         ]\n         if 'predict' in funcs and 'train' in funcs:\n-            return klass\n-    raise RuntimeError('Unable to find model class (has to have `train` and `predict` methods)')\n+            # found\n+            return cls\n+        cls_list.append(cls)\n+    if len(cls_list) == 1:\n+        # only one class in file\n+        return cls_list[0]\n+\n+    raise RuntimeError('Unable to find model class (it has to have `train` and `predict` methods)')\nComment: We need to update this string I think",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/byom_handler/proc_wrapper.py",
    "pr_number": 9453,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1669025785,
    "comment_created_at": "2024-07-08T17:29:37Z"
  },
  {
    "code": "@@ -0,0 +1,165 @@\n+---\n+title: Twilio Chatbot\n+sidebarTitle: Twilio Chatbot\n+---\n+\n+In this tutorial, we'll use MindsDB's integration with Twilio and the custom Jobs feature to implement a chatbot that will reply to text messages. The replies will include a text response generated by OpenAI's GPT-4 model and an image response generated by the OpenAI's DallE 3 model.\n+\n+<p align=\"center\">\n+    <img src=\"/assets/twilio-chatbot-diagram.png\" />\n+</p>\n+\n+Read along to follow the tutorial.\n+\n+## Step 1. Create OpenAI models with a bit of personality\n+\n+In order to create an AI model, you’ll need an [OpenAI account](https://openai.com/) and an [API key](https://platform.openai.com/account/api-keys). You’ll also need a MindsDB installation - you can find an open-source version [here](https://github.com/mindsdb/mindsdb). \n+\n+Then go to your MindsDB SQL Editor and enter the following commands to create AI models:\n+\n+**1. Model to generate a text response:**\n+\n+    Before creating an OpenAI model, please create an engine, providing your OpenAI API key:\n+\n+    ```sql\n+    CREATE ML_ENGINE openai_engine\n+    FROM openai\n+    USING\n+    api_key = 'sk-xxx';\n+    ```\n+\n+    Now you can create a model:\n+\n+    ```sql\n+    CREATE MODEL twilio_bot_model\n+    PREDICT answer\n+    USING\n+    engine = 'openai_engine',\n+    max_tokens = 500,\n+    prompt_template = 'Pretend you are a mashup of Bill Murray and Taylor Swift. Provide a short description of an image using the style of Bill Murray and Taylor Swift that answers users questions: {{body}}';\n+    ```\n+\n+    The `CREATE MODEL` command creates and deploys the model within MindsDB. Here we use the OpenAI GPT-3.5 Turbo model to generate text responses to users’ questions. The `prompt_template` message sets the personality of the bot - here, it is a mashup of Bill Murray and Taylor Swift.",
    "comment": "change we to you",
    "line_number": 42,
    "enriched": "File: docs/sql/tutorials/twilio-chatbot.mdx\nCode: @@ -0,0 +1,165 @@\n+---\n+title: Twilio Chatbot\n+sidebarTitle: Twilio Chatbot\n+---\n+\n+In this tutorial, we'll use MindsDB's integration with Twilio and the custom Jobs feature to implement a chatbot that will reply to text messages. The replies will include a text response generated by OpenAI's GPT-4 model and an image response generated by the OpenAI's DallE 3 model.\n+\n+<p align=\"center\">\n+    <img src=\"/assets/twilio-chatbot-diagram.png\" />\n+</p>\n+\n+Read along to follow the tutorial.\n+\n+## Step 1. Create OpenAI models with a bit of personality\n+\n+In order to create an AI model, you’ll need an [OpenAI account](https://openai.com/) and an [API key](https://platform.openai.com/account/api-keys). You’ll also need a MindsDB installation - you can find an open-source version [here](https://github.com/mindsdb/mindsdb). \n+\n+Then go to your MindsDB SQL Editor and enter the following commands to create AI models:\n+\n+**1. Model to generate a text response:**\n+\n+    Before creating an OpenAI model, please create an engine, providing your OpenAI API key:\n+\n+    ```sql\n+    CREATE ML_ENGINE openai_engine\n+    FROM openai\n+    USING\n+    api_key = 'sk-xxx';\n+    ```\n+\n+    Now you can create a model:\n+\n+    ```sql\n+    CREATE MODEL twilio_bot_model\n+    PREDICT answer\n+    USING\n+    engine = 'openai_engine',\n+    max_tokens = 500,\n+    prompt_template = 'Pretend you are a mashup of Bill Murray and Taylor Swift. Provide a short description of an image using the style of Bill Murray and Taylor Swift that answers users questions: {{body}}';\n+    ```\n+\n+    The `CREATE MODEL` command creates and deploys the model within MindsDB. Here we use the OpenAI GPT-3.5 Turbo model to generate text responses to users’ questions. The `prompt_template` message sets the personality of the bot - here, it is a mashup of Bill Murray and Taylor Swift.\nComment: change we to you",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "docs/sql/tutorials/twilio-chatbot.mdx",
    "pr_number": 8378,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1396126043,
    "comment_created_at": "2023-11-16T17:52:51Z"
  },
  {
    "code": "@@ -60,6 +60,7 @@ def __init__(self, *args, **kwargs):\n         self.default_agent_tools = _DEFAULT_AGENT_TOOLS\n         self.write_privileges = False  # if True, this agent is able to write into other active mindsdb integrations\n \n+    # TODO: refactor to use common function?",
    "comment": "minor nit - perhaps we can link to an issue so we are able to remember to revisit at a suitable time",
    "line_number": 63,
    "enriched": "File: mindsdb/integrations/handlers/langchain_handler/langchain_handler.py\nCode: @@ -60,6 +60,7 @@ def __init__(self, *args, **kwargs):\n         self.default_agent_tools = _DEFAULT_AGENT_TOOLS\n         self.write_privileges = False  # if True, this agent is able to write into other active mindsdb integrations\n \n+    # TODO: refactor to use common function?\nComment: minor nit - perhaps we can link to an issue so we are able to remember to revisit at a suitable time",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/langchain_handler/langchain_handler.py",
    "pr_number": 7318,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1339937313,
    "comment_created_at": "2023-09-28T10:57:37Z"
  },
  {
    "code": "@@ -329,7 +329,9 @@ def gen_hash(v):\n             df[id_col] = df[content_col].apply(gen_hash)\n         else:\n             # generate for empty\n-            df.loc[df[id_col], id_col] = df[content_col].apply(gen_hash)",
    "comment": "looks like here was missed .isna()\r\nIt should be:\r\n```python\r\ndf.loc[df[id_col].isna(), id_col] = df[content_col].apply(gen_hash)\r\n```",
    "line_number": 332,
    "enriched": "File: mindsdb/integrations/libs/vectordatabase_handler.py\nCode: @@ -329,7 +329,9 @@ def gen_hash(v):\n             df[id_col] = df[content_col].apply(gen_hash)\n         else:\n             # generate for empty\n-            df.loc[df[id_col], id_col] = df[content_col].apply(gen_hash)\nComment: looks like here was missed .isna()\r\nIt should be:\r\n```python\r\ndf.loc[df[id_col].isna(), id_col] = df[content_col].apply(gen_hash)\r\n```",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/libs/vectordatabase_handler.py",
    "pr_number": 8475,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1418429745,
    "comment_created_at": "2023-12-07T06:08:42Z"
  },
  {
    "code": "@@ -20,7 +20,6 @@ WITH\n     parameters = {\n       \"aws_access_key_id\": \"AQAXEQK89OX07YS34OP\"\n       \"aws_secret_access_key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\",",
    "comment": "maybe also to remove values which looks like real and replace them with something like 'YOUR ACCESS KEY'?",
    "line_number": 22,
    "enriched": "File: docs/integrations/data-integrations/amazon-s3.mdx\nCode: @@ -20,7 +20,6 @@ WITH\n     parameters = {\n       \"aws_access_key_id\": \"AQAXEQK89OX07YS34OP\"\n       \"aws_secret_access_key\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\",\nComment: maybe also to remove values which looks like real and replace them with something like 'YOUR ACCESS KEY'?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "docs/integrations/data-integrations/amazon-s3.mdx",
    "pr_number": 9979,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1811991684,
    "comment_created_at": "2024-10-23T06:54:08Z"
  },
  {
    "code": "@@ -89,6 +89,7 @@ def connect(self) -> bool:\n         \"\"\"\n         self.is_connected = False\n         self.obb.account.login(pat=self.PAT)\n+        # self.obb.account.refresh()",
    "comment": "Do we need this commented?",
    "line_number": 92,
    "enriched": "File: mindsdb/integrations/handlers/openbb_handler/openbb_handler.py\nCode: @@ -89,6 +89,7 @@ def connect(self) -> bool:\n         \"\"\"\n         self.is_connected = False\n         self.obb.account.login(pat=self.PAT)\n+        # self.obb.account.refresh()\nComment: Do we need this commented?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/openbb_handler/openbb_handler.py",
    "pr_number": 9688,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1743433578,
    "comment_created_at": "2024-09-04T09:58:36Z"
  },
  {
    "code": "@@ -0,0 +1,9 @@\n+{",
    "comment": "We should remove this altogether. What is it used for?",
    "line_number": 1,
    "enriched": "File: config.json\nCode: @@ -0,0 +1,9 @@\n+{\nComment: We should remove this altogether. What is it used for?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "config.json",
    "pr_number": 9097,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1578117940,
    "comment_created_at": "2024-04-24T15:42:30Z"
  },
  {
    "code": "@@ -82,3 +83,61 @@ def get_invoices(self, **kwargs) -> List[Dict]:\n         connection = self.handler.connect()\n         invoices = paypalrestsdk.Invoice.all(kwargs, api=connection)\n         return [invoice.to_dict() for invoice in invoices['invoices']]\n+\n+\n+class OrdersTable(APITable):\n+\n+    def select(self, query: ast.Select) -> pd.DataFrame:\n+        select_statement_parser = SELECTQueryParser(\n+            query,\n+            'orders',\n+            self.get_columns(None)\n+        )\n+        selected_columns, where_conditions, order_by_conditions, result_limit = select_statement_parser.parse_query()\n+\n+        # order_id = '7SP60397AN682533Y'\n+        order_id = None\n+\n+        for condition in where_conditions:\n+            if condition.column.name == 'order_id':\n+               order_id = condition.value\n+            break  \n+\n+        if order_id is None:\n+            raise ValueError(\"order_id must be provided in the WHERE clause of the query\")\n+        \n+        selected_columns = self.get_columns(order_id)\n+        \n+        orders_df = pd.json_normalize(self.get_orders(count=result_limit,order_id=order_id))\n+        select_statement_executor = SELECTQueryExecutor(\n+            orders_df,\n+            selected_columns,\n+            where_conditions,\n+            order_by_conditions\n+        )\n+        orders_df = select_statement_executor.execute_query()\n+\n+        return orders_df\n+    \n+    def get_columns(self,order_id) -> List[Text]:\n+        return pd.json_normalize(self.get_orders(order_id)).columns.tolist()\n+\n+    def get_orders(self, order_id, **kwargs) -> List[Dict]:\n+       headers = {\n+        'Content-Type': 'application/json',\n+        'Authorization': 'Bearer A21AAIUR5R-kPcdVNm6D6zNxxFlSFaE2smdAEZhGFAAsq92VGjrHYBjvZyMh4dzDklXjpDj3rL7vvDRmf5S4fHXcyK0PDwaVg',\n+    }\n+\n+       response = requests.get(f'https://api-m.sandbox.paypal.com/v2/checkout/orders/{order_id}', headers=headers)",
    "comment": "This should be alredy conected using the CREATE DATABASE statement. Check connection method",
    "line_number": 131,
    "enriched": "File: mindsdb/integrations/handlers/paypal_handler/paypal_tables.py\nCode: @@ -82,3 +83,61 @@ def get_invoices(self, **kwargs) -> List[Dict]:\n         connection = self.handler.connect()\n         invoices = paypalrestsdk.Invoice.all(kwargs, api=connection)\n         return [invoice.to_dict() for invoice in invoices['invoices']]\n+\n+\n+class OrdersTable(APITable):\n+\n+    def select(self, query: ast.Select) -> pd.DataFrame:\n+        select_statement_parser = SELECTQueryParser(\n+            query,\n+            'orders',\n+            self.get_columns(None)\n+        )\n+        selected_columns, where_conditions, order_by_conditions, result_limit = select_statement_parser.parse_query()\n+\n+        # order_id = '7SP60397AN682533Y'\n+        order_id = None\n+\n+        for condition in where_conditions:\n+            if condition.column.name == 'order_id':\n+               order_id = condition.value\n+            break  \n+\n+        if order_id is None:\n+            raise ValueError(\"order_id must be provided in the WHERE clause of the query\")\n+        \n+        selected_columns = self.get_columns(order_id)\n+        \n+        orders_df = pd.json_normalize(self.get_orders(count=result_limit,order_id=order_id))\n+        select_statement_executor = SELECTQueryExecutor(\n+            orders_df,\n+            selected_columns,\n+            where_conditions,\n+            order_by_conditions\n+        )\n+        orders_df = select_statement_executor.execute_query()\n+\n+        return orders_df\n+    \n+    def get_columns(self,order_id) -> List[Text]:\n+        return pd.json_normalize(self.get_orders(order_id)).columns.tolist()\n+\n+    def get_orders(self, order_id, **kwargs) -> List[Dict]:\n+       headers = {\n+        'Content-Type': 'application/json',\n+        'Authorization': 'Bearer A21AAIUR5R-kPcdVNm6D6zNxxFlSFaE2smdAEZhGFAAsq92VGjrHYBjvZyMh4dzDklXjpDj3rL7vvDRmf5S4fHXcyK0PDwaVg',\n+    }\n+\n+       response = requests.get(f'https://api-m.sandbox.paypal.com/v2/checkout/orders/{order_id}', headers=headers)\nComment: This should be alredy conected using the CREATE DATABASE statement. Check connection method",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/paypal_handler/paypal_tables.py",
    "pr_number": 7832,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1361837799,
    "comment_created_at": "2023-10-17T09:50:31Z"
  },
  {
    "code": "@@ -1256,6 +1255,18 @@ def answer_drop_view(self, statement):\n \n         return ExecuteAnswer(answer_type=ANSWER_TYPE.OK)\n \n+    def _create_persistent_chroma(self, project_name, kb_name, engine=\"chromadb\"):\n+        \"\"\"Create default vector database for knowledge base, if not specified\"\"\"\n+\n+        vector_store_name = f\"{kb_name}_{engine}\"\n+\n+        persist_directory = f\"{self.session.config.paths['storage']}/vector_databases//{project_name}/{vector_store_name}\"",
    "comment": "Should be fine for local but don't imagine its going to play well on cloud!\r\n\r\nWhats the best parameter to check to see if its cloud or local run?\r\n\r\nHow is user data currently partitioned on cloud?\r\n\r\n",
    "line_number": 1263,
    "enriched": "File: mindsdb/api/mysql/mysql_proxy/executor/executor_commands.py\nCode: @@ -1256,6 +1255,18 @@ def answer_drop_view(self, statement):\n \n         return ExecuteAnswer(answer_type=ANSWER_TYPE.OK)\n \n+    def _create_persistent_chroma(self, project_name, kb_name, engine=\"chromadb\"):\n+        \"\"\"Create default vector database for knowledge base, if not specified\"\"\"\n+\n+        vector_store_name = f\"{kb_name}_{engine}\"\n+\n+        persist_directory = f\"{self.session.config.paths['storage']}/vector_databases//{project_name}/{vector_store_name}\"\nComment: Should be fine for local but don't imagine its going to play well on cloud!\r\n\r\nWhats the best parameter to check to see if its cloud or local run?\r\n\r\nHow is user data currently partitioned on cloud?\r\n\r\n",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/api/mysql/mysql_proxy/executor/executor_commands.py",
    "pr_number": 7886,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1365528338,
    "comment_created_at": "2023-10-19T13:22:12Z"
  },
  {
    "code": "@@ -5,6 +5,7 @@\n \n <div align=\"center\">\n \n+",
    "comment": "Please remove the new line that you added here.",
    "line_number": 8,
    "enriched": "File: README.md\nCode: @@ -5,6 +5,7 @@\n \n <div align=\"center\">\n \n+\nComment: Please remove the new line that you added here.",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "README.md",
    "pr_number": 5952,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1183612105,
    "comment_created_at": "2023-05-03T12:20:40Z"
  },
  {
    "code": "@@ -24,8 +24,8 @@ Here is how to interact with chatbots using MindsDB SQL:\n     ```sql\n     CREATE CHATBOT my_chatbot\n     USING\n-        database = my_slack, -- this must be created with CREATE DATABASE\n-        agent = customer_support_agent, -- this must be created with CREATE AGENT\n+        database = 'my_slack', -- this must be created with CREATE DATABASE\n+        agent = 'customer_support_agent', -- this must be created with CREATE AGENT\n         included_channels = ['support', 'help'], -- Default is all",
    "comment": "Right now you can only interact with a Slack Chatbot through DMs. Not sure if we should include that too",
    "line_number": 29,
    "enriched": "File: docs/agents/chatbot.mdx\nCode: @@ -24,8 +24,8 @@ Here is how to interact with chatbots using MindsDB SQL:\n     ```sql\n     CREATE CHATBOT my_chatbot\n     USING\n-        database = my_slack, -- this must be created with CREATE DATABASE\n-        agent = customer_support_agent, -- this must be created with CREATE AGENT\n+        database = 'my_slack', -- this must be created with CREATE DATABASE\n+        agent = 'customer_support_agent', -- this must be created with CREATE AGENT\n         included_channels = ['support', 'help'], -- Default is all\nComment: Right now you can only interact with a Slack Chatbot through DMs. Not sure if we should include that too",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "docs/agents/chatbot.mdx",
    "pr_number": 8435,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1408232320,
    "comment_created_at": "2023-11-28T18:39:51Z"
  },
  {
    "code": "@@ -0,0 +1,197 @@\n+{",
    "comment": "Hey @HahaBill,\r\nI think we discussed that these JSON files can be removed.",
    "line_number": 1,
    "enriched": "File: mindsdb/integrations/handlers/tripadvisor_handler/json_response_examples/location_details.json\nCode: @@ -0,0 +1,197 @@\n+{\nComment: Hey @HahaBill,\r\nI think we discussed that these JSON files can be removed.",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/tripadvisor_handler/json_response_examples/location_details.json",
    "pr_number": 7542,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1345822640,
    "comment_created_at": "2023-10-04T13:43:41Z"
  },
  {
    "code": "@@ -155,111 +153,116 @@ The following usage applies when **Connection Method 2** was used to connect Sla\n See the usage for **Connection Method 1** [via the `CREATE CHATBOT` syntax](/sql/tutorials/create-chatbot).\n </Warning>\n \n-You can select all messages from a conversation using the below query.\n+Retrieve data from a specified table by providing the integration and table names:\n \n ```sql\n SELECT *\n-FROM mindsdb_slack.messages\n-WHERE channel_id=\"<channel-id>\";\n+FROM slack_datasource.table_name\n+LIMIT 10;\n ```\n \n-<Tip>\n-To find the channel ID of a conversation, you can use the `conversations` table:\n-\n-```sql\n-SELECT *\n-FROM mindsdb_slack.conversations\n-WHERE name = \"<channel-name>\";\n-```\n+## Supported Tables\n \n-Please note that if your workspace has more than 1000 conversations, you may need to use the `LIMIT` clause to retrieve all conversations. More information on this can be found below.\n+The Slack integration supports the following tables:\n \n-You can also find the channel ID by right-clicking on the conversation in Slack, selecting 'View conversation details' or 'View channel details,' and copying the channel ID from the bottom of the 'About' tab.\n-</Tip>\n+### `conversations`",
    "comment": "Maybe we can add `Table` to the title too as `conversations` table",
    "line_number": 168,
    "enriched": "File: docs/integrations/app-integrations/slack.mdx\nCode: @@ -155,111 +153,116 @@ The following usage applies when **Connection Method 2** was used to connect Sla\n See the usage for **Connection Method 1** [via the `CREATE CHATBOT` syntax](/sql/tutorials/create-chatbot).\n </Warning>\n \n-You can select all messages from a conversation using the below query.\n+Retrieve data from a specified table by providing the integration and table names:\n \n ```sql\n SELECT *\n-FROM mindsdb_slack.messages\n-WHERE channel_id=\"<channel-id>\";\n+FROM slack_datasource.table_name\n+LIMIT 10;\n ```\n \n-<Tip>\n-To find the channel ID of a conversation, you can use the `conversations` table:\n-\n-```sql\n-SELECT *\n-FROM mindsdb_slack.conversations\n-WHERE name = \"<channel-name>\";\n-```\n+## Supported Tables\n \n-Please note that if your workspace has more than 1000 conversations, you may need to use the `LIMIT` clause to retrieve all conversations. More information on this can be found below.\n+The Slack integration supports the following tables:\n \n-You can also find the channel ID by right-clicking on the conversation in Slack, selecting 'View conversation details' or 'View channel details,' and copying the channel ID from the bottom of the 'About' tab.\n-</Tip>\n+### `conversations`\nComment: Maybe we can add `Table` to the title too as `conversations` table",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "docs/integrations/app-integrations/slack.mdx",
    "pr_number": 10250,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1875942574,
    "comment_created_at": "2024-12-09T12:58:02Z"
  },
  {
    "code": "@@ -305,9 +321,9 @@ The `default_llm` parameter specifies the default LLM that will be used with [kn\n     }\n ```\n \n-Learn more about the parameters following the [documentation of the `reranking_model` of knowledge bases](/mindsdb_sql/knowledge-bases#reranking-model).\n+The `default_llm` parameter specifies the default LLM that will be used with the [`LLM()` function](/mindsdb_sql/functions/llm_function), the [`TO_MARKDOWN()` function](/mindsdb_sql/functions/to_markdown_function), and as a default model for [agents](/mindsdb_sql/agents/agent).",
    "comment": "@martyna-mindsdb I don't believe support for agents has been provided yet, has it?",
    "line_number": 324,
    "enriched": "File: docs/setup/custom-config.mdx\nCode: @@ -305,9 +321,9 @@ The `default_llm` parameter specifies the default LLM that will be used with [kn\n     }\n ```\n \n-Learn more about the parameters following the [documentation of the `reranking_model` of knowledge bases](/mindsdb_sql/knowledge-bases#reranking-model).\n+The `default_llm` parameter specifies the default LLM that will be used with the [`LLM()` function](/mindsdb_sql/functions/llm_function), the [`TO_MARKDOWN()` function](/mindsdb_sql/functions/to_markdown_function), and as a default model for [agents](/mindsdb_sql/agents/agent).\nComment: @martyna-mindsdb I don't believe support for agents has been provided yet, has it?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "docs/setup/custom-config.mdx",
    "pr_number": 10990,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2121916193,
    "comment_created_at": "2025-06-02T18:45:31Z"
  },
  {
    "code": "@@ -103,14 +112,39 @@ def on_file(file):\n         else:\n             data = request.json\n \n-        if mindsdb_file_name in existing_file_names:\n+        existing_file_names = ca.file_controller.get_files_names(lower=True)",
    "comment": "why should it be case independent? ",
    "line_number": 115,
    "enriched": "File: mindsdb/api/http/namespaces/file.py\nCode: @@ -103,14 +112,39 @@ def on_file(file):\n         else:\n             data = request.json\n \n-        if mindsdb_file_name in existing_file_names:\n+        existing_file_names = ca.file_controller.get_files_names(lower=True)\nComment: why should it be case independent? ",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/api/http/namespaces/file.py",
    "pr_number": 11739,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2434746552,
    "comment_created_at": "2025-10-16T06:40:23Z"
  },
  {
    "code": "@@ -0,0 +1,71 @@\n+from mindsdb.integrations.libs.api_handler import APITable\n+from mindsdb.integrations.libs.response import HandlerResponse as Response\n+from mindsdb.integrations.utilities.sql_utils import extract_comparison_conditions\n+from mindsdb_sql.parser import ast\n+import datetime\n+import pytz\n+import time\n+from tzlocal import get_localzone\n+\n+class GoogleFitTable(APITable):\n+\n+    def time_parser(self, args) -> int:\n+        ymd = args.split('-')\n+        epoch0 = datetime.datetime(1970, 1, 1, tzinfo=pytz.utc)\n+        time = pytz.timezone(str(get_localzone())).localize(datetime.datetime(int(ymd[0].rstrip()), int(ymd[1].rstrip()), int(ymd[2].rstrip())))\n+        return int((time - epoch0).total_seconds() * 1000)\n+    \n+    def select(self, query: ast.Select) -> Response:\n+\n+        conditions = extract_comparison_conditions(query.where)\n+        \n+        params = {}\n+        filters = []\n+        steps = {}\n+        now = int(round(time.time() * 1000))\n+        one_year = 31536000000\n+        one_month = 2629746000\n+        for op, arg1, arg2 in conditions:\n+            if op == 'or':\n+                raise NotImplementedError(f'OR is not supported')\n+            if arg1 == 'date':\n+                print(f'args : {arg2} 2222222222222222222')",
    "comment": "Can we remove prints please?",
    "line_number": 32,
    "enriched": "File: mindsdb/integrations/handlers/google_fit_handler/google_fit_tables.py\nCode: @@ -0,0 +1,71 @@\n+from mindsdb.integrations.libs.api_handler import APITable\n+from mindsdb.integrations.libs.response import HandlerResponse as Response\n+from mindsdb.integrations.utilities.sql_utils import extract_comparison_conditions\n+from mindsdb_sql.parser import ast\n+import datetime\n+import pytz\n+import time\n+from tzlocal import get_localzone\n+\n+class GoogleFitTable(APITable):\n+\n+    def time_parser(self, args) -> int:\n+        ymd = args.split('-')\n+        epoch0 = datetime.datetime(1970, 1, 1, tzinfo=pytz.utc)\n+        time = pytz.timezone(str(get_localzone())).localize(datetime.datetime(int(ymd[0].rstrip()), int(ymd[1].rstrip()), int(ymd[2].rstrip())))\n+        return int((time - epoch0).total_seconds() * 1000)\n+    \n+    def select(self, query: ast.Select) -> Response:\n+\n+        conditions = extract_comparison_conditions(query.where)\n+        \n+        params = {}\n+        filters = []\n+        steps = {}\n+        now = int(round(time.time() * 1000))\n+        one_year = 31536000000\n+        one_month = 2629746000\n+        for op, arg1, arg2 in conditions:\n+            if op == 'or':\n+                raise NotImplementedError(f'OR is not supported')\n+            if arg1 == 'date':\n+                print(f'args : {arg2} 2222222222222222222')\nComment: Can we remove prints please?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/google_fit_handler/google_fit_tables.py",
    "pr_number": 5948,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1183731297,
    "comment_created_at": "2023-05-03T14:00:45Z"
  },
  {
    "code": "@@ -76,6 +77,13 @@ def _download_secret_file(self, secret_file):\n         if self.credentials_file and os.path.isfile(self.credentials_file):\n             copyfile(self.credentials_file, secret_file)\n             return True\n+        \n+        # if credentials_json is set, attempt to write the file\n+        if self.credentials_json:\n+            with open(secret_file, 'w') as creds:",
    "comment": "We need to check how this will work on cloud?",
    "line_number": 83,
    "enriched": "File: mindsdb/integrations/handlers/utilities/auth_utilities/google/google_oauth_utilities.py\nCode: @@ -76,6 +77,13 @@ def _download_secret_file(self, secret_file):\n         if self.credentials_file and os.path.isfile(self.credentials_file):\n             copyfile(self.credentials_file, secret_file)\n             return True\n+        \n+        # if credentials_json is set, attempt to write the file\n+        if self.credentials_json:\n+            with open(secret_file, 'w') as creds:\nComment: We need to check how this will work on cloud?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/utilities/auth_utilities/google/google_oauth_utilities.py",
    "pr_number": 8666,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1464889585,
    "comment_created_at": "2024-01-24T13:07:58Z"
  },
  {
    "code": "@@ -0,0 +1,54 @@\n+import requests\n+from urllib.parse import urljoin\n+\n+\n+def move_under(d, key_contents_to_move, key_to_move_under=None):",
    "comment": "Can you please add comment here what this does?",
    "line_number": 5,
    "enriched": "File: mindsdb/integrations/handlers/sap_erp_handler/api.py\nCode: @@ -0,0 +1,54 @@\n+import requests\n+from urllib.parse import urljoin\n+\n+\n+def move_under(d, key_contents_to_move, key_to_move_under=None):\nComment: Can you please add comment here what this does?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/sap_erp_handler/api.py",
    "pr_number": 8100,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1378792163,
    "comment_created_at": "2023-11-01T13:17:31Z"
  },
  {
    "code": "@@ -131,6 +134,8 @@ def clear_db(self, db):\n         db.session.add(r)\n         r = db.Integration(name=\"lightfm\", data={}, engine=\"lightfm\")\n         db.session.add(r)\n+        r = db.Integration(name=\"ludwig\", data={}, engine=\"ludwig\")",
    "comment": "what do you think about doing\r\n```\r\ncreate ml_engine <name> from <handler>\r\n```\r\ninstead of adding records to integration? \r\n\r\nIt will make this base class less coupled with specific ml handlers\r\n",
    "line_number": 137,
    "enriched": "File: tests/unit/executor_test_base.py\nCode: @@ -131,6 +134,8 @@ def clear_db(self, db):\n         db.session.add(r)\n         r = db.Integration(name=\"lightfm\", data={}, engine=\"lightfm\")\n         db.session.add(r)\n+        r = db.Integration(name=\"ludwig\", data={}, engine=\"ludwig\")\nComment: what do you think about doing\r\n```\r\ncreate ml_engine <name> from <handler>\r\n```\r\ninstead of adding records to integration? \r\n\r\nIt will make this base class less coupled with specific ml handlers\r\n",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "tests/unit/executor_test_base.py",
    "pr_number": 8714,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1472541570,
    "comment_created_at": "2024-01-31T09:31:45Z"
  },
  {
    "code": "@@ -97,3 +103,140 @@ def delete(self, database_name):\n                 + f'Full error: {e}. '\n                 + 'Please check the name and try again.')\n         return '', HTTPStatus.NO_CONTENT\n+\n+\n+def _tables_row_to_obj(table_row: TablesRow) -> Dict:\n+    type = table_row.TABLE_TYPE.lower()\n+    if table_row.TABLE_TYPE == 'BASE TABLE':\n+        type = 'data'\n+    return {\n+        'name': table_row.TABLE_NAME,\n+        'type': type\n+    }\n+\n+\n+@ns_conf.route('/<database_name>/tables')\n+class TablesList(Resource):\n+    @ns_conf.doc('list_tables')\n+    def get(self, database_name):\n+        '''Get all tables in a database'''\n+        session = SessionController()\n+        datanode = session.datahub.get(database_name)\n+        all_tables = datanode.get_tables()\n+        table_objs = [_tables_row_to_obj(t) for t in all_tables]\n+        return table_objs\n+\n+    @ns_conf.doc('create_table')\n+    def post(self, database_name):\n+        '''Creates a table in a database'''\n+        if 'table' not in request.json:\n+            abort(HTTPStatus.BAD_REQUEST, 'Must provide \"table\" parameter in POST body')",
    "comment": "I think we need to have single way to format HTTP errors. `abort` returns to client json with\r\n```\r\n{message: 'Must provide \"table\" parameter in POST body'}\r\n```\r\nBut in some other places we return error in [this form](https://github.com/mindsdb/mindsdb/blob/staging/mindsdb/api/http/utils.py#L7). I prefer second form, because it based on RFC proposals. It is not standard, but at least some kind of agreement. What do you think? If you want, please keep `abort` and merge PR\r\n",
    "line_number": 133,
    "enriched": "File: mindsdb/api/http/namespaces/databases.py\nCode: @@ -97,3 +103,140 @@ def delete(self, database_name):\n                 + f'Full error: {e}. '\n                 + 'Please check the name and try again.')\n         return '', HTTPStatus.NO_CONTENT\n+\n+\n+def _tables_row_to_obj(table_row: TablesRow) -> Dict:\n+    type = table_row.TABLE_TYPE.lower()\n+    if table_row.TABLE_TYPE == 'BASE TABLE':\n+        type = 'data'\n+    return {\n+        'name': table_row.TABLE_NAME,\n+        'type': type\n+    }\n+\n+\n+@ns_conf.route('/<database_name>/tables')\n+class TablesList(Resource):\n+    @ns_conf.doc('list_tables')\n+    def get(self, database_name):\n+        '''Get all tables in a database'''\n+        session = SessionController()\n+        datanode = session.datahub.get(database_name)\n+        all_tables = datanode.get_tables()\n+        table_objs = [_tables_row_to_obj(t) for t in all_tables]\n+        return table_objs\n+\n+    @ns_conf.doc('create_table')\n+    def post(self, database_name):\n+        '''Creates a table in a database'''\n+        if 'table' not in request.json:\n+            abort(HTTPStatus.BAD_REQUEST, 'Must provide \"table\" parameter in POST body')\nComment: I think we need to have single way to format HTTP errors. `abort` returns to client json with\r\n```\r\n{message: 'Must provide \"table\" parameter in POST body'}\r\n```\r\nBut in some other places we return error in [this form](https://github.com/mindsdb/mindsdb/blob/staging/mindsdb/api/http/utils.py#L7). I prefer second form, because it based on RFC proposals. It is not standard, but at least some kind of agreement. What do you think? If you want, please keep `abort` and merge PR\r\n",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/api/http/namespaces/databases.py",
    "pr_number": 5782,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1177858834,
    "comment_created_at": "2023-04-26T13:11:34Z"
  },
  {
    "code": "@@ -0,0 +1,115 @@\n+import os\n+import requests\n+\n+from prometheus_api_client import PrometheusConnect, PrometheusApiClientException, utils\n+\n+from mindsdb.utilities import log\n+from mindsdb.utilities.config import Config\n+\n+from mindsdb_sql.parser import ast\n+\n+from mindsdb.integrations.libs.api_handler import APIHandler, APITable\n+from mindsdb.integrations.utilities.sql_utils import extract_comparison_conditions\n+\n+from mindsdb.integrations.libs.response import (\n+    HandlerStatusResponse as StatusResponse,\n+    HandlerResponse as Response,\n+    RESPONSE_TYPE\n+)\n+\n+class PrometheusTable(APITable):\n+    def select(self, query: ast.Select) -> Response:\n+        conditions = extract_comparison_conditions(query.where)\n+        params = {\n+            \"query\": None,\n+            \"start_time\": utils.parse_datetime(\"5m\"),\n+            \"end_time\": utils.parse_datetime(\"now\")\n+        }\n+\n+        for op, arg1, arg2 in conditions:\n+            if op == 'or':\n+                raise NotImplementedError(f'OR is not supported')\n+            if arg1 == 'start_time':\n+                params['start_time'] = utils.parse_datetime(arg2)\n+            elif arg1 == 'end_time':\n+                params['end_time'] = utils.parse_datetime(arg2)\n+            elif arg1 == 'query':\n+                if op == '=':\n+                    params[arg1] = arg2\n+                else:\n+                    NotImplementedError(f'Unknown op: {op}')\n+\n+        if params.query is None:\n+            ValueError(\"Query must be provided\")\n+\n+        self.handler.call_prometheus_api(params)\n+        return None\n+\n+class PrometheusHandler(APIHandler):\n+    \"\"\"A class for handling connections and interactions with the Prometheus API.\n+\n+    Attributes:\n+        bearer_token (str): The consumer key for the Prometheus app.\n+        api (tweepy.API): The `tweepy.API` object for interacting with the Prometheus API.",
    "comment": "why is this here?",
    "line_number": 53,
    "enriched": "File: mindsdb/integrations/handlers/prometheus_handler/prometheus_handler.py\nCode: @@ -0,0 +1,115 @@\n+import os\n+import requests\n+\n+from prometheus_api_client import PrometheusConnect, PrometheusApiClientException, utils\n+\n+from mindsdb.utilities import log\n+from mindsdb.utilities.config import Config\n+\n+from mindsdb_sql.parser import ast\n+\n+from mindsdb.integrations.libs.api_handler import APIHandler, APITable\n+from mindsdb.integrations.utilities.sql_utils import extract_comparison_conditions\n+\n+from mindsdb.integrations.libs.response import (\n+    HandlerStatusResponse as StatusResponse,\n+    HandlerResponse as Response,\n+    RESPONSE_TYPE\n+)\n+\n+class PrometheusTable(APITable):\n+    def select(self, query: ast.Select) -> Response:\n+        conditions = extract_comparison_conditions(query.where)\n+        params = {\n+            \"query\": None,\n+            \"start_time\": utils.parse_datetime(\"5m\"),\n+            \"end_time\": utils.parse_datetime(\"now\")\n+        }\n+\n+        for op, arg1, arg2 in conditions:\n+            if op == 'or':\n+                raise NotImplementedError(f'OR is not supported')\n+            if arg1 == 'start_time':\n+                params['start_time'] = utils.parse_datetime(arg2)\n+            elif arg1 == 'end_time':\n+                params['end_time'] = utils.parse_datetime(arg2)\n+            elif arg1 == 'query':\n+                if op == '=':\n+                    params[arg1] = arg2\n+                else:\n+                    NotImplementedError(f'Unknown op: {op}')\n+\n+        if params.query is None:\n+            ValueError(\"Query must be provided\")\n+\n+        self.handler.call_prometheus_api(params)\n+        return None\n+\n+class PrometheusHandler(APIHandler):\n+    \"\"\"A class for handling connections and interactions with the Prometheus API.\n+\n+    Attributes:\n+        bearer_token (str): The consumer key for the Prometheus app.\n+        api (tweepy.API): The `tweepy.API` object for interacting with the Prometheus API.\nComment: why is this here?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/prometheus_handler/prometheus_handler.py",
    "pr_number": 5614,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1165189587,
    "comment_created_at": "2023-04-13T08:30:48Z"
  },
  {
    "code": "@@ -7,119 +7,135 @@ MindsDB provides the `LLM()` function that lets users incorporate the LLM-genera\n \n ## Prerequisites\n \n-The `LLM()` function requires a large language model, which can be defined in the [MindsDB configuration under `default_llm`](/setup/custom-config) or via environment variables as below.\n-\n-To use the `LLM()` function with MindsDB, choosing one of the available model providers and define the following environment variables.\n-\n-<AccordionGroup>\n-\n-  <Accordion title=\"OpenAI\">\n-    Here are the environment variables for the OpenAI provider:\n-\n-    ```\n-    LLM_FUNCTION_MODEL_NAME\n-    LLM_FUNCTION_TEMPERATURE\n-    LLM_FUNCTION_MAX_RETRIES\n-    LLM_FUNCTION_MAX_TOKENS\n-    LLM_FUNCTION_BASE_URL\n-    OPENAI_API_KEY\n-    LLM_FUNCTION_API_ORGANIZATION\n-    LLM_FUNCTION_REQUEST_TIMEOUT\n-    ```\n-\n-    Note that the values stored in the environment variables are specific for each provider.\n-  </Accordion>\n-\n-  <Accordion title=\"Anthropic\">\n-    Here are the environment variables for the Anthropic provider:\n-\n-    ```\n-    LLM_FUNCTION_MODEL_NAME\n-    LLM_FUNCTION_TEMPERATURE\n-    LLM_FUNCTION_MAX_TOKENS\n-    LLM_FUNCTION_TOP_P\n-    LLM_FUNCTION_TOP_K\n-    LLM_FUNCTION_DEFAULT_REQUEST_TIMEOUT\n-    LLM_FUNCTION_API_KEY\n-    LLM_FUNCTION_BASE_URL\n-    ```\n-\n-    Note that the values stored in the environment variables are specific for each provider.\n-  </Accordion>\n-\n-  <Accordion title=\"LiteLLM\">\n-    Here are the environment variables for the LiteLLM provider:\n-\n-    ```\n-    LLM_FUNCTION_MODEL_NAME\n-    LLM_FUNCTION_TEMPERATURE\n-    LLM_FUNCTION_API_BASE\n-    LLM_FUNCTION_MAX_RETRIES\n-    LLM_FUNCTION_MAX_TOKENS\n-    LLM_FUNCTION_TOP_P\n-    LLM_FUNCTION_TOP_K\n-    ```\n-\n-    Note that the values stored in the environment variables are specific for each provider.\n-  </Accordion>\n-\n-  <Accordion title=\"Ollama\">\n-    Here are the environment variables for the Ollama provider:\n-\n-    ```\n-    LLM_FUNCTION_BASE_URL\n-    LLM_FUNCTION_MODEL_NAME\n-    LLM_FUNCTION_TEMPERATURE\n-    LLM_FUNCTION_TOP_P\n-    LLM_FUNCTION_TOP_K\n-    LLM_FUNCTION_REQUEST_TIMEOUT\n-    LLM_FUNCTION_FORMAT\n-    LLM_FUNCTION_HEADERS\n-    LLM_FUNCTION_NUM_PREDICT\n-    LLM_FUNCTION_NUM_CTX\n-    LLM_FUNCTION_NUM_GPU\n-    LLM_FUNCTION_REPEAT_PENALTY\n-    LLM_FUNCTION_STOP\n-    LLM_FUNCTION_TEMPLATE\n-    ```\n-\n-    Note that the values stored in the environment variables are specific for each provider.\n-  </Accordion>\n-\n-  <Accordion title=\"Nvidia NIMs\">\n-    Here are the environment variables for the Nvidia NIMs provider:\n-\n-    ```\n-    LLM_FUNCTION_BASE_URL\n-    LLM_FUNCTION_MODEL_NAME\n-    LLM_FUNCTION_TEMPERATURE\n-    LLM_FUNCTION_TOP_P\n-    LLM_FUNCTION_REQUEST_TIMEOUT\n-    LLM_FUNCTION_FORMAT\n-    LLM_FUNCTION_HEADERS\n-    LLM_FUNCTION_NUM_PREDICT\n-    LLM_FUNCTION_NUM_CTX\n-    LLM_FUNCTION_NUM_GPU\n-    LLM_FUNCTION_REPEAT_PENALTY\n-    LLM_FUNCTION_STOP\n-    LLM_FUNCTION_TEMPLATE\n-    LLM_FUNCTION_NVIDIA_API_KEY\n-    ```\n-\n-    Note that the values stored in the environment variables are specific for each provider.\n-  </Accordion>\n-\n-  <Accordion title=\"MindsDB\">\n-    Here are the environment variables for the MindsDB provider:\n-\n-    ```\n-    LLM_FUNCTION_MODEL_NAME\n-    LLM_FUNCTION_PROJECT_NAME\n-    ```\n-\n-    To use MindsDB as a provider, create a model in a project within MindsDB and use its name in the `LLM_FUNCTION_MODEL_NAME` environment variable and the project name in the `LLM_FUNCTION_PROJECT_NAME` environment variable. \n-  </Accordion>\n-</AccordionGroup>\n+The `LLM()` function requires a large language model, which can be defined in the following ways:\n+\n+- By setting the `default_llm` parameter in the [MindsDB configuration file](/setup/custom-config#default-llm).\n+- By saving the default model in the MindsDB Editor under Settings.\n+- By defining the environment variables as below, choosing one of the available model providers.\n+\n+  <AccordionGroup>\n+\n+    <Accordion title=\"OpenAI\">\n+      Here are the environment variables for the OpenAI provider:\n+\n+      ```\n+      LLM_FUNCTION_MODEL_NAME\n+      LLM_FUNCTION_TEMPERATURE\n+      LLM_FUNCTION_MAX_RETRIES\n+      LLM_FUNCTION_MAX_TOKENS\n+      LLM_FUNCTION_BASE_URL\n+      OPENAI_API_KEY\n+      LLM_FUNCTION_API_ORGANIZATION\n+      LLM_FUNCTION_REQUEST_TIMEOUT\n+      ```\n+\n+      Note that the values stored in the environment variables are specific for each provider.\n+    </Accordion>\n+\n+    <Accordion title=\"Anthropic\">\n+      Here are the environment variables for the Anthropic provider:\n+\n+      ```\n+      LLM_FUNCTION_MODEL_NAME\n+      LLM_FUNCTION_TEMPERATURE\n+      LLM_FUNCTION_MAX_TOKENS\n+      LLM_FUNCTION_TOP_P\n+      LLM_FUNCTION_TOP_K\n+      LLM_FUNCTION_DEFAULT_REQUEST_TIMEOUT\n+      LLM_FUNCTION_API_KEY\n+      LLM_FUNCTION_BASE_URL\n+      ```\n+\n+      Note that the values stored in the environment variables are specific for each provider.\n+    </Accordion>\n+\n+    <Accordion title=\"LiteLLM\">\n+      Here are the environment variables for the LiteLLM provider:\n+\n+      ```\n+      LLM_FUNCTION_MODEL_NAME\n+      LLM_FUNCTION_TEMPERATURE\n+      LLM_FUNCTION_API_BASE\n+      LLM_FUNCTION_MAX_RETRIES\n+      LLM_FUNCTION_MAX_TOKENS\n+      LLM_FUNCTION_TOP_P\n+      LLM_FUNCTION_TOP_K\n+      ```\n+\n+      Note that the values stored in the environment variables are specific for each provider.\n+    </Accordion>\n+\n+    <Accordion title=\"Ollama\">\n+      Here are the environment variables for the Ollama provider:\n+\n+      ```\n+      LLM_FUNCTION_BASE_URL\n+      LLM_FUNCTION_MODEL_NAME\n+      LLM_FUNCTION_TEMPERATURE\n+      LLM_FUNCTION_TOP_P\n+      LLM_FUNCTION_TOP_K\n+      LLM_FUNCTION_REQUEST_TIMEOUT\n+      LLM_FUNCTION_FORMAT\n+      LLM_FUNCTION_HEADERS\n+      LLM_FUNCTION_NUM_PREDICT\n+      LLM_FUNCTION_NUM_CTX\n+      LLM_FUNCTION_NUM_GPU\n+      LLM_FUNCTION_REPEAT_PENALTY\n+      LLM_FUNCTION_STOP\n+      LLM_FUNCTION_TEMPLATE\n+      ```\n+\n+      Note that the values stored in the environment variables are specific for each provider.\n+    </Accordion>\n+\n+    <Accordion title=\"Nvidia NIMs\">\n+      Here are the environment variables for the Nvidia NIMs provider:\n+\n+      ```\n+      LLM_FUNCTION_BASE_URL\n+      LLM_FUNCTION_MODEL_NAME\n+      LLM_FUNCTION_TEMPERATURE\n+      LLM_FUNCTION_TOP_P\n+      LLM_FUNCTION_REQUEST_TIMEOUT\n+      LLM_FUNCTION_FORMAT\n+      LLM_FUNCTION_HEADERS\n+      LLM_FUNCTION_NUM_PREDICT\n+      LLM_FUNCTION_NUM_CTX\n+      LLM_FUNCTION_NUM_GPU\n+      LLM_FUNCTION_REPEAT_PENALTY\n+      LLM_FUNCTION_STOP\n+      LLM_FUNCTION_TEMPLATE\n+      LLM_FUNCTION_NVIDIA_API_KEY\n+      ```\n+\n+      Note that the values stored in the environment variables are specific for each provider.\n+    </Accordion>\n+\n+    <Accordion title=\"MindsDB\">\n+      Here are the environment variables for the MindsDB provider:\n+\n+      ```\n+      LLM_FUNCTION_MODEL_NAME\n+      LLM_FUNCTION_PROJECT_NAME\n+      ```\n+\n+      To use MindsDB as a provider, create a model in a project within MindsDB and use its name in the `LLM_FUNCTION_MODEL_NAME` environment variable and the project name in the `LLM_FUNCTION_PROJECT_NAME` environment variable. ",
    "comment": "@martyna-mindsdb Do you think that we need to mention the fact that the model needs to be from one of Gen-AI (LLM-based) integrations here?",
    "line_number": 122,
    "enriched": "File: docs/mindsdb_sql/functions/llm_function.mdx\nCode: @@ -7,119 +7,135 @@ MindsDB provides the `LLM()` function that lets users incorporate the LLM-genera\n \n ## Prerequisites\n \n-The `LLM()` function requires a large language model, which can be defined in the [MindsDB configuration under `default_llm`](/setup/custom-config) or via environment variables as below.\n-\n-To use the `LLM()` function with MindsDB, choosing one of the available model providers and define the following environment variables.\n-\n-<AccordionGroup>\n-\n-  <Accordion title=\"OpenAI\">\n-    Here are the environment variables for the OpenAI provider:\n-\n-    ```\n-    LLM_FUNCTION_MODEL_NAME\n-    LLM_FUNCTION_TEMPERATURE\n-    LLM_FUNCTION_MAX_RETRIES\n-    LLM_FUNCTION_MAX_TOKENS\n-    LLM_FUNCTION_BASE_URL\n-    OPENAI_API_KEY\n-    LLM_FUNCTION_API_ORGANIZATION\n-    LLM_FUNCTION_REQUEST_TIMEOUT\n-    ```\n-\n-    Note that the values stored in the environment variables are specific for each provider.\n-  </Accordion>\n-\n-  <Accordion title=\"Anthropic\">\n-    Here are the environment variables for the Anthropic provider:\n-\n-    ```\n-    LLM_FUNCTION_MODEL_NAME\n-    LLM_FUNCTION_TEMPERATURE\n-    LLM_FUNCTION_MAX_TOKENS\n-    LLM_FUNCTION_TOP_P\n-    LLM_FUNCTION_TOP_K\n-    LLM_FUNCTION_DEFAULT_REQUEST_TIMEOUT\n-    LLM_FUNCTION_API_KEY\n-    LLM_FUNCTION_BASE_URL\n-    ```\n-\n-    Note that the values stored in the environment variables are specific for each provider.\n-  </Accordion>\n-\n-  <Accordion title=\"LiteLLM\">\n-    Here are the environment variables for the LiteLLM provider:\n-\n-    ```\n-    LLM_FUNCTION_MODEL_NAME\n-    LLM_FUNCTION_TEMPERATURE\n-    LLM_FUNCTION_API_BASE\n-    LLM_FUNCTION_MAX_RETRIES\n-    LLM_FUNCTION_MAX_TOKENS\n-    LLM_FUNCTION_TOP_P\n-    LLM_FUNCTION_TOP_K\n-    ```\n-\n-    Note that the values stored in the environment variables are specific for each provider.\n-  </Accordion>\n-\n-  <Accordion title=\"Ollama\">\n-    Here are the environment variables for the Ollama provider:\n-\n-    ```\n-    LLM_FUNCTION_BASE_URL\n-    LLM_FUNCTION_MODEL_NAME\n-    LLM_FUNCTION_TEMPERATURE\n-    LLM_FUNCTION_TOP_P\n-    LLM_FUNCTION_TOP_K\n-    LLM_FUNCTION_REQUEST_TIMEOUT\n-    LLM_FUNCTION_FORMAT\n-    LLM_FUNCTION_HEADERS\n-    LLM_FUNCTION_NUM_PREDICT\n-    LLM_FUNCTION_NUM_CTX\n-    LLM_FUNCTION_NUM_GPU\n-    LLM_FUNCTION_REPEAT_PENALTY\n-    LLM_FUNCTION_STOP\n-    LLM_FUNCTION_TEMPLATE\n-    ```\n-\n-    Note that the values stored in the environment variables are specific for each provider.\n-  </Accordion>\n-\n-  <Accordion title=\"Nvidia NIMs\">\n-    Here are the environment variables for the Nvidia NIMs provider:\n-\n-    ```\n-    LLM_FUNCTION_BASE_URL\n-    LLM_FUNCTION_MODEL_NAME\n-    LLM_FUNCTION_TEMPERATURE\n-    LLM_FUNCTION_TOP_P\n-    LLM_FUNCTION_REQUEST_TIMEOUT\n-    LLM_FUNCTION_FORMAT\n-    LLM_FUNCTION_HEADERS\n-    LLM_FUNCTION_NUM_PREDICT\n-    LLM_FUNCTION_NUM_CTX\n-    LLM_FUNCTION_NUM_GPU\n-    LLM_FUNCTION_REPEAT_PENALTY\n-    LLM_FUNCTION_STOP\n-    LLM_FUNCTION_TEMPLATE\n-    LLM_FUNCTION_NVIDIA_API_KEY\n-    ```\n-\n-    Note that the values stored in the environment variables are specific for each provider.\n-  </Accordion>\n-\n-  <Accordion title=\"MindsDB\">\n-    Here are the environment variables for the MindsDB provider:\n-\n-    ```\n-    LLM_FUNCTION_MODEL_NAME\n-    LLM_FUNCTION_PROJECT_NAME\n-    ```\n-\n-    To use MindsDB as a provider, create a model in a project within MindsDB and use its name in the `LLM_FUNCTION_MODEL_NAME` environment variable and the project name in the `LLM_FUNCTION_PROJECT_NAME` environment variable. \n-  </Accordion>\n-</AccordionGroup>\n+The `LLM()` function requires a large language model, which can be defined in the following ways:\n+\n+- By setting the `default_llm` parameter in the [MindsDB configuration file](/setup/custom-config#default-llm).\n+- By saving the default model in the MindsDB Editor under Settings.\n+- By defining the environment variables as below, choosing one of the available model providers.\n+\n+  <AccordionGroup>\n+\n+    <Accordion title=\"OpenAI\">\n+      Here are the environment variables for the OpenAI provider:\n+\n+      ```\n+      LLM_FUNCTION_MODEL_NAME\n+      LLM_FUNCTION_TEMPERATURE\n+      LLM_FUNCTION_MAX_RETRIES\n+      LLM_FUNCTION_MAX_TOKENS\n+      LLM_FUNCTION_BASE_URL\n+      OPENAI_API_KEY\n+      LLM_FUNCTION_API_ORGANIZATION\n+      LLM_FUNCTION_REQUEST_TIMEOUT\n+      ```\n+\n+      Note that the values stored in the environment variables are specific for each provider.\n+    </Accordion>\n+\n+    <Accordion title=\"Anthropic\">\n+      Here are the environment variables for the Anthropic provider:\n+\n+      ```\n+      LLM_FUNCTION_MODEL_NAME\n+      LLM_FUNCTION_TEMPERATURE\n+      LLM_FUNCTION_MAX_TOKENS\n+      LLM_FUNCTION_TOP_P\n+      LLM_FUNCTION_TOP_K\n+      LLM_FUNCTION_DEFAULT_REQUEST_TIMEOUT\n+      LLM_FUNCTION_API_KEY\n+      LLM_FUNCTION_BASE_URL\n+      ```\n+\n+      Note that the values stored in the environment variables are specific for each provider.\n+    </Accordion>\n+\n+    <Accordion title=\"LiteLLM\">\n+      Here are the environment variables for the LiteLLM provider:\n+\n+      ```\n+      LLM_FUNCTION_MODEL_NAME\n+      LLM_FUNCTION_TEMPERATURE\n+      LLM_FUNCTION_API_BASE\n+      LLM_FUNCTION_MAX_RETRIES\n+      LLM_FUNCTION_MAX_TOKENS\n+      LLM_FUNCTION_TOP_P\n+      LLM_FUNCTION_TOP_K\n+      ```\n+\n+      Note that the values stored in the environment variables are specific for each provider.\n+    </Accordion>\n+\n+    <Accordion title=\"Ollama\">\n+      Here are the environment variables for the Ollama provider:\n+\n+      ```\n+      LLM_FUNCTION_BASE_URL\n+      LLM_FUNCTION_MODEL_NAME\n+      LLM_FUNCTION_TEMPERATURE\n+      LLM_FUNCTION_TOP_P\n+      LLM_FUNCTION_TOP_K\n+      LLM_FUNCTION_REQUEST_TIMEOUT\n+      LLM_FUNCTION_FORMAT\n+      LLM_FUNCTION_HEADERS\n+      LLM_FUNCTION_NUM_PREDICT\n+      LLM_FUNCTION_NUM_CTX\n+      LLM_FUNCTION_NUM_GPU\n+      LLM_FUNCTION_REPEAT_PENALTY\n+      LLM_FUNCTION_STOP\n+      LLM_FUNCTION_TEMPLATE\n+      ```\n+\n+      Note that the values stored in the environment variables are specific for each provider.\n+    </Accordion>\n+\n+    <Accordion title=\"Nvidia NIMs\">\n+      Here are the environment variables for the Nvidia NIMs provider:\n+\n+      ```\n+      LLM_FUNCTION_BASE_URL\n+      LLM_FUNCTION_MODEL_NAME\n+      LLM_FUNCTION_TEMPERATURE\n+      LLM_FUNCTION_TOP_P\n+      LLM_FUNCTION_REQUEST_TIMEOUT\n+      LLM_FUNCTION_FORMAT\n+      LLM_FUNCTION_HEADERS\n+      LLM_FUNCTION_NUM_PREDICT\n+      LLM_FUNCTION_NUM_CTX\n+      LLM_FUNCTION_NUM_GPU\n+      LLM_FUNCTION_REPEAT_PENALTY\n+      LLM_FUNCTION_STOP\n+      LLM_FUNCTION_TEMPLATE\n+      LLM_FUNCTION_NVIDIA_API_KEY\n+      ```\n+\n+      Note that the values stored in the environment variables are specific for each provider.\n+    </Accordion>\n+\n+    <Accordion title=\"MindsDB\">\n+      Here are the environment variables for the MindsDB provider:\n+\n+      ```\n+      LLM_FUNCTION_MODEL_NAME\n+      LLM_FUNCTION_PROJECT_NAME\n+      ```\n+\n+      To use MindsDB as a provider, create a model in a project within MindsDB and use its name in the `LLM_FUNCTION_MODEL_NAME` environment variable and the project name in the `LLM_FUNCTION_PROJECT_NAME` environment variable. \nComment: @martyna-mindsdb Do you think that we need to mention the fact that the model needs to be from one of Gen-AI (LLM-based) integrations here?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "docs/mindsdb_sql/functions/llm_function.mdx",
    "pr_number": 11465,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2289828908,
    "comment_created_at": "2025-08-21T04:34:26Z"
  },
  {
    "code": "@@ -0,0 +1,238 @@\n+import io\n+import pandas as pd\n+from boxsdk import Client, OAuth2",
    "comment": "@vishwamartur This SDK is under maintenance mode and will be deprecated soon. So, you will have to use `box-sdk-gen`. \r\nBut there's already a PR for box integration with the `box-sdk-gen` sdk. I would suggest you to contribute to other issues.\r\n\r\ncc: @chandrevdw31 @ZoranPandovski \r\n",
    "line_number": 3,
    "enriched": "File: mindsdb/integrations/handlers/box_handler/box_handler.py\nCode: @@ -0,0 +1,238 @@\n+import io\n+import pandas as pd\n+from boxsdk import Client, OAuth2\nComment: @vishwamartur This SDK is under maintenance mode and will be deprecated soon. So, you will have to use `box-sdk-gen`. \r\nBut there's already a PR for box integration with the `box-sdk-gen` sdk. I would suggest you to contribute to other issues.\r\n\r\ncc: @chandrevdw31 @ZoranPandovski \r\n",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/box_handler/box_handler.py",
    "pr_number": 10175,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1846130063,
    "comment_created_at": "2024-11-18T09:06:09Z"
  },
  {
    "code": "@@ -14,6 +15,14 @@\n icon_path = 'icon.png'",
    "comment": "We are missing an CKAN inon in png format",
    "line_number": 15,
    "enriched": "File: mindsdb/integrations/handlers/ckan_handler/__init__.py\nCode: @@ -14,6 +15,14 @@\n icon_path = 'icon.png'\nComment: We are missing an CKAN inon in png format",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/ckan_handler/__init__.py",
    "pr_number": 9788,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1795168123,
    "comment_created_at": "2024-10-10T10:33:51Z"
  },
  {
    "code": "@@ -0,0 +1,238 @@\n+import pandas as pd\n+import requests\n+from mindsdb.integrations.libs.api_handler import APITable\n+from mindsdb_sql.parser import ast\n+from mindsdb.integrations.utilities.sql_utils import extract_comparison_conditions\n+\n+\n+class BaseResultsTable(APITable):\n+    def select(self, query: ast.Select) -> pd.DataFrame:\n+        \"\"\"\n+        Selects data from the results table and returns it as a pandas DataFrame.\n+\n+        Args:\n+            query (ast.Select): The SQL query to be executed.\n+\n+        Returns:\n+            pandas.DataFrame: A pandas DataFrame containing the selected data.\n+        \"\"\"\n+        self.handler.connect()\n+\n+        params = {'access_key': self.handler.access_key}\n+        conditions = extract_comparison_conditions(query.where)\n+        params.update({condition[1]: condition[2] for condition in conditions if condition[0] == '='})\n+\n+        if 'query' not in params:\n+            raise ValueError('Query is missing in the SQL query')\n+        if 'type' not in params and hasattr(self, 'default_type'):\n+            params['type'] = self.default_type\n+        api_response = requests.get(self.handler.base_url, params=params)",
    "comment": "Can we check for exceptions here?",
    "line_number": 29,
    "enriched": "File: mindsdb/integrations/handlers/serpstack_handler/serpstack_tables.py\nCode: @@ -0,0 +1,238 @@\n+import pandas as pd\n+import requests\n+from mindsdb.integrations.libs.api_handler import APITable\n+from mindsdb_sql.parser import ast\n+from mindsdb.integrations.utilities.sql_utils import extract_comparison_conditions\n+\n+\n+class BaseResultsTable(APITable):\n+    def select(self, query: ast.Select) -> pd.DataFrame:\n+        \"\"\"\n+        Selects data from the results table and returns it as a pandas DataFrame.\n+\n+        Args:\n+            query (ast.Select): The SQL query to be executed.\n+\n+        Returns:\n+            pandas.DataFrame: A pandas DataFrame containing the selected data.\n+        \"\"\"\n+        self.handler.connect()\n+\n+        params = {'access_key': self.handler.access_key}\n+        conditions = extract_comparison_conditions(query.where)\n+        params.update({condition[1]: condition[2] for condition in conditions if condition[0] == '='})\n+\n+        if 'query' not in params:\n+            raise ValueError('Query is missing in the SQL query')\n+        if 'type' not in params and hasattr(self, 'default_type'):\n+            params['type'] = self.default_type\n+        api_response = requests.get(self.handler.base_url, params=params)\nComment: Can we check for exceptions here?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/serpstack_handler/serpstack_tables.py",
    "pr_number": 9381,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1686353549,
    "comment_created_at": "2024-07-22T10:47:53Z"
  },
  {
    "code": "@@ -20,12 +20,43 @@\n     RESPONSE_TYPE\n )\n import mindsdb.utilities.profiler as profiler\n+from mindsdb.api.mysql.mysql_proxy.libs.constants.mysql import MYSQL_DATA_TYPE\n \n logger = log.getLogger(__name__)\n \n SUBSCRIBE_SLEEP_INTERVAL = 1\n \n \n+def _map_type(internal_type_name: str) -> MYSQL_DATA_TYPE:\n+    \"\"\"Map Postgres types to MySQL types.\n+\n+    Args:\n+        internal_type_name (str): The name of the Postgres type to map.\n+\n+    Returns:\n+        MYSQL_DATA_TYPE: The MySQL type that corresponds to the Postgres type.\n+    \"\"\"\n+    internal_type_name = internal_type_name.lower()\n+    types_map = {\n+        ('smallint', 'integer', 'bigint', 'int', 'smallserial', 'serial', 'bigserial'): MYSQL_DATA_TYPE.INT,\n+        ('real', 'numeric', 'decimal', 'money', 'float'): MYSQL_DATA_TYPE.FLOAT,\n+        ('double precision',): MYSQL_DATA_TYPE.DOUBLE,\n+        ('character varying', 'varchar', 'character', 'char', 'bpchar', 'bpchar', 'text'): MYSQL_DATA_TYPE.TEXT,\n+        ('timestamp', 'timestamp without time zone', 'timestamp with time zone'): MYSQL_DATA_TYPE.DATETIME,\n+        ('date', ): MYSQL_DATA_TYPE.DATE,\n+        ('time', 'time without time zone', 'time with time zone'): MYSQL_DATA_TYPE.TIME,\n+        ('boolean',): MYSQL_DATA_TYPE.BOOL,\n+        ('bytea',): MYSQL_DATA_TYPE.BINARY,\n+    }\n+\n+    for snowflake_types, mysql_data_type in types_map.items():",
    "comment": "Maybe we change snowflake_types to postgres_type",
    "line_number": 52,
    "enriched": "File: mindsdb/integrations/handlers/postgres_handler/postgres_handler.py\nCode: @@ -20,12 +20,43 @@\n     RESPONSE_TYPE\n )\n import mindsdb.utilities.profiler as profiler\n+from mindsdb.api.mysql.mysql_proxy.libs.constants.mysql import MYSQL_DATA_TYPE\n \n logger = log.getLogger(__name__)\n \n SUBSCRIBE_SLEEP_INTERVAL = 1\n \n \n+def _map_type(internal_type_name: str) -> MYSQL_DATA_TYPE:\n+    \"\"\"Map Postgres types to MySQL types.\n+\n+    Args:\n+        internal_type_name (str): The name of the Postgres type to map.\n+\n+    Returns:\n+        MYSQL_DATA_TYPE: The MySQL type that corresponds to the Postgres type.\n+    \"\"\"\n+    internal_type_name = internal_type_name.lower()\n+    types_map = {\n+        ('smallint', 'integer', 'bigint', 'int', 'smallserial', 'serial', 'bigserial'): MYSQL_DATA_TYPE.INT,\n+        ('real', 'numeric', 'decimal', 'money', 'float'): MYSQL_DATA_TYPE.FLOAT,\n+        ('double precision',): MYSQL_DATA_TYPE.DOUBLE,\n+        ('character varying', 'varchar', 'character', 'char', 'bpchar', 'bpchar', 'text'): MYSQL_DATA_TYPE.TEXT,\n+        ('timestamp', 'timestamp without time zone', 'timestamp with time zone'): MYSQL_DATA_TYPE.DATETIME,\n+        ('date', ): MYSQL_DATA_TYPE.DATE,\n+        ('time', 'time without time zone', 'time with time zone'): MYSQL_DATA_TYPE.TIME,\n+        ('boolean',): MYSQL_DATA_TYPE.BOOL,\n+        ('bytea',): MYSQL_DATA_TYPE.BINARY,\n+    }\n+\n+    for snowflake_types, mysql_data_type in types_map.items():\nComment: Maybe we change snowflake_types to postgres_type",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/postgres_handler/postgres_handler.py",
    "pr_number": 10617,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2016310653,
    "comment_created_at": "2025-03-27T11:28:01Z"
  },
  {
    "code": "@@ -27,6 +27,7 @@ parameters={\n     \"instance_id\":\"my-instance\",\n     \"database_id\":\"example-id\",\n     \"project\":\"my-project\",\n+    \"dialect\": \"postgres\" // optinonal, default is 'googlesql'",
    "comment": "This is sql, `//` is not comment symbol. Need to replace to `--` or `/* text */`",
    "line_number": 30,
    "enriched": "File: mindsdb/integrations/handlers/cloud_spanner_handler/README.md\nCode: @@ -27,6 +27,7 @@ parameters={\n     \"instance_id\":\"my-instance\",\n     \"database_id\":\"example-id\",\n     \"project\":\"my-project\",\n+    \"dialect\": \"postgres\" // optinonal, default is 'googlesql'\nComment: This is sql, `//` is not comment symbol. Need to replace to `--` or `/* text */`",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/cloud_spanner_handler/README.md",
    "pr_number": 7518,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1354351217,
    "comment_created_at": "2023-10-11T08:05:08Z"
  },
  {
    "code": "@@ -0,0 +1,109 @@\n+---\n+title: TogetherAI\n+sidebarTitle: TogetherAI\n+---\n+\n+This documentation describes the integration of MindsDB with [TogetherAI](https://www.together.ai/), a research-driven artificial intelligence company. They provide decentralized cloud services which empower developers and researchers at organizations of all sizes to train, fine-tune, and deploy generative AI models.\n+This integration allows using TogetherAI models within MindsDB, providing models with access to data from various data sources.\n+\n+This integration is created by extending [OpenAI Handler](https://github.com/mindsdb/mindsdb/tree/main/mindsdb/integrations/handlers/openai_handler)\n+\n+## Prerequisites\n+\n+Before proceeding, ensure the following prerequisites are met:\n+\n+1. Install MindsDB locally via [Docker](https://docs.mindsdb.com/setup/self-hosted/docker) or [Docker Desktop](https://docs.mindsdb.com/setup/self-hosted/docker-desktop).\n+2. Obtain the TogetherAI API key required to deploy and use TogetherAI models within MindsDB. Follow the [instructions for obtaining the API key](https://docs.together.ai/reference/authentication-1).\n+\n+## Setup\n+\n+Create an AI engine from the [TogetherAI handler](https://github.com/mindsdb/mindsdb/tree/main/mindsdb/integrations/handlers/togetherai_handler).\n+\n+```sql\n+CREATE ML_ENGINE togetherai_engine\n+FROM openai",
    "comment": "maybe change this to togetherai",
    "line_number": 24,
    "enriched": "File: mindsdb/integrations/handlers/togetherai_handler/README.md\nCode: @@ -0,0 +1,109 @@\n+---\n+title: TogetherAI\n+sidebarTitle: TogetherAI\n+---\n+\n+This documentation describes the integration of MindsDB with [TogetherAI](https://www.together.ai/), a research-driven artificial intelligence company. They provide decentralized cloud services which empower developers and researchers at organizations of all sizes to train, fine-tune, and deploy generative AI models.\n+This integration allows using TogetherAI models within MindsDB, providing models with access to data from various data sources.\n+\n+This integration is created by extending [OpenAI Handler](https://github.com/mindsdb/mindsdb/tree/main/mindsdb/integrations/handlers/openai_handler)\n+\n+## Prerequisites\n+\n+Before proceeding, ensure the following prerequisites are met:\n+\n+1. Install MindsDB locally via [Docker](https://docs.mindsdb.com/setup/self-hosted/docker) or [Docker Desktop](https://docs.mindsdb.com/setup/self-hosted/docker-desktop).\n+2. Obtain the TogetherAI API key required to deploy and use TogetherAI models within MindsDB. Follow the [instructions for obtaining the API key](https://docs.together.ai/reference/authentication-1).\n+\n+## Setup\n+\n+Create an AI engine from the [TogetherAI handler](https://github.com/mindsdb/mindsdb/tree/main/mindsdb/integrations/handlers/togetherai_handler).\n+\n+```sql\n+CREATE ML_ENGINE togetherai_engine\n+FROM openai\nComment: maybe change this to togetherai",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/togetherai_handler/README.md",
    "pr_number": 10403,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1930463941,
    "comment_created_at": "2025-01-27T12:39:40Z"
  },
  {
    "code": "@@ -0,0 +1,149 @@\n+from typing import List, Any, Optional, Set\n+import json\n+import logging\n+\n+from langchain_core.documents import Document\n+from langchain_core.retrievers import BaseRetriever\n+from pydantic import Field, PrivateAttr\n+\n+logger = logging.getLogger(__name__)\n+\n+DEFAULT_QUESTION_REFORMULATION_TEMPLATE = \"\"\"Given the original question and the context retrieved, analyze what additional information we need to provide a complete answer.",
    "comment": "Just curious, did you come up with this prompt yourself or is it a known prompt?",
    "line_number": 11,
    "enriched": "File: mindsdb/integrations/utilities/rag/retrievers/multi_hop_retriever.py\nCode: @@ -0,0 +1,149 @@\n+from typing import List, Any, Optional, Set\n+import json\n+import logging\n+\n+from langchain_core.documents import Document\n+from langchain_core.retrievers import BaseRetriever\n+from pydantic import Field, PrivateAttr\n+\n+logger = logging.getLogger(__name__)\n+\n+DEFAULT_QUESTION_REFORMULATION_TEMPLATE = \"\"\"Given the original question and the context retrieved, analyze what additional information we need to provide a complete answer.\nComment: Just curious, did you come up with this prompt yourself or is it a known prompt?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/utilities/rag/retrievers/multi_hop_retriever.py",
    "pr_number": 10344,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1909276105,
    "comment_created_at": "2025-01-09T18:33:55Z"
  },
  {
    "code": "@@ -1,7 +1,88 @@\n+[tool.poetry]\n+name = \"mindsdb\"\n+version = \"0.0.0\"  # leave as 0, it will auto update when pushed to artifact store based on latest git",
    "comment": "To understand a bit bettter - this means we should never update this line? Or does it mean that poetry will automatically change this line to match the latest git?",
    "line_number": 3,
    "enriched": "File: pyproject.toml\nCode: @@ -1,7 +1,88 @@\n+[tool.poetry]\n+name = \"mindsdb\"\n+version = \"0.0.0\"  # leave as 0, it will auto update when pushed to artifact store based on latest git\nComment: To understand a bit bettter - this means we should never update this line? Or does it mean that poetry will automatically change this line to match the latest git?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "pyproject.toml",
    "pr_number": 7027,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1286322146,
    "comment_created_at": "2023-08-07T19:43:33Z"
  },
  {
    "code": "@@ -67,6 +67,7 @@ Here is how to set up a Slack app and generate both a Slack bot token and a Slac\n         - im:write\n         - mpim:read\n         - users.profile:read\n+        - users:read",
    "comment": "I've checked my permissions and I don't have 'users:read' but can read list of users\r\n",
    "line_number": 70,
    "enriched": "File: docs/integrations/app-integrations/slack.mdx\nCode: @@ -67,6 +67,7 @@ Here is how to set up a Slack app and generate both a Slack bot token and a Slac\n         - im:write\n         - mpim:read\n         - users.profile:read\n+        - users:read\nComment: I've checked my permissions and I don't have 'users:read' but can read list of users\r\n",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "docs/integrations/app-integrations/slack.mdx",
    "pr_number": 9657,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1731498742,
    "comment_created_at": "2024-08-26T16:31:07Z"
  },
  {
    "code": "@@ -0,0 +1,58 @@\n+import json\n+import requests\n+from typing import Union\n+from google.oauth2 import service_account\n+\n+from mindsdb.utilities import log\n+\n+from ..exceptions import NoCredentialsException, AuthException\n+\n+\n+logger = log.getLogger(__name__)\n+\n+\n+class GoogleServiceAccountOAuth2Manager:\n+    def __init__(self, credentials_url: str = None, credentials_file: str = None, credentials_json: Union[dict, str] = None) -> None:\n+        # if no credentials provided, raise an exception\n+        if not any([credentials_url, credentials_file, credentials_json]):\n+            raise NoCredentialsException('No valid Google Service Account credentials provided.')\n+        self.credentials_url = credentials_url\n+        self.credentials_file = credentials_file\n+        if credentials_json:\n+            self.credentials_json = self._parse_credentials_json(credentials_json)\n+        else:\n+            self.credentials_json = None\n+\n+    def get_oauth2_credentials(self):\n+        try:\n+            if self.credentials_url:\n+                creds = service_account.Credentials.from_service_account_info(self._download_credentials_file())\n+                return creds\n+\n+            if self.credentials_file:\n+                creds = service_account.Credentials.from_service_account_file(self.credentials_file)\n+                return creds\n+            \n+            if self.credentials_json:\n+                creds = service_account.Credentials.from_service_account_info(self.credentials_json)\n+                return creds\n+        except Exception as e:\n+            raise AuthException(f\"Authentication failed: {e}\")\n+\n+\n+    def _download_credentials_file(self):\n+        response = requests.get(self.credentials_url)\n+        if response.status_code == 200:",
    "comment": "Maybe we do response.raise_for_status in case some error happens here?",
    "line_number": 45,
    "enriched": "File: mindsdb/integrations/handlers/utilities/auth_utilities/google/google_service_account_oauth_utilities.py\nCode: @@ -0,0 +1,58 @@\n+import json\n+import requests\n+from typing import Union\n+from google.oauth2 import service_account\n+\n+from mindsdb.utilities import log\n+\n+from ..exceptions import NoCredentialsException, AuthException\n+\n+\n+logger = log.getLogger(__name__)\n+\n+\n+class GoogleServiceAccountOAuth2Manager:\n+    def __init__(self, credentials_url: str = None, credentials_file: str = None, credentials_json: Union[dict, str] = None) -> None:\n+        # if no credentials provided, raise an exception\n+        if not any([credentials_url, credentials_file, credentials_json]):\n+            raise NoCredentialsException('No valid Google Service Account credentials provided.')\n+        self.credentials_url = credentials_url\n+        self.credentials_file = credentials_file\n+        if credentials_json:\n+            self.credentials_json = self._parse_credentials_json(credentials_json)\n+        else:\n+            self.credentials_json = None\n+\n+    def get_oauth2_credentials(self):\n+        try:\n+            if self.credentials_url:\n+                creds = service_account.Credentials.from_service_account_info(self._download_credentials_file())\n+                return creds\n+\n+            if self.credentials_file:\n+                creds = service_account.Credentials.from_service_account_file(self.credentials_file)\n+                return creds\n+            \n+            if self.credentials_json:\n+                creds = service_account.Credentials.from_service_account_info(self.credentials_json)\n+                return creds\n+        except Exception as e:\n+            raise AuthException(f\"Authentication failed: {e}\")\n+\n+\n+    def _download_credentials_file(self):\n+        response = requests.get(self.credentials_url)\n+        if response.status_code == 200:\nComment: Maybe we do response.raise_for_status in case some error happens here?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/utilities/auth_utilities/google/google_service_account_oauth_utilities.py",
    "pr_number": 9007,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1547538254,
    "comment_created_at": "2024-04-02T10:00:35Z"
  },
  {
    "code": "@@ -310,9 +311,28 @@ def select(\n             payload = {column: payload[column] for column in columns}\n \n         # always include distance\n+        distance_filter = None\n+        distance_col = TableField.DISTANCE.value\n         if distances is not None:\n-            payload[TableField.DISTANCE.value] = distances\n-        return pd.DataFrame(payload)\n+            payload[distance_col] = distances\n+\n+            for cond in conditions:\n+                if cond.column == distance_col:\n+                    distance_filter = cond",
    "comment": "Should this `break` after finding condition?",
    "line_number": 321,
    "enriched": "File: mindsdb/integrations/handlers/chromadb_handler/chromadb_handler.py\nCode: @@ -310,9 +311,28 @@ def select(\n             payload = {column: payload[column] for column in columns}\n \n         # always include distance\n+        distance_filter = None\n+        distance_col = TableField.DISTANCE.value\n         if distances is not None:\n-            payload[TableField.DISTANCE.value] = distances\n-        return pd.DataFrame(payload)\n+            payload[distance_col] = distances\n+\n+            for cond in conditions:\n+                if cond.column == distance_col:\n+                    distance_filter = cond\nComment: Should this `break` after finding condition?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/chromadb_handler/chromadb_handler.py",
    "pr_number": 10567,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2000829771,
    "comment_created_at": "2025-03-18T11:26:56Z"
  },
  {
    "code": "@@ -112,6 +112,48 @@ def post(self, handler_name):\n         )\n \n \n+@ns_conf.route('/<handler_name>/test_connection')",
    "comment": "Maybe we choose a different path name as `health` or `status` ? WDYT?",
    "line_number": 115,
    "enriched": "File: mindsdb/api/http/namespaces/handlers.py\nCode: @@ -112,6 +112,48 @@ def post(self, handler_name):\n         )\n \n \n+@ns_conf.route('/<handler_name>/test_connection')\nComment: Maybe we choose a different path name as `health` or `status` ? WDYT?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/api/http/namespaces/handlers.py",
    "pr_number": 10446,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1949166213,
    "comment_created_at": "2025-02-10T14:29:46Z"
  },
  {
    "code": "@@ -296,8 +307,24 @@ def before_request():\n         else:\n             user_class = 0\n \n+        if (\n+            config.get(\"cloud\", False) is True\n+            and (\n+                company_id is None\n+                or user_class is None\n+            )\n+        ):\n+            logger.error(f\"Got cloud HTTP request with missed headers: company_id={company_id}, user_class={user_class}\")\n+            raise Exception('Request cannot be processed due to insufficient metadata')\n+\n+        if config.get(\"cloud\", False) is True and encryption_key is None:\n+            logger.warn(\"Got cloud HHTP request with missed encryption key\")",
    "comment": "do we need this warning on every request?",
    "line_number": 321,
    "enriched": "File: mindsdb/api/http/initialize.py\nCode: @@ -296,8 +307,24 @@ def before_request():\n         else:\n             user_class = 0\n \n+        if (\n+            config.get(\"cloud\", False) is True\n+            and (\n+                company_id is None\n+                or user_class is None\n+            )\n+        ):\n+            logger.error(f\"Got cloud HTTP request with missed headers: company_id={company_id}, user_class={user_class}\")\n+            raise Exception('Request cannot be processed due to insufficient metadata')\n+\n+        if config.get(\"cloud\", False) is True and encryption_key is None:\n+            logger.warn(\"Got cloud HHTP request with missed encryption key\")\nComment: do we need this warning on every request?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/api/http/initialize.py",
    "pr_number": 8985,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1542765275,
    "comment_created_at": "2024-03-28T11:42:28Z"
  },
  {
    "code": "@@ -16,118 +17,166 @@ class HuggingFaceInferenceAPIHandler(BaseMLEngine):\n \n     name = 'huggingface_api'\n \n+    @staticmethod\n+    def create_validation(target, args=None, **kwargs):\n+\n+        if 'using' in args:\n+            args = args['using']\n+\n+        hf_api = HfApi()\n+\n+        if 'model_name' not in args:\n+            # detect model by task\n+            task = args.get('task')\n+            if task is None:\n+                raise Exception('model_name or task have to be specified')\n+\n+            # get from task",
    "comment": "I don't believe this is necessary, because if the model is not passed, the package itself will take the model defined in the configuration file for that particular task. What do you think?",
    "line_number": 34,
    "enriched": "File: mindsdb/integrations/handlers/huggingface_api_handler/huggingface_api_handler.py\nCode: @@ -16,118 +17,166 @@ class HuggingFaceInferenceAPIHandler(BaseMLEngine):\n \n     name = 'huggingface_api'\n \n+    @staticmethod\n+    def create_validation(target, args=None, **kwargs):\n+\n+        if 'using' in args:\n+            args = args['using']\n+\n+        hf_api = HfApi()\n+\n+        if 'model_name' not in args:\n+            # detect model by task\n+            task = args.get('task')\n+            if task is None:\n+                raise Exception('model_name or task have to be specified')\n+\n+            # get from task\nComment: I don't believe this is necessary, because if the model is not passed, the package itself will take the model defined in the configuration file for that particular task. What do you think?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/huggingface_api_handler/huggingface_api_handler.py",
    "pr_number": 6675,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1244153612,
    "comment_created_at": "2023-06-27T18:03:36Z"
  },
  {
    "code": "@@ -17,129 +15,198 @@\n     RESPONSE_TYPE\n )\n \n+from mindsdb.integrations.libs.base import DatabaseHandler\n+\n \n logger = log.getLogger(__name__)\n \n+\n class S3Handler(DatabaseHandler):\n     \"\"\"\n-    This handler handles connection and execution of the S3 statements.\n+    This handler handles connection and execution of the SQL statements on AWS S3.\n     \"\"\"\n \n     name = 's3'\n+    table_name = 's3_table'\n+    # TODO: Can other file formats be supported?\n+    supported_file_formats = ['csv', 'tsv', 'json', 'parquet']\n \n-    def __init__(self, name: str, connection_data: Optional[dict], **kwargs):\n+    def __init__(self, name: Text, connection_data: Optional[Dict], **kwargs):\n         \"\"\"\n-        Initialize the handler.\n+        Initializes the handler.\n+\n         Args:\n-            name (str): name of particular handler instance\n-            connection_data (dict): parameters for connecting to the database\n-            **kwargs: arbitrary keyword arguments.\n+            name (Text): The name of the handler instance.\n+            connection_data (Dict): The connection data required to connect to the AWS (S3) account.\n+            kwargs: Arbitrary keyword arguments.\n         \"\"\"\n         super().__init__(name)\n-        self.parser = parse_sql\n-        self.dialect = 's3'\n         self.connection_data = connection_data\n         self.kwargs = kwargs\n+        self.key = None\n \n         self.connection = None\n         self.is_connected = False\n \n+        self.is_select_query = False\n+        self.key = None",
    "comment": "We initialize this above too",
    "line_number": 52,
    "enriched": "File: mindsdb/integrations/handlers/s3_handler/s3_handler.py\nCode: @@ -17,129 +15,198 @@\n     RESPONSE_TYPE\n )\n \n+from mindsdb.integrations.libs.base import DatabaseHandler\n+\n \n logger = log.getLogger(__name__)\n \n+\n class S3Handler(DatabaseHandler):\n     \"\"\"\n-    This handler handles connection and execution of the S3 statements.\n+    This handler handles connection and execution of the SQL statements on AWS S3.\n     \"\"\"\n \n     name = 's3'\n+    table_name = 's3_table'\n+    # TODO: Can other file formats be supported?\n+    supported_file_formats = ['csv', 'tsv', 'json', 'parquet']\n \n-    def __init__(self, name: str, connection_data: Optional[dict], **kwargs):\n+    def __init__(self, name: Text, connection_data: Optional[Dict], **kwargs):\n         \"\"\"\n-        Initialize the handler.\n+        Initializes the handler.\n+\n         Args:\n-            name (str): name of particular handler instance\n-            connection_data (dict): parameters for connecting to the database\n-            **kwargs: arbitrary keyword arguments.\n+            name (Text): The name of the handler instance.\n+            connection_data (Dict): The connection data required to connect to the AWS (S3) account.\n+            kwargs: Arbitrary keyword arguments.\n         \"\"\"\n         super().__init__(name)\n-        self.parser = parse_sql\n-        self.dialect = 's3'\n         self.connection_data = connection_data\n         self.kwargs = kwargs\n+        self.key = None\n \n         self.connection = None\n         self.is_connected = False\n \n+        self.is_select_query = False\n+        self.key = None\nComment: We initialize this above too",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/s3_handler/s3_handler.py",
    "pr_number": 9485,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1677746365,
    "comment_created_at": "2024-07-15T12:26:41Z"
  },
  {
    "code": "@@ -1126,6 +1126,17 @@ By default, evaluation results are returned after executing the `EVALUATE KNOWLE\n \n When using `version = 'doc_id'`, the following columns are included in the evaluation results:\n \n+- `total` stores the total number of questions.\n+- `total_found` stores the number of questions to which the knowledge bases provided correct answers.\n+- `retrieved_in_top_k` stores the number of questions to which the knowledge bases provided correct answers.",
    "comment": "This column will be removed ",
    "line_number": 1131,
    "enriched": "File: docs/mindsdb_sql/knowledge-bases.mdx\nCode: @@ -1126,6 +1126,17 @@ By default, evaluation results are returned after executing the `EVALUATE KNOWLE\n \n When using `version = 'doc_id'`, the following columns are included in the evaluation results:\n \n+- `total` stores the total number of questions.\n+- `total_found` stores the number of questions to which the knowledge bases provided correct answers.\n+- `retrieved_in_top_k` stores the number of questions to which the knowledge bases provided correct answers.\nComment: This column will be removed ",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "docs/mindsdb_sql/knowledge-bases.mdx",
    "pr_number": 11080,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2143632546,
    "comment_created_at": "2025-06-12T20:49:55Z"
  },
  {
    "code": "@@ -63,6 +63,10 @@ def get_api_key(\n     if f\"{api_name.lower()}_api_key\" in api_cfg:\n         return api_cfg[f\"{api_name.lower()}_api_key\"]\n \n+    # 6\n+    if 'api_keys' in create_args and api_name in create_args['api_keys']:",
    "comment": "is this use-case used somwhere in this PR? if not, what examples of queries (or requests) activate this option? ",
    "line_number": 67,
    "enriched": "File: mindsdb/integrations/utilities/handler_utils.py\nCode: @@ -63,6 +63,10 @@ def get_api_key(\n     if f\"{api_name.lower()}_api_key\" in api_cfg:\n         return api_cfg[f\"{api_name.lower()}_api_key\"]\n \n+    # 6\n+    if 'api_keys' in create_args and api_name in create_args['api_keys']:\nComment: is this use-case used somwhere in this PR? if not, what examples of queries (or requests) activate this option? ",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/utilities/handler_utils.py",
    "pr_number": 10738,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2052290825,
    "comment_created_at": "2025-04-21T11:33:03Z"
  },
  {
    "code": "@@ -122,27 +146,34 @@ def _split_document(self, doc: Document) -> List[Document]:\n         ) for split_doc in split_docs]\n \n     def process_documents(self, documents: List[Document]) -> List[ProcessedChunk]:\n-        \"\"\"Process documents with contextual enhancement\"\"\"\n         processed_chunks = []\n \n         for doc in documents:\n-            # Split document into chunks\n             chunk_docs = self._split_document(doc)\n \n-            # Process each chunk with context\n-            for chunk_doc in chunk_docs:\n-                context = self._generate_context(chunk_doc.content, doc.content)\n-                processed_content = f\"{context}\\n\\n{chunk_doc.content}\"\n+            # Single chunk case\n+            if len(chunk_docs) == 1:\n+                context = self._generate_context(chunk_docs[0].content, doc.content)\n+                processed_content = f\"{context}\\n\\n{chunk_docs[0].content}\"\n \n-                # Need a unique ID for each document. Can track source ID in metadata.\n-                metadata = chunk_doc.metadata or doc.metadata or {}\n-                metadata['doc_id'] = doc.id\n                 processed_chunks.append(ProcessedChunk(\n-                    id=uuid4().hex,",
    "comment": "these need to be deterministic otherwise upsert won't work @tmichaeldb fyi",
    "line_number": 141,
    "enriched": "File: mindsdb/interfaces/knowledge_base/preprocessing/document_preprocessor.py\nCode: @@ -122,27 +146,34 @@ def _split_document(self, doc: Document) -> List[Document]:\n         ) for split_doc in split_docs]\n \n     def process_documents(self, documents: List[Document]) -> List[ProcessedChunk]:\n-        \"\"\"Process documents with contextual enhancement\"\"\"\n         processed_chunks = []\n \n         for doc in documents:\n-            # Split document into chunks\n             chunk_docs = self._split_document(doc)\n \n-            # Process each chunk with context\n-            for chunk_doc in chunk_docs:\n-                context = self._generate_context(chunk_doc.content, doc.content)\n-                processed_content = f\"{context}\\n\\n{chunk_doc.content}\"\n+            # Single chunk case\n+            if len(chunk_docs) == 1:\n+                context = self._generate_context(chunk_docs[0].content, doc.content)\n+                processed_content = f\"{context}\\n\\n{chunk_docs[0].content}\"\n \n-                # Need a unique ID for each document. Can track source ID in metadata.\n-                metadata = chunk_doc.metadata or doc.metadata or {}\n-                metadata['doc_id'] = doc.id\n                 processed_chunks.append(ProcessedChunk(\n-                    id=uuid4().hex,\nComment: these need to be deterministic otherwise upsert won't work @tmichaeldb fyi",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/interfaces/knowledge_base/preprocessing/document_preprocessor.py",
    "pr_number": 10210,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1860403048,
    "comment_created_at": "2024-11-27T10:25:13Z"
  },
  {
    "code": "@@ -1,254 +1,194 @@\n import os\n import time\n import pytest\n-from unittest.mock import patch\n-\n import pandas as pd\n-\n+from unittest.mock import patch\n from mindsdb_sql import parse_sql\n-\n-from tests.unit.executor_test_base import BaseExecutorTest\n from mindsdb.integrations.handlers.openai_handler.openai_handler import OpenAIHandler\n+from ..executor_test_base import BaseExecutorTest\n \n \n-OPEN_AI_API_KEY = os.environ.get(\"OPEN_AI_API_KEY\")\n-os.environ[\"OPENAI_API_KEY\"] = OPEN_AI_API_KEY\n+class TestOpenAI(BaseExecutorTest):\n+    \"\"\"Test Class for OpenAI Integration Testing\"\"\"\n \n+    @staticmethod\n+    def get_api_key():\n+        \"\"\"Retrieve OpenAI API key from environment variables\"\"\"\n+        return os.environ.get(\"OPENAI_API_KEY\")\n \n-class TestOpenAI(BaseExecutorTest):\n-    def wait_predictor(self, project, name):\n-        # wait\n-        done = False\n-        for attempt in range(200):\n+    def setup_method(self, method):\n+        \"\"\"Setup test environment, creating a project\"\"\"\n+        super().setup_method()\n+        self.run_sql(\"create database proj\")\n+\n+    def wait_predictor(self, project, name, timeout=100):\n+        \"\"\"\n+        Wait for the predictor to be created,\n+        raising an exception if predictor creation fails or exceeds timeout\n+        \"\"\"\n+        for attempt in range(timeout):\n             ret = self.run_sql(f\"select * from {project}.models where name='{name}'\")\n             if not ret.empty:\n-                if ret[\"STATUS\"][0] == \"complete\":\n-                    done = True\n-                    break\n-                elif ret[\"STATUS\"][0] == \"error\":\n-                    break\n+                status = ret[\"STATUS\"][0]\n+                if status == \"complete\":\n+                    return\n+                elif status == \"error\":\n+                    raise RuntimeError(\"Predictor failed\", ret[\"ERROR\"][0])\n             time.sleep(0.5)\n-        if not done:\n-            raise RuntimeError(\"predictor wasn't created\")\n+        raise RuntimeError(\"Predictor wasn't created\")\n \n     def run_sql(self, sql):\n+        \"\"\"Execute SQL and return a DataFrame, raising an AssertionError if an error occurs\"\"\"\n         ret = self.command_executor.execute_command(parse_sql(sql, dialect=\"mindsdb\"))\n-        assert ret.error_code is None\n+        assert ret.error_code is None, f\"SQL execution failed with error: {ret.error_code}\"\n         if ret.data is not None:\n-            columns = [col.alias if col.alias is not None else col.name for col in ret.columns]\n+            columns = [col.alias if col.alias else col.name for col in ret.columns]\n             return pd.DataFrame(ret.data, columns=columns)\n \n     def test_missing_required_keys(self):\n-        # create project\n-        self.run_sql(\"create database proj\")\n+        \"\"\"Test for missing required keys\"\"\"\n         with pytest.raises(Exception):\n             self.run_sql(\n                 f\"\"\"\n-                  create model proj.test_openai_missing_required_keys\n-                  predict answer\n-                  using\n-                    engine='openai',\n-                    openai_api_key='{OPEN_AI_API_KEY}';\n-               \"\"\"\n+                create model proj.test_openai_missing_required_keys\n+                predict answer\n+                using\n+                  engine='openai',\n+                  api_key='{self.get_api_key()}';\n+                \"\"\"\n             )\n \n     def test_invalid_openai_name_parameter(self):\n-        # create project\n-        self.run_sql(\"create database proj\")\n+        \"\"\"Test for invalid OpenAI model name parameter\"\"\"\n         self.run_sql(\n             f\"\"\"\n-              create model proj.test_openai_nonexistant_model\n-              predict answer\n-              using\n-                engine='openai',\n-                question_column='question',\n-                model_name='this-gpt-does-not-exist',\n-                openai_api_key='{OPEN_AI_API_KEY}';\n-           \"\"\"\n+            create model proj.test_openai_nonexistant_model\n+            predict answer\n+            using\n+              engine='openai',\n+              question_column='question',\n+              model_name='this-gpt-does-not-exist',\n+              api_key='{self.get_api_key()}';\n+            \"\"\"\n         )\n         with pytest.raises(Exception):\n             self.wait_predictor(\"proj\", \"test_openai_nonexistant_model\")\n \n     def test_unknown_arguments(self):\n-        self.run_sql(\"create database proj\")\n+        \"\"\"Test for unknown arguments\"\"\"\n         with pytest.raises(Exception):\n             self.run_sql(\n                 f\"\"\"\n                 create model proj.test_openai_unknown_arguments\n                 predict answer\n                 using\n-                    engine='openai',\n-                    question_column='question',\n-                    openai_api_key='{OPEN_AI_API_KEY}',\n-                    evidently_wrong_argument='wrong value';  --- this is a wrong argument name\n-            \"\"\"\n+                  engine='openai',\n+                  question_column='question',\n+                  api_key='{self.get_api_key()}',\n+                  evidently_wrong_argument='wrong value';\n+                \"\"\"\n             )\n \n     @patch(\"mindsdb.integrations.handlers.postgres_handler.Handler\")\n     def test_qa_no_context(self, mock_handler):\n-        # create project\n-        self.run_sql(\"create database proj\")\n+        \"\"\"Test for QA without context\"\"\"\n         df = pd.DataFrame.from_dict({\"question\": [\n             \"What is the capital of Sweden?\",\n             \"What is the second planet of the solar system?\"\n         ]})\n         self.set_handler(mock_handler, name=\"pg\", tables={\"df\": df})\n \n-        self.run_sql(\n-            f\"\"\"\n-           create model proj.test_openai_qa_no_context\n-           predict answer\n-           using\n-             engine='openai',\n-             question_column='question',\n-             openai_api_key='{OPEN_AI_API_KEY}';\n-        \"\"\"\n-        )\n-        self.wait_predictor(\"proj\", \"test_openai_qa_no_context\")\n-\n-        result_df = self.run_sql(\n-            \"\"\"\n-            SELECT p.answer\n-            FROM proj.test_openai_qa_no_context as p\n-            WHERE question='What is the capital of Sweden?'\n-        \"\"\"\n-        )\n-        assert \"stockholm\" in result_df[\"answer\"].iloc[0].lower()\n-\n-        result_df = self.run_sql(\n-            \"\"\"\n-            SELECT p.answer\n-            FROM pg.df as t\n-            JOIN proj.test_openai_qa_no_context as p;\n-        \"\"\"\n-        )\n-        assert \"stockholm\" in result_df[\"answer\"].iloc[0].lower()\n-        assert \"venus\" in result_df[\"answer\"].iloc[1].lower()\n+        # More Test Logic Here",
    "comment": "Looks like this was mistakenly removed?",
    "line_number": 101,
    "enriched": "File: tests/unit/ml_handlers/test_openai.py\nCode: @@ -1,254 +1,194 @@\n import os\n import time\n import pytest\n-from unittest.mock import patch\n-\n import pandas as pd\n-\n+from unittest.mock import patch\n from mindsdb_sql import parse_sql\n-\n-from tests.unit.executor_test_base import BaseExecutorTest\n from mindsdb.integrations.handlers.openai_handler.openai_handler import OpenAIHandler\n+from ..executor_test_base import BaseExecutorTest\n \n \n-OPEN_AI_API_KEY = os.environ.get(\"OPEN_AI_API_KEY\")\n-os.environ[\"OPENAI_API_KEY\"] = OPEN_AI_API_KEY\n+class TestOpenAI(BaseExecutorTest):\n+    \"\"\"Test Class for OpenAI Integration Testing\"\"\"\n \n+    @staticmethod\n+    def get_api_key():\n+        \"\"\"Retrieve OpenAI API key from environment variables\"\"\"\n+        return os.environ.get(\"OPENAI_API_KEY\")\n \n-class TestOpenAI(BaseExecutorTest):\n-    def wait_predictor(self, project, name):\n-        # wait\n-        done = False\n-        for attempt in range(200):\n+    def setup_method(self, method):\n+        \"\"\"Setup test environment, creating a project\"\"\"\n+        super().setup_method()\n+        self.run_sql(\"create database proj\")\n+\n+    def wait_predictor(self, project, name, timeout=100):\n+        \"\"\"\n+        Wait for the predictor to be created,\n+        raising an exception if predictor creation fails or exceeds timeout\n+        \"\"\"\n+        for attempt in range(timeout):\n             ret = self.run_sql(f\"select * from {project}.models where name='{name}'\")\n             if not ret.empty:\n-                if ret[\"STATUS\"][0] == \"complete\":\n-                    done = True\n-                    break\n-                elif ret[\"STATUS\"][0] == \"error\":\n-                    break\n+                status = ret[\"STATUS\"][0]\n+                if status == \"complete\":\n+                    return\n+                elif status == \"error\":\n+                    raise RuntimeError(\"Predictor failed\", ret[\"ERROR\"][0])\n             time.sleep(0.5)\n-        if not done:\n-            raise RuntimeError(\"predictor wasn't created\")\n+        raise RuntimeError(\"Predictor wasn't created\")\n \n     def run_sql(self, sql):\n+        \"\"\"Execute SQL and return a DataFrame, raising an AssertionError if an error occurs\"\"\"\n         ret = self.command_executor.execute_command(parse_sql(sql, dialect=\"mindsdb\"))\n-        assert ret.error_code is None\n+        assert ret.error_code is None, f\"SQL execution failed with error: {ret.error_code}\"\n         if ret.data is not None:\n-            columns = [col.alias if col.alias is not None else col.name for col in ret.columns]\n+            columns = [col.alias if col.alias else col.name for col in ret.columns]\n             return pd.DataFrame(ret.data, columns=columns)\n \n     def test_missing_required_keys(self):\n-        # create project\n-        self.run_sql(\"create database proj\")\n+        \"\"\"Test for missing required keys\"\"\"\n         with pytest.raises(Exception):\n             self.run_sql(\n                 f\"\"\"\n-                  create model proj.test_openai_missing_required_keys\n-                  predict answer\n-                  using\n-                    engine='openai',\n-                    openai_api_key='{OPEN_AI_API_KEY}';\n-               \"\"\"\n+                create model proj.test_openai_missing_required_keys\n+                predict answer\n+                using\n+                  engine='openai',\n+                  api_key='{self.get_api_key()}';\n+                \"\"\"\n             )\n \n     def test_invalid_openai_name_parameter(self):\n-        # create project\n-        self.run_sql(\"create database proj\")\n+        \"\"\"Test for invalid OpenAI model name parameter\"\"\"\n         self.run_sql(\n             f\"\"\"\n-              create model proj.test_openai_nonexistant_model\n-              predict answer\n-              using\n-                engine='openai',\n-                question_column='question',\n-                model_name='this-gpt-does-not-exist',\n-                openai_api_key='{OPEN_AI_API_KEY}';\n-           \"\"\"\n+            create model proj.test_openai_nonexistant_model\n+            predict answer\n+            using\n+              engine='openai',\n+              question_column='question',\n+              model_name='this-gpt-does-not-exist',\n+              api_key='{self.get_api_key()}';\n+            \"\"\"\n         )\n         with pytest.raises(Exception):\n             self.wait_predictor(\"proj\", \"test_openai_nonexistant_model\")\n \n     def test_unknown_arguments(self):\n-        self.run_sql(\"create database proj\")\n+        \"\"\"Test for unknown arguments\"\"\"\n         with pytest.raises(Exception):\n             self.run_sql(\n                 f\"\"\"\n                 create model proj.test_openai_unknown_arguments\n                 predict answer\n                 using\n-                    engine='openai',\n-                    question_column='question',\n-                    openai_api_key='{OPEN_AI_API_KEY}',\n-                    evidently_wrong_argument='wrong value';  --- this is a wrong argument name\n-            \"\"\"\n+                  engine='openai',\n+                  question_column='question',\n+                  api_key='{self.get_api_key()}',\n+                  evidently_wrong_argument='wrong value';\n+                \"\"\"\n             )\n \n     @patch(\"mindsdb.integrations.handlers.postgres_handler.Handler\")\n     def test_qa_no_context(self, mock_handler):\n-        # create project\n-        self.run_sql(\"create database proj\")\n+        \"\"\"Test for QA without context\"\"\"\n         df = pd.DataFrame.from_dict({\"question\": [\n             \"What is the capital of Sweden?\",\n             \"What is the second planet of the solar system?\"\n         ]})\n         self.set_handler(mock_handler, name=\"pg\", tables={\"df\": df})\n \n-        self.run_sql(\n-            f\"\"\"\n-           create model proj.test_openai_qa_no_context\n-           predict answer\n-           using\n-             engine='openai',\n-             question_column='question',\n-             openai_api_key='{OPEN_AI_API_KEY}';\n-        \"\"\"\n-        )\n-        self.wait_predictor(\"proj\", \"test_openai_qa_no_context\")\n-\n-        result_df = self.run_sql(\n-            \"\"\"\n-            SELECT p.answer\n-            FROM proj.test_openai_qa_no_context as p\n-            WHERE question='What is the capital of Sweden?'\n-        \"\"\"\n-        )\n-        assert \"stockholm\" in result_df[\"answer\"].iloc[0].lower()\n-\n-        result_df = self.run_sql(\n-            \"\"\"\n-            SELECT p.answer\n-            FROM pg.df as t\n-            JOIN proj.test_openai_qa_no_context as p;\n-        \"\"\"\n-        )\n-        assert \"stockholm\" in result_df[\"answer\"].iloc[0].lower()\n-        assert \"venus\" in result_df[\"answer\"].iloc[1].lower()\n+        # More Test Logic Here\nComment: Looks like this was mistakenly removed?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "tests/unit/ml_handlers/test_openai.py",
    "pr_number": 7198,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1337275546,
    "comment_created_at": "2023-09-26T14:09:01Z"
  },
  {
    "code": "@@ -0,0 +1,266 @@\n+from typing import Optional\n+from collections import OrderedDict\n+import pandas as pd\n+import phoenixdb\n+from mindsdb_sql import parse_sql\n+from mindsdb_sql.render.sqlalchemy_render import SqlalchemyRender\n+from mindsdb.integrations.libs.base import DatabaseHandler\n+from pyphoenix.sqlalchemy_phoenix import PhoenixDialect\n+from mindsdb_sql.parser.ast.base import ASTNode\n+from mindsdb.utilities import log\n+from mindsdb.integrations.libs.response import (\n+    HandlerStatusResponse as StatusResponse,\n+    HandlerResponse as Response,\n+    RESPONSE_TYPE\n+)\n+from mindsdb.integrations.libs.const import HANDLER_CONNECTION_ARG_TYPE as ARG_TYPE\n+\n+\n+class LindormHandler(DatabaseHandler):\n+    \"\"\"\n+    This handler handles connection and execution of the Apache Phoenix statements.\n+    \"\"\"\n+\n+    name = 'lindorm'\n+\n+    def __init__(self, name: str, connection_data: Optional[dict], **kwargs):\n+        \"\"\"\n+        Initialize the handler.\n+        Args:\n+            name (str): name of particular handler instance\n+            connection_data (dict): parameters for connecting to the database\n+            **kwargs: arbitrary keyword arguments.\n+        \"\"\"\n+        super().__init__(name)\n+        self.parser = parse_sql\n+        self.dialect = 'phoenix'\n+        optional_parameters = ['autocommit', 'lindorm_user', 'lindorm_password']\n+        for parameter in optional_parameters:\n+            if parameter not in connection_data:\n+                connection_data[parameter] = None\n+\n+        self.connection_data = connection_data\n+        self.kwargs = kwargs\n+\n+        self.connection = None\n+        self.is_connected = False\n+\n+    def __del__(self):\n+        if self.is_connected is True:\n+            self.disconnect()\n+\n+    def connect(self) -> StatusResponse:\n+        \"\"\"\n+        Set up the connection required by the handler.\n+        Returns:\n+            HandlerStatusResponse\n+        \"\"\"\n+\n+        if self.is_connected is True:\n+            return self.connection\n+\n+        lindorm_connection_data = {'lindorm_user': self.connection_data['lindorm_user'], 'lindorm_password': self.connection_data['lindorm_password']}\n+\n+        print(self.connection_data['url'])",
    "comment": "Can you remove this?",
    "line_number": 64,
    "enriched": "File: mindsdb/integrations/handlers/lindorm_handler/lindorm_handler.py\nCode: @@ -0,0 +1,266 @@\n+from typing import Optional\n+from collections import OrderedDict\n+import pandas as pd\n+import phoenixdb\n+from mindsdb_sql import parse_sql\n+from mindsdb_sql.render.sqlalchemy_render import SqlalchemyRender\n+from mindsdb.integrations.libs.base import DatabaseHandler\n+from pyphoenix.sqlalchemy_phoenix import PhoenixDialect\n+from mindsdb_sql.parser.ast.base import ASTNode\n+from mindsdb.utilities import log\n+from mindsdb.integrations.libs.response import (\n+    HandlerStatusResponse as StatusResponse,\n+    HandlerResponse as Response,\n+    RESPONSE_TYPE\n+)\n+from mindsdb.integrations.libs.const import HANDLER_CONNECTION_ARG_TYPE as ARG_TYPE\n+\n+\n+class LindormHandler(DatabaseHandler):\n+    \"\"\"\n+    This handler handles connection and execution of the Apache Phoenix statements.\n+    \"\"\"\n+\n+    name = 'lindorm'\n+\n+    def __init__(self, name: str, connection_data: Optional[dict], **kwargs):\n+        \"\"\"\n+        Initialize the handler.\n+        Args:\n+            name (str): name of particular handler instance\n+            connection_data (dict): parameters for connecting to the database\n+            **kwargs: arbitrary keyword arguments.\n+        \"\"\"\n+        super().__init__(name)\n+        self.parser = parse_sql\n+        self.dialect = 'phoenix'\n+        optional_parameters = ['autocommit', 'lindorm_user', 'lindorm_password']\n+        for parameter in optional_parameters:\n+            if parameter not in connection_data:\n+                connection_data[parameter] = None\n+\n+        self.connection_data = connection_data\n+        self.kwargs = kwargs\n+\n+        self.connection = None\n+        self.is_connected = False\n+\n+    def __del__(self):\n+        if self.is_connected is True:\n+            self.disconnect()\n+\n+    def connect(self) -> StatusResponse:\n+        \"\"\"\n+        Set up the connection required by the handler.\n+        Returns:\n+            HandlerStatusResponse\n+        \"\"\"\n+\n+        if self.is_connected is True:\n+            return self.connection\n+\n+        lindorm_connection_data = {'lindorm_user': self.connection_data['lindorm_user'], 'lindorm_password': self.connection_data['lindorm_password']}\n+\n+        print(self.connection_data['url'])\nComment: Can you remove this?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/lindorm_handler/lindorm_handler.py",
    "pr_number": 7955,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1374504394,
    "comment_created_at": "2023-10-27T12:25:58Z"
  },
  {
    "code": "@@ -23,21 +25,26 @@ def create_validation(self,args: Optional[dict] = None) -> None:\n         task_supported = ['regression','classification','backbone']\n         initialization_supported = ['kaiming','xavier','random']\n         args = args[\"using\"]\n-        if \"task\" not in args:\n+        #pattern for layers regex\n+        pattern = r'^\\d+-\\d+-\\d+$'\n+        if \"task\" in args:\n             if args[\"task\"] not in task_supported:\n                 raise Exception(f\"Please specify task parameter supported : {task_supported}\")\n         if \"initialization\" not in args or args[\"initialization\"] not in initialization_supported:\n             raise Exception(f\"Initialization scheme choices are : {initialization_supported}\")\n-        #if not target:\n-            #raise Exception(\"Please provide the target column\")\n+        if \"layers\" in args:\n+            if not re.match(pattern,args[\"layers\"]):\n+                raise Exception(f\"Please specify layers in format : '128-64-32'\")",
    "comment": "Out of ignorance (no experience with pytorch tabular yet), why is a formatted string preferred to a list of integers?\r\n\r\nOur SQL parser does support something like this:\r\n\r\n```sql\r\nCREATE MODEL\r\n...\r\nUSING layers = ['128', '64', '32']  -- integers also work: [128, 64, 32] \r\n```",
    "line_number": 37,
    "enriched": "File: mindsdb/integrations/handlers/pytorch_tabular_handler/pytorch_tabular_handler.py\nCode: @@ -23,21 +25,26 @@ def create_validation(self,args: Optional[dict] = None) -> None:\n         task_supported = ['regression','classification','backbone']\n         initialization_supported = ['kaiming','xavier','random']\n         args = args[\"using\"]\n-        if \"task\" not in args:\n+        #pattern for layers regex\n+        pattern = r'^\\d+-\\d+-\\d+$'\n+        if \"task\" in args:\n             if args[\"task\"] not in task_supported:\n                 raise Exception(f\"Please specify task parameter supported : {task_supported}\")\n         if \"initialization\" not in args or args[\"initialization\"] not in initialization_supported:\n             raise Exception(f\"Initialization scheme choices are : {initialization_supported}\")\n-        #if not target:\n-            #raise Exception(\"Please provide the target column\")\n+        if \"layers\" in args:\n+            if not re.match(pattern,args[\"layers\"]):\n+                raise Exception(f\"Please specify layers in format : '128-64-32'\")\nComment: Out of ignorance (no experience with pytorch tabular yet), why is a formatted string preferred to a list of integers?\r\n\r\nOur SQL parser does support something like this:\r\n\r\n```sql\r\nCREATE MODEL\r\n...\r\nUSING layers = ['128', '64', '32']  -- integers also work: [128, 64, 32] \r\n```",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/pytorch_tabular_handler/pytorch_tabular_handler.py",
    "pr_number": 7463,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1361136006,
    "comment_created_at": "2023-10-16T18:44:06Z"
  },
  {
    "code": "@@ -2,6 +2,7 @@ mindsdb>=22.6.2.1\n mindsdb_sql >= 0.4.0\n pandas <=1.4.3\n transformers==4.21.0\n+sentencepiece==0.1.97",
    "comment": "Why is this needed now? Can't tell why from the rest of the changes 🤔 ",
    "line_number": 5,
    "enriched": "File: mindsdb/integrations/handlers/huggingface_handler/requirements.txt\nCode: @@ -2,6 +2,7 @@ mindsdb>=22.6.2.1\n mindsdb_sql >= 0.4.0\n pandas <=1.4.3\n transformers==4.21.0\n+sentencepiece==0.1.97\nComment: Why is this needed now? Can't tell why from the rest of the changes 🤔 ",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/huggingface_handler/requirements.txt",
    "pr_number": 5428,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1170248072,
    "comment_created_at": "2023-04-18T15:53:23Z"
  },
  {
    "code": "@@ -25,6 +25,7 @@ storage_dir\n docker/dist/*\n .directory\n MindsDB.egg-info/*\n+mindsdb_env",
    "comment": "Hey @HahaBill,\r\nI think this might be a specific Python environment that you use?",
    "line_number": 28,
    "enriched": "File: .gitignore\nCode: @@ -25,6 +25,7 @@ storage_dir\n docker/dist/*\n .directory\n MindsDB.egg-info/*\n+mindsdb_env\nComment: Hey @HahaBill,\r\nI think this might be a specific Python environment that you use?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": ".gitignore",
    "pr_number": 7460,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1342876449,
    "comment_created_at": "2023-10-02T15:52:33Z"
  },
  {
    "code": "@@ -1,48 +1,121 @@\n-from pydantic import BaseModel, AnyUrl, model_validator\n+from typing import Optional\n+from pydantic import BaseModel, AnyUrl, TypeAdapter, model_validator, field_validator, ConfigDict\n from urllib.parse import urlparse\n \n \n+_ANY_URL_ADAPTER = TypeAdapter(AnyUrl)\n+\n+\n class ConnectionConfig(BaseModel):\n-    # TODO: For now validate AnyURL since MySQLDsn wasn't working\n-    url: AnyUrl = None\n-    host: str = None\n+    \"\"\"\n+    MySQL connection configuration with validation.\n+\n+    Supports two connection methods:\n+    1. URL-based: mysql://user:password@host:port/database\n+    2. Parameter-based: individual host, port, user, password, database params\n+    \"\"\"\n+\n+    url: Optional[AnyUrl] = None\n+    host: Optional[str] = None\n     port: int = 3306\n-    user: str = None\n-    password: str = None\n-    database: str = None\n+    user: Optional[str] = None\n+    password: Optional[str] = None\n+    database: Optional[str] = None\n+\n+    @field_validator(\"port\")\n+    @classmethod\n+    def validate_port(cls, v: int) -> int:\n+        \"\"\"Validate that port is within valid range.\"\"\"\n+        if v < 1 or v > 65535:\n+            raise ValueError(f\"Port must be between 1 and 65535, got {v}\")\n+        return v\n+\n+    @field_validator(\"url\", mode=\"before\")\n+    @classmethod\n+    def validate_url(cls, v: Optional[str]) -> Optional[AnyUrl]:\n+        \"\"\"Validate URL using AnyUrl as a fallback option for MySQL DSN parsing.\"\"\"\n+        if v is None or isinstance(v, AnyUrl):\n+            return v\n+        try:\n+            return _ANY_URL_ADAPTER.validate_python(v)\n+        except ValueError as exc:\n+            raise ValueError(f\"Invalid MySQL connection URL: {v}\") from exc\n+\n+    @field_validator(\"host\")\n+    @classmethod\n+    def validate_host(cls, v: Optional[str]) -> Optional[str]:\n+        \"\"\"Validate that host is not empty if provided.\"\"\"\n+        if v is not None and not v.strip():\n+            raise ValueError(\"Host cannot be empty string\")\n+        return v\n+\n+    @field_validator(\"database\")\n+    @classmethod\n+    def validate_database(cls, v: Optional[str]) -> Optional[str]:\n+        \"\"\"Validate that database name is not empty if provided.\"\"\"\n+        if v is not None and not v.strip():\n+            raise ValueError(\"Database name cannot be empty string\")\n+        return v\n \n     @model_validator(mode=\"before\")\n+    @classmethod\n     def check_db_params(cls, values):\n         \"\"\"Ensures either URL is provided or all individual parameters are provided.\"\"\"\n         url = values.get(\"url\")\n         host = values.get(\"host\")\n         user = values.get(\"user\")\n         password = values.get(\"password\")\n         database = values.get(\"database\")\n+\n         if not url and not (host and user and password and database):\n+            missing_params = []\n+            if not host:\n+                missing_params.append(\"host\")\n+            if not user:\n+                missing_params.append(\"user\")\n+            if not password:\n+                missing_params.append(\"password\")\n+            if not database:\n+                missing_params.append(\"database\")\n+\n             raise ValueError(\n-                \"Either a valid URL or required parameters (host, user, password, database) must be provided.\"\n+                f\"Either a valid URL or all required parameters must be provided. Missing: {', '.join(missing_params)}\"",
    "comment": "👏 ",
    "line_number": 82,
    "enriched": "File: mindsdb/integrations/handlers/mysql_handler/settings.py\nCode: @@ -1,48 +1,121 @@\n-from pydantic import BaseModel, AnyUrl, model_validator\n+from typing import Optional\n+from pydantic import BaseModel, AnyUrl, TypeAdapter, model_validator, field_validator, ConfigDict\n from urllib.parse import urlparse\n \n \n+_ANY_URL_ADAPTER = TypeAdapter(AnyUrl)\n+\n+\n class ConnectionConfig(BaseModel):\n-    # TODO: For now validate AnyURL since MySQLDsn wasn't working\n-    url: AnyUrl = None\n-    host: str = None\n+    \"\"\"\n+    MySQL connection configuration with validation.\n+\n+    Supports two connection methods:\n+    1. URL-based: mysql://user:password@host:port/database\n+    2. Parameter-based: individual host, port, user, password, database params\n+    \"\"\"\n+\n+    url: Optional[AnyUrl] = None\n+    host: Optional[str] = None\n     port: int = 3306\n-    user: str = None\n-    password: str = None\n-    database: str = None\n+    user: Optional[str] = None\n+    password: Optional[str] = None\n+    database: Optional[str] = None\n+\n+    @field_validator(\"port\")\n+    @classmethod\n+    def validate_port(cls, v: int) -> int:\n+        \"\"\"Validate that port is within valid range.\"\"\"\n+        if v < 1 or v > 65535:\n+            raise ValueError(f\"Port must be between 1 and 65535, got {v}\")\n+        return v\n+\n+    @field_validator(\"url\", mode=\"before\")\n+    @classmethod\n+    def validate_url(cls, v: Optional[str]) -> Optional[AnyUrl]:\n+        \"\"\"Validate URL using AnyUrl as a fallback option for MySQL DSN parsing.\"\"\"\n+        if v is None or isinstance(v, AnyUrl):\n+            return v\n+        try:\n+            return _ANY_URL_ADAPTER.validate_python(v)\n+        except ValueError as exc:\n+            raise ValueError(f\"Invalid MySQL connection URL: {v}\") from exc\n+\n+    @field_validator(\"host\")\n+    @classmethod\n+    def validate_host(cls, v: Optional[str]) -> Optional[str]:\n+        \"\"\"Validate that host is not empty if provided.\"\"\"\n+        if v is not None and not v.strip():\n+            raise ValueError(\"Host cannot be empty string\")\n+        return v\n+\n+    @field_validator(\"database\")\n+    @classmethod\n+    def validate_database(cls, v: Optional[str]) -> Optional[str]:\n+        \"\"\"Validate that database name is not empty if provided.\"\"\"\n+        if v is not None and not v.strip():\n+            raise ValueError(\"Database name cannot be empty string\")\n+        return v\n \n     @model_validator(mode=\"before\")\n+    @classmethod\n     def check_db_params(cls, values):\n         \"\"\"Ensures either URL is provided or all individual parameters are provided.\"\"\"\n         url = values.get(\"url\")\n         host = values.get(\"host\")\n         user = values.get(\"user\")\n         password = values.get(\"password\")\n         database = values.get(\"database\")\n+\n         if not url and not (host and user and password and database):\n+            missing_params = []\n+            if not host:\n+                missing_params.append(\"host\")\n+            if not user:\n+                missing_params.append(\"user\")\n+            if not password:\n+                missing_params.append(\"password\")\n+            if not database:\n+                missing_params.append(\"database\")\n+\n             raise ValueError(\n-                \"Either a valid URL or required parameters (host, user, password, database) must be provided.\"\n+                f\"Either a valid URL or all required parameters must be provided. Missing: {', '.join(missing_params)}\"\nComment: 👏 ",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/mysql_handler/settings.py",
    "pr_number": 11807,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2471044920,
    "comment_created_at": "2025-10-28T21:09:31Z"
  },
  {
    "code": "@@ -256,3 +263,95 @@ def get_columns(self, table_name: str) -> Response:\n                 table_name = '{table_name}'\n         \"\"\"\n         return self.native_query(query)\n+\n+    def subscribe(self, stop_event, callback, table_name, columns=None, **kwargs):\n+\n+        # psycopg2 is used\n+        import psycopg2",
    "comment": "Why not to use `psycopg`?",
    "line_number": 270,
    "enriched": "File: mindsdb/integrations/handlers/postgres_handler/postgres_handler.py\nCode: @@ -256,3 +263,95 @@ def get_columns(self, table_name: str) -> Response:\n                 table_name = '{table_name}'\n         \"\"\"\n         return self.native_query(query)\n+\n+    def subscribe(self, stop_event, callback, table_name, columns=None, **kwargs):\n+\n+        # psycopg2 is used\n+        import psycopg2\nComment: Why not to use `psycopg`?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/postgres_handler/postgres_handler.py",
    "pr_number": 9218,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1609870274,
    "comment_created_at": "2024-05-22T12:34:55Z"
  },
  {
    "code": "@@ -28,7 +28,13 @@ To enable the MCP API, you can use the [`MINDSDB_APIS` environment variable](/se\n export MINDSDB_APIS='http,mysql,mongodb,postgres,mcp'\n ```\n \n-Or, specify it when starting MindsDB:\n+Start MindsDB in a Docker container enabling the MCP API port:\n+\n+```bash\n+docker run --name mindsdb_container -e MINDSDB_APIS='http,mysql,mongodb,postgres,mcp' -p 47334:47334 -p 47335:47335 -p 47336:47336 -p 55432:55432 -p 47337:47337 mindsdb/mindsdb",
    "comment": "Maybe we run just required services as mcp and mysql",
    "line_number": 34,
    "enriched": "File: docs/mcp/overview.mdx\nCode: @@ -28,7 +28,13 @@ To enable the MCP API, you can use the [`MINDSDB_APIS` environment variable](/se\n export MINDSDB_APIS='http,mysql,mongodb,postgres,mcp'\n ```\n \n-Or, specify it when starting MindsDB:\n+Start MindsDB in a Docker container enabling the MCP API port:\n+\n+```bash\n+docker run --name mindsdb_container -e MINDSDB_APIS='http,mysql,mongodb,postgres,mcp' -p 47334:47334 -p 47335:47335 -p 47336:47336 -p 55432:55432 -p 47337:47337 mindsdb/mindsdb\nComment: Maybe we run just required services as mcp and mysql",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "docs/mcp/overview.mdx",
    "pr_number": 10648,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2027042035,
    "comment_created_at": "2025-04-03T13:49:19Z"
  },
  {
    "code": "@@ -9,9 +9,9 @@ ENV PYTHONUNBUFFERED=1\n RUN apt update && apt install -y build-essential\n \n # db2 requirement\n-RUN apt install -y libxml2 libmagic1 || true\n+RUN apt install -y libxml2 libmagic1 libkrb5-dev || true\n \n-RUN python -m pip install --prefer-binary --no-cache-dir --upgrade pip==22.1.2 && \\\n+RUN python -m pip install --prefer-binary --no-cache-dir --upgrade pip==23.1.2 && \\",
    "comment": "Should we always use the latest pip or the >= 23 version?",
    "line_number": 14,
    "enriched": "File: docker/release\nCode: @@ -9,9 +9,9 @@ ENV PYTHONUNBUFFERED=1\n RUN apt update && apt install -y build-essential\n \n # db2 requirement\n-RUN apt install -y libxml2 libmagic1 || true\n+RUN apt install -y libxml2 libmagic1 libkrb5-dev || true\n \n-RUN python -m pip install --prefer-binary --no-cache-dir --upgrade pip==22.1.2 && \\\n+RUN python -m pip install --prefer-binary --no-cache-dir --upgrade pip==23.1.2 && \\\nComment: Should we always use the latest pip or the >= 23 version?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "docker/release",
    "pr_number": 5957,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1186082238,
    "comment_created_at": "2023-05-05T13:15:20Z"
  },
  {
    "code": "@@ -48,10 +47,41 @@ class RESOURCE_GROUP:\n SERVICE_FILES_NAMES = (DIR_LOCK_FILE_NAME, DIR_LAST_MODIFIED_FILE_NAME)\n \n \n+def compare_recursive(comparison: filecmp.dircmp) -> bool:\n+    \"\"\"Check output of dircmp and return True if the directories do not differ\n+\n+    Args:\n+        comparison (filecmp.dircmp): dirs comparison\n+\n+    Returns:\n+        bool: True if dirs do not differ\n+    \"\"\"\n+    if comparison.left_only or comparison.right_only or comparison.diff_files:\n+        return False\n+    for sub_comparison in comparison.subdirs.values():\n+        if compare_recursive(sub_comparison) is False:\n+            return False\n+    return True\n+\n+\n+def compare_directories(dir1: str, dir2: str) -> bool:\n+    \"\"\"Compare two directories\n+\n+    Args:\n+        dir1 (str): dir to compare\n+        dir2 (str): dir to compare\n+\n+    Returns:\n+        bool: True if dirs do not differ\n+    \"\"\"\n+    dcmp = filecmp.dircmp(dir1, dir2)",
    "comment": "do you need to compare a content of files inside? \r\nAs I see, it compares only time of the files by default\r\nhttps://docs.python.org/3.11/library/filecmp.html#filecmp.cmpfiles",
    "line_number": 77,
    "enriched": "File: mindsdb/interfaces/storage/fs.py\nCode: @@ -48,10 +47,41 @@ class RESOURCE_GROUP:\n SERVICE_FILES_NAMES = (DIR_LOCK_FILE_NAME, DIR_LAST_MODIFIED_FILE_NAME)\n \n \n+def compare_recursive(comparison: filecmp.dircmp) -> bool:\n+    \"\"\"Check output of dircmp and return True if the directories do not differ\n+\n+    Args:\n+        comparison (filecmp.dircmp): dirs comparison\n+\n+    Returns:\n+        bool: True if dirs do not differ\n+    \"\"\"\n+    if comparison.left_only or comparison.right_only or comparison.diff_files:\n+        return False\n+    for sub_comparison in comparison.subdirs.values():\n+        if compare_recursive(sub_comparison) is False:\n+            return False\n+    return True\n+\n+\n+def compare_directories(dir1: str, dir2: str) -> bool:\n+    \"\"\"Compare two directories\n+\n+    Args:\n+        dir1 (str): dir to compare\n+        dir2 (str): dir to compare\n+\n+    Returns:\n+        bool: True if dirs do not differ\n+    \"\"\"\n+    dcmp = filecmp.dircmp(dir1, dir2)\nComment: do you need to compare a content of files inside? \r\nAs I see, it compares only time of the files by default\r\nhttps://docs.python.org/3.11/library/filecmp.html#filecmp.cmpfiles",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/interfaces/storage/fs.py",
    "pr_number": 10327,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1897933999,
    "comment_created_at": "2024-12-26T13:46:25Z"
  },
  {
    "code": "@@ -116,6 +125,10 @@ The following is an explanation of the syntax and parameters:\n * `<id_col>` is a column that contains unique identifiers of data. The knowledge base expects the `id` column name, unless specified differently in the `id_column` parameter when creating the knowledge base.\n * `<text_col>` is a column that contains the text content. The knowledge base expects the `content` column name, unless specified differently in the `content_columns` parameter when creating the knowledge base.\n \n+<Note>\n+Note that when inserting the same data into the knowledge base again, then no new rows are inserted, as the content already exists in the knowledge base.",
    "comment": "when inserted row has existed id but different content - the existed record will be updated (embeddings and content)",
    "line_number": 129,
    "enriched": "File: docs/mindsdb_sql/knowledge-bases.mdx\nCode: @@ -116,6 +125,10 @@ The following is an explanation of the syntax and parameters:\n * `<id_col>` is a column that contains unique identifiers of data. The knowledge base expects the `id` column name, unless specified differently in the `id_column` parameter when creating the knowledge base.\n * `<text_col>` is a column that contains the text content. The knowledge base expects the `content` column name, unless specified differently in the `content_columns` parameter when creating the knowledge base.\n \n+<Note>\n+Note that when inserting the same data into the knowledge base again, then no new rows are inserted, as the content already exists in the knowledge base.\nComment: when inserted row has existed id but different content - the existed record will be updated (embeddings and content)",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "docs/mindsdb_sql/knowledge-bases.mdx",
    "pr_number": 10746,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2055538370,
    "comment_created_at": "2025-04-23T08:34:07Z"
  },
  {
    "code": "@@ -0,0 +1,82 @@\n+\"\"\"backfill_agent_id\n+\n+Revision ID: 011e6f2dd9c2\n+Revises: f16d4ab03091\n+Create Date: 2023-09-18 11:02:36.795544\n+\n+\"\"\"\n+from alembic import op\n+import datetime\n+import sqlalchemy as sa\n+\n+\n+# revision identifiers, used by Alembic.\n+revision = '011e6f2dd9c2'\n+down_revision = 'f16d4ab03091'\n+branch_labels = None\n+depends_on = None\n+\n+\n+def upgrade():\n+    conn = op.get_bind()\n+    chatbots_table = sa.Table(\n+        'chat_bots',\n+        sa.MetaData(),\n+        sa.Column('id', sa.Integer()),\n+        sa.Column('project_id', sa.Integer()),\n+        sa.Column('agent_id', sa.Integer()),\n+        sa.Column('name', sa.String()),\n+        sa.Column('model_name', sa.String())\n+    )\n+\n+    agents_table = sa.Table(\n+        'agents',\n+        sa.MetaData(),\n+        sa.Column('id', sa.Integer()),\n+        sa.Column('company_id', sa.Integer()),\n+        sa.Column('user_class', sa.Integer()),\n+        sa.Column('name', sa.String()),\n+        sa.Column('project_id', sa.Integer()),\n+        sa.Column('model_name', sa.String()),\n+        sa.Column('updated_at', sa.DateTime()),\n+        sa.Column('created_at', sa.DateTime())\n+    )\n+\n+    tasks_table = sa.Table(\n+        'tasks',\n+        sa.MetaData(),\n+        sa.Column('company_id', sa.Integer()),\n+        sa.Column('user_class', sa.Integer()),\n+        sa.Column('object_type', sa.String()),\n+        sa.Column('object_id', sa.Integer())\n+    )\n+\n+    all_chatbots = conn.execute(chatbots_table.select()).fetchall()\n+    for chatbot_row in all_chatbots:\n+        id, project_id, _, name, model_name = chatbot_row\n+\n+        # Get the corresponding task.\n+        task_select = tasks_table.select().where(tasks_table.c.object_type == 'chatbot').where(tasks_table.c.object_id == id)\n+        task_row = conn.execute(task_select).first()\n+        if task_row is None:\n+            continue\n+        company_id, user_class, _, _ = task_row\n+        # Create the new agent.\n+        op.execute(agents_table.insert().values(\n+            company_id=company_id,\n+            user_class=user_class,\n+            name=name,\n+            project_id=project_id,\n+            model_name=model_name,",
    "comment": "the purpose to create agents is to move model_name from chatbot, right? if so and if we keep model_name in chatbot, are we going to copy this column once again in future?",
    "line_number": 70,
    "enriched": "File: mindsdb/migrations/versions/2023-09-18_011e6f2dd9c2_backfill_agent_id.py\nCode: @@ -0,0 +1,82 @@\n+\"\"\"backfill_agent_id\n+\n+Revision ID: 011e6f2dd9c2\n+Revises: f16d4ab03091\n+Create Date: 2023-09-18 11:02:36.795544\n+\n+\"\"\"\n+from alembic import op\n+import datetime\n+import sqlalchemy as sa\n+\n+\n+# revision identifiers, used by Alembic.\n+revision = '011e6f2dd9c2'\n+down_revision = 'f16d4ab03091'\n+branch_labels = None\n+depends_on = None\n+\n+\n+def upgrade():\n+    conn = op.get_bind()\n+    chatbots_table = sa.Table(\n+        'chat_bots',\n+        sa.MetaData(),\n+        sa.Column('id', sa.Integer()),\n+        sa.Column('project_id', sa.Integer()),\n+        sa.Column('agent_id', sa.Integer()),\n+        sa.Column('name', sa.String()),\n+        sa.Column('model_name', sa.String())\n+    )\n+\n+    agents_table = sa.Table(\n+        'agents',\n+        sa.MetaData(),\n+        sa.Column('id', sa.Integer()),\n+        sa.Column('company_id', sa.Integer()),\n+        sa.Column('user_class', sa.Integer()),\n+        sa.Column('name', sa.String()),\n+        sa.Column('project_id', sa.Integer()),\n+        sa.Column('model_name', sa.String()),\n+        sa.Column('updated_at', sa.DateTime()),\n+        sa.Column('created_at', sa.DateTime())\n+    )\n+\n+    tasks_table = sa.Table(\n+        'tasks',\n+        sa.MetaData(),\n+        sa.Column('company_id', sa.Integer()),\n+        sa.Column('user_class', sa.Integer()),\n+        sa.Column('object_type', sa.String()),\n+        sa.Column('object_id', sa.Integer())\n+    )\n+\n+    all_chatbots = conn.execute(chatbots_table.select()).fetchall()\n+    for chatbot_row in all_chatbots:\n+        id, project_id, _, name, model_name = chatbot_row\n+\n+        # Get the corresponding task.\n+        task_select = tasks_table.select().where(tasks_table.c.object_type == 'chatbot').where(tasks_table.c.object_id == id)\n+        task_row = conn.execute(task_select).first()\n+        if task_row is None:\n+            continue\n+        company_id, user_class, _, _ = task_row\n+        # Create the new agent.\n+        op.execute(agents_table.insert().values(\n+            company_id=company_id,\n+            user_class=user_class,\n+            name=name,\n+            project_id=project_id,\n+            model_name=model_name,\nComment: the purpose to create agents is to move model_name from chatbot, right? if so and if we keep model_name in chatbot, are we going to copy this column once again in future?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/migrations/versions/2023-09-18_011e6f2dd9c2_backfill_agent_id.py",
    "pr_number": 7363,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1330046854,
    "comment_created_at": "2023-09-19T12:28:05Z"
  },
  {
    "code": "@@ -202,7 +245,7 @@ def select(self, query: ast.Select) -> pd.DataFrame:\n \n     def get_channel_details(self, channel_id):\n         details = (\n-            self.handler.connect().channels().list(part=\"statistics,snippet,contentDetails\", id=channel_id).execute()\n+            self.handler.connect.channels().list(part=\"statistics,snippet,contentDetails\", id=channel_id).execute()",
    "comment": "Why have the parenthesis been removed here when calling the `connect()` method?",
    "line_number": 248,
    "enriched": "File: mindsdb/integrations/handlers/youtube_handler/youtube_tables.py\nCode: @@ -202,7 +245,7 @@ def select(self, query: ast.Select) -> pd.DataFrame:\n \n     def get_channel_details(self, channel_id):\n         details = (\n-            self.handler.connect().channels().list(part=\"statistics,snippet,contentDetails\", id=channel_id).execute()\n+            self.handler.connect.channels().list(part=\"statistics,snippet,contentDetails\", id=channel_id).execute()\nComment: Why have the parenthesis been removed here when calling the `connect()` method?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/youtube_handler/youtube_tables.py",
    "pr_number": 8203,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1388952660,
    "comment_created_at": "2023-11-10T06:09:45Z"
  },
  {
    "code": "@@ -119,26 +133,49 @@ def native_query(self, query: str) -> Response:\n \n     def query(self, query: ASTNode) -> Response:\n         \"\"\"\n-        Retrieve the data from the SQL statement with eliminated rows that dont satisfy the WHERE condition\n+        Executes a SQL query represented by an ASTNode and retrieves the data.\n+\n+        Args:\n+            query (ASTNode): An ASTNode representing the SQL query to be executed.\n+\n+        Returns:\n+            Response: The response from the `native_query` method, containing the result of the SQL query execution.\n         \"\"\"\n         renderer = SqlalchemyRender(BigQueryDialect)\n         query_str = renderer.get_string(query, with_failback=True)\n         return self.native_query(query_str)\n \n     def get_tables(self) -> Response:\n         \"\"\"\n-        Get a list with all of the tabels in BigQuery\n+        Retrieves a list of all non-system tables and views in the configured dataset of the BigQuery warehouse.\n+\n+        Returns:\n+            Response: A response object containing the list of tables and views, formatted as per the `Response` class.\n+        \"\"\"\n+        query = f\"\"\"\n+            SELECT table_name, table_schema, table_type,",
    "comment": "This shouldn't be an issue but better to remove the comma after `table_type`",
    "line_number": 156,
    "enriched": "File: mindsdb/integrations/handlers/bigquery_handler/bigquery_handler.py\nCode: @@ -119,26 +133,49 @@ def native_query(self, query: str) -> Response:\n \n     def query(self, query: ASTNode) -> Response:\n         \"\"\"\n-        Retrieve the data from the SQL statement with eliminated rows that dont satisfy the WHERE condition\n+        Executes a SQL query represented by an ASTNode and retrieves the data.\n+\n+        Args:\n+            query (ASTNode): An ASTNode representing the SQL query to be executed.\n+\n+        Returns:\n+            Response: The response from the `native_query` method, containing the result of the SQL query execution.\n         \"\"\"\n         renderer = SqlalchemyRender(BigQueryDialect)\n         query_str = renderer.get_string(query, with_failback=True)\n         return self.native_query(query_str)\n \n     def get_tables(self) -> Response:\n         \"\"\"\n-        Get a list with all of the tabels in BigQuery\n+        Retrieves a list of all non-system tables and views in the configured dataset of the BigQuery warehouse.\n+\n+        Returns:\n+            Response: A response object containing the list of tables and views, formatted as per the `Response` class.\n+        \"\"\"\n+        query = f\"\"\"\n+            SELECT table_name, table_schema, table_type,\nComment: This shouldn't be an issue but better to remove the comma after `table_type`",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/bigquery_handler/bigquery_handler.py",
    "pr_number": 9310,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1642927504,
    "comment_created_at": "2024-06-17T14:39:22Z"
  },
  {
    "code": "@@ -72,11 +70,10 @@ def adapt_query(node, is_table, **kwargs):\n         )\n         query_str = render.get_string(query_ast, with_failback=True)\n \n-    # workaround to prevent duckdb.TypeMismatchException: serialize and deserialize with feather\n+    # workaround to prevent duckdb.TypeMismatchException\n     if len(df) > 0 and table_name.lower() in ('models', 'predictors'):\n-        fd = io.BytesIO()\n-        df.to_feather(fd)\n-        df = pd.read_feather(fd)\n+        if 'TRAINING_OPTIONS' in df.columns:",
    "comment": "This effectively fixes the issue with nested dicts, but how about the three lines you've removed here? Shouldn't they stay? If not, then feel free to merge 👍 ",
    "line_number": 75,
    "enriched": "File: mindsdb/api/mysql/mysql_proxy/utilities/sql.py\nCode: @@ -72,11 +70,10 @@ def adapt_query(node, is_table, **kwargs):\n         )\n         query_str = render.get_string(query_ast, with_failback=True)\n \n-    # workaround to prevent duckdb.TypeMismatchException: serialize and deserialize with feather\n+    # workaround to prevent duckdb.TypeMismatchException\n     if len(df) > 0 and table_name.lower() in ('models', 'predictors'):\n-        fd = io.BytesIO()\n-        df.to_feather(fd)\n-        df = pd.read_feather(fd)\n+        if 'TRAINING_OPTIONS' in df.columns:\nComment: This effectively fixes the issue with nested dicts, but how about the three lines you've removed here? Shouldn't they stay? If not, then feel free to merge 👍 ",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/api/mysql/mysql_proxy/utilities/sql.py",
    "pr_number": 6396,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1212341354,
    "comment_created_at": "2023-05-31T21:35:57Z"
  },
  {
    "code": "@@ -0,0 +1,75 @@\n+---\n+title: Portkey\n+sidebarTitle: Portkey\n+---\n+\n+This documentation describes the integration of MindsDB with [Portkey](https://www.portkey.com/), an AI Gateway that allows developers to connect to All the AI models in the world with a single API.\n+Portkey also brings in observability, caching, and other features that are useful for building production-grade AI applications.\n+\n+## Prerequisites\n+\n+Before proceeding, ensure the following prerequisites are met:\n+\n+1. Install MindsDB locally via [Docker](https://docs.mindsdb.com/setup/self-hosted/docker) or [Docker Desktop](https://docs.mindsdb.com/setup/self-hosted/docker-desktop).\n+2. To use Portkey within MindsDB, install the required dependencies following [this instruction](https://docs.mindsdb.com/setup/self-hosted/docker#install-dependencies).\n+3. Obtain the Portkey API key required to deploy and use Portkey within MindsDB. Follow the [instructions for obtaining the API key](https://docs.portkey.ai/docs/api-reference/introduction).\n+\n+## Setup\n+\n+Create an AI engine from the [Portkey handler](https://github.com/mindsdb/mindsdb/tree/main/mindsdb/integrations/handlers/portkey_handler).\n+\n+<Info>\n+You can pass all the parameters that are supported by Portkey inside the `USING` clause.\n+</Info>\n+\n+\n+```sql\n+CREATE ML_ENGINE portkey_engine\n+FROM portkey\n+USING\n+    portkey_api_key = '{PORTKEY_API_KEY}',\n+    config = '{PORTKEY_CONFIG_ID}';",
    "comment": "What exactly is this config? Is it something specific to Portkey? If so, can we mention it here?",
    "line_number": 31,
    "enriched": "File: docs/integrations/ai-engines/portkey.mdx\nCode: @@ -0,0 +1,75 @@\n+---\n+title: Portkey\n+sidebarTitle: Portkey\n+---\n+\n+This documentation describes the integration of MindsDB with [Portkey](https://www.portkey.com/), an AI Gateway that allows developers to connect to All the AI models in the world with a single API.\n+Portkey also brings in observability, caching, and other features that are useful for building production-grade AI applications.\n+\n+## Prerequisites\n+\n+Before proceeding, ensure the following prerequisites are met:\n+\n+1. Install MindsDB locally via [Docker](https://docs.mindsdb.com/setup/self-hosted/docker) or [Docker Desktop](https://docs.mindsdb.com/setup/self-hosted/docker-desktop).\n+2. To use Portkey within MindsDB, install the required dependencies following [this instruction](https://docs.mindsdb.com/setup/self-hosted/docker#install-dependencies).\n+3. Obtain the Portkey API key required to deploy and use Portkey within MindsDB. Follow the [instructions for obtaining the API key](https://docs.portkey.ai/docs/api-reference/introduction).\n+\n+## Setup\n+\n+Create an AI engine from the [Portkey handler](https://github.com/mindsdb/mindsdb/tree/main/mindsdb/integrations/handlers/portkey_handler).\n+\n+<Info>\n+You can pass all the parameters that are supported by Portkey inside the `USING` clause.\n+</Info>\n+\n+\n+```sql\n+CREATE ML_ENGINE portkey_engine\n+FROM portkey\n+USING\n+    portkey_api_key = '{PORTKEY_API_KEY}',\n+    config = '{PORTKEY_CONFIG_ID}';\nComment: What exactly is this config? Is it something specific to Portkey? If so, can we mention it here?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "docs/integrations/ai-engines/portkey.mdx",
    "pr_number": 9641,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1785712461,
    "comment_created_at": "2024-10-03T06:48:28Z"
  },
  {
    "code": "@@ -0,0 +1,309 @@\n+from dataclasses import dataclass\n+import copy\n+from typing import List, Optional, Union\n+\n+from mindsdb_sql_parser.ast import BinaryOperation, Identifier, Constant, UnaryOperation, Select, Star, Tuple, ASTNode\n+import pandas as pd\n+\n+from mindsdb.integrations.utilities.query_traversal import query_traversal\n+\n+\n+@dataclass\n+class ConditionBlock:\n+    op: str\n+    items: list\n+\n+\n+class KnowledgeBaseQueryExecutor:\n+    def __init__(self, kb, content_column=\"content\", id_column=\"chunk_id\"):\n+        self.kb = kb\n+        self.content_column = content_column.lower()\n+        self.id_column = id_column\n+        self.limit = None\n+        self._negative_set_size = 100\n+\n+    def is_content_condition(self, node: ASTNode) -> bool:\n+        \"\"\"\n+        Checks if the node is a condition to Content column\n+\n+        :param node: condition to check\n+        \"\"\"\n+        if isinstance(node, BinaryOperation):\n+            if isinstance(node.args[0], Identifier):\n+                parts = node.args[0].parts\n+                if len(parts) == 1 and parts[0].lower() == self.content_column:\n+                    return True\n+        return False\n+\n+    @staticmethod\n+    def invert_content_op(node: BinaryOperation) -> BinaryOperation:\n+        # Change operator of binary operation to opposite one\n+        op_map = {\"=\": \"!=\", \"!=\": \"=\", \"LIKE\": \"!=\", \"NOT LIKE\": \"=\", \"IN\": \"NOT IN\"}",
    "comment": "should {\"NOT IN\": \"IN\"}  be here?",
    "line_number": 41,
    "enriched": "File: mindsdb/interfaces/knowledge_base/executor.py\nCode: @@ -0,0 +1,309 @@\n+from dataclasses import dataclass\n+import copy\n+from typing import List, Optional, Union\n+\n+from mindsdb_sql_parser.ast import BinaryOperation, Identifier, Constant, UnaryOperation, Select, Star, Tuple, ASTNode\n+import pandas as pd\n+\n+from mindsdb.integrations.utilities.query_traversal import query_traversal\n+\n+\n+@dataclass\n+class ConditionBlock:\n+    op: str\n+    items: list\n+\n+\n+class KnowledgeBaseQueryExecutor:\n+    def __init__(self, kb, content_column=\"content\", id_column=\"chunk_id\"):\n+        self.kb = kb\n+        self.content_column = content_column.lower()\n+        self.id_column = id_column\n+        self.limit = None\n+        self._negative_set_size = 100\n+\n+    def is_content_condition(self, node: ASTNode) -> bool:\n+        \"\"\"\n+        Checks if the node is a condition to Content column\n+\n+        :param node: condition to check\n+        \"\"\"\n+        if isinstance(node, BinaryOperation):\n+            if isinstance(node.args[0], Identifier):\n+                parts = node.args[0].parts\n+                if len(parts) == 1 and parts[0].lower() == self.content_column:\n+                    return True\n+        return False\n+\n+    @staticmethod\n+    def invert_content_op(node: BinaryOperation) -> BinaryOperation:\n+        # Change operator of binary operation to opposite one\n+        op_map = {\"=\": \"!=\", \"!=\": \"=\", \"LIKE\": \"!=\", \"NOT LIKE\": \"=\", \"IN\": \"NOT IN\"}\nComment: should {\"NOT IN\": \"IN\"}  be here?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/interfaces/knowledge_base/executor.py",
    "pr_number": 11132,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2163694258,
    "comment_created_at": "2025-06-24T11:20:30Z"
  },
  {
    "code": "@@ -1285,14 +1299,23 @@ def answer_create_kb(self, statement: CreateKnowledgeBase):\n         embedding_model_id = model_record[\"model_record\"].id\n \n         # search for the vector database table\n-        if len(statement.storage.parts) < 2:\n+        if statement.storage and len(statement.storage.parts) < 2:\n             raise SqlApiException(\n                 f\"Invalid vectordatabase table name: {statement.storage}\"\n                 \"Need the form 'database_name.table_name'\"\n             )\n \n-        vector_db_name = statement.storage.parts[0]\n-        vector_table_name = statement.storage.parts[-1]\n+        vector_table_name = (\n+            statement.storage.parts[-1] if statement.storage else \"default_collection\"\n+        )\n+\n+        vector_db_name = (",
    "comment": "If I'm understanding this correctly, we're always creating a persistent Chroma if `statement.storage` isn't specified. However, we don't want to do that in Cloud, only locally. This is because we plan to use PostgreSQL with `pgvector` instead for Cloud (to be implemented).\r\n\r\nSo we would need to check if we're running in the Cloud:\r\n`is_cloud = self.session.config.get('cloud', False)`",
    "line_number": 1312,
    "enriched": "File: mindsdb/api/mysql/mysql_proxy/executor/executor_commands.py\nCode: @@ -1285,14 +1299,23 @@ def answer_create_kb(self, statement: CreateKnowledgeBase):\n         embedding_model_id = model_record[\"model_record\"].id\n \n         # search for the vector database table\n-        if len(statement.storage.parts) < 2:\n+        if statement.storage and len(statement.storage.parts) < 2:\n             raise SqlApiException(\n                 f\"Invalid vectordatabase table name: {statement.storage}\"\n                 \"Need the form 'database_name.table_name'\"\n             )\n \n-        vector_db_name = statement.storage.parts[0]\n-        vector_table_name = statement.storage.parts[-1]\n+        vector_table_name = (\n+            statement.storage.parts[-1] if statement.storage else \"default_collection\"\n+        )\n+\n+        vector_db_name = (\nComment: If I'm understanding this correctly, we're always creating a persistent Chroma if `statement.storage` isn't specified. However, we don't want to do that in Cloud, only locally. This is because we plan to use PostgreSQL with `pgvector` instead for Cloud (to be implemented).\r\n\r\nSo we would need to check if we're running in the Cloud:\r\n`is_cloud = self.session.config.get('cloud', False)`",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/api/mysql/mysql_proxy/executor/executor_commands.py",
    "pr_number": 8015,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1369362416,
    "comment_created_at": "2023-10-23T23:12:44Z"
  },
  {
    "code": "@@ -108,18 +108,36 @@ def _get_chat_history(self, chat_id, table_name):\n         time_col = t_params['time_col']\n         chat_id_cols = t_params['chat_id_col'] if isinstance(t_params['chat_id_col'], list) else [t_params['chat_id_col']]\n \n-        ast_query = Select(\n-            targets=[Identifier(text_col),\n-                     Identifier(username_col),\n-                     Identifier(time_col)],\n-            from_table=Identifier(t_params['name']),\n-            where=[BinaryOperation(\n+        chat_id = chat_id if isinstance(chat_id, tuple) else (chat_id,)\n+        # Add a WHERE clause for each chat_id column.\n+        where_conditions = [",
    "comment": "as I know where have to be ASTNode object, not array:\r\nwhere=BinaryOperation(op='and',\r\n      args=[\r\n          BinaryOperation(op='=', args=[Identifier(text_col), Constant(1)]),\r\n          BinaryOperation(op='=', args=[Identifier(text_col), Constant(1)]),\r\n      ]\r\n  )",
    "line_number": 113,
    "enriched": "File: mindsdb/interfaces/chatbot/memory.py\nCode: @@ -108,18 +108,36 @@ def _get_chat_history(self, chat_id, table_name):\n         time_col = t_params['time_col']\n         chat_id_cols = t_params['chat_id_col'] if isinstance(t_params['chat_id_col'], list) else [t_params['chat_id_col']]\n \n-        ast_query = Select(\n-            targets=[Identifier(text_col),\n-                     Identifier(username_col),\n-                     Identifier(time_col)],\n-            from_table=Identifier(t_params['name']),\n-            where=[BinaryOperation(\n+        chat_id = chat_id if isinstance(chat_id, tuple) else (chat_id,)\n+        # Add a WHERE clause for each chat_id column.\n+        where_conditions = [\nComment: as I know where have to be ASTNode object, not array:\r\nwhere=BinaryOperation(op='and',\r\n      args=[\r\n          BinaryOperation(op='=', args=[Identifier(text_col), Constant(1)]),\r\n          BinaryOperation(op='=', args=[Identifier(text_col), Constant(1)]),\r\n      ]\r\n  )",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/interfaces/chatbot/memory.py",
    "pr_number": 10286,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1897399627,
    "comment_created_at": "2024-12-25T15:09:31Z"
  },
  {
    "code": "@@ -1 +1,2 @@\n google-api-python-client\n+youtube_transcript_api",
    "comment": "Shouldn't it be with `-` instead of `_`? Like this, `youtube-transcript-api`.",
    "line_number": 2,
    "enriched": "File: mindsdb/integrations/handlers/youtube_handler/requirements.txt\nCode: @@ -1 +1,2 @@\n google-api-python-client\n+youtube_transcript_api\nComment: Shouldn't it be with `-` instead of `_`? Like this, `youtube-transcript-api`.",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/youtube_handler/requirements.txt",
    "pr_number": 7918,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1366746611,
    "comment_created_at": "2023-10-20T09:50:30Z"
  },
  {
    "code": "@@ -495,7 +495,7 @@ def raise_warnings(self, logger) -> None:\n \n         for env_name in ('MINDSDB_HTTP_SERVER_TYPE', 'MINDSDB_DEFAULT_SERVER'):\n             env_value = os.environ.get(env_name, '')\n-            if env_value.lower() not in ('waitress', 'flask', 'gunicorn'):\n+            if env_value.lower() not in ('waitress', 'flask', 'gunicorn', ''):",
    "comment": "does adding of '' change the behavior?  ",
    "line_number": 498,
    "enriched": "File: mindsdb/utilities/config.py\nCode: @@ -495,7 +495,7 @@ def raise_warnings(self, logger) -> None:\n \n         for env_name in ('MINDSDB_HTTP_SERVER_TYPE', 'MINDSDB_DEFAULT_SERVER'):\n             env_value = os.environ.get(env_name, '')\n-            if env_value.lower() not in ('waitress', 'flask', 'gunicorn'):\n+            if env_value.lower() not in ('waitress', 'flask', 'gunicorn', ''):\nComment: does adding of '' change the behavior?  ",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/utilities/config.py",
    "pr_number": 10497,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1987171640,
    "comment_created_at": "2025-03-10T12:17:42Z"
  },
  {
    "code": "@@ -311,15 +313,37 @@ def get_completion(\n         Raises:\n             ValueError: Agent's model does not exist.\n         '''\n+        # Log the human question\n+        human_question = \"\\n\".join(f\"{key}: {value}\" for message in messages for key, value in message.items())\n+        self.save_agent_history(\n+            agent_id=agent.id,\n+            type='question',\n+            text=human_question,\n+            sender_type='Human',",
    "comment": "are not `sender_type` and `type` duplicated? \r\nHuman sends question, AI - completion",
    "line_number": 322,
    "enriched": "File: mindsdb/interfaces/agents/agents_controller.py\nCode: @@ -311,15 +313,37 @@ def get_completion(\n         Raises:\n             ValueError: Agent's model does not exist.\n         '''\n+        # Log the human question\n+        human_question = \"\\n\".join(f\"{key}: {value}\" for message in messages for key, value in message.items())\n+        self.save_agent_history(\n+            agent_id=agent.id,\n+            type='question',\n+            text=human_question,\n+            sender_type='Human',\nComment: are not `sender_type` and `type` duplicated? \r\nHuman sends question, AI - completion",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/interfaces/agents/agents_controller.py",
    "pr_number": 9591,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1711554833,
    "comment_created_at": "2024-08-09T14:17:18Z"
  },
  {
    "code": "@@ -0,0 +1,83 @@\n+import smtplib\n+import imaplib\n+import email\n+from email.mime.multipart import MIMEMultipart\n+from email.mime.text import MIMEText\n+import pandas as pd\n+\n+\n+class EmailClient:\n+    def __init__(self, email, password, smtp_server='smtp.gmail.com', smtp_port=587, imap_server=\"imap.gmail.com\"):\n+        self.email = email\n+        self.password = password\n+        self.smtp_server = smtplib.SMTP(smtp_server, smtp_port)\n+        self.imap_server = imaplib.IMAP4_SSL(imap_server)\n+\n+    def send_email(self, to_addr, subject, body):\n+        msg = MIMEMultipart()\n+        msg['From'] = self.email\n+        msg['To'] = to_addr",
    "comment": "Is this working with multiple recipients?",
    "line_number": 19,
    "enriched": "File: mindsdb/integrations/handlers/email_handler/email_helpers.py\nCode: @@ -0,0 +1,83 @@\n+import smtplib\n+import imaplib\n+import email\n+from email.mime.multipart import MIMEMultipart\n+from email.mime.text import MIMEText\n+import pandas as pd\n+\n+\n+class EmailClient:\n+    def __init__(self, email, password, smtp_server='smtp.gmail.com', smtp_port=587, imap_server=\"imap.gmail.com\"):\n+        self.email = email\n+        self.password = password\n+        self.smtp_server = smtplib.SMTP(smtp_server, smtp_port)\n+        self.imap_server = imaplib.IMAP4_SSL(imap_server)\n+\n+    def send_email(self, to_addr, subject, body):\n+        msg = MIMEMultipart()\n+        msg['From'] = self.email\n+        msg['To'] = to_addr\nComment: Is this working with multiple recipients?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/email_handler/email_helpers.py",
    "pr_number": 6931,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1273373741,
    "comment_created_at": "2023-07-25T11:09:01Z"
  },
  {
    "code": "@@ -0,0 +1,198 @@\n+from collections import OrderedDict\n+\n+import pandas as pd\n+import psycopg\n+from mindsdb_sql import ASTNode, CreateTable, Insert, Select\n+\n+from mindsdb.integrations.handlers.postgres_handler.postgres_handler import (\n+    PostgresHandler,\n+)\n+from mindsdb.integrations.libs.base import VectorStoreHandler\n+from mindsdb.integrations.libs.const import HANDLER_CONNECTION_ARG_TYPE as ARG_TYPE\n+from mindsdb.integrations.libs.response import RESPONSE_TYPE\n+from mindsdb.integrations.libs.response import HandlerResponse as Response\n+from mindsdb.utilities import log\n+from mindsdb.utilities.profiler import profiler\n+\n+\n+class PgVectorHandler(PostgresHandler, VectorStoreHandler):\n+    \"\"\"This handler handles connection and execution of the PostgreSQL with pgvector extension statements.\"\"\"\n+\n+    name = \"pgvector\"\n+\n+    def __init__(self, name: str, **kwargs):\n+\n+        super().__init__(name=name, **kwargs)\n+\n+    @profiler.profile()\n+    def connect(self):\n+        \"\"\"\n+        Handles the connection to a PostgreSQL database instance.\n+        \"\"\"\n+        if self.is_connected is True:\n+            return self.connection\n+\n+        config = {\n+            \"host\": self.connection_args.get(\"host\"),\n+            \"port\": self.connection_args.get(\"port\"),\n+            \"user\": self.connection_args.get(\"user\"),\n+            \"password\": self.connection_args.get(\"password\"),\n+            \"dbname\": self.connection_args.get(\"database\"),\n+        }\n+\n+        if self.connection_args.get(\"sslmode\"):\n+            config[\"sslmode\"] = self.connection_args.get(\"sslmode\")\n+\n+        if self.connection_args.get(\"schema\"):\n+            config[\n+                \"options\"\n+            ] = f'-c search_path={self.connection_args.get(\"schema\")},public'\n+\n+        connection = psycopg.connect(**config, connect_timeout=10)\n+\n+        self.is_connected = True\n+        self.connection = connection\n+\n+        with self.connection.cursor() as cur:\n+            try:\n+                # load pg_vector extension\n+                cur.execute(\"CREATE EXTENSION IF NOT EXISTS vector\")\n+                self.connection.commit()\n+\n+            except psycopg.Error as e:\n+                log.logger.error(\n+                    f\"Error loading pg_vector extension, ensure you have installed it before running, {e}!\"\n+                )\n+\n+        return self.connection\n+\n+    def similarity_search(self, query: ASTNode) -> Response:\n+        \"\"\"\n+        Run a select query on the vectorpg database using the <-> operator.\n+        \"\"\"\n+\n+        collection_name = query.from_table.parts[-1]\n+\n+        with self.connection.cursor() as cur:\n+            try:\n+                # convert search embedding to string\n+                string_embeddings_search = str(query.where.args[1].items[0].value)\n+                # get limit from query\n+                limit = query.limit.value if query.limit else 5\n+                # we need to use the <-> operator to search for similar vectors,\n+                # so we need to convert the string to a vector and also use a threshold (e.g. 0.5)",
    "comment": "Shouldn't this threshold be configurable through `using`?",
    "line_number": 83,
    "enriched": "File: mindsdb/integrations/handlers/pgvector_handler/pgvector_handler.py\nCode: @@ -0,0 +1,198 @@\n+from collections import OrderedDict\n+\n+import pandas as pd\n+import psycopg\n+from mindsdb_sql import ASTNode, CreateTable, Insert, Select\n+\n+from mindsdb.integrations.handlers.postgres_handler.postgres_handler import (\n+    PostgresHandler,\n+)\n+from mindsdb.integrations.libs.base import VectorStoreHandler\n+from mindsdb.integrations.libs.const import HANDLER_CONNECTION_ARG_TYPE as ARG_TYPE\n+from mindsdb.integrations.libs.response import RESPONSE_TYPE\n+from mindsdb.integrations.libs.response import HandlerResponse as Response\n+from mindsdb.utilities import log\n+from mindsdb.utilities.profiler import profiler\n+\n+\n+class PgVectorHandler(PostgresHandler, VectorStoreHandler):\n+    \"\"\"This handler handles connection and execution of the PostgreSQL with pgvector extension statements.\"\"\"\n+\n+    name = \"pgvector\"\n+\n+    def __init__(self, name: str, **kwargs):\n+\n+        super().__init__(name=name, **kwargs)\n+\n+    @profiler.profile()\n+    def connect(self):\n+        \"\"\"\n+        Handles the connection to a PostgreSQL database instance.\n+        \"\"\"\n+        if self.is_connected is True:\n+            return self.connection\n+\n+        config = {\n+            \"host\": self.connection_args.get(\"host\"),\n+            \"port\": self.connection_args.get(\"port\"),\n+            \"user\": self.connection_args.get(\"user\"),\n+            \"password\": self.connection_args.get(\"password\"),\n+            \"dbname\": self.connection_args.get(\"database\"),\n+        }\n+\n+        if self.connection_args.get(\"sslmode\"):\n+            config[\"sslmode\"] = self.connection_args.get(\"sslmode\")\n+\n+        if self.connection_args.get(\"schema\"):\n+            config[\n+                \"options\"\n+            ] = f'-c search_path={self.connection_args.get(\"schema\")},public'\n+\n+        connection = psycopg.connect(**config, connect_timeout=10)\n+\n+        self.is_connected = True\n+        self.connection = connection\n+\n+        with self.connection.cursor() as cur:\n+            try:\n+                # load pg_vector extension\n+                cur.execute(\"CREATE EXTENSION IF NOT EXISTS vector\")\n+                self.connection.commit()\n+\n+            except psycopg.Error as e:\n+                log.logger.error(\n+                    f\"Error loading pg_vector extension, ensure you have installed it before running, {e}!\"\n+                )\n+\n+        return self.connection\n+\n+    def similarity_search(self, query: ASTNode) -> Response:\n+        \"\"\"\n+        Run a select query on the vectorpg database using the <-> operator.\n+        \"\"\"\n+\n+        collection_name = query.from_table.parts[-1]\n+\n+        with self.connection.cursor() as cur:\n+            try:\n+                # convert search embedding to string\n+                string_embeddings_search = str(query.where.args[1].items[0].value)\n+                # get limit from query\n+                limit = query.limit.value if query.limit else 5\n+                # we need to use the <-> operator to search for similar vectors,\n+                # so we need to convert the string to a vector and also use a threshold (e.g. 0.5)\nComment: Shouldn't this threshold be configurable through `using`?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/pgvector_handler/pgvector_handler.py",
    "pr_number": 7069,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1296569832,
    "comment_created_at": "2023-08-17T01:00:03Z"
  },
  {
    "code": "@@ -37,11 +37,17 @@ protobuf==3.20.3\n hierarchicalforecast~=0.4.0\n google-auth-oauthlib\n msal\n-langchain==0.1.11\n-langchain-core==0.1.46\n-langchain-community==0.0.27\n-langchain-openai==0.1.6\n-langchain-text_splitters==0.0.1\n+#langchain==0.1.11\n+transformers",
    "comment": "Do we need transformers here?",
    "line_number": 41,
    "enriched": "File: requirements/requirements.txt\nCode: @@ -37,11 +37,17 @@ protobuf==3.20.3\n hierarchicalforecast~=0.4.0\n google-auth-oauthlib\n msal\n-langchain==0.1.11\n-langchain-core==0.1.46\n-langchain-community==0.0.27\n-langchain-openai==0.1.6\n-langchain-text_splitters==0.0.1\n+#langchain==0.1.11\n+transformers\nComment: Do we need transformers here?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "requirements/requirements.txt",
    "pr_number": 9552,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1695627416,
    "comment_created_at": "2024-07-29T17:40:44Z"
  },
  {
    "code": "@@ -10,3 +13,19 @@ class CassandraHandler(ScyllaHandler):\n \n     def __init__(self, name, **kwargs):\n         super().__init__(name, **kwargs)\n+\n+    def get_tables(self) -> Response:\n+        \"\"\"\n+        Get the list of tables in the connected Cassandra database.\n+\n+        :return: List of table names.\n+        \"\"\"\n+        sql = \"DESCRIBE TABLES\"\n+        result = self.native_query(sql)\n+        df = result.data_frame\n+        table_data = pd.DataFrame(",
    "comment": "Can we use `df.rename()` to rename the columns here?\r\n\r\nFurther, the columns that we expect this function to return are `TABLE_NAME`, `TABLE_SCHEMA` and `TABLE_TYPE`. `TABLE_NAME` is self-explanatory. `TABLE_SCHEMA` can be the schema (or equivalent) the table belongs to in a database that organizes its objects in such a manner. Maybe `keyspace_name` applies here? I am not too sure. `TABLE_TYPE` typically denotes whether the table is a base table or a view. Only `TABLE_NAME` is mandatory here though, but the other information is also welcome.",
    "line_number": 26,
    "enriched": "File: mindsdb/integrations/handlers/cassandra_handler/cassandra_handler.py\nCode: @@ -10,3 +13,19 @@ class CassandraHandler(ScyllaHandler):\n \n     def __init__(self, name, **kwargs):\n         super().__init__(name, **kwargs)\n+\n+    def get_tables(self) -> Response:\n+        \"\"\"\n+        Get the list of tables in the connected Cassandra database.\n+\n+        :return: List of table names.\n+        \"\"\"\n+        sql = \"DESCRIBE TABLES\"\n+        result = self.native_query(sql)\n+        df = result.data_frame\n+        table_data = pd.DataFrame(\nComment: Can we use `df.rename()` to rename the columns here?\r\n\r\nFurther, the columns that we expect this function to return are `TABLE_NAME`, `TABLE_SCHEMA` and `TABLE_TYPE`. `TABLE_NAME` is self-explanatory. `TABLE_SCHEMA` can be the schema (or equivalent) the table belongs to in a database that organizes its objects in such a manner. Maybe `keyspace_name` applies here? I am not too sure. `TABLE_TYPE` typically denotes whether the table is a base table or a view. Only `TABLE_NAME` is mandatory here though, but the other information is also welcome.",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/cassandra_handler/cassandra_handler.py",
    "pr_number": 10025,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1818399055,
    "comment_created_at": "2024-10-28T05:34:34Z"
  },
  {
    "code": "@@ -1260,13 +1260,13 @@ def _create_embedding_model(self, project_name, engine=\"openai\", params: dict =\n             raise ValueError(\"'provider' parameter is required for embedding model\")\n \n         # check available providers\n-        avail_providers = (\"openai\", \"azure_openai\", \"bedrock\", \"gemini\", \"google\")\n+        avail_providers = (\"openai\", \"azure_openai\", \"bedrock\", \"gemini\", \"google\", \"ollama\")",
    "comment": "there  is still a `custom_openai` that can be set from the UI which will make this check to fail",
    "line_number": 1263,
    "enriched": "File: mindsdb/interfaces/knowledge_base/controller.py\nCode: @@ -1260,13 +1260,13 @@ def _create_embedding_model(self, project_name, engine=\"openai\", params: dict =\n             raise ValueError(\"'provider' parameter is required for embedding model\")\n \n         # check available providers\n-        avail_providers = (\"openai\", \"azure_openai\", \"bedrock\", \"gemini\", \"google\")\n+        avail_providers = (\"openai\", \"azure_openai\", \"bedrock\", \"gemini\", \"google\", \"ollama\")\nComment: there  is still a `custom_openai` that can be set from the UI which will make this check to fail",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/interfaces/knowledge_base/controller.py",
    "pr_number": 11616,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2380142723,
    "comment_created_at": "2025-09-25T19:38:05Z"
  },
  {
    "code": "@@ -0,0 +1,64 @@\n+---\n+title: MindsDB'c MCP Server Usage and Tools\n+sidebarTitle: Usage\n+---\n+\n+**MindsDB** is an MCP server that enables your MCP applications to answer questions over large-scale federated data spanning databases, data warehouses, and SaaS applications.\n+\n+## Start MindsDB as an MCP Server\n+\n+Follow the steps below to use MindsDB as an MCP server.\n+\n+1. Install MindsDB locally via [Docker](https://docs.mindsdb.com/setup/self-hosted/docker) or [Docker Desktop](https://docs.mindsdb.com/setup/self-hosted/docker-desktop).",
    "comment": "Should we mention here that user may also install MindsDB via pip?\r\n",
    "line_number": 12,
    "enriched": "File: docs/mcp/usage.mdx\nCode: @@ -0,0 +1,64 @@\n+---\n+title: MindsDB'c MCP Server Usage and Tools\n+sidebarTitle: Usage\n+---\n+\n+**MindsDB** is an MCP server that enables your MCP applications to answer questions over large-scale federated data spanning databases, data warehouses, and SaaS applications.\n+\n+## Start MindsDB as an MCP Server\n+\n+Follow the steps below to use MindsDB as an MCP server.\n+\n+1. Install MindsDB locally via [Docker](https://docs.mindsdb.com/setup/self-hosted/docker) or [Docker Desktop](https://docs.mindsdb.com/setup/self-hosted/docker-desktop).\nComment: Should we mention here that user may also install MindsDB via pip?\r\n",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "docs/mcp/usage.mdx",
    "pr_number": 10876,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2104507004,
    "comment_created_at": "2025-05-23T12:41:18Z"
  },
  {
    "code": "@@ -157,26 +158,48 @@ def _get_rag_params(pred_args: Dict) -> Dict:\n     return rag_params\n \n \n-def _build_retrieval_tool(tool: dict, pred_args: dict):\n+def _build_retrieval_tool(tool: dict, pred_args: dict, skill: db.Skills):\n     \"\"\"\n     Builds a retrieval tool i.e RAG\n     \"\"\"\n     # build RAG config\n \n     tools_config = tool['config']\n \n-    mindsdb_path = pred_args['mindsdb_path']\n \n     # we update the config with the pred_args to allow for custom config\n     tools_config.update(pred_args)\n \n     rag_params = _get_rag_params(tools_config)\n \n     if 'vector_store_config' not in rag_params:\n-\n-        rag_params['vector_store_config'] = {'persist_directory': mindsdb_path('persisted_chroma')}\n-\n-    rag_config = RAGPipelineModel(**rag_params)",
    "comment": "what errors do you run into? \r\n\r\nPerhaps we could fix by pinning pydantic version in langchain_handler requirements? (fine for now though)",
    "line_number": 176,
    "enriched": "File: mindsdb/integrations/handlers/langchain_handler/tools.py\nCode: @@ -157,26 +158,48 @@ def _get_rag_params(pred_args: Dict) -> Dict:\n     return rag_params\n \n \n-def _build_retrieval_tool(tool: dict, pred_args: dict):\n+def _build_retrieval_tool(tool: dict, pred_args: dict, skill: db.Skills):\n     \"\"\"\n     Builds a retrieval tool i.e RAG\n     \"\"\"\n     # build RAG config\n \n     tools_config = tool['config']\n \n-    mindsdb_path = pred_args['mindsdb_path']\n \n     # we update the config with the pred_args to allow for custom config\n     tools_config.update(pred_args)\n \n     rag_params = _get_rag_params(tools_config)\n \n     if 'vector_store_config' not in rag_params:\n-\n-        rag_params['vector_store_config'] = {'persist_directory': mindsdb_path('persisted_chroma')}\n-\n-    rag_config = RAGPipelineModel(**rag_params)\nComment: what errors do you run into? \r\n\r\nPerhaps we could fix by pinning pydantic version in langchain_handler requirements? (fine for now though)",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/langchain_handler/tools.py",
    "pr_number": 9098,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1572233734,
    "comment_created_at": "2024-04-19T11:30:11Z"
  },
  {
    "code": "@@ -0,0 +1,428 @@\n+from __future__ import annotations\n+\n+import asyncio\n+import logging\n+import math\n+import os\n+import random\n+from typing import Any, Dict, List, Optional, Sequence, Tuple\n+\n+from openai import AsyncOpenAI, AsyncAzureOpenAI\n+\n+from mindsdb.integrations.utilities.rag.settings import DEFAULT_RERANKING_MODEL, DEFAULT_LLM_ENDPOINT\n+\n+log = logging.getLogger(__name__)\n+\n+\n+class Document:\n+    \"\"\"A simple Document class to represent documents with content and metadata.\"\"\"\n+    def __init__(self, page_content: str, metadata: Optional[Dict[str, Any]] = None):\n+        self.page_content = page_content\n+        self.metadata = metadata or {}\n+\n+    def __repr__(self):\n+        return f\"Document(content={self.page_content}, metadata={self.metadata})\"\n+\n+\n+class LLMReranker:",
    "comment": "it is almost copy of  reranker_complressor.LLMReranker\r\nwhy not replace it? or langchain dependend version is going to be used? \r\n",
    "line_number": 27,
    "enriched": "File: mindsdb/integrations/utilities/rag/rerankers/reranker_async.py\nCode: @@ -0,0 +1,428 @@\n+from __future__ import annotations\n+\n+import asyncio\n+import logging\n+import math\n+import os\n+import random\n+from typing import Any, Dict, List, Optional, Sequence, Tuple\n+\n+from openai import AsyncOpenAI, AsyncAzureOpenAI\n+\n+from mindsdb.integrations.utilities.rag.settings import DEFAULT_RERANKING_MODEL, DEFAULT_LLM_ENDPOINT\n+\n+log = logging.getLogger(__name__)\n+\n+\n+class Document:\n+    \"\"\"A simple Document class to represent documents with content and metadata.\"\"\"\n+    def __init__(self, page_content: str, metadata: Optional[Dict[str, Any]] = None):\n+        self.page_content = page_content\n+        self.metadata = metadata or {}\n+\n+    def __repr__(self):\n+        return f\"Document(content={self.page_content}, metadata={self.metadata})\"\n+\n+\n+class LLMReranker:\nComment: it is almost copy of  reranker_complressor.LLMReranker\r\nwhy not replace it? or langchain dependend version is going to be used? \r\n",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/utilities/rag/rerankers/reranker_async.py",
    "pr_number": 10757,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2060474296,
    "comment_created_at": "2025-04-25T15:37:47Z"
  },
  {
    "code": "@@ -36,4 +36,5 @@ pyarrow >= 10.0.1, < 10.1.0\n dataprep_ml\n grpcio-tools\n python-magic >= 0.4.27\n+slack_sdk",
    "comment": "Please remove it from here. Each handler has its own dependency in handlers dir",
    "line_number": 39,
    "enriched": "File: requirements.txt\nCode: @@ -36,4 +36,5 @@ pyarrow >= 10.0.1, < 10.1.0\n dataprep_ml\n grpcio-tools\n python-magic >= 0.4.27\n+slack_sdk\nComment: Please remove it from here. Each handler has its own dependency in handlers dir",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "requirements.txt",
    "pr_number": 6180,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1198730295,
    "comment_created_at": "2023-05-19T09:08:15Z"
  },
  {
    "code": "@@ -168,6 +169,12 @@ AND last_name = 'Doe'\n AND email = 'john.doe@example.com';\n ~~~~\n \n+~~~~sql\n+DELETE FROM shopify_datasource.orders\n+WHERE id=5632671580477 \n+AND contact_email='egnition_sample_21@egnition.com';",
    "comment": "To delete the record you need just the id column. Please remove this line `AND contact_email='egnition_sample_21@egnition.com'` from this sample statement.",
    "line_number": 175,
    "enriched": "File: mindsdb/integrations/handlers/shopify_handler/README.md\nCode: @@ -168,6 +169,12 @@ AND last_name = 'Doe'\n AND email = 'john.doe@example.com';\n ~~~~\n \n+~~~~sql\n+DELETE FROM shopify_datasource.orders\n+WHERE id=5632671580477 \n+AND contact_email='egnition_sample_21@egnition.com';\nComment: To delete the record you need just the id column. Please remove this line `AND contact_email='egnition_sample_21@egnition.com'` from this sample statement.",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/shopify_handler/README.md",
    "pr_number": 8424,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1404219642,
    "comment_created_at": "2023-11-24T10:53:04Z"
  },
  {
    "code": "@@ -1,142 +1,157 @@\n-from typing import Optional\n-from collections import OrderedDict\n+from typing import Text, Dict, Optional\n \n-import pandas as pd\n from elasticsearch import Elasticsearch\n-\n-from elasticsearch.exceptions import ConnectionError, AuthenticationException\n-\n-from mindsdb_sql import parse_sql\n-from mindsdb_sql.render.sqlalchemy_render import SqlalchemyRender\n+from elasticsearch.exceptions import ConnectionError, AuthenticationException, TransportError, RequestError\n from es.elastic.sqlalchemy import ESDialect\n-from mindsdb.integrations.libs.base import DatabaseHandler\n-\n+from pandas import DataFrame\n from mindsdb_sql.parser.ast.base import ASTNode\n+from mindsdb_sql.render.sqlalchemy_render import SqlalchemyRender\n \n-from mindsdb.utilities import log\n+from mindsdb.integrations.libs.base import DatabaseHandler\n from mindsdb.integrations.libs.response import (\n-    HandlerStatusResponse as StatusResponse,\n     HandlerResponse as Response,\n-    RESPONSE_TYPE\n+    HandlerStatusResponse as StatusResponse,\n+    RESPONSE_TYPE,\n )\n-from mindsdb.integrations.libs.const import HANDLER_CONNECTION_ARG_TYPE as ARG_TYPE\n+from mindsdb.utilities import log\n+\n \n logger = log.getLogger(__name__)\n \n+\n class ElasticsearchHandler(DatabaseHandler):\n     \"\"\"\n-    This handler handles connection and execution of the Airtable statements.\n+    This handler handles the connection and execution of SQL statements on Elasticsearch.\n     \"\"\"\n \n     name = 'elasticsearch'\n \n-    def __init__(self, name: str, connection_data: Optional[dict], **kwargs):\n+    def __init__(self, name: Text, connection_data: Optional[Dict], **kwargs) -> None:\n         \"\"\"\n-        Initialize the handler.\n+        Initializes the handler.\n+\n         Args:\n-            name (str): name of particular handler instance\n-            connection_data (dict): parameters for connecting to the database\n-            **kwargs: arbitrary keyword arguments.\n+            name (Text): The name of the handler instance.\n+            connection_data (Dict): The connection data required to connect to the AWS (S3) account.\n+            kwargs: Arbitrary keyword arguments.\n         \"\"\"\n         super().__init__(name)\n-        self.parser = parse_sql\n-        self.dialect = 'elasticsearch'\n-\n-        if ('hosts' not in connection_data) and ('cloud_id' not in connection_data):\n-            raise Exception(\"Either the hosts or cloud_id parameter should be provided!\")\n-\n-        optional_parameters = ['hosts', 'cloud_id', 'username', 'password']\n-        for parameter in optional_parameters:\n-            if parameter not in connection_data:\n-                connection_data[parameter] = None\n-\n         self.connection_data = connection_data\n         self.kwargs = kwargs\n \n         self.connection = None\n         self.is_connected = False\n \n-    def __del__(self):\n-        if self.is_connected is True:\n+    def __del__(self) -> None:\n+        \"\"\"\n+        Closes the connection when the handler instance is deleted.\n+        \"\"\"\n+        if self.is_connected:\n             self.disconnect()\n \n-    def connect(self) -> StatusResponse:\n+    def connect(self) -> Elasticsearch:\n         \"\"\"\n-        Set up the connection required by the handler.\n+        Establishes a connection to the Elasticsearch host.\n+\n+        Raises:\n+            ValueError: If the expected connection parameters are not provided.\n+\n         Returns:\n-            HandlerStatusResponse\n+            elasticsearch.Elasticsearch: A connection object to the Elasticsearch host.\n         \"\"\"\n-\n         if self.is_connected is True:\n-            return StatusResponse(True)\n+            return self.connection\n+\n+        config = {}\n+\n+        # Mandatory connection parameters.\n+        if ('hosts' not in self.connection_data) and ('cloud_id' not in self.connection_data):",
    "comment": "Shouldn't this be `or` ?",
    "line_number": 68,
    "enriched": "File: mindsdb/integrations/handlers/elasticsearch_handler/elasticsearch_handler.py\nCode: @@ -1,142 +1,157 @@\n-from typing import Optional\n-from collections import OrderedDict\n+from typing import Text, Dict, Optional\n \n-import pandas as pd\n from elasticsearch import Elasticsearch\n-\n-from elasticsearch.exceptions import ConnectionError, AuthenticationException\n-\n-from mindsdb_sql import parse_sql\n-from mindsdb_sql.render.sqlalchemy_render import SqlalchemyRender\n+from elasticsearch.exceptions import ConnectionError, AuthenticationException, TransportError, RequestError\n from es.elastic.sqlalchemy import ESDialect\n-from mindsdb.integrations.libs.base import DatabaseHandler\n-\n+from pandas import DataFrame\n from mindsdb_sql.parser.ast.base import ASTNode\n+from mindsdb_sql.render.sqlalchemy_render import SqlalchemyRender\n \n-from mindsdb.utilities import log\n+from mindsdb.integrations.libs.base import DatabaseHandler\n from mindsdb.integrations.libs.response import (\n-    HandlerStatusResponse as StatusResponse,\n     HandlerResponse as Response,\n-    RESPONSE_TYPE\n+    HandlerStatusResponse as StatusResponse,\n+    RESPONSE_TYPE,\n )\n-from mindsdb.integrations.libs.const import HANDLER_CONNECTION_ARG_TYPE as ARG_TYPE\n+from mindsdb.utilities import log\n+\n \n logger = log.getLogger(__name__)\n \n+\n class ElasticsearchHandler(DatabaseHandler):\n     \"\"\"\n-    This handler handles connection and execution of the Airtable statements.\n+    This handler handles the connection and execution of SQL statements on Elasticsearch.\n     \"\"\"\n \n     name = 'elasticsearch'\n \n-    def __init__(self, name: str, connection_data: Optional[dict], **kwargs):\n+    def __init__(self, name: Text, connection_data: Optional[Dict], **kwargs) -> None:\n         \"\"\"\n-        Initialize the handler.\n+        Initializes the handler.\n+\n         Args:\n-            name (str): name of particular handler instance\n-            connection_data (dict): parameters for connecting to the database\n-            **kwargs: arbitrary keyword arguments.\n+            name (Text): The name of the handler instance.\n+            connection_data (Dict): The connection data required to connect to the AWS (S3) account.\n+            kwargs: Arbitrary keyword arguments.\n         \"\"\"\n         super().__init__(name)\n-        self.parser = parse_sql\n-        self.dialect = 'elasticsearch'\n-\n-        if ('hosts' not in connection_data) and ('cloud_id' not in connection_data):\n-            raise Exception(\"Either the hosts or cloud_id parameter should be provided!\")\n-\n-        optional_parameters = ['hosts', 'cloud_id', 'username', 'password']\n-        for parameter in optional_parameters:\n-            if parameter not in connection_data:\n-                connection_data[parameter] = None\n-\n         self.connection_data = connection_data\n         self.kwargs = kwargs\n \n         self.connection = None\n         self.is_connected = False\n \n-    def __del__(self):\n-        if self.is_connected is True:\n+    def __del__(self) -> None:\n+        \"\"\"\n+        Closes the connection when the handler instance is deleted.\n+        \"\"\"\n+        if self.is_connected:\n             self.disconnect()\n \n-    def connect(self) -> StatusResponse:\n+    def connect(self) -> Elasticsearch:\n         \"\"\"\n-        Set up the connection required by the handler.\n+        Establishes a connection to the Elasticsearch host.\n+\n+        Raises:\n+            ValueError: If the expected connection parameters are not provided.\n+\n         Returns:\n-            HandlerStatusResponse\n+            elasticsearch.Elasticsearch: A connection object to the Elasticsearch host.\n         \"\"\"\n-\n         if self.is_connected is True:\n-            return StatusResponse(True)\n+            return self.connection\n+\n+        config = {}\n+\n+        # Mandatory connection parameters.\n+        if ('hosts' not in self.connection_data) and ('cloud_id' not in self.connection_data):\nComment: Shouldn't this be `or` ?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/elasticsearch_handler/elasticsearch_handler.py",
    "pr_number": 9501,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1689758976,
    "comment_created_at": "2024-07-24T13:03:50Z"
  },
  {
    "code": "@@ -15,19 +15,37 @@ def default(self, obj):\n             return obj.strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n         if isinstance(obj, date):\n             return obj.strftime(\"%Y-%m-%d\")\n+        if (\n+            isinstance(obj, np.float16)\n+            or isinstance(obj, np.float32)\n+            or isinstance(obj, np.float64)\n+            or isinstance(obj, Decimal)\n+        ):\n+            return float(obj)\n         if isinstance(obj, np.bool_):\n             return bool(obj)\n-        if isinstance(obj, np.int8) or isinstance(obj, np.int16) or isinstance(obj, np.int32) or isinstance(obj, np.int64):",
    "comment": "why this block is removed?",
    "line_number": 20,
    "enriched": "File: mindsdb/utilities/json_encoder.py\nCode: @@ -15,19 +15,37 @@ def default(self, obj):\n             return obj.strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n         if isinstance(obj, date):\n             return obj.strftime(\"%Y-%m-%d\")\n+        if (\n+            isinstance(obj, np.float16)\n+            or isinstance(obj, np.float32)\n+            or isinstance(obj, np.float64)\n+            or isinstance(obj, Decimal)\n+        ):\n+            return float(obj)\n         if isinstance(obj, np.bool_):\n             return bool(obj)\n-        if isinstance(obj, np.int8) or isinstance(obj, np.int16) or isinstance(obj, np.int32) or isinstance(obj, np.int64):\nComment: why this block is removed?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/utilities/json_encoder.py",
    "pr_number": 11664,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2401722736,
    "comment_created_at": "2025-10-03T12:24:57Z"
  },
  {
    "code": "@@ -89,7 +90,7 @@ def describe(self, attribute: Optional[str] = None) -> pd.DataFrame:\n     def _set_models(self, args):\n         if 'api_key' in args:\n             args['openai_api_key'] = args['api_key']  # remove this once #7496 is fixed\n-        client = openai.OpenAI(api_key=get_api_key('openai', args, self.engine_storage), base_url=ANYSCALE_API_BASE)\n+        client = self._get_client(get_api_key('openai', args, self.engine_storage))",
    "comment": "We now use this instead, as base class also calls the method, so we overwrite with `ANYSCALE_API_BASE`.",
    "line_number": 93,
    "enriched": "File: mindsdb/integrations/handlers/anyscale_endpoints_handler/anyscale_endpoints_handler.py\nCode: @@ -89,7 +90,7 @@ def describe(self, attribute: Optional[str] = None) -> pd.DataFrame:\n     def _set_models(self, args):\n         if 'api_key' in args:\n             args['openai_api_key'] = args['api_key']  # remove this once #7496 is fixed\n-        client = openai.OpenAI(api_key=get_api_key('openai', args, self.engine_storage), base_url=ANYSCALE_API_BASE)\n+        client = self._get_client(get_api_key('openai', args, self.engine_storage))\nComment: We now use this instead, as base class also calls the method, so we overwrite with `ANYSCALE_API_BASE`.",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/anyscale_endpoints_handler/anyscale_endpoints_handler.py",
    "pr_number": 8579,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1441870168,
    "comment_created_at": "2024-01-04T15:07:32Z"
  },
  {
    "code": "@@ -884,42 +870,41 @@ def add(\n                 return kb\n             raise EntityExistsError(\"Knowledge base already exists\", name)\n \n-        embedding_model_params = get_model_params(params.get('embedding_model', {}), 'default_embedding_model')\n-        reranking_model_params = get_model_params(params.get('reranking_model', {}), 'default_llm')\n+        embedding_params = copy.deepcopy(config.get('default_embedding_model', {}))\n \n+        model_name = None\n+        model_project = project\n         if embedding_model:\n             model_name = embedding_model.parts[-1]\n+            if len(embedding_model.parts) > 1:\n+                model_project = self.session.database_controller.get_project(embedding_model.parts[-2])\n \n-        elif embedding_model_params:\n-            # Get embedding model from params.\n-            # This is called here to check validaity of the parameters.\n-            get_embedding_model_from_params(\n-                embedding_model_params\n-            )\n+        elif 'embedding_model' in params:\n+            if isinstance(params['embedding_model'], str):\n+                # it is model name\n+                model_name = params['embedding_model']\n+            else:\n+                # it is params for model\n+                embedding_params.update(params['embedding_model'])\n \n-        else:\n-            model_name = self._get_default_embedding_model(\n+        if model_name is None:\n+            model_name = self._create_embedding_model(",
    "comment": "@ea-rus I don't know if I understand the reason to create a new model when the new syntax is used? Is it necessary? This will create a new model for each new knowledge base, right?\r\nIs this so that we can avoid using LangChain?",
    "line_number": 891,
    "enriched": "File: mindsdb/interfaces/knowledge_base/controller.py\nCode: @@ -884,42 +870,41 @@ def add(\n                 return kb\n             raise EntityExistsError(\"Knowledge base already exists\", name)\n \n-        embedding_model_params = get_model_params(params.get('embedding_model', {}), 'default_embedding_model')\n-        reranking_model_params = get_model_params(params.get('reranking_model', {}), 'default_llm')\n+        embedding_params = copy.deepcopy(config.get('default_embedding_model', {}))\n \n+        model_name = None\n+        model_project = project\n         if embedding_model:\n             model_name = embedding_model.parts[-1]\n+            if len(embedding_model.parts) > 1:\n+                model_project = self.session.database_controller.get_project(embedding_model.parts[-2])\n \n-        elif embedding_model_params:\n-            # Get embedding model from params.\n-            # This is called here to check validaity of the parameters.\n-            get_embedding_model_from_params(\n-                embedding_model_params\n-            )\n+        elif 'embedding_model' in params:\n+            if isinstance(params['embedding_model'], str):\n+                # it is model name\n+                model_name = params['embedding_model']\n+            else:\n+                # it is params for model\n+                embedding_params.update(params['embedding_model'])\n \n-        else:\n-            model_name = self._get_default_embedding_model(\n+        if model_name is None:\n+            model_name = self._create_embedding_model(\nComment: @ea-rus I don't know if I understand the reason to create a new model when the new syntax is used? Is it necessary? This will create a new model for each new knowledge base, right?\r\nIs this so that we can avoid using LangChain?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/interfaces/knowledge_base/controller.py",
    "pr_number": 10749,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2059868404,
    "comment_created_at": "2025-04-25T09:12:55Z"
  },
  {
    "code": "@@ -68,8 +75,7 @@ CREATE DATABASE paypal_datasource\n WITH ENGINE = 'paypal',\n PARAMETERS = {\n   \"mode\": \"sandbox\",\n-  \"client_id\": \"EBWKjlELKMYqRNQ6sYvFo64FtaRLRR5BdHEESmha49TM\",\n-  \"client_secret\": \"EO422dn3gQLgDbuwqTjzrFgFtaRLRR5BdHEESmha49TM\"\n+  \"client_id\": \"AUv8rrc_P-EbP2E0mpb49BV7rFt3Usr-vdUZO8VGOnjRehGHBXkSzchr37SYF2GNdQFYSp72jh5QUhzG\",\"client_secret\":\"EMnAWe06ioGtouJs7gLYT9chK9-2jJ--7MKRXpI8FesmY_2Kp-d_7aCqff7M9moEJBvuXoBO4clKtY0v\"",
    "comment": "Please remove your client_id and client_secret and use some placeholders here.",
    "line_number": 78,
    "enriched": "File: mindsdb/integrations/handlers/paypal_handler/README.md\nCode: @@ -68,8 +75,7 @@ CREATE DATABASE paypal_datasource\n WITH ENGINE = 'paypal',\n PARAMETERS = {\n   \"mode\": \"sandbox\",\n-  \"client_id\": \"EBWKjlELKMYqRNQ6sYvFo64FtaRLRR5BdHEESmha49TM\",\n-  \"client_secret\": \"EO422dn3gQLgDbuwqTjzrFgFtaRLRR5BdHEESmha49TM\"\n+  \"client_id\": \"AUv8rrc_P-EbP2E0mpb49BV7rFt3Usr-vdUZO8VGOnjRehGHBXkSzchr37SYF2GNdQFYSp72jh5QUhzG\",\"client_secret\":\"EMnAWe06ioGtouJs7gLYT9chK9-2jJ--7MKRXpI8FesmY_2Kp-d_7aCqff7M9moEJBvuXoBO4clKtY0v\"\nComment: Please remove your client_id and client_secret and use some placeholders here.",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/paypal_handler/README.md",
    "pr_number": 7841,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1362280263,
    "comment_created_at": "2023-10-17T14:59:53Z"
  },
  {
    "code": "@@ -0,0 +1,257 @@\n+from mindsdb_sql_parser.ast import Identifier, Function, Constant, BinaryOperation\n+\n+\n+def adapt_char_fn(node: Function) -> Function | None:\n+    \"\"\"Replace MySQL's multy-arg CHAR call to chain of DuckDB's CHR calls\n+\n+    Example:\n+        CHAR(77, 78, 79) => CHR(77) || CHR(78) || CHR(79)\n+\n+    Args:\n+        node (Function): Function node to adapt\n+\n+    Returns:\n+        Function | None: Adapted function node\n+    \"\"\"\n+    if len(node.args) == 1:\n+        node.op = \"chr\"\n+        return node\n+\n+    acc = None\n+    for arg in node.args:\n+        fn = Function(op=\"chr\", args=[arg])\n+        if acc is None:\n+            acc = fn\n+            continue\n+        acc = BinaryOperation(\"||\", args=[acc, fn])\n+\n+    acc.parentheses = True\n+    acc.alias = node.alias\n+    return acc\n+\n+\n+def adapt_locate_fn(node: Function) -> Function | None:\n+    \"\"\"Replace MySQL's LOCATE (or INSTR) call to DuckDB's STRPOS call\n+\n+    Example:\n+        LOCATE('bar', 'foobarbar') => STRPOS('foobarbar', 'bar')\n+        INSTR('foobarbar', 'bar') => STRPOS('foobarbar', 'bar')\n+        LOCATE('bar', 'foobarbar', 3) => ValueError (there is no analogue in DuckDB)\n+\n+    Args:\n+        node (Function): Function node to adapt\n+\n+    Returns:\n+        Function | None: Adapted function node\n+\n+    Raises:\n+        ValueError: If the function has 3 arguments\n+    \"\"\"\n+    if len(node.args) == 3:\n+        raise ValueError(\"MySQL LOCATE function with 3 arguments is not supported\")\n+    if node.op == \"locate\":\n+        node.args = [node.args[1], node.args[0]]\n+    elif node.op == \"insrt\":\n+        node.args = [node.args[0], node.args[1]]\n+    node.op = \"strpos\"\n+\n+\n+def adapt_unhex_fn(node: Function) -> None:\n+    \"\"\"Check MySQL's UNHEX function call arguments to ensure they are strings,\n+    because DuckDB's UNHEX accepts only string arguments, while MySQL's UNHEX can accept integer arguments.\n+    NOTE: if return dataframe from duckdb then unhex values are array - this may be an issue\n+\n+    Args:\n+        node (Function): Function node to adapt\n+\n+    Returns:\n+        None\n+\n+    Raises:\n+        ValueError: If the function argument is not a string\n+    \"\"\"\n+    for arg in node.args:\n+        if not isinstance(arg, (str, bytes)):\n+            raise ValueError(\"MySQL UNHEX function argument must be a string\")\n+\n+\n+def adapt_format_fn(node: Function) -> None:\n+    \"\"\"Adapt MySQL's FORMAT function to DuckDB's FORMAT function\n+\n+    Example:\n+        FORMAT(1234567.89, 0) => FORMAT('{:,.0f}', 1234567.89)\n+        FORMAT(1234567.89, 2) => FORMAT('{:,.2f}', 1234567.89)\n+        FORMAT('{:.2f}', 1234567.89) => FORMAT('{:,.2f}', 1234567.89)  # no changes for original style\n+\n+    Args:\n+        node (Function): Function node to adapt\n+\n+    Returns:\n+        None\n+\n+    Raises:\n+        ValueError: If MySQL's function has 3rd 'locale' argument, like FORMAT(12332.2, 2, 'de_DE')\n+    \"\"\"\n+    if (\n+        not isinstance(node.args[0], Constant)",
    "comment": "what if the first argument is the value from table (not a constant)? ",
    "line_number": 96,
    "enriched": "File: mindsdb/api/executor/utilities/mysql_to_duckdb_functions.py\nCode: @@ -0,0 +1,257 @@\n+from mindsdb_sql_parser.ast import Identifier, Function, Constant, BinaryOperation\n+\n+\n+def adapt_char_fn(node: Function) -> Function | None:\n+    \"\"\"Replace MySQL's multy-arg CHAR call to chain of DuckDB's CHR calls\n+\n+    Example:\n+        CHAR(77, 78, 79) => CHR(77) || CHR(78) || CHR(79)\n+\n+    Args:\n+        node (Function): Function node to adapt\n+\n+    Returns:\n+        Function | None: Adapted function node\n+    \"\"\"\n+    if len(node.args) == 1:\n+        node.op = \"chr\"\n+        return node\n+\n+    acc = None\n+    for arg in node.args:\n+        fn = Function(op=\"chr\", args=[arg])\n+        if acc is None:\n+            acc = fn\n+            continue\n+        acc = BinaryOperation(\"||\", args=[acc, fn])\n+\n+    acc.parentheses = True\n+    acc.alias = node.alias\n+    return acc\n+\n+\n+def adapt_locate_fn(node: Function) -> Function | None:\n+    \"\"\"Replace MySQL's LOCATE (or INSTR) call to DuckDB's STRPOS call\n+\n+    Example:\n+        LOCATE('bar', 'foobarbar') => STRPOS('foobarbar', 'bar')\n+        INSTR('foobarbar', 'bar') => STRPOS('foobarbar', 'bar')\n+        LOCATE('bar', 'foobarbar', 3) => ValueError (there is no analogue in DuckDB)\n+\n+    Args:\n+        node (Function): Function node to adapt\n+\n+    Returns:\n+        Function | None: Adapted function node\n+\n+    Raises:\n+        ValueError: If the function has 3 arguments\n+    \"\"\"\n+    if len(node.args) == 3:\n+        raise ValueError(\"MySQL LOCATE function with 3 arguments is not supported\")\n+    if node.op == \"locate\":\n+        node.args = [node.args[1], node.args[0]]\n+    elif node.op == \"insrt\":\n+        node.args = [node.args[0], node.args[1]]\n+    node.op = \"strpos\"\n+\n+\n+def adapt_unhex_fn(node: Function) -> None:\n+    \"\"\"Check MySQL's UNHEX function call arguments to ensure they are strings,\n+    because DuckDB's UNHEX accepts only string arguments, while MySQL's UNHEX can accept integer arguments.\n+    NOTE: if return dataframe from duckdb then unhex values are array - this may be an issue\n+\n+    Args:\n+        node (Function): Function node to adapt\n+\n+    Returns:\n+        None\n+\n+    Raises:\n+        ValueError: If the function argument is not a string\n+    \"\"\"\n+    for arg in node.args:\n+        if not isinstance(arg, (str, bytes)):\n+            raise ValueError(\"MySQL UNHEX function argument must be a string\")\n+\n+\n+def adapt_format_fn(node: Function) -> None:\n+    \"\"\"Adapt MySQL's FORMAT function to DuckDB's FORMAT function\n+\n+    Example:\n+        FORMAT(1234567.89, 0) => FORMAT('{:,.0f}', 1234567.89)\n+        FORMAT(1234567.89, 2) => FORMAT('{:,.2f}', 1234567.89)\n+        FORMAT('{:.2f}', 1234567.89) => FORMAT('{:,.2f}', 1234567.89)  # no changes for original style\n+\n+    Args:\n+        node (Function): Function node to adapt\n+\n+    Returns:\n+        None\n+\n+    Raises:\n+        ValueError: If MySQL's function has 3rd 'locale' argument, like FORMAT(12332.2, 2, 'de_DE')\n+    \"\"\"\n+    if (\n+        not isinstance(node.args[0], Constant)\nComment: what if the first argument is the value from table (not a constant)? ",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/api/executor/utilities/mysql_to_duckdb_functions.py",
    "pr_number": 11380,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2251214211,
    "comment_created_at": "2025-08-04T11:34:48Z"
  },
  {
    "code": "@@ -96,11 +100,38 @@ def add_skill(self, name: str, project_name: str, type: str, params: Dict[str, s\n             raise ValueError(f\"Skill with name already exists: {name}\")\n \n         # Load metadata to data catalog (if enabled) if the skill is Text-to-SQL.\n-        if config.get(\"data_catalog\", {}).get(\"enabled\", False) and type == \"sql\":\n-            data_catalog_loader = DataCatalogLoader(\n-                database_name=params[\"database\"], table_names=params[\"tables\"] if \"tables\" in params else None\n-            )\n-            data_catalog_loader.load_metadata()\n+        if config.get(\"data_catalog\", {}).get(\"enabled\", False):\n+            if type == SkillType.TEXT2SQL.value:\n+                # TODO: Is it possible to create a skill with complete access to the database with the new agent syntax?\n+                # TODO: Handle the case where `ignore_tables` is provided. Is this a valid parameter?\n+                # TODO: Knowledge Bases?\n+                if \"include_tables\" not in params:",
    "comment": "is there a plan to also make data_catalog also include knowledge bases? so shouldn't we also consider `include_knowledge_base` param?",
    "line_number": 108,
    "enriched": "File: mindsdb/interfaces/skills/skills_controller.py\nCode: @@ -96,11 +100,38 @@ def add_skill(self, name: str, project_name: str, type: str, params: Dict[str, s\n             raise ValueError(f\"Skill with name already exists: {name}\")\n \n         # Load metadata to data catalog (if enabled) if the skill is Text-to-SQL.\n-        if config.get(\"data_catalog\", {}).get(\"enabled\", False) and type == \"sql\":\n-            data_catalog_loader = DataCatalogLoader(\n-                database_name=params[\"database\"], table_names=params[\"tables\"] if \"tables\" in params else None\n-            )\n-            data_catalog_loader.load_metadata()\n+        if config.get(\"data_catalog\", {}).get(\"enabled\", False):\n+            if type == SkillType.TEXT2SQL.value:\n+                # TODO: Is it possible to create a skill with complete access to the database with the new agent syntax?\n+                # TODO: Handle the case where `ignore_tables` is provided. Is this a valid parameter?\n+                # TODO: Knowledge Bases?\n+                if \"include_tables\" not in params:\nComment: is there a plan to also make data_catalog also include knowledge bases? so shouldn't we also consider `include_knowledge_base` param?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/interfaces/skills/skills_controller.py",
    "pr_number": 11014,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2131983250,
    "comment_created_at": "2025-06-06T10:55:34Z"
  },
  {
    "code": "@@ -228,7 +228,14 @@ def put(self, database_name):\n                     HTTPStatus.BAD_REQUEST, \"Connection error\", status.error_message or \"Connection error\"\n                 )\n \n-        session.integration_controller.modify(database_name, parameters)\n+        try:\n+            session.integration_controller.modify(database_name, parameters, check_connection=check_connection)",
    "comment": "there is check connection above, need to delete it or this",
    "line_number": 232,
    "enriched": "File: mindsdb/api/http/namespaces/databases.py\nCode: @@ -228,7 +228,14 @@ def put(self, database_name):\n                     HTTPStatus.BAD_REQUEST, \"Connection error\", status.error_message or \"Connection error\"\n                 )\n \n-        session.integration_controller.modify(database_name, parameters)\n+        try:\n+            session.integration_controller.modify(database_name, parameters, check_connection=check_connection)\nComment: there is check connection above, need to delete it or this",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/api/http/namespaces/databases.py",
    "pr_number": 11861,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2504807280,
    "comment_created_at": "2025-11-07T17:53:08Z"
  },
  {
    "code": "@@ -37,6 +37,7 @@ class OpenAIHandler(BaseMLEngine):\n \n     def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n+        # client=OpenAI(api_key=get_api_key('openai', args, self.engine_storage))",
    "comment": "@parthiv11 - seems to be unused. Can you pls delete?",
    "line_number": 40,
    "enriched": "File: mindsdb/integrations/handlers/openai_handler/openai_handler.py\nCode: @@ -37,6 +37,7 @@ class OpenAIHandler(BaseMLEngine):\n \n     def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n+        # client=OpenAI(api_key=get_api_key('openai', args, self.engine_storage))\nComment: @parthiv11 - seems to be unused. Can you pls delete?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/openai_handler/openai_handler.py",
    "pr_number": 8541,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1437155361,
    "comment_created_at": "2023-12-27T16:59:22Z"
  },
  {
    "code": "@@ -1236,6 +1272,930 @@ paths:\n                 type: object\n                 items:\n                   type: string\n+        '500':\n+          description: Server error\n+          content:\n+            application/json:\n+              schema:\n+                type: object\n+                items:\n+                  type: string\n+  '/api/projects/{projectName}/skills':\n+    get:\n+      security:\n+        - ApiKeyAuth: []\n+      summary: Returns a list of all created skills.\n+      description: Gets all skills created by the user.\n+      parameters:\n+        - name: projectName\n+          in: path\n+          description: The name of the project where agent resides\n+          required: true\n+          schema:\n+            type: string\n+      responses:\n+        '200':\n+          description: A JSON array of skill names\n+          content:\n+            application/json:\n+              schema:\n+                type: array\n+                items:\n+                  $ref: '#/components/schemas/Skill'\n+        '401':\n+          description: Invalid API key error message\n+          content:\n+            application/json:\n+              schema:\n+                type: object\n+                items:\n+                  type: string\n+        '500':\n+          description: Server error\n+          content:\n+            application/json:\n+              schema:\n+                type: object\n+                items:\n+                  type: string\n+    post:\n+      security:\n+        - ApiKeyAuth: []\n+      summary: Creates a new skill.\n+      description: Creates a new skill.\n+      parameters:\n+        - name: projectName\n+          in: path\n+          description: The name of the project where agent resides\n+          required: true\n+          schema:\n+            type: string\n+      requestBody:\n+        required: true\n+        content:\n+          application/json:\n+            schema:\n+              type: object\n+              properties:\n+                skill:\n+                  type: object\n+                  properties:\n+                    name:\n+                      type: string\n+                    type:\n+                      type: string\n+                      description: Type of skill (text2sql | knowledge_base).\n+                    source: # used when type = knowledge_base\n+                      type: string\n+                      description: Used to store a knowledge_base object when type is set to knowledge_base.\n+                    database: # used when type = text2sql\n+                      type: string\n+                      description: Used to store a data source connection when type is set to text2sql.\n+                    tables: # used when type = text2sql\n+                      type: array\n+                      items:\n+                        type: string\n+                      description: Used to store table(s) names when type is set to text2sql.\n+                    description:\n+                      type: string\n+                      description: Skill description is important for an agent to decide which skill to use.\n+      responses:\n+        '200':\n+          description: Created a skill\n+          content:\n+            application/json:\n+              schema:\n+                $ref: '#/components/schemas/Skill'\n+        '401':\n+          description: Invalid API key error message\n+          content:\n+            application/json:\n+              schema:\n+                type: object\n+                items:\n+                  type: string\n+        '400':\n+          description: Bad request format\n+          content:\n+            application/json:\n+              schema:\n+                type: object\n+                items:\n+                  type: string\n+        '409':\n+          description: Skill already exists\n+          content:\n+            application/json:\n+              schema:\n+                type: object\n+                items:\n+                  type: string\n+        '500':\n+          description: Server error\n+          content:\n+            application/json:\n+              schema:\n+                type: object\n+                items:\n+                  type: string\n+  '/api/projects/{projectName}/skills/{skillName}':\n+    get:\n+      security:\n+        - ApiKeyAuth: []\n+      summary: Gets info about existing skills.\n+      description: Gets info about an existing skill.\n+      parameters:\n+        - name: projectName\n+          in: path\n+          description: The name of the project where agent resides\n+          required: true\n+          schema:\n+            type: string\n+        - name: skillName\n+          in: path\n+          description: Name of existing skill\n+          required: true\n+          schema:\n+            type: string\n+      responses:\n+        '200':\n+          description: A JSON object with skills\n+          content:\n+            application/json:\n+              schema:\n+                $ref: '#/components/schemas/Skill'\n+        '401':\n+          description: Invalid API key error message\n+          content:\n+            application/json:\n+              schema:\n+                type: object\n+                items:\n+                  type: string\n+        '404':\n+          description: Skill not found\n+          content:\n+            application/json:\n+              schema:\n+                type: object\n+                items:\n+                  type: string\n+        '500':\n+          description: Server error\n+          content:\n+            application/json:\n+              schema:\n+                type: object\n+                items:\n+                  type: string\n+    put:\n+      security:\n+        - ApiKeyAuth: []\n+      summary: Updates an existing skill.\n+      description: Updates an existing skill, or creates a new skill if one doesn't exist.\n+      parameters:\n+        - name: projectName\n+          in: path\n+          description: The name of the project where agent resides\n+          required: true\n+          schema:\n+            type: string\n+        - name: skillName\n+          in: path\n+          description: The name of the skill\n+          required: true\n+          schema:\n+            type: string\n+      requestBody:\n+        required: true\n+        content:\n+          application/json:\n+            schema:\n+              type: object\n+              properties:\n+                skill:\n+                  type: object\n+                  properties:\n+                    name:\n+                      type: string\n+                    type:\n+                      type: string\n+                      description: Type of skill (text2sql | knowledge_base).\n+                    source: # used when type = knowledge_base\n+                      type: string\n+                      description: Used to store a knowledge_base object when type is set to knowledge_base.\n+                    database: # used when type = text2sql\n+                      type: string\n+                      description: Used to store a data source connection when type is set to text2sql.\n+                    tables: # used when type = text2sql\n+                      type: array\n+                      items:\n+                        type: string\n+                      description: Used to store table(s) names when type is set to text2sql.\n+                    description:\n+                      type: string\n+                      description: Skill description is important for an agent to decide which skill to use.\n+      responses:\n+        '200':\n+          description: Skill was successfully updated\n+          content:\n+            application/json:\n+              schema:\n+                $ref: '#/components/schemas/Skill'\n+        '400':\n+          description: Bad request format\n+          content:\n+            application/json:\n+              schema:\n+                type: object\n+                items:\n+                  type: string\n+        '401':\n+          description: Invalid API key error message\n+          content:\n+            application/json:\n+              schema:\n+                type: object\n+                items:\n+                  type: string\n+        '500':\n+          description: Server error\n+          content:\n+            application/json:\n+              schema:\n+                type: object\n+                items:\n+                  type: string\n+    delete:\n+      security:\n+        - ApiKeyAuth: []",
    "comment": "We can remove this for now",
    "line_number": 1531,
    "enriched": "File: docs/openapi.yml\nCode: @@ -1236,6 +1272,930 @@ paths:\n                 type: object\n                 items:\n                   type: string\n+        '500':\n+          description: Server error\n+          content:\n+            application/json:\n+              schema:\n+                type: object\n+                items:\n+                  type: string\n+  '/api/projects/{projectName}/skills':\n+    get:\n+      security:\n+        - ApiKeyAuth: []\n+      summary: Returns a list of all created skills.\n+      description: Gets all skills created by the user.\n+      parameters:\n+        - name: projectName\n+          in: path\n+          description: The name of the project where agent resides\n+          required: true\n+          schema:\n+            type: string\n+      responses:\n+        '200':\n+          description: A JSON array of skill names\n+          content:\n+            application/json:\n+              schema:\n+                type: array\n+                items:\n+                  $ref: '#/components/schemas/Skill'\n+        '401':\n+          description: Invalid API key error message\n+          content:\n+            application/json:\n+              schema:\n+                type: object\n+                items:\n+                  type: string\n+        '500':\n+          description: Server error\n+          content:\n+            application/json:\n+              schema:\n+                type: object\n+                items:\n+                  type: string\n+    post:\n+      security:\n+        - ApiKeyAuth: []\n+      summary: Creates a new skill.\n+      description: Creates a new skill.\n+      parameters:\n+        - name: projectName\n+          in: path\n+          description: The name of the project where agent resides\n+          required: true\n+          schema:\n+            type: string\n+      requestBody:\n+        required: true\n+        content:\n+          application/json:\n+            schema:\n+              type: object\n+              properties:\n+                skill:\n+                  type: object\n+                  properties:\n+                    name:\n+                      type: string\n+                    type:\n+                      type: string\n+                      description: Type of skill (text2sql | knowledge_base).\n+                    source: # used when type = knowledge_base\n+                      type: string\n+                      description: Used to store a knowledge_base object when type is set to knowledge_base.\n+                    database: # used when type = text2sql\n+                      type: string\n+                      description: Used to store a data source connection when type is set to text2sql.\n+                    tables: # used when type = text2sql\n+                      type: array\n+                      items:\n+                        type: string\n+                      description: Used to store table(s) names when type is set to text2sql.\n+                    description:\n+                      type: string\n+                      description: Skill description is important for an agent to decide which skill to use.\n+      responses:\n+        '200':\n+          description: Created a skill\n+          content:\n+            application/json:\n+              schema:\n+                $ref: '#/components/schemas/Skill'\n+        '401':\n+          description: Invalid API key error message\n+          content:\n+            application/json:\n+              schema:\n+                type: object\n+                items:\n+                  type: string\n+        '400':\n+          description: Bad request format\n+          content:\n+            application/json:\n+              schema:\n+                type: object\n+                items:\n+                  type: string\n+        '409':\n+          description: Skill already exists\n+          content:\n+            application/json:\n+              schema:\n+                type: object\n+                items:\n+                  type: string\n+        '500':\n+          description: Server error\n+          content:\n+            application/json:\n+              schema:\n+                type: object\n+                items:\n+                  type: string\n+  '/api/projects/{projectName}/skills/{skillName}':\n+    get:\n+      security:\n+        - ApiKeyAuth: []\n+      summary: Gets info about existing skills.\n+      description: Gets info about an existing skill.\n+      parameters:\n+        - name: projectName\n+          in: path\n+          description: The name of the project where agent resides\n+          required: true\n+          schema:\n+            type: string\n+        - name: skillName\n+          in: path\n+          description: Name of existing skill\n+          required: true\n+          schema:\n+            type: string\n+      responses:\n+        '200':\n+          description: A JSON object with skills\n+          content:\n+            application/json:\n+              schema:\n+                $ref: '#/components/schemas/Skill'\n+        '401':\n+          description: Invalid API key error message\n+          content:\n+            application/json:\n+              schema:\n+                type: object\n+                items:\n+                  type: string\n+        '404':\n+          description: Skill not found\n+          content:\n+            application/json:\n+              schema:\n+                type: object\n+                items:\n+                  type: string\n+        '500':\n+          description: Server error\n+          content:\n+            application/json:\n+              schema:\n+                type: object\n+                items:\n+                  type: string\n+    put:\n+      security:\n+        - ApiKeyAuth: []\n+      summary: Updates an existing skill.\n+      description: Updates an existing skill, or creates a new skill if one doesn't exist.\n+      parameters:\n+        - name: projectName\n+          in: path\n+          description: The name of the project where agent resides\n+          required: true\n+          schema:\n+            type: string\n+        - name: skillName\n+          in: path\n+          description: The name of the skill\n+          required: true\n+          schema:\n+            type: string\n+      requestBody:\n+        required: true\n+        content:\n+          application/json:\n+            schema:\n+              type: object\n+              properties:\n+                skill:\n+                  type: object\n+                  properties:\n+                    name:\n+                      type: string\n+                    type:\n+                      type: string\n+                      description: Type of skill (text2sql | knowledge_base).\n+                    source: # used when type = knowledge_base\n+                      type: string\n+                      description: Used to store a knowledge_base object when type is set to knowledge_base.\n+                    database: # used when type = text2sql\n+                      type: string\n+                      description: Used to store a data source connection when type is set to text2sql.\n+                    tables: # used when type = text2sql\n+                      type: array\n+                      items:\n+                        type: string\n+                      description: Used to store table(s) names when type is set to text2sql.\n+                    description:\n+                      type: string\n+                      description: Skill description is important for an agent to decide which skill to use.\n+      responses:\n+        '200':\n+          description: Skill was successfully updated\n+          content:\n+            application/json:\n+              schema:\n+                $ref: '#/components/schemas/Skill'\n+        '400':\n+          description: Bad request format\n+          content:\n+            application/json:\n+              schema:\n+                type: object\n+                items:\n+                  type: string\n+        '401':\n+          description: Invalid API key error message\n+          content:\n+            application/json:\n+              schema:\n+                type: object\n+                items:\n+                  type: string\n+        '500':\n+          description: Server error\n+          content:\n+            application/json:\n+              schema:\n+                type: object\n+                items:\n+                  type: string\n+    delete:\n+      security:\n+        - ApiKeyAuth: []\nComment: We can remove this for now",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "docs/openapi.yml",
    "pr_number": 9113,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1576215835,
    "comment_created_at": "2024-04-23T13:01:19Z"
  },
  {
    "code": "@@ -15,7 +15,7 @@ RUN pip install \".[grpc]\" \".[telemetry]\"\n \n RUN pip install git+https://github.com/mindsdb/lightwood.git@staging --upgrade --no-cache-dir\n # Install our app\n-# COPY ./mindsdb /mindsdb/mindsdb\n+# COPY ../mindsdb /mindsdb/mindsdb",
    "comment": "Please exclude this File from the PR",
    "line_number": 18,
    "enriched": "File: docker/executor.Dockerfile\nCode: @@ -15,7 +15,7 @@ RUN pip install \".[grpc]\" \".[telemetry]\"\n \n RUN pip install git+https://github.com/mindsdb/lightwood.git@staging --upgrade --no-cache-dir\n # Install our app\n-# COPY ./mindsdb /mindsdb/mindsdb\n+# COPY ../mindsdb /mindsdb/mindsdb\nComment: Please exclude this File from the PR",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "docker/executor.Dockerfile",
    "pr_number": 7616,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1350070474,
    "comment_created_at": "2023-10-09T09:37:16Z"
  },
  {
    "code": "@@ -0,0 +1,234 @@\n+import os\n+import requests\n+import time\n+from typing import Dict, Optional\n+from mindsdb.integrations.libs.llm_utils import get_completed_prompts\n+\n+import pandas as pd\n+\n+from mindsdb.integrations.libs.base import BaseMLEngine\n+from mindsdb.utilities import log\n+from mindsdb.interfaces.storage.model_fs import HandlerStorage\n+from mindsdb.utilities.config import Config\n+\n+logger = log.getLogger(__name__)\n+\n+LEONARDO_API_BASE = 'https://cloud.leonardo.ai/api/rest/v1'\n+\n+\n+class LeonardoAIHandler(BaseMLEngine):\n+    \"\"\"\n+    This integration seamlessly combines MindsDB and Leonardo AI to create a powerful\n+    AI-driven solution for creative content generation.\n+\n+    Content Generation with Leonardo AI: Harness the power of advanced generative\n+    models for creative content production. From realistic images to artistic text, Leonardo\n+    AI opens up new possibilities for content creators.\n+    \"\"\"\n+    name = \"leonardoai\"",
    "comment": "In the README we are referencing `leonardo_ai`. So maybe change this or the README info",
    "line_number": 28,
    "enriched": "File: mindsdb/integrations/handlers/leonardoai_handler/leonardo_ai_handler.py\nCode: @@ -0,0 +1,234 @@\n+import os\n+import requests\n+import time\n+from typing import Dict, Optional\n+from mindsdb.integrations.libs.llm_utils import get_completed_prompts\n+\n+import pandas as pd\n+\n+from mindsdb.integrations.libs.base import BaseMLEngine\n+from mindsdb.utilities import log\n+from mindsdb.interfaces.storage.model_fs import HandlerStorage\n+from mindsdb.utilities.config import Config\n+\n+logger = log.getLogger(__name__)\n+\n+LEONARDO_API_BASE = 'https://cloud.leonardo.ai/api/rest/v1'\n+\n+\n+class LeonardoAIHandler(BaseMLEngine):\n+    \"\"\"\n+    This integration seamlessly combines MindsDB and Leonardo AI to create a powerful\n+    AI-driven solution for creative content generation.\n+\n+    Content Generation with Leonardo AI: Harness the power of advanced generative\n+    models for creative content production. From realistic images to artistic text, Leonardo\n+    AI opens up new possibilities for content creators.\n+    \"\"\"\n+    name = \"leonardoai\"\nComment: In the README we are referencing `leonardo_ai`. So maybe change this or the README info",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/leonardoai_handler/leonardo_ai_handler.py",
    "pr_number": 8553,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1444699435,
    "comment_created_at": "2024-01-08T14:06:40Z"
  },
  {
    "code": "@@ -66,7 +73,22 @@ def _make_text_to_sql_tools(self, skill: db.Skills) -> dict:\n         return dict(\n             name='sql_db_query',\n             func=sql_agent.query_safe,\n-            description=description\n+            description=description,\n+            type=skill.type\n+        )\n+\n+    def _make_retrieval_tools(self, skill: db.Skills) -> dict:\n+        \"\"\"\n+        creates advanced retrieval tool i.e. RAG\n+        \"\"\"\n+        params = skill.params\n+        return dict(\n+            name=params.get('name', skill.name),\n+            config=params.get('retriever_config', {}),\n+            description=f'You must use this tool to get more context or information '\n+                        f'to answer a question about {params[\"description\"]}. '\n+                        f'The input should be the exact question the user is asking.',",
    "comment": "updated with base template for now, we can replace with llm gen if needed but this suffices for now",
    "line_number": 90,
    "enriched": "File: mindsdb/interfaces/skills/skill_tool.py\nCode: @@ -66,7 +73,22 @@ def _make_text_to_sql_tools(self, skill: db.Skills) -> dict:\n         return dict(\n             name='sql_db_query',\n             func=sql_agent.query_safe,\n-            description=description\n+            description=description,\n+            type=skill.type\n+        )\n+\n+    def _make_retrieval_tools(self, skill: db.Skills) -> dict:\n+        \"\"\"\n+        creates advanced retrieval tool i.e. RAG\n+        \"\"\"\n+        params = skill.params\n+        return dict(\n+            name=params.get('name', skill.name),\n+            config=params.get('retriever_config', {}),\n+            description=f'You must use this tool to get more context or information '\n+                        f'to answer a question about {params[\"description\"]}. '\n+                        f'The input should be the exact question the user is asking.',\nComment: updated with base template for now, we can replace with llm gen if needed but this suffices for now",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/interfaces/skills/skill_tool.py",
    "pr_number": 9068,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1562385046,
    "comment_created_at": "2024-04-12T10:48:26Z"
  },
  {
    "code": "@@ -25,17 +25,32 @@ my_model = project.create_model (\n )\n ```\n \n-Here is the syntax to create an OpenAI model:\n+Please note that in the case of LLM models, the parameters are stored in `options`. Here is the syntax to create an OpenAI model:\n \n ```bash\n-sentiment_classifier = project.create_model(\n+sentiment_classifier = project.create_model (\n       name='sentiment_classifier',\n-      engine='huggingface',\n+      engine='openai', # alternatively: engine=server.ml_engines.openai\n       predict='sentiment',\n+      prompt_template='answer this question: {{questions}}'\n       options={\n-          'model_name':'cardiffnlp/twitter-roberta-base-sentiment',\n-          'input_column': 'comment',\n-          'labels': ['negative', 'neutral', 'positive']\n+          'model_name':'gpt4'",
    "comment": "it is possible to move 'model_name' also from options. All unknown params are recognized as options now. So these commands are equal:\r\n\r\n```\r\nsentiment_classifier = project.create_model (\r\n      name='sentiment_classifier',\r\n      engine='openai', # alternatively: engine=server.ml_engines.openai\r\n      predict='sentiment',\r\n      options={\r\n          'prompt_template':'answer this question: {{questions}}',\r\n          'model_name':'gpt4'\r\n      }\r\n)\r\n\r\nsentiment_classifier = project.create_model (\r\n      name='sentiment_classifier',\r\n      engine='openai', # alternatively: engine=server.ml_engines.openai\r\n      predict='sentiment',\r\n      prompt_template='answer this question: {{questions}}',\r\n      model_name = 'gpt4'\r\n)\r\n```",
    "line_number": 37,
    "enriched": "File: docs/sdk_python/create_model.mdx\nCode: @@ -25,17 +25,32 @@ my_model = project.create_model (\n )\n ```\n \n-Here is the syntax to create an OpenAI model:\n+Please note that in the case of LLM models, the parameters are stored in `options`. Here is the syntax to create an OpenAI model:\n \n ```bash\n-sentiment_classifier = project.create_model(\n+sentiment_classifier = project.create_model (\n       name='sentiment_classifier',\n-      engine='huggingface',\n+      engine='openai', # alternatively: engine=server.ml_engines.openai\n       predict='sentiment',\n+      prompt_template='answer this question: {{questions}}'\n       options={\n-          'model_name':'cardiffnlp/twitter-roberta-base-sentiment',\n-          'input_column': 'comment',\n-          'labels': ['negative', 'neutral', 'positive']\n+          'model_name':'gpt4'\nComment: it is possible to move 'model_name' also from options. All unknown params are recognized as options now. So these commands are equal:\r\n\r\n```\r\nsentiment_classifier = project.create_model (\r\n      name='sentiment_classifier',\r\n      engine='openai', # alternatively: engine=server.ml_engines.openai\r\n      predict='sentiment',\r\n      options={\r\n          'prompt_template':'answer this question: {{questions}}',\r\n          'model_name':'gpt4'\r\n      }\r\n)\r\n\r\nsentiment_classifier = project.create_model (\r\n      name='sentiment_classifier',\r\n      engine='openai', # alternatively: engine=server.ml_engines.openai\r\n      predict='sentiment',\r\n      prompt_template='answer this question: {{questions}}',\r\n      model_name = 'gpt4'\r\n)\r\n```",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "docs/sdk_python/create_model.mdx",
    "pr_number": 7376,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1331729063,
    "comment_created_at": "2023-09-20T14:30:47Z"
  },
  {
    "code": "@@ -262,6 +603,121 @@ def add_raw_values(self, values):\n         )\n         self.add_raw_df(df)\n \n+    def get_ast_columns(self) -> list[TableColumn]:\n+        \"\"\"Converts ResultSet columns to a list of TableColumn objects with SQLAlchemy types.\n+\n+        This method processes each column in the ResultSet, determines its MySQL data type\n+        (inferring it if necessary), and maps it to the appropriate SQLAlchemy type.\n+        The resulting TableColumn objects most likely will be used in CREATE TABLE statement.\n+\n+        Returns:\n+            list[TableColumn]: A list of TableColumn objects with properly mapped SQLAlchemy types\n+        \"\"\"\n+        columns: list[TableColumn] = []\n+\n+        type_mapping = {\n+            MYSQL_DATA_TYPE.TINYINT: sqlalchemy_types.INTEGER,\n+            MYSQL_DATA_TYPE.SMALLINT: sqlalchemy_types.INTEGER,\n+            MYSQL_DATA_TYPE.MEDIUMINT: sqlalchemy_types.INTEGER,\n+            MYSQL_DATA_TYPE.INT: sqlalchemy_types.INTEGER,\n+            MYSQL_DATA_TYPE.BIGINT: sqlalchemy_types.INTEGER,\n+            MYSQL_DATA_TYPE.YEAR: sqlalchemy_types.INTEGER,\n+            MYSQL_DATA_TYPE.BOOL: sqlalchemy_types.BOOLEAN,\n+            MYSQL_DATA_TYPE.BOOLEAN: sqlalchemy_types.BOOLEAN,\n+            MYSQL_DATA_TYPE.FLOAT: sqlalchemy_types.FLOAT,\n+            MYSQL_DATA_TYPE.DOUBLE: sqlalchemy_types.FLOAT,\n+            MYSQL_DATA_TYPE.TIME: sqlalchemy_types.TIME,\n+            MYSQL_DATA_TYPE.DATE: sqlalchemy_types.DATE,\n+            MYSQL_DATA_TYPE.DATETIME: sqlalchemy_types.DATETIME,\n+            MYSQL_DATA_TYPE.TIMESTAMP: sqlalchemy_types.TIMESTAMP,\n+        }\n+\n+        for i, column in enumerate(self._columns):\n+            column_type: MYSQL_DATA_TYPE | None = column.type\n+\n+            # infer MYSQL_DATA_TYPE if not set\n+            if isinstance(column_type, MYSQL_DATA_TYPE) is False:\n+                if column_type is not None:\n+                    logger.warning(f'Unexpected column type: {column_type}')\n+                if self._df is None:\n+                    column_type = MYSQL_DATA_TYPE.TEXT\n+                else:\n+                    column_type = get_mysql_data_type_from_series(self._df.iloc[:, i])\n+\n+            sqlalchemy_type = type_mapping.get(column_type, sqlalchemy_types.TEXT)\n+\n+            columns.append(\n+                TableColumn(\n+                    name=column.alias,\n+                    type=sqlalchemy_type\n+                )\n+            )\n+        return columns\n+\n+    def dump_to_mysql(self, infer_column_size: bool = False) -> tuple[pd.DataFrame, list[dict[str, str | int]]]:",
    "comment": "how about to move this and other functions (including `to_mysql_column_dict`) which prepare data for mysql server - to mysql server? ",
    "line_number": 657,
    "enriched": "File: mindsdb/api/executor/sql_query/result_set.py\nCode: @@ -262,6 +603,121 @@ def add_raw_values(self, values):\n         )\n         self.add_raw_df(df)\n \n+    def get_ast_columns(self) -> list[TableColumn]:\n+        \"\"\"Converts ResultSet columns to a list of TableColumn objects with SQLAlchemy types.\n+\n+        This method processes each column in the ResultSet, determines its MySQL data type\n+        (inferring it if necessary), and maps it to the appropriate SQLAlchemy type.\n+        The resulting TableColumn objects most likely will be used in CREATE TABLE statement.\n+\n+        Returns:\n+            list[TableColumn]: A list of TableColumn objects with properly mapped SQLAlchemy types\n+        \"\"\"\n+        columns: list[TableColumn] = []\n+\n+        type_mapping = {\n+            MYSQL_DATA_TYPE.TINYINT: sqlalchemy_types.INTEGER,\n+            MYSQL_DATA_TYPE.SMALLINT: sqlalchemy_types.INTEGER,\n+            MYSQL_DATA_TYPE.MEDIUMINT: sqlalchemy_types.INTEGER,\n+            MYSQL_DATA_TYPE.INT: sqlalchemy_types.INTEGER,\n+            MYSQL_DATA_TYPE.BIGINT: sqlalchemy_types.INTEGER,\n+            MYSQL_DATA_TYPE.YEAR: sqlalchemy_types.INTEGER,\n+            MYSQL_DATA_TYPE.BOOL: sqlalchemy_types.BOOLEAN,\n+            MYSQL_DATA_TYPE.BOOLEAN: sqlalchemy_types.BOOLEAN,\n+            MYSQL_DATA_TYPE.FLOAT: sqlalchemy_types.FLOAT,\n+            MYSQL_DATA_TYPE.DOUBLE: sqlalchemy_types.FLOAT,\n+            MYSQL_DATA_TYPE.TIME: sqlalchemy_types.TIME,\n+            MYSQL_DATA_TYPE.DATE: sqlalchemy_types.DATE,\n+            MYSQL_DATA_TYPE.DATETIME: sqlalchemy_types.DATETIME,\n+            MYSQL_DATA_TYPE.TIMESTAMP: sqlalchemy_types.TIMESTAMP,\n+        }\n+\n+        for i, column in enumerate(self._columns):\n+            column_type: MYSQL_DATA_TYPE | None = column.type\n+\n+            # infer MYSQL_DATA_TYPE if not set\n+            if isinstance(column_type, MYSQL_DATA_TYPE) is False:\n+                if column_type is not None:\n+                    logger.warning(f'Unexpected column type: {column_type}')\n+                if self._df is None:\n+                    column_type = MYSQL_DATA_TYPE.TEXT\n+                else:\n+                    column_type = get_mysql_data_type_from_series(self._df.iloc[:, i])\n+\n+            sqlalchemy_type = type_mapping.get(column_type, sqlalchemy_types.TEXT)\n+\n+            columns.append(\n+                TableColumn(\n+                    name=column.alias,\n+                    type=sqlalchemy_type\n+                )\n+            )\n+        return columns\n+\n+    def dump_to_mysql(self, infer_column_size: bool = False) -> tuple[pd.DataFrame, list[dict[str, str | int]]]:\nComment: how about to move this and other functions (including `to_mysql_column_dict`) which prepare data for mysql server - to mysql server? ",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/api/executor/sql_query/result_set.py",
    "pr_number": 10716,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2091058598,
    "comment_created_at": "2025-05-15T12:27:32Z"
  },
  {
    "code": "@@ -395,7 +397,11 @@ def finetune(\n                     deleted_at=null()\n                 ).scalar_subquery()\n             ),\n-            active=False\n+            active=False,\n+            training_metadata={\n+                'hostname': socket.gethostname(),\n+                'reason': 'finetune'",
    "comment": "how is this new value ('reason') used? ",
    "line_number": 403,
    "enriched": "File: mindsdb/integrations/libs/ml_exec_base.py\nCode: @@ -395,7 +397,11 @@ def finetune(\n                     deleted_at=null()\n                 ).scalar_subquery()\n             ),\n-            active=False\n+            active=False,\n+            training_metadata={\n+                'hostname': socket.gethostname(),\n+                'reason': 'finetune'\nComment: how is this new value ('reason') used? ",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/libs/ml_exec_base.py",
    "pr_number": 10217,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1868795032,
    "comment_created_at": "2024-12-04T06:36:44Z"
  },
  {
    "code": "@@ -0,0 +1,79 @@\n+from stability_sdk import client\n+from stability_sdk.client import generation\n+from PIL import Image\n+import io, requests\n+\n+class StabilityAPIClient:\n+    \n+    def __init__(self, api_key, dir_to_save, \n+                 engine=\"stable-diffusion-xl-1024-v1-0\", upscale_engine=\"esrgan-v1-x2plus\"):\n+        self.api_key = api_key",
    "comment": "Please mention in the comments about what this function does.",
    "line_number": 10,
    "enriched": "File: mindsdb/integrations/handlers/stabilityai_handler/stabilityai.py\nCode: @@ -0,0 +1,79 @@\n+from stability_sdk import client\n+from stability_sdk.client import generation\n+from PIL import Image\n+import io, requests\n+\n+class StabilityAPIClient:\n+    \n+    def __init__(self, api_key, dir_to_save, \n+                 engine=\"stable-diffusion-xl-1024-v1-0\", upscale_engine=\"esrgan-v1-x2plus\"):\n+        self.api_key = api_key\nComment: Please mention in the comments about what this function does.",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/stabilityai_handler/stabilityai.py",
    "pr_number": 8114,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1382440538,
    "comment_created_at": "2023-11-04T18:27:03Z"
  },
  {
    "code": "@@ -232,7 +232,23 @@ def _resolve_table_names(self, table_names: List[str], all_tables: List[Identifi\n             table_identifier = tables_idx.get(tuple(table_parts))\n \n             if table_identifier is None:\n-                raise ValueError(f\"Table {table} not found in the database\")\n+                # If the full table name (along with the database and schema) is enclosed in backticks\n+                # raise an error for the agent to correct it\n+                supported_file_formats = ['csv', 'tsv', 'json', 'parquet']\n+                if (\n+                    table_name.startswith('`')\n+                    and table_name.endswith('`')\n+                    and table_name.count('`') == 2\n+                    and len(table_parts) == 1\n+                    # Ensure the table name has at least one dot and it is not in relation to a file e.g. `file.csv`\n+                    and '.' in table_name\n+                    and table_name.split('.')[-1] not in supported_file_formats\n+                ):\n+                    raise ValueError(\n+                        \"Only the table name should be enclosed in backticks.\\n\"\n+                        \"Do not include the database or schema inside the backticks.\"\n+                    )",
    "comment": "why it is prohibited to add backticks to database or schema? ",
    "line_number": 250,
    "enriched": "File: mindsdb/interfaces/skills/sql_agent.py\nCode: @@ -232,7 +232,23 @@ def _resolve_table_names(self, table_names: List[str], all_tables: List[Identifi\n             table_identifier = tables_idx.get(tuple(table_parts))\n \n             if table_identifier is None:\n-                raise ValueError(f\"Table {table} not found in the database\")\n+                # If the full table name (along with the database and schema) is enclosed in backticks\n+                # raise an error for the agent to correct it\n+                supported_file_formats = ['csv', 'tsv', 'json', 'parquet']\n+                if (\n+                    table_name.startswith('`')\n+                    and table_name.endswith('`')\n+                    and table_name.count('`') == 2\n+                    and len(table_parts) == 1\n+                    # Ensure the table name has at least one dot and it is not in relation to a file e.g. `file.csv`\n+                    and '.' in table_name\n+                    and table_name.split('.')[-1] not in supported_file_formats\n+                ):\n+                    raise ValueError(\n+                        \"Only the table name should be enclosed in backticks.\\n\"\n+                        \"Do not include the database or schema inside the backticks.\"\n+                    )\nComment: why it is prohibited to add backticks to database or schema? ",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/interfaces/skills/sql_agent.py",
    "pr_number": 10839,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2092520430,
    "comment_created_at": "2025-05-16T07:45:59Z"
  },
  {
    "code": "@@ -0,0 +1,102 @@\n+import os\n+import time\n+import pytest\n+from unittest.mock import patch\n+\n+import pandas as pd\n+from mindsdb_sql import parse_sql\n+\n+from ..executor_test_base import BaseExecutorTest\n+\n+\n+class TestTimeGPT(BaseExecutorTest):\n+    @staticmethod\n+    def get_api_key():\n+        \"\"\"Retrieve TimeGPT API key from environment variables\"\"\"\n+        return os.environ.get(\"TIME_GPT_API_KEY\") or os.environ.get(\"TIMEGPT_API_KEY\")\n+\n+    def wait_predictor(self, project, name):\n+        # wait\n+        done = False\n+        for attempt in range(200):\n+            ret = self.run_sql(f\"select * from {project}.models where name='{name}'\")\n+            if not ret.empty:\n+                if ret[\"STATUS\"][0] == \"complete\":\n+                    done = True\n+                    break\n+                elif ret[\"STATUS\"][0] == \"error\":\n+                    raise RuntimeError(\"predictor failed\", ret[\"ERROR\"][0])\n+            time.sleep(0.5)\n+        if not done:\n+            raise RuntimeError(\"predictor wasn't created\")\n+\n+    def run_sql(self, sql):\n+        ret = self.command_executor.execute_command(parse_sql(sql, dialect=\"mindsdb\"))\n+        assert ret.error_code is None\n+        if ret.data is not None:\n+            columns = [col.alias if col.alias is not None else col.name for col in ret.columns]\n+            return pd.DataFrame(ret.data, columns=columns)\n+\n+    def test_missing_required_keys(self):\n+        # create project\n+        self.run_sql(\"create database proj\")\n+        self.run_sql(f\"\"\"create ml_engine timegpt from timegpt using api_key='{self.get_api_key()}';\"\"\")\n+        # with pytest.raises(Exception):\n+        self.run_sql(\n+            \"\"\"\n+              create model proj.test_timegpt_missing_required_keys\n+              predict answer\n+              using\n+                engine='timegpt';\n+           \"\"\"\n+        )\n+\n+    def test_unknown_arguments(self):\n+        self.run_sql(\"create database proj\")\n+        self.run_sql(f\"\"\"create ml_engine timegpt from timegpt using api_key='{self.get_api_key()}';\"\"\")\n+        with pytest.raises(Exception):\n+            self.run_sql(\n+                f\"\"\"\n+                create model proj.test_timegpt_unknown_arguments\n+                predict answer\n+                using\n+                    engine='timegpt',\n+                    api_key='{self.get_api_key()}',\n+                    evidently_wrong_argument='wrong value';  --- this is a wrong argument name\n+            \"\"\"\n+            )\n+\n+    @patch(\"mindsdb.integrations.handlers.postgres_handler.Handler\")\n+    def test_forecast_group(self, mock_handler):\n+        # create project\n+        self.run_sql(\"create database proj\")\n+        df = pd.read_csv('tests/unit/ml_handlers/data/house_sales.csv')\n+        self.set_handler(mock_handler, name=\"pg\", tables={\"df\": df})\n+\n+        self.run_sql(f\"\"\"create ml_engine timegpt from timegpt using api_key='{self.get_api_key()}';\"\"\")\n+\n+        self.run_sql(\n+            f\"\"\"\n+           create model proj.test_timegpt_forecast\n+           predict ma\n+           order by saledate\n+           group by type, bedrooms\n+           window 128\n+           horizon 5\n+           using\n+             engine='timegpt',\n+             api_key='{self.get_api_key()}';\n+        \"\"\"\n+        )\n+        self.wait_predictor(\"proj\", \"test_timegpt_forecast\")\n+\n+        self.run_sql(\n+            \"\"\"\n+            SELECT p.ma\n+            FROM proj.test_timegpt_forecast as p\n+            JOIN pg.df as t\n+            WHERE p.saledate > LATEST;\n+        \"\"\"\n+        )\n+\n+        # asserts",
    "comment": "Looks great. Are we missing asserts here ?",
    "line_number": 102,
    "enriched": "File: tests/unit/ml_handlers/test_timegpt.py\nCode: @@ -0,0 +1,102 @@\n+import os\n+import time\n+import pytest\n+from unittest.mock import patch\n+\n+import pandas as pd\n+from mindsdb_sql import parse_sql\n+\n+from ..executor_test_base import BaseExecutorTest\n+\n+\n+class TestTimeGPT(BaseExecutorTest):\n+    @staticmethod\n+    def get_api_key():\n+        \"\"\"Retrieve TimeGPT API key from environment variables\"\"\"\n+        return os.environ.get(\"TIME_GPT_API_KEY\") or os.environ.get(\"TIMEGPT_API_KEY\")\n+\n+    def wait_predictor(self, project, name):\n+        # wait\n+        done = False\n+        for attempt in range(200):\n+            ret = self.run_sql(f\"select * from {project}.models where name='{name}'\")\n+            if not ret.empty:\n+                if ret[\"STATUS\"][0] == \"complete\":\n+                    done = True\n+                    break\n+                elif ret[\"STATUS\"][0] == \"error\":\n+                    raise RuntimeError(\"predictor failed\", ret[\"ERROR\"][0])\n+            time.sleep(0.5)\n+        if not done:\n+            raise RuntimeError(\"predictor wasn't created\")\n+\n+    def run_sql(self, sql):\n+        ret = self.command_executor.execute_command(parse_sql(sql, dialect=\"mindsdb\"))\n+        assert ret.error_code is None\n+        if ret.data is not None:\n+            columns = [col.alias if col.alias is not None else col.name for col in ret.columns]\n+            return pd.DataFrame(ret.data, columns=columns)\n+\n+    def test_missing_required_keys(self):\n+        # create project\n+        self.run_sql(\"create database proj\")\n+        self.run_sql(f\"\"\"create ml_engine timegpt from timegpt using api_key='{self.get_api_key()}';\"\"\")\n+        # with pytest.raises(Exception):\n+        self.run_sql(\n+            \"\"\"\n+              create model proj.test_timegpt_missing_required_keys\n+              predict answer\n+              using\n+                engine='timegpt';\n+           \"\"\"\n+        )\n+\n+    def test_unknown_arguments(self):\n+        self.run_sql(\"create database proj\")\n+        self.run_sql(f\"\"\"create ml_engine timegpt from timegpt using api_key='{self.get_api_key()}';\"\"\")\n+        with pytest.raises(Exception):\n+            self.run_sql(\n+                f\"\"\"\n+                create model proj.test_timegpt_unknown_arguments\n+                predict answer\n+                using\n+                    engine='timegpt',\n+                    api_key='{self.get_api_key()}',\n+                    evidently_wrong_argument='wrong value';  --- this is a wrong argument name\n+            \"\"\"\n+            )\n+\n+    @patch(\"mindsdb.integrations.handlers.postgres_handler.Handler\")\n+    def test_forecast_group(self, mock_handler):\n+        # create project\n+        self.run_sql(\"create database proj\")\n+        df = pd.read_csv('tests/unit/ml_handlers/data/house_sales.csv')\n+        self.set_handler(mock_handler, name=\"pg\", tables={\"df\": df})\n+\n+        self.run_sql(f\"\"\"create ml_engine timegpt from timegpt using api_key='{self.get_api_key()}';\"\"\")\n+\n+        self.run_sql(\n+            f\"\"\"\n+           create model proj.test_timegpt_forecast\n+           predict ma\n+           order by saledate\n+           group by type, bedrooms\n+           window 128\n+           horizon 5\n+           using\n+             engine='timegpt',\n+             api_key='{self.get_api_key()}';\n+        \"\"\"\n+        )\n+        self.wait_predictor(\"proj\", \"test_timegpt_forecast\")\n+\n+        self.run_sql(\n+            \"\"\"\n+            SELECT p.ma\n+            FROM proj.test_timegpt_forecast as p\n+            JOIN pg.df as t\n+            WHERE p.saledate > LATEST;\n+        \"\"\"\n+        )\n+\n+        # asserts\nComment: Looks great. Are we missing asserts here ?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "tests/unit/ml_handlers/test_timegpt.py",
    "pr_number": 7204,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1356671803,
    "comment_created_at": "2023-10-12T11:20:51Z"
  },
  {
    "code": "@@ -144,17 +144,39 @@ def get_chat_config(self) -> Dict:\n         params = {\n             'polling': {\n                 'type': 'message_count',\n-                'table': 'chats',\n-                'chat_id_col': 'id',\n-                'count_col': 'lastMessagePreview_id'\n             },\n-            'chat_table': {\n-                'name': 'chat_messages',\n-                'chat_id_col': 'chatId',\n-                'username_col': 'from_user_displayName',\n-                'text_col': 'body_content',\n-                'time_col': 'createdDateTime',\n-            }\n+            'tables': [\n+                {\n+                    'polling': {\n+                        'type': 'message_count',",
    "comment": "what is the purpose of  'type': 'message_count' here if we have the same setting in the line 146? \r\nwe can't use 'realtime' here",
    "line_number": 151,
    "enriched": "File: mindsdb/integrations/handlers/ms_teams_handler/ms_teams_handler.py\nCode: @@ -144,17 +144,39 @@ def get_chat_config(self) -> Dict:\n         params = {\n             'polling': {\n                 'type': 'message_count',\n-                'table': 'chats',\n-                'chat_id_col': 'id',\n-                'count_col': 'lastMessagePreview_id'\n             },\n-            'chat_table': {\n-                'name': 'chat_messages',\n-                'chat_id_col': 'chatId',\n-                'username_col': 'from_user_displayName',\n-                'text_col': 'body_content',\n-                'time_col': 'createdDateTime',\n-            }\n+            'tables': [\n+                {\n+                    'polling': {\n+                        'type': 'message_count',\nComment: what is the purpose of  'type': 'message_count' here if we have the same setting in the line 146? \r\nwe can't use 'realtime' here",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/ms_teams_handler/ms_teams_handler.py",
    "pr_number": 9768,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1778731556,
    "comment_created_at": "2024-09-27T14:38:36Z"
  },
  {
    "code": "@@ -21,14 +21,13 @@ Hybrid search can be enabled at the time of querying the knowledge base by speci\n SELECT * from my_kb\n WHERE\n \tcontent = ”ACME-213”\n-AND hybrid_search_alpha = 0.8 -- optional, this is 0.5 by default\n-AND hybrid_search = true;\n+AND hybrid_search_alpha = 0.8 -- optional, this is 0.5 by default;",
    "comment": "the default value is 1. by default keyword search doesn't work, only semantic\r\n\r\nalso it is possible to use `hybrid_search` param instead of `hybrid_search_alpha`. these two settings do the same:\r\n- hybrid_search_alpha=0.5\r\n- hybrid_search=true\r\n\r\n",
    "line_number": 24,
    "enriched": "File: docs/mindsdb_sql/knowledge_bases/hybrid_search.mdx\nCode: @@ -21,14 +21,13 @@ Hybrid search can be enabled at the time of querying the knowledge base by speci\n SELECT * from my_kb\n WHERE\n \tcontent = ”ACME-213”\n-AND hybrid_search_alpha = 0.8 -- optional, this is 0.5 by default\n-AND hybrid_search = true;\n+AND hybrid_search_alpha = 0.8 -- optional, this is 0.5 by default;\nComment: the default value is 1. by default keyword search doesn't work, only semantic\r\n\r\nalso it is possible to use `hybrid_search` param instead of `hybrid_search_alpha`. these two settings do the same:\r\n- hybrid_search_alpha=0.5\r\n- hybrid_search=true\r\n\r\n",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "docs/mindsdb_sql/knowledge_bases/hybrid_search.mdx",
    "pr_number": 11958,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2581597404,
    "comment_created_at": "2025-12-02T15:05:47Z"
  },
  {
    "code": "@@ -33,7 +33,7 @@ def get_a2a_app(\n     agent_card = AgentCard(\n         name=\"MindsDB Agent Connector\",\n         description=(f\"A2A connector that proxies requests to MindsDB agents in project '{project_name}'.\"),\n-        url=f\"http://127.0.0.1:{mindsdb_port}\",\n+        url=f\"http://127.0.0.1:{mindsdb_port}/a2a/\",",
    "comment": "isn't it brake anything? is it used by something now?",
    "line_number": 36,
    "enriched": "File: mindsdb/api/a2a/__init__.py\nCode: @@ -33,7 +33,7 @@ def get_a2a_app(\n     agent_card = AgentCard(\n         name=\"MindsDB Agent Connector\",\n         description=(f\"A2A connector that proxies requests to MindsDB agents in project '{project_name}'.\"),\n-        url=f\"http://127.0.0.1:{mindsdb_port}\",\n+        url=f\"http://127.0.0.1:{mindsdb_port}/a2a/\",\nComment: isn't it brake anything? is it used by something now?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/api/a2a/__init__.py",
    "pr_number": 11606,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2387504641,
    "comment_created_at": "2025-09-29T10:48:01Z"
  },
  {
    "code": "@@ -23,6 +23,7 @@\n DEFAULT_CHUNK_OVERLAP = 200\n DEFAULT_POOL_RECYCLE = 3600\n DEFAULT_LLM_MODEL = \"gpt-4o\"\n+DEFAULT_LLM_MODEL_PROVIDER = \"openai\"",
    "comment": "How can we define other providers?",
    "line_number": 26,
    "enriched": "File: mindsdb/integrations/utilities/rag/settings.py\nCode: @@ -23,6 +23,7 @@\n DEFAULT_CHUNK_OVERLAP = 200\n DEFAULT_POOL_RECYCLE = 3600\n DEFAULT_LLM_MODEL = \"gpt-4o\"\n+DEFAULT_LLM_MODEL_PROVIDER = \"openai\"\nComment: How can we define other providers?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/utilities/rag/settings.py",
    "pr_number": 10198,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1853157880,
    "comment_created_at": "2024-11-22T01:24:27Z"
  },
  {
    "code": "@@ -637,18 +640,60 @@ def answer_create_chatbot(self, statement):\n \n         name = statement.name\n         project_name = name.parts[-2] if len(name.parts) > 1 else self.session.database\n+        chat_engine = statement.params.pop('chat_engine', None)\n+        is_running = statement.params.pop('is_running', True)\n \n         database = self.session.integration_controller.get(statement.database.parts[-1])\n-        if database is None:\n-            raise SqlApiException(f'Database not found: {statement.database}')\n+        if database is None and chat_engine is None:\n+            raise SqlApiException(f'Database not found and no chat engine provided: {statement.database}')\n+\n+        # Database ID cannot be null\n+        database_id = database['id'] if database is not None else -1",
    "comment": "what does '-1' means? ",
    "line_number": 651,
    "enriched": "File: mindsdb/api/mysql/mysql_proxy/executor/executor_commands.py\nCode: @@ -637,18 +640,60 @@ def answer_create_chatbot(self, statement):\n \n         name = statement.name\n         project_name = name.parts[-2] if len(name.parts) > 1 else self.session.database\n+        chat_engine = statement.params.pop('chat_engine', None)\n+        is_running = statement.params.pop('is_running', True)\n \n         database = self.session.integration_controller.get(statement.database.parts[-1])\n-        if database is None:\n-            raise SqlApiException(f'Database not found: {statement.database}')\n+        if database is None and chat_engine is None:\n+            raise SqlApiException(f'Database not found and no chat engine provided: {statement.database}')\n+\n+        # Database ID cannot be null\n+        database_id = database['id'] if database is not None else -1\nComment: what does '-1' means? ",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/api/mysql/mysql_proxy/executor/executor_commands.py",
    "pr_number": 6730,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1255361607,
    "comment_created_at": "2023-07-07T07:16:21Z"
  },
  {
    "code": "@@ -51,32 +52,42 @@ def _make_text_to_sql_tools(self, skill: db.Skills) -> dict:\n         '''\n            Uses SQLAgent to execute tool\n         '''\n-\n+        # To prevent dependency on Langchain unless an actual tool uses it.\n+        try:\n+            from mindsdb.integrations.handlers.langchain_handler.mindsdb_database_agent import MindsDBSQL\n+            from langchain_community.agent_toolkits.sql.toolkit import SQLDatabaseToolkit\n+            from langchain_community.tools.sql_database.tool import QuerySQLDataBaseTool\n+            from langchain_community.chat_models import ChatOpenAI\n+        except ImportError:\n+            raise ImportError('To use the text-to-SQL skill, please install langchain with `pip install mindsdb[langchain]`')\n         database = skill.params['database']\n         tables = skill.params['tables']\n-\n-        sql_agent = SQLAgent(\n-            self.get_command_executor(),\n-            database,\n-            include_tables=tables\n-        )\n-\n-        description = (\n-            \"Use the conversation context to decide which table to query. \"\n-            \"Input to this tool is a detailed and correct SQL query, output is a result from the database. \"\n-            \"If the query is not correct, an error message will be returned. \"\n-            \"If an error is returned, rewrite the query, check the query, and try again. \"\n-            f\"These are the available tables: {','.join(tables)}\\n\"\n-        )\n-        for table in tables:\n-            description += f'Table name: \"{table}\", columns {sql_agent.get_table_columns(table)}\\n'\n-\n-        return dict(\n-            name='sql_db_query',\n-            func=sql_agent.query_safe,\n-            description=description,\n-            type=skill.type\n+        tables_to_include = [f'{database}.{table}' for table in tables]\n+        db = MindsDBSQL(\n+            engine=self.get_command_executor(),\n+            metadata=self.get_command_executor().session.integration_controller,\n+            include_tables=tables_to_include\n         )\n+        # Users probably don't need to configure this for now.\n+        llm = ChatOpenAI(model=_DEFAULT_SQL_LLM_MODEL, temperature=0)",
    "comment": "Are you sure this won't cause issues if user hasn't got openai env variabel set? see https://github.com/mindsdb/mindsdb/pull/9080/files",
    "line_number": 72,
    "enriched": "File: mindsdb/interfaces/skills/skill_tool.py\nCode: @@ -51,32 +52,42 @@ def _make_text_to_sql_tools(self, skill: db.Skills) -> dict:\n         '''\n            Uses SQLAgent to execute tool\n         '''\n-\n+        # To prevent dependency on Langchain unless an actual tool uses it.\n+        try:\n+            from mindsdb.integrations.handlers.langchain_handler.mindsdb_database_agent import MindsDBSQL\n+            from langchain_community.agent_toolkits.sql.toolkit import SQLDatabaseToolkit\n+            from langchain_community.tools.sql_database.tool import QuerySQLDataBaseTool\n+            from langchain_community.chat_models import ChatOpenAI\n+        except ImportError:\n+            raise ImportError('To use the text-to-SQL skill, please install langchain with `pip install mindsdb[langchain]`')\n         database = skill.params['database']\n         tables = skill.params['tables']\n-\n-        sql_agent = SQLAgent(\n-            self.get_command_executor(),\n-            database,\n-            include_tables=tables\n-        )\n-\n-        description = (\n-            \"Use the conversation context to decide which table to query. \"\n-            \"Input to this tool is a detailed and correct SQL query, output is a result from the database. \"\n-            \"If the query is not correct, an error message will be returned. \"\n-            \"If an error is returned, rewrite the query, check the query, and try again. \"\n-            f\"These are the available tables: {','.join(tables)}\\n\"\n-        )\n-        for table in tables:\n-            description += f'Table name: \"{table}\", columns {sql_agent.get_table_columns(table)}\\n'\n-\n-        return dict(\n-            name='sql_db_query',\n-            func=sql_agent.query_safe,\n-            description=description,\n-            type=skill.type\n+        tables_to_include = [f'{database}.{table}' for table in tables]\n+        db = MindsDBSQL(\n+            engine=self.get_command_executor(),\n+            metadata=self.get_command_executor().session.integration_controller,\n+            include_tables=tables_to_include\n         )\n+        # Users probably don't need to configure this for now.\n+        llm = ChatOpenAI(model=_DEFAULT_SQL_LLM_MODEL, temperature=0)\nComment: Are you sure this won't cause issues if user hasn't got openai env variabel set? see https://github.com/mindsdb/mindsdb/pull/9080/files",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/interfaces/skills/skill_tool.py",
    "pr_number": 9132,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1579320566,
    "comment_created_at": "2024-04-25T11:35:08Z"
  },
  {
    "code": "@@ -0,0 +1,372 @@\n+from collections import OrderedDict\n+from typing import List, Optional\n+\n+import lancedb\n+import pandas as pd\n+import pyarrow as pa\n+from lance.vector import vec_to_table\n+import duckdb\n+import json\n+\n+from mindsdb.integrations.libs.const import HANDLER_CONNECTION_ARG_TYPE as ARG_TYPE\n+from mindsdb.integrations.libs.response import RESPONSE_TYPE\n+from mindsdb.integrations.libs.response import HandlerResponse\n+from mindsdb.integrations.libs.response import HandlerResponse as Response\n+from mindsdb.integrations.libs.response import HandlerStatusResponse as StatusResponse\n+from mindsdb.integrations.libs.vectordatabase_handler import (\n+    FilterCondition,\n+    FilterOperator,\n+    TableField,\n+    VectorStoreHandler,\n+)\n+from mindsdb.utilities import log\n+\n+\n+class LanceDBHandler(VectorStoreHandler):\n+    \"\"\"This handler handles connection and execution of the LanceDB statements.\"\"\"\n+\n+    name = \"lancedb\"\n+\n+    def __init__(self, name: str, **kwargs):\n+        super().__init__(name, **kwargs)\n+        self._connection_data = kwargs.get(\"connection_data\")\n+\n+        self._client_config = {\n+            \"uri\": self._connection_data.get(\"persist_directory\"),\n+            \"api_key\": self._connection_data.get(\"api_key\", None),\n+            \"region\": self._connection_data.get(\"region\"),\n+            \"host_override\": self._connection_data.get(\"host_override\"),\n+        }\n+\n+        # uri is required either for LanceDB Cloud or local\n+        if not self._client_config[\"uri\"]:\n+            raise Exception(\n+                \"persist_directory is required for LanceDB connection!\"\n+            )\n+        # uri, api_key and region is required either for LanceDB Cloud\n+        elif self._client_config[\"uri\"] and self._client_config[\"api_key\"] and not self._client_config[\"region\"]:\n+            raise Exception(\n+                \"region is required for LanceDB Cloud connection!\"\n+            )\n+\n+        self._client = None\n+        self.is_connected = False\n+        self.connect()\n+\n+    def _get_client(self):\n+        client_config = self._client_config\n+        if client_config is None:\n+            raise Exception(\"Client config is not set!\")\n+        return lancedb.connect(**client_config)\n+\n+    def __del__(self):\n+        if self.is_connected is True:\n+            self.disconnect()\n+\n+    def connect(self):\n+        \"\"\"Connect to a LanceDB database.\"\"\"\n+        if self.is_connected is True:\n+            return\n+        try:\n+            self._client = self._get_client()\n+            self.is_connected = True\n+        except Exception as e:\n+            log.logger.error(f\"Error connecting to LanceDB client, {e}!\")\n+            self.is_connected = False\n+\n+    def disconnect(self):\n+        \"\"\"Close the database connection.\"\"\"\n+        if self.is_connected is False:\n+            return\n+        self._client = None\n+        self.is_connected = False\n+\n+    def check_connection(self):\n+        \"\"\"Check the connection to the LanceDB database.\"\"\"\n+        response_code = StatusResponse(False)\n+        need_to_close = self.is_connected is False\n+\n+        try:\n+            # self._client.heartbeat()\n+            response_code.success = True",
    "comment": "This doesn't check the connection? Do we need the heartbeat uncommented? ",
    "line_number": 91,
    "enriched": "File: mindsdb/integrations/handlers/lancedb_handler/lancedb_handler.py\nCode: @@ -0,0 +1,372 @@\n+from collections import OrderedDict\n+from typing import List, Optional\n+\n+import lancedb\n+import pandas as pd\n+import pyarrow as pa\n+from lance.vector import vec_to_table\n+import duckdb\n+import json\n+\n+from mindsdb.integrations.libs.const import HANDLER_CONNECTION_ARG_TYPE as ARG_TYPE\n+from mindsdb.integrations.libs.response import RESPONSE_TYPE\n+from mindsdb.integrations.libs.response import HandlerResponse\n+from mindsdb.integrations.libs.response import HandlerResponse as Response\n+from mindsdb.integrations.libs.response import HandlerStatusResponse as StatusResponse\n+from mindsdb.integrations.libs.vectordatabase_handler import (\n+    FilterCondition,\n+    FilterOperator,\n+    TableField,\n+    VectorStoreHandler,\n+)\n+from mindsdb.utilities import log\n+\n+\n+class LanceDBHandler(VectorStoreHandler):\n+    \"\"\"This handler handles connection and execution of the LanceDB statements.\"\"\"\n+\n+    name = \"lancedb\"\n+\n+    def __init__(self, name: str, **kwargs):\n+        super().__init__(name, **kwargs)\n+        self._connection_data = kwargs.get(\"connection_data\")\n+\n+        self._client_config = {\n+            \"uri\": self._connection_data.get(\"persist_directory\"),\n+            \"api_key\": self._connection_data.get(\"api_key\", None),\n+            \"region\": self._connection_data.get(\"region\"),\n+            \"host_override\": self._connection_data.get(\"host_override\"),\n+        }\n+\n+        # uri is required either for LanceDB Cloud or local\n+        if not self._client_config[\"uri\"]:\n+            raise Exception(\n+                \"persist_directory is required for LanceDB connection!\"\n+            )\n+        # uri, api_key and region is required either for LanceDB Cloud\n+        elif self._client_config[\"uri\"] and self._client_config[\"api_key\"] and not self._client_config[\"region\"]:\n+            raise Exception(\n+                \"region is required for LanceDB Cloud connection!\"\n+            )\n+\n+        self._client = None\n+        self.is_connected = False\n+        self.connect()\n+\n+    def _get_client(self):\n+        client_config = self._client_config\n+        if client_config is None:\n+            raise Exception(\"Client config is not set!\")\n+        return lancedb.connect(**client_config)\n+\n+    def __del__(self):\n+        if self.is_connected is True:\n+            self.disconnect()\n+\n+    def connect(self):\n+        \"\"\"Connect to a LanceDB database.\"\"\"\n+        if self.is_connected is True:\n+            return\n+        try:\n+            self._client = self._get_client()\n+            self.is_connected = True\n+        except Exception as e:\n+            log.logger.error(f\"Error connecting to LanceDB client, {e}!\")\n+            self.is_connected = False\n+\n+    def disconnect(self):\n+        \"\"\"Close the database connection.\"\"\"\n+        if self.is_connected is False:\n+            return\n+        self._client = None\n+        self.is_connected = False\n+\n+    def check_connection(self):\n+        \"\"\"Check the connection to the LanceDB database.\"\"\"\n+        response_code = StatusResponse(False)\n+        need_to_close = self.is_connected is False\n+\n+        try:\n+            # self._client.heartbeat()\n+            response_code.success = True\nComment: This doesn't check the connection? Do we need the heartbeat uncommented? ",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/lancedb_handler/lancedb_handler.py",
    "pr_number": 8135,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1376098652,
    "comment_created_at": "2023-10-30T12:10:56Z"
  },
  {
    "code": "@@ -0,0 +1,123 @@\n+---\n+title: Create Agent\n+sidebarTitle: Create Agent\n+---\n+\n+**POST `/api/projects/{project_name}/agents`**\n+\n+This API endpoint creates an agent using the `POST` method.\n+\n+<Tip>\n+Learn more about agents and the available parameters following [this doc page](/mindsdb_sql/agents/agent).\n+</Tip>\n+\n+### Path Parameters\n+\n+<ParamField body='project_name' type='string' required>\n+Defines the project where the agents are located. Note that the default project name is `mindsdb`.\n+</ParamField>\n+\n+### Body\n+\n+<ParamField body='name' type='string' required>\n+Name of the agent.\n+</ParamField>\n+\n+<ParamField body='model' type='string'>",
    "comment": "@martyna-mindsdb I don't quite recall if Azure OpenAI can be used with agents, but if so, should we mention the other parameters that are required for that provider?",
    "line_number": 26,
    "enriched": "File: docs/rest/agents/create.mdx\nCode: @@ -0,0 +1,123 @@\n+---\n+title: Create Agent\n+sidebarTitle: Create Agent\n+---\n+\n+**POST `/api/projects/{project_name}/agents`**\n+\n+This API endpoint creates an agent using the `POST` method.\n+\n+<Tip>\n+Learn more about agents and the available parameters following [this doc page](/mindsdb_sql/agents/agent).\n+</Tip>\n+\n+### Path Parameters\n+\n+<ParamField body='project_name' type='string' required>\n+Defines the project where the agents are located. Note that the default project name is `mindsdb`.\n+</ParamField>\n+\n+### Body\n+\n+<ParamField body='name' type='string' required>\n+Name of the agent.\n+</ParamField>\n+\n+<ParamField body='model' type='string'>\nComment: @martyna-mindsdb I don't quite recall if Azure OpenAI can be used with agents, but if so, should we mention the other parameters that are required for that provider?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "docs/rest/agents/create.mdx",
    "pr_number": 11264,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2230340307,
    "comment_created_at": "2025-07-25T07:11:11Z"
  },
  {
    "code": "@@ -0,0 +1,16 @@\n+---",
    "comment": "looks like extra file, not related to PR",
    "line_number": 1,
    "enriched": "File: docs/faqs/a2a-unavailable.mdx\nCode: @@ -0,0 +1,16 @@\n+---\nComment: looks like extra file, not related to PR",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "docs/faqs/a2a-unavailable.mdx",
    "pr_number": 11521,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2324280753,
    "comment_created_at": "2025-09-05T07:04:06Z"
  },
  {
    "code": "@@ -171,3 +174,44 @@ def get_columns(self, table_name: str) -> Response:\n                 table_name = '{table_name}'\n         \"\"\"\n         return self.native_query(query)\n+\n+connection_args = OrderedDict(\n+    user={\n+        'type': ARG_TYPE.STR,\n+        'description': 'The user name used to authenticate with the PostgreSQL server.',\n+        'required': True,\n+        'label': 'User'\n+    },\n+    password={\n+        'type': ARG_TYPE.PWD,\n+        'description': 'The password to authenticate the user with the PostgreSQL server.',\n+        'required': True,\n+        'label': 'Password'\n+    },\n+    database={\n+        'type': ARG_TYPE.STR,\n+        'description': 'The database name to use when connecting with the PostgreSQL server.',\n+        'required': True,\n+        'label': 'Database'\n+    },\n+    host={\n+        'type': ARG_TYPE.STR,\n+        'description': 'The host name or IP address of the PostgreSQL server. NOTE: use \\'127.0.0.1\\' instead of \\'localhost\\' to connect to local server.',\n+        'required': True,\n+        'label': 'Host'\n+    },\n+    port={\n+        'type': ARG_TYPE.INT,\n+        'description': 'The TCP/IP port of the PostgreSQL server. Must be an integer.',\n+        'required': True,\n+        'label': 'Port'\n+    },",
    "comment": "We are missing the schema as optional param",
    "line_number": 208,
    "enriched": "File: mindsdb/integrations/handlers/postgres_handler/postgres_handler.py\nCode: @@ -171,3 +174,44 @@ def get_columns(self, table_name: str) -> Response:\n                 table_name = '{table_name}'\n         \"\"\"\n         return self.native_query(query)\n+\n+connection_args = OrderedDict(\n+    user={\n+        'type': ARG_TYPE.STR,\n+        'description': 'The user name used to authenticate with the PostgreSQL server.',\n+        'required': True,\n+        'label': 'User'\n+    },\n+    password={\n+        'type': ARG_TYPE.PWD,\n+        'description': 'The password to authenticate the user with the PostgreSQL server.',\n+        'required': True,\n+        'label': 'Password'\n+    },\n+    database={\n+        'type': ARG_TYPE.STR,\n+        'description': 'The database name to use when connecting with the PostgreSQL server.',\n+        'required': True,\n+        'label': 'Database'\n+    },\n+    host={\n+        'type': ARG_TYPE.STR,\n+        'description': 'The host name or IP address of the PostgreSQL server. NOTE: use \\'127.0.0.1\\' instead of \\'localhost\\' to connect to local server.',\n+        'required': True,\n+        'label': 'Host'\n+    },\n+    port={\n+        'type': ARG_TYPE.INT,\n+        'description': 'The TCP/IP port of the PostgreSQL server. Must be an integer.',\n+        'required': True,\n+        'label': 'Port'\n+    },\nComment: We are missing the schema as optional param",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/postgres_handler/postgres_handler.py",
    "pr_number": 6623,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1233314197,
    "comment_created_at": "2023-06-18T14:17:51Z"
  },
  {
    "code": "@@ -0,0 +1,191 @@\n+import pandas as pd\n+\n+from typing import List\n+\n+from mindsdb.integrations.libs.api_handler import APITable\n+from mindsdb.integrations.utilities.sql_utils import extract_comparison_conditions\n+from mindsdb.utilities.log import get_log\n+\n+from mindsdb_sql.parser import ast\n+\n+logger = get_log(\"integrations.gitlab_handler\")\n+\n+class GitlabIssuesTable(APITable):\n+    \"\"\"The GitLab Issue Table implementation\"\"\"\n+\n+    def select(self, query: ast.Select) -> pd.DataFrame:\n+        \"\"\"Pulls data from the GitLab \"List repository issues\" API\n+        Args:\n+            query: SELECT\n+        Returns:\n+            DataFrame\n+        Raises:\n+            ValueError\n+        \"\"\"\n+\n+        conditions = extract_comparison_conditions(query.where)\n+\n+        if query.limit:\n+            total_results = query.limit.value\n+        else:\n+            total_results = 20\n+\n+        issues_kwargs = {}\n+        order_by_conditions = {}\n+\n+        if query.order_by and len(query.order_by) > 0:\n+            order_by_conditions[\"columns\"] = []\n+            order_by_conditions[\"ascending\"] = []\n+\n+            for an_order in query.order_by:\n+                if an_order.field.parts[0] != \"issues\":\n+                    next",
    "comment": "Did you mean 'continue' ?",
    "line_number": 42,
    "enriched": "File: mindsdb/integrations/handlers/gitlab_handler/gitlab_tables.py\nCode: @@ -0,0 +1,191 @@\n+import pandas as pd\n+\n+from typing import List\n+\n+from mindsdb.integrations.libs.api_handler import APITable\n+from mindsdb.integrations.utilities.sql_utils import extract_comparison_conditions\n+from mindsdb.utilities.log import get_log\n+\n+from mindsdb_sql.parser import ast\n+\n+logger = get_log(\"integrations.gitlab_handler\")\n+\n+class GitlabIssuesTable(APITable):\n+    \"\"\"The GitLab Issue Table implementation\"\"\"\n+\n+    def select(self, query: ast.Select) -> pd.DataFrame:\n+        \"\"\"Pulls data from the GitLab \"List repository issues\" API\n+        Args:\n+            query: SELECT\n+        Returns:\n+            DataFrame\n+        Raises:\n+            ValueError\n+        \"\"\"\n+\n+        conditions = extract_comparison_conditions(query.where)\n+\n+        if query.limit:\n+            total_results = query.limit.value\n+        else:\n+            total_results = 20\n+\n+        issues_kwargs = {}\n+        order_by_conditions = {}\n+\n+        if query.order_by and len(query.order_by) > 0:\n+            order_by_conditions[\"columns\"] = []\n+            order_by_conditions[\"ascending\"] = []\n+\n+            for an_order in query.order_by:\n+                if an_order.field.parts[0] != \"issues\":\n+                    next\nComment: Did you mean 'continue' ?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/gitlab_handler/gitlab_tables.py",
    "pr_number": 5768,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1179173897,
    "comment_created_at": "2023-04-27T13:39:04Z"
  },
  {
    "code": "@@ -0,0 +1,102 @@\n+from mindsdb.integrations.handlers.strava_handler.strava_tables import StravaAllClubsTable\n+from mindsdb.integrations.handlers.strava_handler.strava_tables import StravaClubActivitesTable\n+from mindsdb.integrations.libs.api_handler import APIHandler\n+from mindsdb.integrations.libs.response import (\n+    HandlerStatusResponse as StatusResponse,\n+)\n+from mindsdb.utilities.log import get_log\n+from mindsdb_sql import parse_sql\n+\n+import requests\n+import pandas as pd\n+import json\n+\n+logger = get_log(\"integrations.stravahandler\")\n+\n+class StravaHandler(APIHandler):\n+    \"\"\"Strava handler implementation\"\"\"\n+\n+    def __init__(self, name=None, **kwargs):\n+        \"\"\"Initialize the Confluence handler.",
    "comment": "Please change this",
    "line_number": 20,
    "enriched": "File: mindsdb/integrations/handlers/strava_handler/strava_handler.py\nCode: @@ -0,0 +1,102 @@\n+from mindsdb.integrations.handlers.strava_handler.strava_tables import StravaAllClubsTable\n+from mindsdb.integrations.handlers.strava_handler.strava_tables import StravaClubActivitesTable\n+from mindsdb.integrations.libs.api_handler import APIHandler\n+from mindsdb.integrations.libs.response import (\n+    HandlerStatusResponse as StatusResponse,\n+)\n+from mindsdb.utilities.log import get_log\n+from mindsdb_sql import parse_sql\n+\n+import requests\n+import pandas as pd\n+import json\n+\n+logger = get_log(\"integrations.stravahandler\")\n+\n+class StravaHandler(APIHandler):\n+    \"\"\"Strava handler implementation\"\"\"\n+\n+    def __init__(self, name=None, **kwargs):\n+        \"\"\"Initialize the Confluence handler.\nComment: Please change this",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/strava_handler/strava_handler.py",
    "pr_number": 5771,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1175088012,
    "comment_created_at": "2023-04-24T10:24:10Z"
  },
  {
    "code": "@@ -0,0 +1,81 @@\n+\"\"\"Utility functions for RAG pipeline configuration\"\"\"\n+from typing import Dict, Any, Optional\n+\n+from mindsdb.utilities import log\n+from mindsdb.integrations.utilities.rag.settings import (\n+    RetrieverType, MultiVectorRetrieverMode, SearchType,\n+    SearchKwargs, SummarizationConfig, VectorStoreConfig,\n+    RerankerConfig, RAGPipelineModel, DEFAULT_COLLECTION_NAME\n+)\n+\n+logger = log.getlogger(__name__)\n+\n+\n+def load_rag_config(base_config: Dict[str, Any], kb_params: Optional[Dict[str, Any]] = None, embedding_model: Any = None) -> RAGPipelineModel:",
    "comment": "we should centralising rag config loading for retrieval tool on agent and knoweldge base standalone RAG",
    "line_number": 14,
    "enriched": "File: mindsdb/integrations/utilities/rag/config_loader.py\nCode: @@ -0,0 +1,81 @@\n+\"\"\"Utility functions for RAG pipeline configuration\"\"\"\n+from typing import Dict, Any, Optional\n+\n+from mindsdb.utilities import log\n+from mindsdb.integrations.utilities.rag.settings import (\n+    RetrieverType, MultiVectorRetrieverMode, SearchType,\n+    SearchKwargs, SummarizationConfig, VectorStoreConfig,\n+    RerankerConfig, RAGPipelineModel, DEFAULT_COLLECTION_NAME\n+)\n+\n+logger = log.getlogger(__name__)\n+\n+\n+def load_rag_config(base_config: Dict[str, Any], kb_params: Optional[Dict[str, Any]] = None, embedding_model: Any = None) -> RAGPipelineModel:\nComment: we should centralising rag config loading for retrieval tool on agent and knoweldge base standalone RAG",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/utilities/rag/config_loader.py",
    "pr_number": 10271,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1879883073,
    "comment_created_at": "2024-12-11T11:17:39Z"
  },
  {
    "code": "@@ -57,13 +57,22 @@ def _anyscale_base_api(self, args: dict, key='OPENAI_API_BASE'):\n \n     @staticmethod\n     def create_validation(target, args=None, **kwargs):\n-        # remove original key for the validation check in `OpenAIHandler`\n-        api_key_name = 'anyscale_endpoints_api_key'\n-        if api_key_name in args['using']:\n-            del args['using'][api_key_name]\n-        if api_key_name in args:\n-            del args[api_key_name]\n-        OpenAIHandler.create_validation(target, args, **kwargs)\n+        api_key = AnyscaleEndpointsHandler._get_api_key(args, kwargs['handler_storage'])\n+        args['using']['openai_api_key'] = api_key\n+\n+        key = 'OPENAI_API_BASE'",
    "comment": "why openai in anyscale handler?",
    "line_number": 63,
    "enriched": "File: mindsdb/integrations/handlers/anyscale_endpoints_handler/anyscale_endpoints_handler.py\nCode: @@ -57,13 +57,22 @@ def _anyscale_base_api(self, args: dict, key='OPENAI_API_BASE'):\n \n     @staticmethod\n     def create_validation(target, args=None, **kwargs):\n-        # remove original key for the validation check in `OpenAIHandler`\n-        api_key_name = 'anyscale_endpoints_api_key'\n-        if api_key_name in args['using']:\n-            del args['using'][api_key_name]\n-        if api_key_name in args:\n-            del args[api_key_name]\n-        OpenAIHandler.create_validation(target, args, **kwargs)\n+        api_key = AnyscaleEndpointsHandler._get_api_key(args, kwargs['handler_storage'])\n+        args['using']['openai_api_key'] = api_key\n+\n+        key = 'OPENAI_API_BASE'\nComment: why openai in anyscale handler?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/anyscale_endpoints_handler/anyscale_endpoints_handler.py",
    "pr_number": 8847,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1506218352,
    "comment_created_at": "2024-02-28T16:12:37Z"
  },
  {
    "code": "@@ -1,18 +1,52 @@\n-To run test, execute from project root:\n+## Test Structure\n \n+Our tests are organized into several subdirectories, each focusing on different aspects of our application:",
    "comment": "There is another sub directory called `handler_tests`; should we ignore that for now (if we are planning on moving it to unit) or does it need to be mentioned?",
    "line_number": 3,
    "enriched": "File: tests/README.md\nCode: @@ -1,18 +1,52 @@\n-To run test, execute from project root:\n+## Test Structure\n \n+Our tests are organized into several subdirectories, each focusing on different aspects of our application:\nComment: There is another sub directory called `handler_tests`; should we ignore that for now (if we are planning on moving it to unit) or does it need to be mentioned?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "tests/README.md",
    "pr_number": 8730,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1490621121,
    "comment_created_at": "2024-02-15T08:39:58Z"
  },
  {
    "code": "@@ -455,3 +439,234 @@ def get_columns(self, table_name) -> Response:\n         result.to_columns_table_response(map_type_fn=_map_type)\n \n         return result\n+\n+    def meta_get_tables(self, table_names: Optional[List[str]] = None) -> Response:\n+        \"\"\"\n+        Retrieves metadata information about the tables in the Snowflake database to be stored in the data catalog.\n+\n+        Args:\n+            table_names (list): A list of table names for which to retrieve metadata information.\n+\n+        Returns:\n+            Response: A response object containing the metadata information, formatted as per the `Response` class.\n+        \"\"\"\n+        query = \"\"\"\n+            SELECT\n+                TABLE_CATALOG,\n+                TABLE_SCHEMA,\n+                TABLE_NAME,\n+                TABLE_TYPE,\n+                COMMENT AS TABLE_DESCRIPTION,\n+                ROW_COUNT,\n+                CREATED,\n+                LAST_ALTERED\n+            FROM INFORMATION_SCHEMA.TABLES\n+            WHERE TABLE_SCHEMA = current_schema()\n+            AND TABLE_TYPE IN ('BASE TABLE', 'VIEW')\n+        \"\"\"\n+\n+        if table_names is not None and len(table_names) > 0:\n+            table_names_str = \", \".join([f\"'{t.upper()}'\" for t in table_names])\n+            query += f\" AND TABLE_NAME IN ({table_names_str})\"\n+\n+        result = self.native_query(query)\n+        return result\n+\n+    def meta_get_columns(self, table_names: Optional[List[str]] = None) -> Response:\n+        \"\"\"\n+        Retrieves column metadata for the specified tables (or all tables if no list is provided).\n+\n+        Args:\n+            table_names (list): A list of table names for which to retrieve column metadata.\n+\n+        Returns:\n+            Response: A response object containing the column metadata.\n+        \"\"\"\n+        query = \"\"\"\n+            SELECT\n+                TABLE_NAME,\n+                COLUMN_NAME,\n+                DATA_TYPE,\n+                COMMENT AS COLUMN_DESCRIPTION,\n+                COLUMN_DEFAULT,\n+                (IS_NULLABLE = 'YES') AS IS_NULLABLE\n+                CHARACTER_MAXIMUM_LENGTH,\n+                CHARACTER_OCTET_LENGTH,",
    "comment": "**correctness**: `meta_get_columns` query is missing a comma after `(IS_NULLABLE = 'YES') AS IS_NULLABLE`, causing SQL syntax error and incorrect results.\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n                (IS_NULLABLE = 'YES') AS IS_NULLABLE,\n                CHARACTER_MAXIMUM_LENGTH,\n```\n</details>\n<!-- suggestion_end -->\n",
    "line_number": 494,
    "enriched": "File: mindsdb/integrations/handlers/snowflake_handler/snowflake_handler.py\nCode: @@ -455,3 +439,234 @@ def get_columns(self, table_name) -> Response:\n         result.to_columns_table_response(map_type_fn=_map_type)\n \n         return result\n+\n+    def meta_get_tables(self, table_names: Optional[List[str]] = None) -> Response:\n+        \"\"\"\n+        Retrieves metadata information about the tables in the Snowflake database to be stored in the data catalog.\n+\n+        Args:\n+            table_names (list): A list of table names for which to retrieve metadata information.\n+\n+        Returns:\n+            Response: A response object containing the metadata information, formatted as per the `Response` class.\n+        \"\"\"\n+        query = \"\"\"\n+            SELECT\n+                TABLE_CATALOG,\n+                TABLE_SCHEMA,\n+                TABLE_NAME,\n+                TABLE_TYPE,\n+                COMMENT AS TABLE_DESCRIPTION,\n+                ROW_COUNT,\n+                CREATED,\n+                LAST_ALTERED\n+            FROM INFORMATION_SCHEMA.TABLES\n+            WHERE TABLE_SCHEMA = current_schema()\n+            AND TABLE_TYPE IN ('BASE TABLE', 'VIEW')\n+        \"\"\"\n+\n+        if table_names is not None and len(table_names) > 0:\n+            table_names_str = \", \".join([f\"'{t.upper()}'\" for t in table_names])\n+            query += f\" AND TABLE_NAME IN ({table_names_str})\"\n+\n+        result = self.native_query(query)\n+        return result\n+\n+    def meta_get_columns(self, table_names: Optional[List[str]] = None) -> Response:\n+        \"\"\"\n+        Retrieves column metadata for the specified tables (or all tables if no list is provided).\n+\n+        Args:\n+            table_names (list): A list of table names for which to retrieve column metadata.\n+\n+        Returns:\n+            Response: A response object containing the column metadata.\n+        \"\"\"\n+        query = \"\"\"\n+            SELECT\n+                TABLE_NAME,\n+                COLUMN_NAME,\n+                DATA_TYPE,\n+                COMMENT AS COLUMN_DESCRIPTION,\n+                COLUMN_DEFAULT,\n+                (IS_NULLABLE = 'YES') AS IS_NULLABLE\n+                CHARACTER_MAXIMUM_LENGTH,\n+                CHARACTER_OCTET_LENGTH,\nComment: **correctness**: `meta_get_columns` query is missing a comma after `(IS_NULLABLE = 'YES') AS IS_NULLABLE`, causing SQL syntax error and incorrect results.\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n                (IS_NULLABLE = 'YES') AS IS_NULLABLE,\n                CHARACTER_MAXIMUM_LENGTH,\n```\n</details>\n<!-- suggestion_end -->\n",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "mindsdb/integrations/handlers/snowflake_handler/snowflake_handler.py",
    "pr_number": 10951,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2116083987,
    "comment_created_at": "2025-05-30T14:55:34Z"
  },
  {
    "code": "@@ -0,0 +1,87 @@\n+---\n+title: Teradata\n+sidebarTitle: Teradata\n+---\n+\n+# Teradata Handler\n+\n+This is the implementation of the Teradata handler for MindsDB.\n+\n+## Teradata Database\n+\n+Teradata Vantage Advanced SQL Engine (formerly known as Teradata Database) is a connected multi-cloud data platform offering for enterprise analytics, that enables users to solve complex data challenges from start to scale. It supports huge data warehouse applications and was designed with a patented massively parallel processing (MPP) architecture. [Read more](https://docs.teradata.com/r/Teradata-VantageTM-Advanced-SQL-Engine-Release-Summary/June-2022/Introduction/What-is-Teradata-Vantage).\n+\n+## Implementation\n+\n+This handler was implemented using `teradatasql` - the Python driver for Teradata.\n+\n+The required arguments to establish a connection are,\n+\n+* `host`: the host name or IP address of the Teradata Vantage instance\n+* `user`: specifies the user name\n+* `password`: specifies the password for the user\n+* `database`: sets the database for the connection\n+\n+## Usage\n+\n+Assuming you created a database in Teradata called `HR` and you have a table called `Employees` that was created using\n+the following SQL statements:\n+\n+~~~~sql",
    "comment": "Please use the below code block:\r\n\r\n> \\```sql\r\n> SQL GOES HERE\r\n> \\```",
    "line_number": 30,
    "enriched": "File: docs/app-integrations/teradata.mdx\nCode: @@ -0,0 +1,87 @@\n+---\n+title: Teradata\n+sidebarTitle: Teradata\n+---\n+\n+# Teradata Handler\n+\n+This is the implementation of the Teradata handler for MindsDB.\n+\n+## Teradata Database\n+\n+Teradata Vantage Advanced SQL Engine (formerly known as Teradata Database) is a connected multi-cloud data platform offering for enterprise analytics, that enables users to solve complex data challenges from start to scale. It supports huge data warehouse applications and was designed with a patented massively parallel processing (MPP) architecture. [Read more](https://docs.teradata.com/r/Teradata-VantageTM-Advanced-SQL-Engine-Release-Summary/June-2022/Introduction/What-is-Teradata-Vantage).\n+\n+## Implementation\n+\n+This handler was implemented using `teradatasql` - the Python driver for Teradata.\n+\n+The required arguments to establish a connection are,\n+\n+* `host`: the host name or IP address of the Teradata Vantage instance\n+* `user`: specifies the user name\n+* `password`: specifies the password for the user\n+* `database`: sets the database for the connection\n+\n+## Usage\n+\n+Assuming you created a database in Teradata called `HR` and you have a table called `Employees` that was created using\n+the following SQL statements:\n+\n+~~~~sql\nComment: Please use the below code block:\r\n\r\n> \\```sql\r\n> SQL GOES HERE\r\n> \\```",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "docs/app-integrations/teradata.mdx",
    "pr_number": 5386,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1155352739,
    "comment_created_at": "2023-04-02T18:07:05Z"
  },
  {
    "code": "@@ -334,3 +334,38 @@ class ChatBotsHistory(Base):\n     destination = Column(String)\n     sent_at = Column(DateTime, default=datetime.datetime.now)\n     error = Column(String)\n+\n+\n+class Triggers(Base):\n+    __tablename__ = 'triggers'\n+    id = Column(Integer, primary_key=True)\n+\n+    name = Column(String, nullable=False)\n+    project_id = Column(Integer, nullable=False)\n+\n+    database_id = Column(Integer)\n+    table_name = Column(String, nullable=False)\n+    query_str = Column(String, nullable=False)\n+    columns = Column(String, nullable=False)  # list of columns separated by delimiter\n+\n+    created_at = Column(DateTime, default=datetime.datetime.now)\n+\n+\n+class Tasks(Base):\n+    __tablename__ = 'tasks'\n+    id = Column(Integer, primary_key=True)\n+    company_id = Column(Integer)\n+    user_class = Column(Integer, nullable=True)\n+\n+    # trigger, chatbot\n+    object_type = Column(String, nullable=False)\n+    object_id = Column(Integer, nullable=False)\n+\n+    last_error = Column(String)",
    "comment": "Looks like data in table could be updated. In that case let add `updated_at` field",
    "line_number": 364,
    "enriched": "File: mindsdb/interfaces/storage/db.py\nCode: @@ -334,3 +334,38 @@ class ChatBotsHistory(Base):\n     destination = Column(String)\n     sent_at = Column(DateTime, default=datetime.datetime.now)\n     error = Column(String)\n+\n+\n+class Triggers(Base):\n+    __tablename__ = 'triggers'\n+    id = Column(Integer, primary_key=True)\n+\n+    name = Column(String, nullable=False)\n+    project_id = Column(Integer, nullable=False)\n+\n+    database_id = Column(Integer)\n+    table_name = Column(String, nullable=False)\n+    query_str = Column(String, nullable=False)\n+    columns = Column(String, nullable=False)  # list of columns separated by delimiter\n+\n+    created_at = Column(DateTime, default=datetime.datetime.now)\n+\n+\n+class Tasks(Base):\n+    __tablename__ = 'tasks'\n+    id = Column(Integer, primary_key=True)\n+    company_id = Column(Integer)\n+    user_class = Column(Integer, nullable=True)\n+\n+    # trigger, chatbot\n+    object_type = Column(String, nullable=False)\n+    object_id = Column(Integer, nullable=False)\n+\n+    last_error = Column(String)\nComment: Looks like data in table could be updated. In that case let add `updated_at` field",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "mindsdb/interfaces/storage/db.py",
    "pr_number": 6819,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1283008734,
    "comment_created_at": "2023-08-03T10:35:22Z"
  },
  {
    "code": "@@ -210,10 +210,8 @@ def do_clean_process_marks():\n         if import_meta.get(\"success\", False) is not True:\n             logger.info(\n                 dedent(\n-                    \"\"\"\n-                Some handlers cannot be imported. You can check list of available handlers by execute command in sql editor:\n-                    select * from information_schema.handlers;\n-            \"\"\"\n+                    \"\"\"Some handlers cannot be imported. You can check list of available handlers by execute command in sql editor:\n+                    select * from information_schema.handlers;\"\"\"",
    "comment": "will be good to remove most of heading spaces in second line of message",
    "line_number": 214,
    "enriched": "File: mindsdb/__main__.py\nCode: @@ -210,10 +210,8 @@ def do_clean_process_marks():\n         if import_meta.get(\"success\", False) is not True:\n             logger.info(\n                 dedent(\n-                    \"\"\"\n-                Some handlers cannot be imported. You can check list of available handlers by execute command in sql editor:\n-                    select * from information_schema.handlers;\n-            \"\"\"\n+                    \"\"\"Some handlers cannot be imported. You can check list of available handlers by execute command in sql editor:\n+                    select * from information_schema.handlers;\"\"\"\nComment: will be good to remove most of heading spaces in second line of message",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "mindsdb/__main__.py",
    "pr_number": 8869,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1514677618,
    "comment_created_at": "2024-03-06T15:22:22Z"
  },
  {
    "code": "@@ -127,6 +134,36 @@ def list_databases() -> Dict[str, Any]:\n         }\n \n \n+class CustomAuthMiddleware(BaseHTTPMiddleware):\n+    \"\"\"Custom middleware to handle authentication basing on header 'Authorization'\n+    \"\"\"\n+    async def dispatch(self, request: Request, call_next):\n+        mcp_access_token = os.environ.get('MDB_MCP_ACCESS_TOKEN')\n+        if mcp_access_token is not None:\n+            auth_token = request.headers.get('Authorization', '').partition('Bearer ')[-1]\n+            if mcp_access_token != auth_token:\n+                return Response(status_code=401, content=\"Unauthorized\", media_type=\"text/plain\")\n+\n+        response = await call_next(request)\n+\n+        return response",
    "comment": "**Correctness**: The `dispatch` method in `CustomAuthMiddleware` is missing proper indentation for its method body, which will cause Python syntax errors when executed.\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n    async def dispatch(self, request: Request, call_next):\n        mcp_access_token = os.environ.get('MDB_MCP_ACCESS_TOKEN')\n        if mcp_access_token is not None:\n            auth_token = request.headers.get('Authorization', '').partition('Bearer ')[-1]\n            if mcp_access_token != auth_token:\n                return Response(status_code=401, content=\"Unauthorized\", media_type=\"text/plain\")\n\n        response = await call_next(request)\n\n        return response\n```\n</details>\n<!-- suggestion_end -->\n",
    "line_number": 149,
    "enriched": "File: mindsdb/api/mcp/start.py\nCode: @@ -127,6 +134,36 @@ def list_databases() -> Dict[str, Any]:\n         }\n \n \n+class CustomAuthMiddleware(BaseHTTPMiddleware):\n+    \"\"\"Custom middleware to handle authentication basing on header 'Authorization'\n+    \"\"\"\n+    async def dispatch(self, request: Request, call_next):\n+        mcp_access_token = os.environ.get('MDB_MCP_ACCESS_TOKEN')\n+        if mcp_access_token is not None:\n+            auth_token = request.headers.get('Authorization', '').partition('Bearer ')[-1]\n+            if mcp_access_token != auth_token:\n+                return Response(status_code=401, content=\"Unauthorized\", media_type=\"text/plain\")\n+\n+        response = await call_next(request)\n+\n+        return response\nComment: **Correctness**: The `dispatch` method in `CustomAuthMiddleware` is missing proper indentation for its method body, which will cause Python syntax errors when executed.\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n    async def dispatch(self, request: Request, call_next):\n        mcp_access_token = os.environ.get('MDB_MCP_ACCESS_TOKEN')\n        if mcp_access_token is not None:\n            auth_token = request.headers.get('Authorization', '').partition('Bearer ')[-1]\n            if mcp_access_token != auth_token:\n                return Response(status_code=401, content=\"Unauthorized\", media_type=\"text/plain\")\n\n        response = await call_next(request)\n\n        return response\n```\n</details>\n<!-- suggestion_end -->\n",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "mindsdb/api/mcp/start.py",
    "pr_number": 10874,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2103099380,
    "comment_created_at": "2025-05-22T17:44:44Z"
  },
  {
    "code": "@@ -94,6 +87,20 @@ def get_tools(self, prefix=\"\") -> List[BaseTool]:\n             8. Identity and Purpose:\n                - When asked about yourself or your maker, state that you are a Data-Mind, created by MindsDB to help answer data questions.\n                - When asked about your purpose or how you can help, explore the available data sources and then explain that you can answer questions based on the connected data. Provide a few relevant example questions that you could answer for the user about their data.\n+            9. Knowledge Base Queries:\n+               - To query a knowledge base, use a `SELECT` statement. For semantic search, use a `WHERE` clause on the `content` column.\n+               - Example: `SELECT * FROM my_kb WHERE content = 'find information about X';`\n+               - The `LIKE` operator is not supported for semantic search on the `content` column.\n+               - The following SQL operators are supported for filtering by metadata and content:\n+                 - Comparison: =, <>, !=, <, >, <=, >=\n+                 - Logical: AND, OR, NOT\n+                 - Pattern matching: LIKE, NOT LIKE\n+                 - Range: BETWEEN, NOT BETWEEN, IN, NOT IN\n+                 - NULL filtering: IS NULL, IS NOT NULL\n+               - Examples:\n+                 - `SELECT * FROM my_kb WHERE metadata.author = 'John Doe' AND metadata.year > 2020;`\n+                 - `SELECT * FROM my_kb WHERE content LIKE '%search_term%';`",
    "comment": "I'm not sure we support syntax with LIKE command in knowledge bases. \r\nI believe it works with `=` sign rathen than LIKE:\r\n```SELECT * FROM my_kb WHERE content = '%search_term%';```\r\ncan you please double check",
    "line_number": 102,
    "enriched": "File: mindsdb/interfaces/skills/custom/text2sql/mindsdb_sql_toolkit.py\nCode: @@ -94,6 +87,20 @@ def get_tools(self, prefix=\"\") -> List[BaseTool]:\n             8. Identity and Purpose:\n                - When asked about yourself or your maker, state that you are a Data-Mind, created by MindsDB to help answer data questions.\n                - When asked about your purpose or how you can help, explore the available data sources and then explain that you can answer questions based on the connected data. Provide a few relevant example questions that you could answer for the user about their data.\n+            9. Knowledge Base Queries:\n+               - To query a knowledge base, use a `SELECT` statement. For semantic search, use a `WHERE` clause on the `content` column.\n+               - Example: `SELECT * FROM my_kb WHERE content = 'find information about X';`\n+               - The `LIKE` operator is not supported for semantic search on the `content` column.\n+               - The following SQL operators are supported for filtering by metadata and content:\n+                 - Comparison: =, <>, !=, <, >, <=, >=\n+                 - Logical: AND, OR, NOT\n+                 - Pattern matching: LIKE, NOT LIKE\n+                 - Range: BETWEEN, NOT BETWEEN, IN, NOT IN\n+                 - NULL filtering: IS NULL, IS NOT NULL\n+               - Examples:\n+                 - `SELECT * FROM my_kb WHERE metadata.author = 'John Doe' AND metadata.year > 2020;`\n+                 - `SELECT * FROM my_kb WHERE content LIKE '%search_term%';`\nComment: I'm not sure we support syntax with LIKE command in knowledge bases. \r\nI believe it works with `=` sign rathen than LIKE:\r\n```SELECT * FROM my_kb WHERE content = '%search_term%';```\r\ncan you please double check",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "mindsdb/interfaces/skills/custom/text2sql/mindsdb_sql_toolkit.py",
    "pr_number": 11304,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2279111816,
    "comment_created_at": "2025-08-15T14:38:07Z"
  },
  {
    "code": "@@ -87,6 +95,56 @@ def call(self, step: FetchDataframeStepPartition) -> ResultSet:\n         else:\n             return self.fetch_iterate(run_query, query, on_error=on_error)\n \n+    def repeat_till_reach_limit(self, step, limit):\n+        first_table_limit = limit * 2\n+        dn = self.session.datahub.get(step.integration)\n+\n+        query = step.query\n+\n+        # fill params\n+        query, context_callback = query_context_controller.handle_db_context_vars(query, dn, self.session)\n+\n+        try_num = 1\n+        while True:\n+            self.substeps = copy.deepcopy(step.steps)\n+            query2 = copy.deepcopy(query)\n+\n+            if first_table_limit is not None:\n+                query2.limit = Constant(int(first_table_limit))\n+            else:\n+                query2.limit = None\n+\n+            response = dn.query(query=query2, session=self.session)\n+            df = response.data_frame\n+\n+            result = self.exec_sub_steps(df)\n+\n+            if len(result) >= limit or len(df) < first_table_limit or first_table_limit is None:\n+                # we have enough results\n+                #  OR first table doesn't return requested count of rows\n+                #  OR it is a flag to stop\n+                result = result[:limit]\n+                break\n+\n+            if try_num > 3:\n+                # the last try without the limit\n+                first_table_limit = None\n+                continue\n+\n+            # no enough results\n+            if len(result) > 0:\n+                # forecast the required limit (depending on how much row we don't have)\n+                first_table_limit = first_table_limit * limit / len(result) * try_num + 10**try_num",
    "comment": "first_table_limit must be rounded here, instead of line 113\r\nOtherwise check in line 122 may be wrong (for example `len([1,2,3]) < 3.1` )",
    "line_number": 137,
    "enriched": "File: mindsdb/api/executor/sql_query/steps/fetch_dataframe_partition.py\nCode: @@ -87,6 +95,56 @@ def call(self, step: FetchDataframeStepPartition) -> ResultSet:\n         else:\n             return self.fetch_iterate(run_query, query, on_error=on_error)\n \n+    def repeat_till_reach_limit(self, step, limit):\n+        first_table_limit = limit * 2\n+        dn = self.session.datahub.get(step.integration)\n+\n+        query = step.query\n+\n+        # fill params\n+        query, context_callback = query_context_controller.handle_db_context_vars(query, dn, self.session)\n+\n+        try_num = 1\n+        while True:\n+            self.substeps = copy.deepcopy(step.steps)\n+            query2 = copy.deepcopy(query)\n+\n+            if first_table_limit is not None:\n+                query2.limit = Constant(int(first_table_limit))\n+            else:\n+                query2.limit = None\n+\n+            response = dn.query(query=query2, session=self.session)\n+            df = response.data_frame\n+\n+            result = self.exec_sub_steps(df)\n+\n+            if len(result) >= limit or len(df) < first_table_limit or first_table_limit is None:\n+                # we have enough results\n+                #  OR first table doesn't return requested count of rows\n+                #  OR it is a flag to stop\n+                result = result[:limit]\n+                break\n+\n+            if try_num > 3:\n+                # the last try without the limit\n+                first_table_limit = None\n+                continue\n+\n+            # no enough results\n+            if len(result) > 0:\n+                # forecast the required limit (depending on how much row we don't have)\n+                first_table_limit = first_table_limit * limit / len(result) * try_num + 10**try_num\nComment: first_table_limit must be rounded here, instead of line 113\r\nOtherwise check in line 122 may be wrong (for example `len([1,2,3]) < 3.1` )",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "mindsdb/api/executor/sql_query/steps/fetch_dataframe_partition.py",
    "pr_number": 11904,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2572074572,
    "comment_created_at": "2025-11-28T16:09:42Z"
  },
  {
    "code": "@@ -0,0 +1,190 @@\n+import sys\n+import time\n+import threading\n+from dataclasses import dataclass, field\n+from collections import defaultdict\n+\n+from mindsdb.integrations.libs.base import DatabaseHandler\n+from mindsdb.utilities.context import context as ctx\n+from mindsdb.utilities import log\n+\n+logger = log.getLogger(__name__)\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class HandlersCacheRecord:\n+    \"\"\"Record for a handler in the cache\n+\n+    Args:\n+        handler (DatabaseHandler): handler instance\n+        expired_at (float): time when the handler will be expired\n+    \"\"\"\n+\n+    handler: DatabaseHandler\n+    expired_at: float\n+    connect_attempt_done: threading.Event = field(default_factory=threading.Event)\n+\n+    @property\n+    def expired(self) -> bool:\n+        \"\"\"check if the handler is expired\n+\n+        Returns:\n+            bool: True if the handler is expired, False otherwise\n+        \"\"\"\n+        return self.expired_at < time.time()\n+\n+    @property\n+    def has_references(self) -> bool:\n+        \"\"\"check if the handler has references\n+\n+        Returns:\n+            bool: True if the handler has references, False otherwise\n+        \"\"\"\n+        return sys.getrefcount(self.handler) > 2",
    "comment": "`if it is not in use at the moment` is detected by this line, right? ",
    "line_number": 43,
    "enriched": "File: mindsdb/interfaces/database/data_handlers_cache.py\nCode: @@ -0,0 +1,190 @@\n+import sys\n+import time\n+import threading\n+from dataclasses import dataclass, field\n+from collections import defaultdict\n+\n+from mindsdb.integrations.libs.base import DatabaseHandler\n+from mindsdb.utilities.context import context as ctx\n+from mindsdb.utilities import log\n+\n+logger = log.getLogger(__name__)\n+\n+\n+@dataclass(kw_only=True, slots=True)\n+class HandlersCacheRecord:\n+    \"\"\"Record for a handler in the cache\n+\n+    Args:\n+        handler (DatabaseHandler): handler instance\n+        expired_at (float): time when the handler will be expired\n+    \"\"\"\n+\n+    handler: DatabaseHandler\n+    expired_at: float\n+    connect_attempt_done: threading.Event = field(default_factory=threading.Event)\n+\n+    @property\n+    def expired(self) -> bool:\n+        \"\"\"check if the handler is expired\n+\n+        Returns:\n+            bool: True if the handler is expired, False otherwise\n+        \"\"\"\n+        return self.expired_at < time.time()\n+\n+    @property\n+    def has_references(self) -> bool:\n+        \"\"\"check if the handler has references\n+\n+        Returns:\n+            bool: True if the handler has references, False otherwise\n+        \"\"\"\n+        return sys.getrefcount(self.handler) > 2\nComment: `if it is not in use at the moment` is detected by this line, right? ",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "mindsdb/interfaces/database/data_handlers_cache.py",
    "pr_number": 11653,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2412882093,
    "comment_created_at": "2025-10-08T07:37:03Z"
  },
  {
    "code": "@@ -0,0 +1,325 @@\n+from concurrent.futures import as_completed, TimeoutError\n+from typing import Dict, List\n+import os\n+import re\n+\n+from langchain.agents import AgentExecutor\n+from langchain.agents.initialize import initialize_agent\n+from langchain.chains.conversation.memory import ConversationSummaryBufferMemory\n+from langchain.schema import SystemMessage\n+from langchain_community.chat_models import ChatAnthropic, ChatOpenAI, ChatAnyscale, ChatLiteLLM, ChatOllama\n+from langchain_core.prompts import PromptTemplate\n+from langfuse import Langfuse\n+from langfuse.callback import CallbackHandler\n+\n+import numpy as np\n+import pandas as pd\n+\n+from mindsdb.integrations.handlers.openai_handler.constants import CHAT_MODELS as OPEN_AI_CHAT_MODELS\n+from mindsdb.integrations.libs.llm.utils import get_llm_config\n+from mindsdb.integrations.utilities.handler_utils import get_api_key\n+from mindsdb.integrations.handlers.langchain_embedding_handler.langchain_embedding_handler import construct_model_from_args\n+from mindsdb.utilities import log\n+from mindsdb.utilities.context_executor import ContextThreadPoolExecutor\n+from mindsdb.interfaces.storage import db\n+\n+from .mindsdb_chat_model import ChatMindsdb\n+from .log_callback_handler import LogCallbackHandler\n+from .langfuse_callback_handler import LangfuseCallbackHandler\n+from .tools import langchain_tools_from_skill\n+from .safe_output_parser import SafeOutputParser\n+\n+from .constants import (\n+    DEFAULT_AGENT_TIMEOUT_SECONDS,\n+    DEFAULT_AGENT_TYPE,\n+    DEFAULT_MAX_ITERATIONS,\n+    DEFAULT_MAX_TOKENS,\n+    SUPPORTED_PROVIDERS,\n+    ANTHROPIC_CHAT_MODELS,\n+    OLLAMA_CHAT_MODELS,\n+    USER_COLUMN,\n+    ASSISTANT_COLUMN\n+)\n+\n+_PARSING_ERROR_PREFIXES = ['An output parsing error occured', 'Could not parse LLM output']\n+\n+logger = log.getLogger(__name__)\n+\n+\n+def get_llm_provider(args: Dict) -> str:\n+    if 'provider' in args:\n+        return args['provider']\n+    if args['model_name'] in ANTHROPIC_CHAT_MODELS:\n+        return 'anthropic'\n+    if args['model_name'] in OPEN_AI_CHAT_MODELS:\n+        return 'openai'\n+    if args['model_name'] in OLLAMA_CHAT_MODELS:\n+        return 'ollama'\n+    raise ValueError(\"Invalid model name. Please define provider\")\n+\n+\n+def get_embedding_model_provider(args: Dict) -> str:\n+    if 'embedding_model_provider' in args:\n+        return args['embedding_model_provider']\n+    if 'embedding_model_provider' not in args:\n+        logger.warning('No embedding model provider specified. trying to use llm provider.')\n+        return args.get('embedding_model_provider', get_llm_provider(args))\n+    raise ValueError(\"Invalid model name. Please define provider\")\n+\n+\n+def get_chat_model_params(args: Dict) -> Dict:\n+    model_config = args.copy()\n+    # Include API keys.\n+    model_config['api_keys'] = {\n+        p: get_api_key(p, model_config, None, strict=False) for p in SUPPORTED_PROVIDERS\n+    }\n+    llm_config = get_llm_config(args.get('provider', get_llm_provider(args)), model_config)\n+    config_dict = llm_config.model_dump()\n+    config_dict = {k: v for k, v in config_dict.items() if v is not None}\n+    return config_dict\n+\n+\n+def create_chat_model(args: Dict):",
    "comment": "These utility functions would probably be better off in a separate file",
    "line_number": 82,
    "enriched": "File: mindsdb/interfaces/agents/langchain_agent.py\nCode: @@ -0,0 +1,325 @@\n+from concurrent.futures import as_completed, TimeoutError\n+from typing import Dict, List\n+import os\n+import re\n+\n+from langchain.agents import AgentExecutor\n+from langchain.agents.initialize import initialize_agent\n+from langchain.chains.conversation.memory import ConversationSummaryBufferMemory\n+from langchain.schema import SystemMessage\n+from langchain_community.chat_models import ChatAnthropic, ChatOpenAI, ChatAnyscale, ChatLiteLLM, ChatOllama\n+from langchain_core.prompts import PromptTemplate\n+from langfuse import Langfuse\n+from langfuse.callback import CallbackHandler\n+\n+import numpy as np\n+import pandas as pd\n+\n+from mindsdb.integrations.handlers.openai_handler.constants import CHAT_MODELS as OPEN_AI_CHAT_MODELS\n+from mindsdb.integrations.libs.llm.utils import get_llm_config\n+from mindsdb.integrations.utilities.handler_utils import get_api_key\n+from mindsdb.integrations.handlers.langchain_embedding_handler.langchain_embedding_handler import construct_model_from_args\n+from mindsdb.utilities import log\n+from mindsdb.utilities.context_executor import ContextThreadPoolExecutor\n+from mindsdb.interfaces.storage import db\n+\n+from .mindsdb_chat_model import ChatMindsdb\n+from .log_callback_handler import LogCallbackHandler\n+from .langfuse_callback_handler import LangfuseCallbackHandler\n+from .tools import langchain_tools_from_skill\n+from .safe_output_parser import SafeOutputParser\n+\n+from .constants import (\n+    DEFAULT_AGENT_TIMEOUT_SECONDS,\n+    DEFAULT_AGENT_TYPE,\n+    DEFAULT_MAX_ITERATIONS,\n+    DEFAULT_MAX_TOKENS,\n+    SUPPORTED_PROVIDERS,\n+    ANTHROPIC_CHAT_MODELS,\n+    OLLAMA_CHAT_MODELS,\n+    USER_COLUMN,\n+    ASSISTANT_COLUMN\n+)\n+\n+_PARSING_ERROR_PREFIXES = ['An output parsing error occured', 'Could not parse LLM output']\n+\n+logger = log.getLogger(__name__)\n+\n+\n+def get_llm_provider(args: Dict) -> str:\n+    if 'provider' in args:\n+        return args['provider']\n+    if args['model_name'] in ANTHROPIC_CHAT_MODELS:\n+        return 'anthropic'\n+    if args['model_name'] in OPEN_AI_CHAT_MODELS:\n+        return 'openai'\n+    if args['model_name'] in OLLAMA_CHAT_MODELS:\n+        return 'ollama'\n+    raise ValueError(\"Invalid model name. Please define provider\")\n+\n+\n+def get_embedding_model_provider(args: Dict) -> str:\n+    if 'embedding_model_provider' in args:\n+        return args['embedding_model_provider']\n+    if 'embedding_model_provider' not in args:\n+        logger.warning('No embedding model provider specified. trying to use llm provider.')\n+        return args.get('embedding_model_provider', get_llm_provider(args))\n+    raise ValueError(\"Invalid model name. Please define provider\")\n+\n+\n+def get_chat_model_params(args: Dict) -> Dict:\n+    model_config = args.copy()\n+    # Include API keys.\n+    model_config['api_keys'] = {\n+        p: get_api_key(p, model_config, None, strict=False) for p in SUPPORTED_PROVIDERS\n+    }\n+    llm_config = get_llm_config(args.get('provider', get_llm_provider(args)), model_config)\n+    config_dict = llm_config.model_dump()\n+    config_dict = {k: v for k, v in config_dict.items() if v is not None}\n+    return config_dict\n+\n+\n+def create_chat_model(args: Dict):\nComment: These utility functions would probably be better off in a separate file",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "mindsdb/interfaces/agents/langchain_agent.py",
    "pr_number": 9481,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1679992216,
    "comment_created_at": "2024-07-16T19:52:07Z"
  },
  {
    "code": "@@ -28,7 +28,7 @@ The optional arguments are as follows:\n In order to make use of this handler and connect to the DuckDB database in MindsDB, the following syntax can be used:\n \n ```sql\n-CREATE DATABASE duckdb_datasource\n+CREATE DATABASE duckdb_datasource;",
    "comment": "The semicolon should be at the end of the statement and not in its first line. Please correct it.",
    "line_number": 31,
    "enriched": "File: docs/data-integrations/duckdb.mdx\nCode: @@ -28,7 +28,7 @@ The optional arguments are as follows:\n In order to make use of this handler and connect to the DuckDB database in MindsDB, the following syntax can be used:\n \n ```sql\n-CREATE DATABASE duckdb_datasource\n+CREATE DATABASE duckdb_datasource;\nComment: The semicolon should be at the end of the statement and not in its first line. Please correct it.",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "docs/data-integrations/duckdb.mdx",
    "pr_number": 5810,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1177913152,
    "comment_created_at": "2023-04-26T13:49:29Z"
  },
  {
    "code": "@@ -61,7 +61,14 @@ def post(self):\n             )\n \n         if check_connection:\n-            handler = session.integration_controller.create_tmp_handler(name, database['engine'], parameters)\n+            try:\n+                handler = session.integration_controller.create_tmp_handler(name, database['engine'], parameters)\n+            except ImportError as imort_error:",
    "comment": "typo im**P**ort_error",
    "line_number": 66,
    "enriched": "File: mindsdb/api/http/namespaces/databases.py\nCode: @@ -61,7 +61,14 @@ def post(self):\n             )\n \n         if check_connection:\n-            handler = session.integration_controller.create_tmp_handler(name, database['engine'], parameters)\n+            try:\n+                handler = session.integration_controller.create_tmp_handler(name, database['engine'], parameters)\n+            except ImportError as imort_error:\nComment: typo im**P**ort_error",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "mindsdb/api/http/namespaces/databases.py",
    "pr_number": 10233,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1871347622,
    "comment_created_at": "2024-12-05T13:01:23Z"
  },
  {
    "code": "@@ -1,53 +1,59 @@\n ---\n-title: UPDATE FROM SELECT Statement\n+title: UPDATE Statement\n sidebarTitle: UPDATE\n ---\n \n ## Description\n \n-The `UPDATE FROM SELECT` statement updates data in existing table. The data comes from a subselect query.\n-It can be used as alternative to 'create table' and 'insert into' for store predictions in distinct columns of existing rows\n+MindsDB provides two ways of using the `UPDATE` statement:\n+\n+1. The regular `UPDATE` statement updates specific column values in an existing table.\n+\n+2. The `UPDATE FROM SELECT` statement updates data in an existing table from a subselect query. It can be used as an alternative to `CREATE TABLE` or `INSERT INTO` to store predictions.\n \n ## Syntax\n \n-Here is an example:\n+Here is an example of the regular `UPDATE` statement:\n+\n+```sql\n+UPDATE integration_name.table_name\n+SET column_name = new_value\n+WHERE column_name = old_value\n+```\n+\n+<Info>\n+Please replace the placeholders as follows:\n+\n+- `integration_name` is the name of the connected data source.\n+- `table_name` is the table name within that data asource.",
    "comment": "```suggestion\r\n- `table_name` is the table name within that data source.\r\n```",
    "line_number": 28,
    "enriched": "File: docs/sql/api/update.mdx\nCode: @@ -1,53 +1,59 @@\n ---\n-title: UPDATE FROM SELECT Statement\n+title: UPDATE Statement\n sidebarTitle: UPDATE\n ---\n \n ## Description\n \n-The `UPDATE FROM SELECT` statement updates data in existing table. The data comes from a subselect query.\n-It can be used as alternative to 'create table' and 'insert into' for store predictions in distinct columns of existing rows\n+MindsDB provides two ways of using the `UPDATE` statement:\n+\n+1. The regular `UPDATE` statement updates specific column values in an existing table.\n+\n+2. The `UPDATE FROM SELECT` statement updates data in an existing table from a subselect query. It can be used as an alternative to `CREATE TABLE` or `INSERT INTO` to store predictions.\n \n ## Syntax\n \n-Here is an example:\n+Here is an example of the regular `UPDATE` statement:\n+\n+```sql\n+UPDATE integration_name.table_name\n+SET column_name = new_value\n+WHERE column_name = old_value\n+```\n+\n+<Info>\n+Please replace the placeholders as follows:\n+\n+- `integration_name` is the name of the connected data source.\n+- `table_name` is the table name within that data asource.\nComment: ```suggestion\r\n- `table_name` is the table name within that data source.\r\n```",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "docs/sql/api/update.mdx",
    "pr_number": 6734,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1248708520,
    "comment_created_at": "2023-07-01T08:50:01Z"
  },
  {
    "code": "@@ -7,9 +7,7 @@ sidebarTitle: Remove a Trigger\n \n Triggers enable users to define event-based actions. For example, if a table is updated, then run a query to update predictions.\n \n-<Info>\n-Currently, you can create triggers on MongoDB data sources.\n-</Info>\n+you can create triggers",
    "comment": "is it lost line? ",
    "line_number": 10,
    "enriched": "File: docs/mindsdb_sql/sql/drop/trigger.mdx\nCode: @@ -7,9 +7,7 @@ sidebarTitle: Remove a Trigger\n \n Triggers enable users to define event-based actions. For example, if a table is updated, then run a query to update predictions.\n \n-<Info>\n-Currently, you can create triggers on MongoDB data sources.\n-</Info>\n+you can create triggers\nComment: is it lost line? ",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "docs/mindsdb_sql/sql/drop/trigger.mdx",
    "pr_number": 9116,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1575966039,
    "comment_created_at": "2024-04-23T09:43:51Z"
  },
  {
    "code": "@@ -2,80 +2,109 @@\n from typing import Callable, List, Union\n \n from langchain_core.documents import Document\n-from langchain_text_splitters import MarkdownHeaderTextSplitter, HTMLHeaderTextSplitter, RecursiveCharacterTextSplitter\n+from langchain_text_splitters import (\n+    MarkdownHeaderTextSplitter,\n+    HTMLHeaderTextSplitter,\n+    RecursiveCharacterTextSplitter,\n+)\n+\n+from mindsdb.interfaces.knowledge_base.preprocessing.models import TextChunkingConfig\n \n from mindsdb.utilities import log\n \n DEFAULT_CHUNK_SIZE = 1000\n DEFAULT_CHUNK_OVERLAP = 50\n DEFAULT_MARKDOWN_HEADERS_TO_SPLIT_ON = [\n-    ('#', 'Header 1'),\n-    ('##', 'Header 2'),\n-    ('###', 'Header 3')",
    "comment": "nit: would rather put formatting changes in a separate PR to make it clearer what functionality is being change",
    "line_number": 14,
    "enriched": "File: mindsdb/integrations/utilities/rag/splitters/file_splitter.py\nCode: @@ -2,80 +2,109 @@\n from typing import Callable, List, Union\n \n from langchain_core.documents import Document\n-from langchain_text_splitters import MarkdownHeaderTextSplitter, HTMLHeaderTextSplitter, RecursiveCharacterTextSplitter\n+from langchain_text_splitters import (\n+    MarkdownHeaderTextSplitter,\n+    HTMLHeaderTextSplitter,\n+    RecursiveCharacterTextSplitter,\n+)\n+\n+from mindsdb.interfaces.knowledge_base.preprocessing.models import TextChunkingConfig\n \n from mindsdb.utilities import log\n \n DEFAULT_CHUNK_SIZE = 1000\n DEFAULT_CHUNK_OVERLAP = 50\n DEFAULT_MARKDOWN_HEADERS_TO_SPLIT_ON = [\n-    ('#', 'Header 1'),\n-    ('##', 'Header 2'),\n-    ('###', 'Header 3')\nComment: nit: would rather put formatting changes in a separate PR to make it clearer what functionality is being change",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "mindsdb/integrations/utilities/rag/splitters/file_splitter.py",
    "pr_number": 10243,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1871421124,
    "comment_created_at": "2024-12-05T13:49:09Z"
  },
  {
    "code": "@@ -0,0 +1,256 @@\n+import streamlit as st\n+import mindsdb_sdk\n+import pandas as pd\n+import time\n+import altair as alt\n+\n+# --- UI Config ---\n+st.set_page_config(page_title=\"KEV Security Analytics\", page_icon=\"🛡️\", layout=\"wide\")\n+\n+# Sidebar Menu\n+menu = st.sidebar.radio(\"📌 Navigation\", [\"Dashboard\", \"KEV Insight Copilot\"])\n+\n+st.sidebar.title(\"KEV Security Agent\")\n+server_url = st.sidebar.text_input(\"MindsDB Server URL\", \"http://127.0.0.1:47334\")\n+project_name = st.sidebar.text_input(\"Project Name\", \"mindsdb\")\n+agent_name = st.sidebar.text_input(\"Agent/Table Name\", \"kev_security_agent\")\n+\n+# --- Connect to MindsDB ---\n+@st.cache_resource\n+def connect_mindsdb(url):\n+    \"\"\"Establishes connection to MindsDB server.\"\"\"\n+    try:\n+        return mindsdb_sdk.connect(url)\n+    except Exception:\n+        return None\n+\n+server = connect_mindsdb(server_url)\n+\n+if server is None:\n+    st.error(\"❌ Failed to connect to MindsDB. Please check the URL.\")\n+    st.stop()\n+\n+# Get the MindsDB project\n+try:\n+    project = server.get_project(project_name)\n+except Exception as e:\n+    st.error(f\"❌ Failed to get MindsDB project: {e}\")\n+    st.stop()\n+\n+\n+# ============================================================\n+# ✅ PAGE 1 – DASHBOARD\n+# ============================================================\n+if menu == \"Dashboard\":\n+    st.title(\"📊 CISA KEV Catalog Vulnerability Dashboard\")\n+\n+    query_str = \"\"\"\n+    SELECT\n+        *\n+    FROM cyber_postgres.public.cisa_known_exploited;\n+    \"\"\"\n+    \n+    # --- Data Fetching ---\n+    try:\n+        df = project.query(query_str).fetch()\n+    except Exception as e:\n+        st.error(f\"Query Error: {e}\")\n+        st.stop()\n+    \n+    # --- Data Cleaning and Preprocessing ---\n+    # Rename columns for clarity\n+    df = df.rename(columns={\n+    \"dateAdded\": \"date_added\",\n+    \"vendorProject\": \"vendor\",\n+    \"cveID\": \"cve_id\",\n+    \"knownRansomwareCampaignUse\": \"ransomware_use\"\n+    })\n+\n+    # Convert date to datetime and drop missing dates\n+    df[\"date_added\"] = pd.to_datetime(df[\"date_added\"], errors=\"coerce\")\n+    df.dropna(subset=['date_added'], inplace=True) \n+\n+    # --- KPIs ---\n+    total_vuln = len(df)\n+    unique_vendor = df[\"vendor\"].nunique()\n+    ransomware_related = df[df[\"ransomware_use\"] == \"Yes\"].shape[0]\n+\n+    col1, col2, col3 = st.columns(3)\n+    col1.metric(\"Total KEVs Listed\", total_vuln)\n+    col2.metric(\"Unique Vendors Affected\", unique_vendor)\n+    col3.metric(\"Ransomware-Related CVEs\", ransomware_related)\n+\n+    # --- Monthly Submission Trend Plot ---\n+    st.markdown(\"### 📈 Monthly Submission Trend of KEVs\")\n+    \n+    # Group by month and convert index to DatetimeIndex for correct plotting\n+    monthly_trend_series = df.groupby(df[\"date_added\"].dt.to_period(\"M\")).size()\n+    monthly_trend_series.index = monthly_trend_series.index.to_timestamp() \n+    \n+    # Convert to DataFrame for explicit x/y mapping in st.line_chart (Altair)\n+    monthly_trend_df = monthly_trend_series.rename(\"Count\").reset_index()\n+    monthly_trend_df = monthly_trend_df.rename(columns={'date_added': 'Date'})\n+\n+    # Use st.line_chart for interactive time series plot\n+    st.line_chart(monthly_trend_df, x='Date', y='Count', color=\"#1f77b4\") \n+\n+\n+    # --- Other Charts (using Altair for color and interactivity) ---\n+    colA, colB = st.columns(2)\n+\n+    with colA:\n+        # Top 10 Vendor Distribution\n+        st.markdown(\"### 🏢 Top 10 Frequency Distribution by Vendor\")\n+        top_vendors = df[\"vendor\"].value_counts().head(10).reset_index()\n+        top_vendors.columns = ['Vendor', 'Count']\n+\n+        # Altair chart: color by category\n+        chart_vendors = alt.Chart(top_vendors).mark_bar().encode(\n+            x=alt.X('Count', title='Number of KEVs'),\n+            y=alt.Y('Vendor', sort='-x', title='Vendor'),\n+            color=alt.Color('Vendor', title='Vendor', legend=None), \n+            tooltip=['Vendor', 'Count']\n+        ).properties(\n+            title='Top 10 Frequency Distribution by Vendor'\n+        ).interactive()\n+        st.altair_chart(chart_vendors, use_container_width=True)\n+\n+    with colB:\n+        # Top 10 Product Concentration\n+        st.markdown(\"### 🛠️ Top 10 Vulnerability Concentration by Product\")\n+        top_products = df[\"product\"].value_counts().head(10).reset_index()\n+        top_products.columns = ['Product', 'Count']\n+\n+        # Altair chart: color by category\n+        chart_products = alt.Chart(top_products).mark_bar().encode(\n+            x=alt.X('Count', title='Number of KEVs'),\n+            y=alt.Y('Product', sort='-x', title='Product'),\n+            color=alt.Color('Product', title='Product', legend=None), \n+            tooltip=['Product', 'Count']\n+        ).properties(\n+            title='Top 10 Vulnerability Concentration by Product'\n+        ).interactive()\n+        st.altair_chart(chart_products, use_container_width=True)\n+\n+    # Ransomware Campaign Distribution\n+    st.markdown(\"### 💥 Distribution of Vulnerabilities Used in Ransomware Campaigns\")\n+    ransomware_dist = df[\"ransomware_use\"].value_counts().reset_index()\n+    ransomware_dist.columns = ['Ransomware Use', 'Count'] \n+\n+    # Altair chart: custom colors for 'Yes/No/Unknown'\n+    chart_ransomware = alt.Chart(ransomware_dist).mark_bar().encode(\n+        x=alt.X('Ransomware Use', title='Used in Ransomware Campaign'),\n+        y=alt.Y('Count', title='Number of KEVs'),\n+        color=alt.Color('Ransomware Use', \n+                        scale=alt.Scale(domain=['Yes', 'No', 'Unknown'], range=['#e45757', '#57a44c', '#7f7f7f']), \n+                        title='Ransomware Use'),\n+        tooltip=['Ransomware Use', 'Count']\n+    ).properties(\n+        title='Distribution of Vulnerabilities Used in Ransomware Campaigns'\n+    ).interactive()\n+    st.altair_chart(chart_ransomware, use_container_width=True)\n+\n+\n+    # Prevalent CWE Weakness Frequencies\n+    st.markdown(\"### 🧩 Top 15 Prevalent CWE Weakness Frequencies\")\n+    expanded_cwes = df[\"cwes\"].explode().dropna()\n+    cwe_counts = expanded_cwes.value_counts().head(15).reset_index()\n+    cwe_counts.columns = ['CWE ID', 'Count'] \n+\n+    # Altair chart: color by CWE ID\n+    chart_cwes = alt.Chart(cwe_counts).mark_bar().encode(\n+        x=alt.X('Count', title='Frequency'),\n+        y=alt.Y('CWE ID', sort='-x', title='CWE Weakness'),\n+        color=alt.Color('CWE ID', title='CWE ID', legend=None), \n+        tooltip=['CWE ID', 'Count']\n+    ).properties(\n+        title='Top 15 Prevalent CWE Weakness Frequencies'\n+    ).interactive()\n+    st.altair_chart(chart_cwes, use_container_width=True)\n+\n+\n+    # --- Data Preview ---\n+    st.markdown(\"---\")\n+    st.subheader(\"🔎 KEV Core Data Preview (First 50 Entries)\")\n+    \n+    # Define the core columns explicitly to exclude metadata\n+    core_columns = [\n+        \"cve_id\",         \n+        \"vendor\",         \n+        \"product\",\n+        \"vulnerabilityName\", \n+        \"date_added\",       \n+        \"requiredAction\",\n+        \"dueDate\",\n+        \"ransomware_use\", \n+        \"cwes\"\n+    ]\n+    \n+    # Select only the core columns and the first 50 rows\n+    try:\n+        df_preview = df[core_columns].head(50)\n+        st.dataframe(df_preview, use_container_width=True)\n+    except KeyError as e:\n+        st.error(f\"Configuration Error: One or more specified columns are missing after renaming. Missing columns: {e}\")\n+        st.dataframe(df.head(5), use_container_width=True) \n+\n+\n+# ============================================================\n+# ✅ PAGE 2 – KEV Insight Copilot (Chat Agent)\n+# ============================================================\n+if menu == \"KEV Insight Copilot\":\n+\n+    st.title(\"🤖 KEV Insight Copilot\")\n+    st.markdown(\"Ask anything related to known exploited vulnerabilities.\")\n+\n+    # Init State for chat history\n+    if \"processing\" not in st.session_state:\n+        st.session_state.processing = False\n+    if \"last_answer\" not in st.session_state:\n+        st.session_state.last_answer = None\n+    if \"history\" not in st.session_state:\n+        st.session_state.history = []\n+\n+    question = st.text_input(\"Your Question:\")\n+    ask_btn = st.button(\"Ask\", disabled=st.session_state.processing or len(question.strip()) == 0)\n+\n+    # Send Query to MindsDB Agent\n+    if ask_btn:\n+        st.session_state.processing = True\n+        st.session_state.last_answer = None\n+        st.rerun()\n+\n+    if st.session_state.processing:\n+        with st.spinner(\"Analyzing vulnerability database…\"):\n+            try:\n+                # Query the MindsDB agent table\n+                query_str = f\"SELECT answer FROM {agent_name} WHERE question = '{question}';\"",
    "comment": "**correctness**: `question` is interpolated directly into the SQL query string in `query_str`, allowing for SQL injection if the user enters malicious input.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb hacktoberfest/use-cases/kev_insight_copilot/app.py, lines 222-227, the code constructs an SQL query by directly interpolating the user-provided `question` variable, which can lead to SQL injection vulnerabilities. Refactor this code to use parameterized queries with the `params` argument to safely pass the `question` variable to the query.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n                query_str = f\"SELECT answer FROM {agent_name} WHERE question = %s;\"\n                result = project.query(query_str, params=[question]).fetch()\n```\n</details>\n<!-- suggestion_end -->\n",
    "line_number": 227,
    "enriched": "File: mindsdb hacktoberfest/use-cases/kev_insight_copilot/app.py\nCode: @@ -0,0 +1,256 @@\n+import streamlit as st\n+import mindsdb_sdk\n+import pandas as pd\n+import time\n+import altair as alt\n+\n+# --- UI Config ---\n+st.set_page_config(page_title=\"KEV Security Analytics\", page_icon=\"🛡️\", layout=\"wide\")\n+\n+# Sidebar Menu\n+menu = st.sidebar.radio(\"📌 Navigation\", [\"Dashboard\", \"KEV Insight Copilot\"])\n+\n+st.sidebar.title(\"KEV Security Agent\")\n+server_url = st.sidebar.text_input(\"MindsDB Server URL\", \"http://127.0.0.1:47334\")\n+project_name = st.sidebar.text_input(\"Project Name\", \"mindsdb\")\n+agent_name = st.sidebar.text_input(\"Agent/Table Name\", \"kev_security_agent\")\n+\n+# --- Connect to MindsDB ---\n+@st.cache_resource\n+def connect_mindsdb(url):\n+    \"\"\"Establishes connection to MindsDB server.\"\"\"\n+    try:\n+        return mindsdb_sdk.connect(url)\n+    except Exception:\n+        return None\n+\n+server = connect_mindsdb(server_url)\n+\n+if server is None:\n+    st.error(\"❌ Failed to connect to MindsDB. Please check the URL.\")\n+    st.stop()\n+\n+# Get the MindsDB project\n+try:\n+    project = server.get_project(project_name)\n+except Exception as e:\n+    st.error(f\"❌ Failed to get MindsDB project: {e}\")\n+    st.stop()\n+\n+\n+# ============================================================\n+# ✅ PAGE 1 – DASHBOARD\n+# ============================================================\n+if menu == \"Dashboard\":\n+    st.title(\"📊 CISA KEV Catalog Vulnerability Dashboard\")\n+\n+    query_str = \"\"\"\n+    SELECT\n+        *\n+    FROM cyber_postgres.public.cisa_known_exploited;\n+    \"\"\"\n+    \n+    # --- Data Fetching ---\n+    try:\n+        df = project.query(query_str).fetch()\n+    except Exception as e:\n+        st.error(f\"Query Error: {e}\")\n+        st.stop()\n+    \n+    # --- Data Cleaning and Preprocessing ---\n+    # Rename columns for clarity\n+    df = df.rename(columns={\n+    \"dateAdded\": \"date_added\",\n+    \"vendorProject\": \"vendor\",\n+    \"cveID\": \"cve_id\",\n+    \"knownRansomwareCampaignUse\": \"ransomware_use\"\n+    })\n+\n+    # Convert date to datetime and drop missing dates\n+    df[\"date_added\"] = pd.to_datetime(df[\"date_added\"], errors=\"coerce\")\n+    df.dropna(subset=['date_added'], inplace=True) \n+\n+    # --- KPIs ---\n+    total_vuln = len(df)\n+    unique_vendor = df[\"vendor\"].nunique()\n+    ransomware_related = df[df[\"ransomware_use\"] == \"Yes\"].shape[0]\n+\n+    col1, col2, col3 = st.columns(3)\n+    col1.metric(\"Total KEVs Listed\", total_vuln)\n+    col2.metric(\"Unique Vendors Affected\", unique_vendor)\n+    col3.metric(\"Ransomware-Related CVEs\", ransomware_related)\n+\n+    # --- Monthly Submission Trend Plot ---\n+    st.markdown(\"### 📈 Monthly Submission Trend of KEVs\")\n+    \n+    # Group by month and convert index to DatetimeIndex for correct plotting\n+    monthly_trend_series = df.groupby(df[\"date_added\"].dt.to_period(\"M\")).size()\n+    monthly_trend_series.index = monthly_trend_series.index.to_timestamp() \n+    \n+    # Convert to DataFrame for explicit x/y mapping in st.line_chart (Altair)\n+    monthly_trend_df = monthly_trend_series.rename(\"Count\").reset_index()\n+    monthly_trend_df = monthly_trend_df.rename(columns={'date_added': 'Date'})\n+\n+    # Use st.line_chart for interactive time series plot\n+    st.line_chart(monthly_trend_df, x='Date', y='Count', color=\"#1f77b4\") \n+\n+\n+    # --- Other Charts (using Altair for color and interactivity) ---\n+    colA, colB = st.columns(2)\n+\n+    with colA:\n+        # Top 10 Vendor Distribution\n+        st.markdown(\"### 🏢 Top 10 Frequency Distribution by Vendor\")\n+        top_vendors = df[\"vendor\"].value_counts().head(10).reset_index()\n+        top_vendors.columns = ['Vendor', 'Count']\n+\n+        # Altair chart: color by category\n+        chart_vendors = alt.Chart(top_vendors).mark_bar().encode(\n+            x=alt.X('Count', title='Number of KEVs'),\n+            y=alt.Y('Vendor', sort='-x', title='Vendor'),\n+            color=alt.Color('Vendor', title='Vendor', legend=None), \n+            tooltip=['Vendor', 'Count']\n+        ).properties(\n+            title='Top 10 Frequency Distribution by Vendor'\n+        ).interactive()\n+        st.altair_chart(chart_vendors, use_container_width=True)\n+\n+    with colB:\n+        # Top 10 Product Concentration\n+        st.markdown(\"### 🛠️ Top 10 Vulnerability Concentration by Product\")\n+        top_products = df[\"product\"].value_counts().head(10).reset_index()\n+        top_products.columns = ['Product', 'Count']\n+\n+        # Altair chart: color by category\n+        chart_products = alt.Chart(top_products).mark_bar().encode(\n+            x=alt.X('Count', title='Number of KEVs'),\n+            y=alt.Y('Product', sort='-x', title='Product'),\n+            color=alt.Color('Product', title='Product', legend=None), \n+            tooltip=['Product', 'Count']\n+        ).properties(\n+            title='Top 10 Vulnerability Concentration by Product'\n+        ).interactive()\n+        st.altair_chart(chart_products, use_container_width=True)\n+\n+    # Ransomware Campaign Distribution\n+    st.markdown(\"### 💥 Distribution of Vulnerabilities Used in Ransomware Campaigns\")\n+    ransomware_dist = df[\"ransomware_use\"].value_counts().reset_index()\n+    ransomware_dist.columns = ['Ransomware Use', 'Count'] \n+\n+    # Altair chart: custom colors for 'Yes/No/Unknown'\n+    chart_ransomware = alt.Chart(ransomware_dist).mark_bar().encode(\n+        x=alt.X('Ransomware Use', title='Used in Ransomware Campaign'),\n+        y=alt.Y('Count', title='Number of KEVs'),\n+        color=alt.Color('Ransomware Use', \n+                        scale=alt.Scale(domain=['Yes', 'No', 'Unknown'], range=['#e45757', '#57a44c', '#7f7f7f']), \n+                        title='Ransomware Use'),\n+        tooltip=['Ransomware Use', 'Count']\n+    ).properties(\n+        title='Distribution of Vulnerabilities Used in Ransomware Campaigns'\n+    ).interactive()\n+    st.altair_chart(chart_ransomware, use_container_width=True)\n+\n+\n+    # Prevalent CWE Weakness Frequencies\n+    st.markdown(\"### 🧩 Top 15 Prevalent CWE Weakness Frequencies\")\n+    expanded_cwes = df[\"cwes\"].explode().dropna()\n+    cwe_counts = expanded_cwes.value_counts().head(15).reset_index()\n+    cwe_counts.columns = ['CWE ID', 'Count'] \n+\n+    # Altair chart: color by CWE ID\n+    chart_cwes = alt.Chart(cwe_counts).mark_bar().encode(\n+        x=alt.X('Count', title='Frequency'),\n+        y=alt.Y('CWE ID', sort='-x', title='CWE Weakness'),\n+        color=alt.Color('CWE ID', title='CWE ID', legend=None), \n+        tooltip=['CWE ID', 'Count']\n+    ).properties(\n+        title='Top 15 Prevalent CWE Weakness Frequencies'\n+    ).interactive()\n+    st.altair_chart(chart_cwes, use_container_width=True)\n+\n+\n+    # --- Data Preview ---\n+    st.markdown(\"---\")\n+    st.subheader(\"🔎 KEV Core Data Preview (First 50 Entries)\")\n+    \n+    # Define the core columns explicitly to exclude metadata\n+    core_columns = [\n+        \"cve_id\",         \n+        \"vendor\",         \n+        \"product\",\n+        \"vulnerabilityName\", \n+        \"date_added\",       \n+        \"requiredAction\",\n+        \"dueDate\",\n+        \"ransomware_use\", \n+        \"cwes\"\n+    ]\n+    \n+    # Select only the core columns and the first 50 rows\n+    try:\n+        df_preview = df[core_columns].head(50)\n+        st.dataframe(df_preview, use_container_width=True)\n+    except KeyError as e:\n+        st.error(f\"Configuration Error: One or more specified columns are missing after renaming. Missing columns: {e}\")\n+        st.dataframe(df.head(5), use_container_width=True) \n+\n+\n+# ============================================================\n+# ✅ PAGE 2 – KEV Insight Copilot (Chat Agent)\n+# ============================================================\n+if menu == \"KEV Insight Copilot\":\n+\n+    st.title(\"🤖 KEV Insight Copilot\")\n+    st.markdown(\"Ask anything related to known exploited vulnerabilities.\")\n+\n+    # Init State for chat history\n+    if \"processing\" not in st.session_state:\n+        st.session_state.processing = False\n+    if \"last_answer\" not in st.session_state:\n+        st.session_state.last_answer = None\n+    if \"history\" not in st.session_state:\n+        st.session_state.history = []\n+\n+    question = st.text_input(\"Your Question:\")\n+    ask_btn = st.button(\"Ask\", disabled=st.session_state.processing or len(question.strip()) == 0)\n+\n+    # Send Query to MindsDB Agent\n+    if ask_btn:\n+        st.session_state.processing = True\n+        st.session_state.last_answer = None\n+        st.rerun()\n+\n+    if st.session_state.processing:\n+        with st.spinner(\"Analyzing vulnerability database…\"):\n+            try:\n+                # Query the MindsDB agent table\n+                query_str = f\"SELECT answer FROM {agent_name} WHERE question = '{question}';\"\nComment: **correctness**: `question` is interpolated directly into the SQL query string in `query_str`, allowing for SQL injection if the user enters malicious input.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb hacktoberfest/use-cases/kev_insight_copilot/app.py, lines 222-227, the code constructs an SQL query by directly interpolating the user-provided `question` variable, which can lead to SQL injection vulnerabilities. Refactor this code to use parameterized queries with the `params` argument to safely pass the `question` variable to the query.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n                query_str = f\"SELECT answer FROM {agent_name} WHERE question = %s;\"\n                result = project.query(query_str, params=[question]).fetch()\n```\n</details>\n<!-- suggestion_end -->\n",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "mindsdb hacktoberfest/use-cases/kev_insight_copilot/app.py",
    "pr_number": 11806,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2469990246,
    "comment_created_at": "2025-10-28T15:20:49Z"
  },
  {
    "code": "@@ -123,10 +123,11 @@ def insert(self, df: pd.DataFrame):\n \n         # add embeddings\n         df_emb = self._df_to_embeddings(df)\n+        df = pd.concat([df, df_emb], axis=1)\n \n         # send to vector db\n         db_handler = self._get_vector_db()\n-        db_handler.do_upsert(self._kb.vector_database_table, df_emb)\n+        db_handler.do_upsert(self._kb.vector_database_table, df)",
    "comment": "@TylerSandman , I returned this. Content is stored in vector db too ",
    "line_number": 130,
    "enriched": "File: mindsdb/interfaces/knowledge_base/controller.py\nCode: @@ -123,10 +123,11 @@ def insert(self, df: pd.DataFrame):\n \n         # add embeddings\n         df_emb = self._df_to_embeddings(df)\n+        df = pd.concat([df, df_emb], axis=1)\n \n         # send to vector db\n         db_handler = self._get_vector_db()\n-        db_handler.do_upsert(self._kb.vector_database_table, df_emb)\n+        db_handler.do_upsert(self._kb.vector_database_table, df)\nComment: @TylerSandman , I returned this. Content is stored in vector db too ",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "mindsdb/interfaces/knowledge_base/controller.py",
    "pr_number": 8458,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1412109774,
    "comment_created_at": "2023-12-01T13:34:51Z"
  },
  {
    "code": "@@ -8,7 +8,7 @@ WORKDIR /mindsdb\n # This will almost always invalidate the cache for this stage\n COPY . .\n # Find every FILE that is not a requirements file and delete it\n-RUN find ./ -type f -not -name \"requirements*.txt\" -print | xargs rm -f \\\n+RUN find ./ -type f -not -name \"requirements*.txt\" -print0 | xargs -0 rm -f \\",
    "comment": "**Correctness**: The Docker build process uses 'print | xargs' which breaks when filenames contain spaces. This is fixed by using 'print0 | xargs -0' for proper null-termination.\n\n",
    "line_number": 11,
    "enriched": "File: docker/mindsdb.Dockerfile\nCode: @@ -8,7 +8,7 @@ WORKDIR /mindsdb\n # This will almost always invalidate the cache for this stage\n COPY . .\n # Find every FILE that is not a requirements file and delete it\n-RUN find ./ -type f -not -name \"requirements*.txt\" -print | xargs rm -f \\\n+RUN find ./ -type f -not -name \"requirements*.txt\" -print0 | xargs -0 rm -f \\\nComment: **Correctness**: The Docker build process uses 'print | xargs' which breaks when filenames contain spaces. This is fixed by using 'print0 | xargs -0' for proper null-termination.\n\n",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "docker/mindsdb.Dockerfile",
    "pr_number": 11761,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2440430600,
    "comment_created_at": "2025-10-17T15:43:23Z"
  },
  {
    "code": "@@ -43,6 +44,83 @@ def __init__(self, name: str = None, **kwargs):\n \n         document_data = FrappeDocumentsTable(self)\n         self._register_table('documents', document_data)\n+        self.connection_data = args\n+\n+    def back_office_config(self):\n+        tools = {\n+            'register_sales_invoice': 'have to be used by assistant to register a sales invoice. Input is JSON object serialized as a string. Due date have to be passed in format: \"yyyy-mm-dd\".',\n+            'check_company_exists': 'useful to check the company is exist. Input is company',\n+            'check_expense_type': 'useful to check the expense_type is exist. Input is expense_type',\n+            'check_customer':  'useful to check the customer is exist. Input is customer',\n+\n+        }\n+        return {\n+            'tools': tools,\n+        }\n+\n+    def register_sales_invoice(self, data):\n+        \"\"\"\n+          input is:\n+            {\n+              \"due_date\": \"2023-05-31\",\n+              \"customer\": \"ksim\",\n+              \"items\": [\n+                {\n+                  \"name\": \"T-shirt--\",\n+                  \"description\": \"T-shirt\",\n+                  \"quantity\": 1\n+                }\n+              ]\n+            }\n+        \"\"\"\n+        invoice = json.loads(data)\n+        date = dt.datetime.strptime(invoice['due_date'], '%Y-%m-%d')\n+        if date <= dt.datetime.today():\n+            return 'Error: due_date have to be in the future'\n+\n+        for item in invoice['items']:\n+            # rename column\n+            item['item_name'] = item['name']\n+            item['qty'] = item['quantity']\n+            del item['name']\n+            del item['quantity']\n+\n+            # add required fields\n+            item['uom'] = \"Nos\"\n+            item['conversion_factor'] = 1\n+\n+            income_account = self.connection_data.get('income_account', \"Sales Income - C8\")\n+            item['income_account'] = income_account\n+\n+        try:\n+            self.client.post_document('Sales Invoice', invoice)\n+        except Exception as e:\n+            return f\"Error: {e}\"\n+        return f\"Success\"\n+\n+    # def check_employee_exists(self, name):",
    "comment": "Please delete commented out code",
    "line_number": 101,
    "enriched": "File: mindsdb/integrations/handlers/frappe_handler/frappe_handler.py\nCode: @@ -43,6 +44,83 @@ def __init__(self, name: str = None, **kwargs):\n \n         document_data = FrappeDocumentsTable(self)\n         self._register_table('documents', document_data)\n+        self.connection_data = args\n+\n+    def back_office_config(self):\n+        tools = {\n+            'register_sales_invoice': 'have to be used by assistant to register a sales invoice. Input is JSON object serialized as a string. Due date have to be passed in format: \"yyyy-mm-dd\".',\n+            'check_company_exists': 'useful to check the company is exist. Input is company',\n+            'check_expense_type': 'useful to check the expense_type is exist. Input is expense_type',\n+            'check_customer':  'useful to check the customer is exist. Input is customer',\n+\n+        }\n+        return {\n+            'tools': tools,\n+        }\n+\n+    def register_sales_invoice(self, data):\n+        \"\"\"\n+          input is:\n+            {\n+              \"due_date\": \"2023-05-31\",\n+              \"customer\": \"ksim\",\n+              \"items\": [\n+                {\n+                  \"name\": \"T-shirt--\",\n+                  \"description\": \"T-shirt\",\n+                  \"quantity\": 1\n+                }\n+              ]\n+            }\n+        \"\"\"\n+        invoice = json.loads(data)\n+        date = dt.datetime.strptime(invoice['due_date'], '%Y-%m-%d')\n+        if date <= dt.datetime.today():\n+            return 'Error: due_date have to be in the future'\n+\n+        for item in invoice['items']:\n+            # rename column\n+            item['item_name'] = item['name']\n+            item['qty'] = item['quantity']\n+            del item['name']\n+            del item['quantity']\n+\n+            # add required fields\n+            item['uom'] = \"Nos\"\n+            item['conversion_factor'] = 1\n+\n+            income_account = self.connection_data.get('income_account', \"Sales Income - C8\")\n+            item['income_account'] = income_account\n+\n+        try:\n+            self.client.post_document('Sales Invoice', invoice)\n+        except Exception as e:\n+            return f\"Error: {e}\"\n+        return f\"Success\"\n+\n+    # def check_employee_exists(self, name):\nComment: Please delete commented out code",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "mindsdb/integrations/handlers/frappe_handler/frappe_handler.py",
    "pr_number": 6128,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1221895890,
    "comment_created_at": "2023-06-07T16:37:44Z"
  },
  {
    "code": "@@ -0,0 +1,61 @@\n+# NewsAPI API Handler\n+\n+This handler integrates with the [News API](https://newsapi.org/docs) to make aggregate article (data available to use for model training and predictions.\n+\n+## Example: Selet artcles from news api",
    "comment": "Can you please fix the typo in Select",
    "line_number": 5,
    "enriched": "File: mindsdb/integrations/handlers/newsapi_handler/README.md\nCode: @@ -0,0 +1,61 @@\n+# NewsAPI API Handler\n+\n+This handler integrates with the [News API](https://newsapi.org/docs) to make aggregate article (data available to use for model training and predictions.\n+\n+## Example: Selet artcles from news api\nComment: Can you please fix the typo in Select",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "mindsdb/integrations/handlers/newsapi_handler/README.md",
    "pr_number": 5783,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1176279521,
    "comment_created_at": "2023-04-25T09:49:41Z"
  },
  {
    "code": "@@ -0,0 +1,35 @@\n+---\n+title: SingleStore\n+sidebarTitle: SingleStore\n+---\n+\n+# SingleStore Handler\n+\n+This is the implementation of the SingleStore handler for MindsDB.\n+\n+## SingleStore\n+SingleStore is a proprietary, cloud-native database designed for data-intensive applications. A distributed, relational, SQL database management system that features ANSI SQL support, it is known for speed in data ingest, transaction processing, and query processing.\n+\n+## Implementation\n+This handler was implemented using the `mysql-connector-python` library.\n+\n+The required arguments to establish a connection are,\n+* `host`: host name or IP address\n+* `port`: port used to make TCP/IP connection\n+* `database`: database name\n+* `user`: database user\n+* `password`: database password\n+\n+## Usage\n+In order to make use of this handler and connect to Databricks in MindsDB, the following syntax can be used,\n+~~~~sql",
    "comment": "The code block should be as below:\r\n\r\n> \\```sql\r\n> SQL GOES HERE\r\n> \\```",
    "line_number": 25,
    "enriched": "File: docs/data-integrations/singlestore.mdx\nCode: @@ -0,0 +1,35 @@\n+---\n+title: SingleStore\n+sidebarTitle: SingleStore\n+---\n+\n+# SingleStore Handler\n+\n+This is the implementation of the SingleStore handler for MindsDB.\n+\n+## SingleStore\n+SingleStore is a proprietary, cloud-native database designed for data-intensive applications. A distributed, relational, SQL database management system that features ANSI SQL support, it is known for speed in data ingest, transaction processing, and query processing.\n+\n+## Implementation\n+This handler was implemented using the `mysql-connector-python` library.\n+\n+The required arguments to establish a connection are,\n+* `host`: host name or IP address\n+* `port`: port used to make TCP/IP connection\n+* `database`: database name\n+* `user`: database user\n+* `password`: database password\n+\n+## Usage\n+In order to make use of this handler and connect to Databricks in MindsDB, the following syntax can be used,\n+~~~~sql\nComment: The code block should be as below:\r\n\r\n> \\```sql\r\n> SQL GOES HERE\r\n> \\```",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "docs/data-integrations/singlestore.mdx",
    "pr_number": 5035,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1138981637,
    "comment_created_at": "2023-03-16T16:16:01Z"
  },
  {
    "code": "@@ -179,3 +179,135 @@ def get_columns(self, table_name) -> Response:\n         \"\"\"",
    "comment": "**security**: User-supplied `table_name` in `get_columns` is directly interpolated into SQL, allowing SQL injection if untrusted input is passed.\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n        if not isinstance(table_name, str) or not table_name.isidentifier():\n            raise ValueError('Invalid table_name provided.')\n        query = f\"\"\"\n            SELECT column_name AS Field, data_type as Type\n            FROM `{self.connection_data['project_id']}.{self.connection_data['dataset']}.INFORMATION_SCHEMA.COLUMNS`\n            WHERE table_name = '{table_name}'\n        \"\"\"\n```\n</details>\n<!-- suggestion_end -->\n",
    "line_number": 179,
    "enriched": "File: mindsdb/integrations/handlers/bigquery_handler/bigquery_handler.py\nCode: @@ -179,3 +179,135 @@ def get_columns(self, table_name) -> Response:\n         \"\"\"\nComment: **security**: User-supplied `table_name` in `get_columns` is directly interpolated into SQL, allowing SQL injection if untrusted input is passed.\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n        if not isinstance(table_name, str) or not table_name.isidentifier():\n            raise ValueError('Invalid table_name provided.')\n        query = f\"\"\"\n            SELECT column_name AS Field, data_type as Type\n            FROM `{self.connection_data['project_id']}.{self.connection_data['dataset']}.INFORMATION_SCHEMA.COLUMNS`\n            WHERE table_name = '{table_name}'\n        \"\"\"\n```\n</details>\n<!-- suggestion_end -->\n",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "mindsdb/integrations/handlers/bigquery_handler/bigquery_handler.py",
    "pr_number": 10947,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2115896375,
    "comment_created_at": "2025-05-30T13:06:31Z"
  },
  {
    "code": "@@ -30,12 +31,32 @@ def call(self, step):\n             if step.is_replace:\n                 is_replace = True\n \n+        if len(step.table.parts) > 1:\n+            integration_name = step.table.parts[0]\n+            table_name = Identifier(parts=step.table.parts[1:])\n+        else:\n+            integration_name = self.context['database']\n+            table_name = step.table\n+\n+        dn = self.session.datahub.get(integration_name)\n+\n+        if hasattr(dn, 'create_table') is False:\n+            raise NotSupportedYet(f\"Creating table in '{integration_name}' is not supported\")\n+\n         if step.dataframe is not None:\n             data = self.steps_data[step.dataframe.result.step_num]\n         elif step.query is not None:\n             data = ResultSet()\n-            for col in step.query.columns:\n-                data.add_column(Column(col.name))\n+            if step.query.columns is None:\n+                # Is query like: INSERT INTO table VALUES (...)\n+                table_columns_df = dn.get_table_columns_df(str(table_name))\n+                columns_names = table_columns_df[INF_SCHEMA_COLUMNS_NAMES.COLUMN_NAME].to_list()",
    "comment": "will it work if count of columns in table is equal count of columns in query? \r\n ",
    "line_number": 53,
    "enriched": "File: mindsdb/api/executor/sql_query/steps/insert_step.py\nCode: @@ -30,12 +31,32 @@ def call(self, step):\n             if step.is_replace:\n                 is_replace = True\n \n+        if len(step.table.parts) > 1:\n+            integration_name = step.table.parts[0]\n+            table_name = Identifier(parts=step.table.parts[1:])\n+        else:\n+            integration_name = self.context['database']\n+            table_name = step.table\n+\n+        dn = self.session.datahub.get(integration_name)\n+\n+        if hasattr(dn, 'create_table') is False:\n+            raise NotSupportedYet(f\"Creating table in '{integration_name}' is not supported\")\n+\n         if step.dataframe is not None:\n             data = self.steps_data[step.dataframe.result.step_num]\n         elif step.query is not None:\n             data = ResultSet()\n-            for col in step.query.columns:\n-                data.add_column(Column(col.name))\n+            if step.query.columns is None:\n+                # Is query like: INSERT INTO table VALUES (...)\n+                table_columns_df = dn.get_table_columns_df(str(table_name))\n+                columns_names = table_columns_df[INF_SCHEMA_COLUMNS_NAMES.COLUMN_NAME].to_list()\nComment: will it work if count of columns in table is equal count of columns in query? \r\n ",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "mindsdb/api/executor/sql_query/steps/insert_step.py",
    "pr_number": 10872,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2104409461,
    "comment_created_at": "2025-05-23T11:38:55Z"
  },
  {
    "code": "@@ -49,3 +49,7 @@ You can use this established connection to query your table as follows.\n SELECT *\n FROM maria_datasource.example_table;\n ```\n+\n+<Tip>",
    "comment": "Please look at the issue instructions. This box should be placed before the `## Usage` section - that is at line 28.",
    "line_number": 53,
    "enriched": "File: docs/data-integrations/mariadb.mdx\nCode: @@ -49,3 +49,7 @@ You can use this established connection to query your table as follows.\n SELECT *\n FROM maria_datasource.example_table;\n ```\n+\n+<Tip>\nComment: Please look at the issue instructions. This box should be placed before the `## Usage` section - that is at line 28.",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "docs/data-integrations/mariadb.mdx",
    "pr_number": 6674,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1242332099,
    "comment_created_at": "2023-06-26T14:54:43Z"
  },
  {
    "code": "@@ -29,6 +29,7 @@ numpy\n pytz\n botocore\n boto3 >= 1.34.131\n+openai<2.0.0,>=1.54.0",
    "comment": "to fix dependency check failure",
    "line_number": 32,
    "enriched": "File: requirements/requirements.txt\nCode: @@ -29,6 +29,7 @@ numpy\n pytz\n botocore\n boto3 >= 1.34.131\n+openai<2.0.0,>=1.54.0\nComment: to fix dependency check failure",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "requirements/requirements.txt",
    "pr_number": 10376,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1920062438,
    "comment_created_at": "2025-01-17T11:32:57Z"
  },
  {
    "code": "@@ -0,0 +1,55 @@\n+---\n+title: TDEngine\n+sidebarTitle: TDEngine\n+---\n+\n+# TDEngine Handler\n+\n+This is the implementation of the  TDEngine handler for MindsDB.\n+\n+##  TDEngine\n+TDengine is an open source, high-performance, cloud native time-series database optimized for Internet of Things (IoT), Connected Cars, and Industrial IoT. It enables efficient, real-time data ingestion, processing, and monitoring of TB and even PB scale data per day, generated by billions of sensors and data collectors. TDengine differentiates itself from other time-series databases with the following advantages:\n+\n+* High Performance\n+* Simplified Solution\n+* Cloud Native\n+* Ease of Use\n+* Easy Data Analytics\n+* Open Source\n+  \n+\n+\n+## Implementation\n+This handler was implemented using the `taos/taosrest`, a Python library that allows you to use Python code to run SQL commands on TDEngine Server.\n+\n+The required arguments to establish a connection are,\n+* `user`: username asscociated with server\n+* `password`: password to authenticate your access\n+* `url`: Url to TDEngine server. For local server url is localhost:6041 (Default)\n+* `token`: Unique token provide while using TDEngine Cloud.\n+* `database`: Database name to be connected\n+\n+\n+## Usage\n+In order to make use of this handler and connect to TDEngine in MindsDB, the following syntax can be used,\n+\n+~~~~sql",
    "comment": "Please use the below code block:\r\n\r\n> \\```sql\r\n> SQL GOES HERE\r\n> \\```",
    "line_number": 36,
    "enriched": "File: docs/app-integrations/tdengine.mdx\nCode: @@ -0,0 +1,55 @@\n+---\n+title: TDEngine\n+sidebarTitle: TDEngine\n+---\n+\n+# TDEngine Handler\n+\n+This is the implementation of the  TDEngine handler for MindsDB.\n+\n+##  TDEngine\n+TDengine is an open source, high-performance, cloud native time-series database optimized for Internet of Things (IoT), Connected Cars, and Industrial IoT. It enables efficient, real-time data ingestion, processing, and monitoring of TB and even PB scale data per day, generated by billions of sensors and data collectors. TDengine differentiates itself from other time-series databases with the following advantages:\n+\n+* High Performance\n+* Simplified Solution\n+* Cloud Native\n+* Ease of Use\n+* Easy Data Analytics\n+* Open Source\n+  \n+\n+\n+## Implementation\n+This handler was implemented using the `taos/taosrest`, a Python library that allows you to use Python code to run SQL commands on TDEngine Server.\n+\n+The required arguments to establish a connection are,\n+* `user`: username asscociated with server\n+* `password`: password to authenticate your access\n+* `url`: Url to TDEngine server. For local server url is localhost:6041 (Default)\n+* `token`: Unique token provide while using TDEngine Cloud.\n+* `database`: Database name to be connected\n+\n+\n+## Usage\n+In order to make use of this handler and connect to TDEngine in MindsDB, the following syntax can be used,\n+\n+~~~~sql\nComment: Please use the below code block:\r\n\r\n> \\```sql\r\n> SQL GOES HERE\r\n> \\```",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "docs/app-integrations/tdengine.mdx",
    "pr_number": 5387,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1155352604,
    "comment_created_at": "2023-04-02T18:05:44Z"
  },
  {
    "code": "@@ -223,6 +230,7 @@ def _run(self, tool_input: str) -> str:\n         try:\n             # Execute the query\n             query = llm_str_strip(query)\n+            query = self._remove_backticks_from_identifiers(query)",
    "comment": "this should brake there queries that have table names and column names with special characters \r\nIt was fixed in this PR https://github.com/mindsdb/mindsdb/pull/11101\r\n@MinuraPunchihewa, is it correct? ",
    "line_number": 233,
    "enriched": "File: mindsdb/interfaces/skills/custom/text2sql/mindsdb_kb_tools.py\nCode: @@ -223,6 +230,7 @@ def _run(self, tool_input: str) -> str:\n         try:\n             # Execute the query\n             query = llm_str_strip(query)\n+            query = self._remove_backticks_from_identifiers(query)\nComment: this should brake there queries that have table names and column names with special characters \r\nIt was fixed in this PR https://github.com/mindsdb/mindsdb/pull/11101\r\n@MinuraPunchihewa, is it correct? ",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "mindsdb/interfaces/skills/custom/text2sql/mindsdb_kb_tools.py",
    "pr_number": 11159,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2166445948,
    "comment_created_at": "2025-06-25T11:05:37Z"
  },
  {
    "code": "@@ -136,8 +136,20 @@ def test_create_table(self, handler):\n         tables = self.get_table_names(handler)\n         assert new_table in tables, f\"expected to have {new_table} in database, but got: {tables}\"\n \n+    def test_insert_table(self, handler):\n+        tablename = \"test_mdb\"\n+        res = handler.native_query(f\"INSERT INTO {new_table} (test_col) values (1), (2), (3))\")\n+        self.check_valid_response(res)\n+        handler.disconnect()\n+        handler.connect()\n+        res = handler.query(f\"SELECT count(*) FROM {new_table}\")\n+        self.check_valid_response(res)\n+        got_rows = res.data_frame.shape[0]\n+        assert got_rows == 3\n+\n+\n     def test_drop_table(self, handler):\n-        drop_table = \"test_md\"",
    "comment": "I am guessing that this is a typo?",
    "line_number": 140,
    "enriched": "File: tests/handler_tests/test_mysql_handler.py\nCode: @@ -136,8 +136,20 @@ def test_create_table(self, handler):\n         tables = self.get_table_names(handler)\n         assert new_table in tables, f\"expected to have {new_table} in database, but got: {tables}\"\n \n+    def test_insert_table(self, handler):\n+        tablename = \"test_mdb\"\n+        res = handler.native_query(f\"INSERT INTO {new_table} (test_col) values (1), (2), (3))\")\n+        self.check_valid_response(res)\n+        handler.disconnect()\n+        handler.connect()\n+        res = handler.query(f\"SELECT count(*) FROM {new_table}\")\n+        self.check_valid_response(res)\n+        got_rows = res.data_frame.shape[0]\n+        assert got_rows == 3\n+\n+\n     def test_drop_table(self, handler):\n-        drop_table = \"test_md\"\nComment: I am guessing that this is a typo?",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "tests/handler_tests/test_mysql_handler.py",
    "pr_number": 8429,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1405622198,
    "comment_created_at": "2023-11-27T05:07:49Z"
  },
  {
    "code": "@@ -53,3 +53,18 @@ WHERE youtube_video_id = \"raWFGQ20OfA\"\n ORDER BY display_name ASC\n LIMIT 5;\n ```\n+\n+\n+Get information about any youtube video using video_id:\n+```sql\n+SELECT * FROM mindsdb_youtube.videos\n+WHERE video_id=\"id\";\n+```\n+\n+\n+Given a channel_id, get information about the channel",
    "comment": "Please just add : at the end.",
    "line_number": 65,
    "enriched": "File: docs/integrations/app-integrations/youtube.mdx\nCode: @@ -53,3 +53,18 @@ WHERE youtube_video_id = \"raWFGQ20OfA\"\n ORDER BY display_name ASC\n LIMIT 5;\n ```\n+\n+\n+Get information about any youtube video using video_id:\n+```sql\n+SELECT * FROM mindsdb_youtube.videos\n+WHERE video_id=\"id\";\n+```\n+\n+\n+Given a channel_id, get information about the channel\nComment: Please just add : at the end.",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "docs/integrations/app-integrations/youtube.mdx",
    "pr_number": 8621,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1452321478,
    "comment_created_at": "2024-01-15T12:32:02Z"
  },
  {
    "code": "@@ -41,17 +41,17 @@ class TextChunkingConfig(BaseModel):\n     \"\"\"Configuration for text chunking preprocessor using Pydantic\"\"\"\n     chunk_size: int = Field(\n         default=1000,\n-        description=\"The target size of each text chunk\",\n+        description=\"The target size of each text chunk in tokens\",\n         gt=0\n     )\n     chunk_overlap: int = Field(\n         default=200,\n-        description=\"The number of characters to overlap between chunks\",\n+        description=\"The number of tokens to overlap between chunks\",\n         ge=0\n     )\n-    length_function: Callable = Field(\n-        default=len,\n-        description=\"Function to measure text length\"\n+    encoding_name: str = Field(",
    "comment": "we currently split by character, splitting by tokens makes more sense",
    "line_number": 52,
    "enriched": "File: mindsdb/interfaces/knowledge_base/preprocessing/models.py\nCode: @@ -41,17 +41,17 @@ class TextChunkingConfig(BaseModel):\n     \"\"\"Configuration for text chunking preprocessor using Pydantic\"\"\"\n     chunk_size: int = Field(\n         default=1000,\n-        description=\"The target size of each text chunk\",\n+        description=\"The target size of each text chunk in tokens\",\n         gt=0\n     )\n     chunk_overlap: int = Field(\n         default=200,\n-        description=\"The number of characters to overlap between chunks\",\n+        description=\"The number of tokens to overlap between chunks\",\n         ge=0\n     )\n-    length_function: Callable = Field(\n-        default=len,\n-        description=\"Function to measure text length\"\n+    encoding_name: str = Field(\nComment: we currently split by character, splitting by tokens makes more sense",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "mindsdb/interfaces/knowledge_base/preprocessing/models.py",
    "pr_number": 10407,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1932142313,
    "comment_created_at": "2025-01-28T13:12:27Z"
  },
  {
    "code": "@@ -107,7 +107,8 @@ RUN python -m pip install --prefer-binary --no-cache-dir --upgrade pip==23.1.2 &\n     pip install --prefer-binary --no-cache-dir 'weaviate-client~=3.24.2' || true && \\\n     pip install --prefer-binary --no-cache-dir 'pgvector' || true && \\\n     pip install --prefer-binary --no-cache-dir 'anthropic==0.3.4' || true  && \\\n-    pip install --prefer-binary --no-cache-dir python-gitlab || true\n+    pip install --prefer-binary --no-cache-dir python-gitlab || true && \\\n+    pip install --prefer-binary --no-cache-dir 'chromadb~=0.4.8' 'pysqlite3-binary' || true && \\",
    "comment": "don't think `&&  \\` is required at end of line 111, otherwise looks good",
    "line_number": 111,
    "enriched": "File: docker/release\nCode: @@ -107,7 +107,8 @@ RUN python -m pip install --prefer-binary --no-cache-dir --upgrade pip==23.1.2 &\n     pip install --prefer-binary --no-cache-dir 'weaviate-client~=3.24.2' || true && \\\n     pip install --prefer-binary --no-cache-dir 'pgvector' || true && \\\n     pip install --prefer-binary --no-cache-dir 'anthropic==0.3.4' || true  && \\\n-    pip install --prefer-binary --no-cache-dir python-gitlab || true\n+    pip install --prefer-binary --no-cache-dir python-gitlab || true && \\\n+    pip install --prefer-binary --no-cache-dir 'chromadb~=0.4.8' 'pysqlite3-binary' || true && \\\nComment: don't think `&&  \\` is required at end of line 111, otherwise looks good",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "docker/release",
    "pr_number": 8268,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1381741533,
    "comment_created_at": "2023-11-03T14:09:02Z"
  },
  {
    "code": "@@ -0,0 +1,33 @@\n+---\n+title: SQLite\n+sidebarTitle: SQLite\n+---\n+\n+#SQLite Handler\n+This is the implementation of the SQLite handler for MindsDB.\n+\n+##SQLite\n+SQLite is an in-process library that implements a self-contained, serverless, zero-configuration, transactional SQL database engine. The code for SQLite is in the public domain and is thus free for use for any purpose, commercial or private. SQLite is the most widely deployed database in the world with more applications than we can count, including several high-profile projects. https://www.sqlite.org/about.html\n+\n+##Implementation\n+This handler was implemented using the standard sqlite3 library that comes with Python.\n+\n+The only required argument to establish a connection is db_file. This points to the database file that the connection is to be made to.\n+\n+Optionally, this may also be set to :memory:, which will create an in-memory database.\n+\n+##Usage\n+In order to make use of this handler and connect to a SQLite database in MindsDB, the following syntax can be used,\n+\n+~~~~sql",
    "comment": "Please correct the code blocks to be like this:\r\n\r\n> \\```sql\r\n> SQL GOES HERE\r\n> \\```",
    "line_number": 22,
    "enriched": "File: docs/data-integrations/SQLite.mdx\nCode: @@ -0,0 +1,33 @@\n+---\n+title: SQLite\n+sidebarTitle: SQLite\n+---\n+\n+#SQLite Handler\n+This is the implementation of the SQLite handler for MindsDB.\n+\n+##SQLite\n+SQLite is an in-process library that implements a self-contained, serverless, zero-configuration, transactional SQL database engine. The code for SQLite is in the public domain and is thus free for use for any purpose, commercial or private. SQLite is the most widely deployed database in the world with more applications than we can count, including several high-profile projects. https://www.sqlite.org/about.html\n+\n+##Implementation\n+This handler was implemented using the standard sqlite3 library that comes with Python.\n+\n+The only required argument to establish a connection is db_file. This points to the database file that the connection is to be made to.\n+\n+Optionally, this may also be set to :memory:, which will create an in-memory database.\n+\n+##Usage\n+In order to make use of this handler and connect to a SQLite database in MindsDB, the following syntax can be used,\n+\n+~~~~sql\nComment: Please correct the code blocks to be like this:\r\n\r\n> \\```sql\r\n> SQL GOES HERE\r\n> \\```",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "docs/data-integrations/SQLite.mdx",
    "pr_number": 5163,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1147596677,
    "comment_created_at": "2023-03-24T13:41:04Z"
  },
  {
    "code": "@@ -109,6 +109,8 @@ def clear_db(self, db):\n         db.session.add(r)\n         r = db.Integration(name='lightwood', data={}, engine='lightwood')\n         db.session.add(r)\n+        r = db.Integration(name='fbprophet', data={}, engine='fbprophet')\n+        db.session.add(r)\n         db.session.flush()\n         self.lw_integration_id = r.id",
    "comment": "lw_integration_id should point at lightwood integration.\r\nthis line should be moved where lightwood integration is creating",
    "line_number": 115,
    "enriched": "File: tests/unit/executor_test_base.py\nCode: @@ -109,6 +109,8 @@ def clear_db(self, db):\n         db.session.add(r)\n         r = db.Integration(name='lightwood', data={}, engine='lightwood')\n         db.session.add(r)\n+        r = db.Integration(name='fbprophet', data={}, engine='fbprophet')\n+        db.session.add(r)\n         db.session.flush()\n         self.lw_integration_id = r.id\nComment: lw_integration_id should point at lightwood integration.\r\nthis line should be moved where lightwood integration is creating",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "tests/unit/executor_test_base.py",
    "pr_number": 5759,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1176259097,
    "comment_created_at": "2023-04-25T09:33:14Z"
  },
  {
    "code": "@@ -0,0 +1,76 @@\n+# Cohere Handler\n+\n+Cohere ML handler for MindsDB provides interfaces to connect with Cohere ML ia APIs and pull Cohere ML Capabilites into MindsDB.",
    "comment": "Please change the typo in ia => via",
    "line_number": 3,
    "enriched": "File: mindsdb/integrations/handlers/cohere_handler/README.md\nCode: @@ -0,0 +1,76 @@\n+# Cohere Handler\n+\n+Cohere ML handler for MindsDB provides interfaces to connect with Cohere ML ia APIs and pull Cohere ML Capabilites into MindsDB.\nComment: Please change the typo in ia => via",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "mindsdb/integrations/handlers/cohere_handler/README.md",
    "pr_number": 6176,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1200670562,
    "comment_created_at": "2023-05-22T15:23:38Z"
  },
  {
    "code": "@@ -736,8 +800,33 @@ def _get_handler_info(self, handler_dir: Path):\n         for item in code.body:\n             if not isinstance(item, ast.Assign):\n                 continue\n-            if isinstance(item.targets[0], ast.Name) and isinstance(item.value, ast.Constant):\n-                info[item.targets[0].id] = item.value.value\n+            if isinstance(item.targets[0], ast.Name):\n+                name = item.targets[0].id\n+                if isinstance(item.value, ast.Constant):\n+                    info[name] = item.value.value\n+                if isinstance(item.value, ast.Attribute) and name == 'type':\n+                    if item.value.attr == 'ML':\n+                        info[name] = HANDLER_TYPE.ML\n+                        info['class_type'] = 'ml'\n+                    else:\n+                        info[name] = HANDLER_TYPE.DATA\n+                        info['class_type'] = self._get_base_class_type(code, handler_dir) or 'sql'\n+\n+        # connection args\n+        if info['type'] == HANDLER_TYPE.ML:\n+            args_file = handler_dir / 'creation_args.py'\n+            if args_file.exists():\n+                code = ast.parse(open(args_file).read())",
    "comment": "`open(args_file).read()` == `args_file.read_text()`",
    "line_number": 819,
    "enriched": "File: mindsdb/interfaces/database/integrations.py\nCode: @@ -736,8 +800,33 @@ def _get_handler_info(self, handler_dir: Path):\n         for item in code.body:\n             if not isinstance(item, ast.Assign):\n                 continue\n-            if isinstance(item.targets[0], ast.Name) and isinstance(item.value, ast.Constant):\n-                info[item.targets[0].id] = item.value.value\n+            if isinstance(item.targets[0], ast.Name):\n+                name = item.targets[0].id\n+                if isinstance(item.value, ast.Constant):\n+                    info[name] = item.value.value\n+                if isinstance(item.value, ast.Attribute) and name == 'type':\n+                    if item.value.attr == 'ML':\n+                        info[name] = HANDLER_TYPE.ML\n+                        info['class_type'] = 'ml'\n+                    else:\n+                        info[name] = HANDLER_TYPE.DATA\n+                        info['class_type'] = self._get_base_class_type(code, handler_dir) or 'sql'\n+\n+        # connection args\n+        if info['type'] == HANDLER_TYPE.ML:\n+            args_file = handler_dir / 'creation_args.py'\n+            if args_file.exists():\n+                code = ast.parse(open(args_file).read())\nComment: `open(args_file).read()` == `args_file.read_text()`",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "mindsdb/interfaces/database/integrations.py",
    "pr_number": 10230,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1865803060,
    "comment_created_at": "2024-12-02T12:57:25Z"
  },
  {
    "code": "@@ -46,18 +46,35 @@ def __find_last_columns(self, query: ASTNode) -> Union[dict, None]:\n                     'table': <table identifier>,\n                     'column': <column name>,\n                     'links': [<link to ast node>, ... ],\n-                    'target_idx': <number of column in select target>\n+                    'target_idx': <number of column in select target>,\n+                    'gen_init_query': if true: to generate query to initial values for las",
    "comment": "typo?",
    "line_number": 50,
    "enriched": "File: mindsdb/interfaces/query_context/last_query.py\nCode: @@ -46,18 +46,35 @@ def __find_last_columns(self, query: ASTNode) -> Union[dict, None]:\n                     'table': <table identifier>,\n                     'column': <column name>,\n                     'links': [<link to ast node>, ... ],\n-                    'target_idx': <number of column in select target>\n+                    'target_idx': <number of column in select target>,\n+                    'gen_init_query': if true: to generate query to initial values for las\nComment: typo?",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "mindsdb/interfaces/query_context/last_query.py",
    "pr_number": 9786,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1789999015,
    "comment_created_at": "2024-10-07T11:00:35Z"
  },
  {
    "code": "@@ -34,7 +34,8 @@ services:\n       - '47334:47334'\n       - '47335:47335'\n     environment:\n-      MINDSDB_DB_CON: \"postgresql://postgres:postgres@postgres/mindsdb\"\n+      MINDSDB_DB_CON: \"postgresql://mindsdb:mindsdb@postgres/mindsdb\"\n+      KB_PGVECTOR_URL: \"postgresql://mindsdb:mindsdb@postgres/kb\"",
    "comment": "**correctness**: `postgres` service uses default user/password, but `MINDSDB_DB_CON` and `KB_PGVECTOR_URL` expect a `mindsdb` user which is not created, causing connection/authentication failures.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn docker-compose.yml, lines 37-38, the environment variables `MINDSDB_DB_CON` and `KB_PGVECTOR_URL` use the `mindsdb` user, but the `postgres` service only creates the `postgres` user. Update these variables to use `postgres:postgres` as the credentials to prevent authentication failures.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n      MINDSDB_DB_CON: \"postgresql://postgres:postgres@postgres/mindsdb\"\n      KB_PGVECTOR_URL: \"postgresql://postgres:postgres@postgres/kb\"\n```\n</details>\n<!-- suggestion_end -->\n",
    "line_number": 38,
    "enriched": "File: docker-compose.yml\nCode: @@ -34,7 +34,8 @@ services:\n       - '47334:47334'\n       - '47335:47335'\n     environment:\n-      MINDSDB_DB_CON: \"postgresql://postgres:postgres@postgres/mindsdb\"\n+      MINDSDB_DB_CON: \"postgresql://mindsdb:mindsdb@postgres/mindsdb\"\n+      KB_PGVECTOR_URL: \"postgresql://mindsdb:mindsdb@postgres/kb\"\nComment: **correctness**: `postgres` service uses default user/password, but `MINDSDB_DB_CON` and `KB_PGVECTOR_URL` expect a `mindsdb` user which is not created, causing connection/authentication failures.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn docker-compose.yml, lines 37-38, the environment variables `MINDSDB_DB_CON` and `KB_PGVECTOR_URL` use the `mindsdb` user, but the `postgres` service only creates the `postgres` user. Update these variables to use `postgres:postgres` as the credentials to prevent authentication failures.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n      MINDSDB_DB_CON: \"postgresql://postgres:postgres@postgres/mindsdb\"\n      KB_PGVECTOR_URL: \"postgresql://postgres:postgres@postgres/kb\"\n```\n</details>\n<!-- suggestion_end -->\n",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "docker-compose.yml",
    "pr_number": 11937,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2561750649,
    "comment_created_at": "2025-11-25T22:40:24Z"
  },
  {
    "code": "@@ -133,6 +133,6 @@ def predict(self, df: pd.DataFrame = None, args: dict = None):\n \n         # get question from sql query\n         # e.g. where question = 'What is the capital of France?'\n-        response = question_answerer(df[\"question\"].tolist()[0])\n+        response = question_answerer(df[args[\"input_column\"]].tolist()[0])",
    "comment": "It should be `args.input_column`",
    "line_number": 136,
    "enriched": "File: mindsdb/integrations/handlers/rag_handler/rag_handler.py\nCode: @@ -133,6 +133,6 @@ def predict(self, df: pd.DataFrame = None, args: dict = None):\n \n         # get question from sql query\n         # e.g. where question = 'What is the capital of France?'\n-        response = question_answerer(df[\"question\"].tolist()[0])\n+        response = question_answerer(df[args[\"input_column\"]].tolist()[0])\nComment: It should be `args.input_column`",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "mindsdb/integrations/handlers/rag_handler/rag_handler.py",
    "pr_number": 8781,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1492778464,
    "comment_created_at": "2024-02-16T17:25:05Z"
  },
  {
    "code": "@@ -0,0 +1,21 @@\n+from mindsdb.integrations.libs.const import HANDLER_TYPE\n+\n+from .__about__ import __version__ as version, __description__ as description\n+try:\n+    from .quickbooks_handler import (\n+        QuickbooksHandler as Handler\n+    )\n+    import_error = None\n+except Exception as e:\n+    Handler = None\n+    import_error = e\n+\n+title = 'QuickBooks'\n+name = 'quickbooks'\n+type = HANDLER_TYPE.DATA\n+icon_path = 'icon.svg'",
    "comment": "Please remove the Y icon from the hackernews and use the correct one",
    "line_number": 16,
    "enriched": "File: mindsdb/integrations/handlers/quickbooks_handler/__init__.py\nCode: @@ -0,0 +1,21 @@\n+from mindsdb.integrations.libs.const import HANDLER_TYPE\n+\n+from .__about__ import __version__ as version, __description__ as description\n+try:\n+    from .quickbooks_handler import (\n+        QuickbooksHandler as Handler\n+    )\n+    import_error = None\n+except Exception as e:\n+    Handler = None\n+    import_error = e\n+\n+title = 'QuickBooks'\n+name = 'quickbooks'\n+type = HANDLER_TYPE.DATA\n+icon_path = 'icon.svg'\nComment: Please remove the Y icon from the hackernews and use the correct one",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "mindsdb/integrations/handlers/quickbooks_handler/__init__.py",
    "pr_number": 5770,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1183590207,
    "comment_created_at": "2023-05-03T11:58:53Z"
  },
  {
    "code": "@@ -16,10 +16,30 @@\n )\n from mindsdb.utilities import log\n \n-import sys\n-__import__(\"pysqlite3\")\n-sys.modules[\"sqlite3\"] = sys.modules.pop(\"pysqlite3\")\n-import chromadb  # noqa: E402\n+\n+def get_chromadb():\n+    \"\"\"\n+    Import and return the chromadb module, using pysqlite3 if available.\n+    # this is a hack to make chromadb work with pysqlite3 instead of sqlite3 for cloud usage",
    "comment": "Nit: redundant #",
    "line_number": 23,
    "enriched": "File: mindsdb/integrations/handlers/chromadb_handler/chromadb_handler.py\nCode: @@ -16,10 +16,30 @@\n )\n from mindsdb.utilities import log\n \n-import sys\n-__import__(\"pysqlite3\")\n-sys.modules[\"sqlite3\"] = sys.modules.pop(\"pysqlite3\")\n-import chromadb  # noqa: E402\n+\n+def get_chromadb():\n+    \"\"\"\n+    Import and return the chromadb module, using pysqlite3 if available.\n+    # this is a hack to make chromadb work with pysqlite3 instead of sqlite3 for cloud usage\nComment: Nit: redundant #",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "mindsdb/integrations/handlers/chromadb_handler/chromadb_handler.py",
    "pr_number": 8122,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1374771504,
    "comment_created_at": "2023-10-27T15:57:12Z"
  },
  {
    "code": "@@ -711,8 +711,8 @@ def insert(self, df: pd.DataFrame, params: dict = None):\n             ...\n \n         # First adapt column names to identify content and metadata columns\n-        adapted_df = self._adapt_column_names(df)\n-        content_columns = self._kb.params.get(\"content_columns\", [TableField.CONTENT.value])\n+        adapted_df, normalized_columns = self._adapt_column_names(df)",
    "comment": "I think the update of `_adapt_column_names` function is missing in this PR",
    "line_number": 714,
    "enriched": "File: mindsdb/interfaces/knowledge_base/controller.py\nCode: @@ -711,8 +711,8 @@ def insert(self, df: pd.DataFrame, params: dict = None):\n             ...\n \n         # First adapt column names to identify content and metadata columns\n-        adapted_df = self._adapt_column_names(df)\n-        content_columns = self._kb.params.get(\"content_columns\", [TableField.CONTENT.value])\n+        adapted_df, normalized_columns = self._adapt_column_names(df)\nComment: I think the update of `_adapt_column_names` function is missing in this PR",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "mindsdb/interfaces/knowledge_base/controller.py",
    "pr_number": 11936,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2560693758,
    "comment_created_at": "2025-11-25T16:41:10Z"
  },
  {
    "code": "@@ -13,11 +13,12 @@ This handler is implemented using `psycopg2`, a Python library that allows you t\n \n The required arguments to establish a connection are as follows:\n \n-* `user` is the database user.\n-* `password` is the database password.\n-* `host` is the host name, IP address, or URL.\n-* `port` is the port used to make TCP/IP connection.\n-* `database` is the database name.\n+- `user` is the database user.\n+- `password` is the database password.\n+- `host` is the host name, IP address, or URL.\n+- `port` is the port used to make TCP/IP connection.\n+- `database` is the database name.\n+- `schema` Schema to which your table is asscociated.",
    "comment": "Please correct this line to the following:\r\n\r\n> \\- \\`schema\\` is the schema to which your table belongs.",
    "line_number": 21,
    "enriched": "File: docs/data-integrations/yugabytedb.mdx\nCode: @@ -13,11 +13,12 @@ This handler is implemented using `psycopg2`, a Python library that allows you t\n \n The required arguments to establish a connection are as follows:\n \n-* `user` is the database user.\n-* `password` is the database password.\n-* `host` is the host name, IP address, or URL.\n-* `port` is the port used to make TCP/IP connection.\n-* `database` is the database name.\n+- `user` is the database user.\n+- `password` is the database password.\n+- `host` is the host name, IP address, or URL.\n+- `port` is the port used to make TCP/IP connection.\n+- `database` is the database name.\n+- `schema` Schema to which your table is asscociated.\nComment: Please correct this line to the following:\r\n\r\n> \\- \\`schema\\` is the schema to which your table belongs.",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "docs/data-integrations/yugabytedb.mdx",
    "pr_number": 6108,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1198776242,
    "comment_created_at": "2023-05-19T09:56:57Z"
  },
  {
    "code": "@@ -50,20 +50,54 @@ FROM writer;\n \n -- Create a Writer model and embed input data\n CREATE MODEL writer_demo\n-FROM mysql_demo_db (select * from demo_fda_context)\n+FROM mysql_demo_db (select * from demo_fda_context limit 10) --limit to 10 rows for testing purposes\n PREDICT answer ---specify any column name that exists inside context table, 'answer' used for illustrative purposes\n USING\n    engine=\"writer\",\n-   run_embeddings=true, --if context should be transformed to embeddings and loaded to vectorDB\n-   model_name=\"palmyra-x\",  --specify which model to use\n    writer_org_id=\"\",\n    writer_api_key=\"\",\n-   chromadb_folder_name='full_context', --specify folder name for where chromadb will be persisted locally\n-   embeddings_model_name=\"sentence-transformers/all-mpnet-base-v2\", --this can be any sentence transformer that is compatible with Hugging Face sentence_transformer library, if none provided defaults to \"sentence-transformers/all-mpnet-base-v2\"\n-   prompt_template='{question}';\n+   embeddings_model_name=\"sentence-transformers/all-mpnet-base-v2\",\n+   vector_store_folder_name=\"writer_demo_vector_store\",\n+   prompt_template=\"Use the following pieces of context to answer the question at the end. If you do not know the answer,\n+just say that you do not know, do not try to make up an answer.\n+Context: {context}\n+Question: {question}\n+Helpful Answer:\"; --this can be any sentence transformer that is compatible with Hugging Face sentence_transformer library, if none provided defaults to \"sentence-transformers/all-mpnet-base-v2\"\n \n -- Ask a question on your data using Writer LLM API\n SELECT *\n FROM writer_demo\n WHERE question='what product is best for treating a cold?';\n ```\n+\n+--Run evaluation of configured model\n+```sql\n+-- Create Writer model\n+CREATE MODEL writer_demo_evaluate\n+PREDICT answer\n+USING\n+   engine=\"writer\",\n+     writer_org_id=\"\",\n+writer_api_key=\"\",\n+   embeddings_model_name=\"sentence-transformers/all-mpnet-base-v2\",\n+evaluate_dataset='squad_v2_val_100_sample',\n+n_rows_evaluation=10,\n+vector_store_folder_name=\"writer_demo_eval_vector_store\",\n+prompt_template=\"Use the following pieces of context to answer the question at the end. If you do not know the answer,\n+just say that you do not know, do not try to make up an answer.\n+Context: {context}\n+Question: {question}\n+Helpful Answer:\";\n+\n+-- Evaluate model\n+select * from writer_demo_evaluate where run_evaluation = True;\n+\n+-- Get evaluation metrics and output from evaluation\n+\n+NB this will only work if you have run the evaluation query above",
    "comment": "Missing `--`",
    "line_number": 97,
    "enriched": "File: mindsdb/integrations/handlers/writer_handler/README.md\nCode: @@ -50,20 +50,54 @@ FROM writer;\n \n -- Create a Writer model and embed input data\n CREATE MODEL writer_demo\n-FROM mysql_demo_db (select * from demo_fda_context)\n+FROM mysql_demo_db (select * from demo_fda_context limit 10) --limit to 10 rows for testing purposes\n PREDICT answer ---specify any column name that exists inside context table, 'answer' used for illustrative purposes\n USING\n    engine=\"writer\",\n-   run_embeddings=true, --if context should be transformed to embeddings and loaded to vectorDB\n-   model_name=\"palmyra-x\",  --specify which model to use\n    writer_org_id=\"\",\n    writer_api_key=\"\",\n-   chromadb_folder_name='full_context', --specify folder name for where chromadb will be persisted locally\n-   embeddings_model_name=\"sentence-transformers/all-mpnet-base-v2\", --this can be any sentence transformer that is compatible with Hugging Face sentence_transformer library, if none provided defaults to \"sentence-transformers/all-mpnet-base-v2\"\n-   prompt_template='{question}';\n+   embeddings_model_name=\"sentence-transformers/all-mpnet-base-v2\",\n+   vector_store_folder_name=\"writer_demo_vector_store\",\n+   prompt_template=\"Use the following pieces of context to answer the question at the end. If you do not know the answer,\n+just say that you do not know, do not try to make up an answer.\n+Context: {context}\n+Question: {question}\n+Helpful Answer:\"; --this can be any sentence transformer that is compatible with Hugging Face sentence_transformer library, if none provided defaults to \"sentence-transformers/all-mpnet-base-v2\"\n \n -- Ask a question on your data using Writer LLM API\n SELECT *\n FROM writer_demo\n WHERE question='what product is best for treating a cold?';\n ```\n+\n+--Run evaluation of configured model\n+```sql\n+-- Create Writer model\n+CREATE MODEL writer_demo_evaluate\n+PREDICT answer\n+USING\n+   engine=\"writer\",\n+     writer_org_id=\"\",\n+writer_api_key=\"\",\n+   embeddings_model_name=\"sentence-transformers/all-mpnet-base-v2\",\n+evaluate_dataset='squad_v2_val_100_sample',\n+n_rows_evaluation=10,\n+vector_store_folder_name=\"writer_demo_eval_vector_store\",\n+prompt_template=\"Use the following pieces of context to answer the question at the end. If you do not know the answer,\n+just say that you do not know, do not try to make up an answer.\n+Context: {context}\n+Question: {question}\n+Helpful Answer:\";\n+\n+-- Evaluate model\n+select * from writer_demo_evaluate where run_evaluation = True;\n+\n+-- Get evaluation metrics and output from evaluation\n+\n+NB this will only work if you have run the evaluation query above\nComment: Missing `--`",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "mindsdb/integrations/handlers/writer_handler/README.md",
    "pr_number": 7508,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1344236746,
    "comment_created_at": "2023-10-03T14:42:14Z"
  },
  {
    "code": "@@ -117,18 +133,61 @@ def get_table_info(self, table_names: Optional[List[str]] = None) -> str:\n         If `sample_rows_in_table_info`, the specified number of sample rows will be\n         appended to each table description. This can increase performance as demonstrated in the paper.\n         \"\"\"\n+        try:\n+            cache_key = self._generate_cache_key(table_names)",
    "comment": "How often the llm use more than 1 table in get_table_info?\r\nIf there is 1 table in 99% cases - we could cache per table  (and simplify generation key and update of cache)",
    "line_number": 137,
    "enriched": "File: mindsdb/interfaces/skills/sql_agent.py\nCode: @@ -117,18 +133,61 @@ def get_table_info(self, table_names: Optional[List[str]] = None) -> str:\n         If `sample_rows_in_table_info`, the specified number of sample rows will be\n         appended to each table description. This can increase performance as demonstrated in the paper.\n         \"\"\"\n+        try:\n+            cache_key = self._generate_cache_key(table_names)\nComment: How often the llm use more than 1 table in get_table_info?\r\nIf there is 1 table in 99% cases - we could cache per table  (and simplify generation key and update of cache)",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "mindsdb/interfaces/skills/sql_agent.py",
    "pr_number": 9484,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1673997218,
    "comment_created_at": "2024-07-11T13:13:50Z"
  },
  {
    "code": "@@ -0,0 +1,44 @@\n+---\n+title: D0lt\n+sidebarTitle: D0lt\n+---\n+\n+# D0lt Handler\n+\n+This is the implementation of the  D0lt handler for MindsDB.\n+\n+##  Dolt is Git for Data!\n+Dolt is a single-node and embedded DBMS that incorporates Git-style versioning as a first-class entity. Dolt behaves like Git where it is a content addressable local database where the main objects are tables instead of files. In Dolt, a user creates a database locally. The database contains tables that can be read and updated using SQL. Similar to Git, writes are staged until the user issues a commit. Upon commit, the writes are appended to permanent storage.\n+\n+Branch/merge semantics are supported allowing for the tables to evolve at a different pace for multiple users. This allows for loose collaboration on data as well as multiple views on the same core data. Merge conflicts are detected for schema and data conflicts. Data conflicts are cell-based, not line-based. Remote repositories allow for cooperation among repository instances. Clone, push, and pull semantics are all available.\n+\n+## Implementation\n+This handler was implemented using the `mysql-connector`, a Python library that allows you to use Python code to run SQL commands on D0lt Database.\n+\n+The required arguments to establish a connection are,\n+* `user`: username asscociated with database\n+* `password`: password to authenticate your access\n+* `host`: host to server IP Address or hostname\n+* `port`: port through which TCPIP connection is to be made\n+* `database`: Database name to be connected\n+\n+\n+## Usage\n+In order to make use of this handler and connect to D0lt in MindsDB, the following syntax can be used,\n+~~~~sql",
    "comment": "Please use the below code block:\r\n\r\n> \\```sql\r\n> SQL GOES HERE\r\n> \\```",
    "line_number": 28,
    "enriched": "File: docs/app-integrations/d0lt.mdx\nCode: @@ -0,0 +1,44 @@\n+---\n+title: D0lt\n+sidebarTitle: D0lt\n+---\n+\n+# D0lt Handler\n+\n+This is the implementation of the  D0lt handler for MindsDB.\n+\n+##  Dolt is Git for Data!\n+Dolt is a single-node and embedded DBMS that incorporates Git-style versioning as a first-class entity. Dolt behaves like Git where it is a content addressable local database where the main objects are tables instead of files. In Dolt, a user creates a database locally. The database contains tables that can be read and updated using SQL. Similar to Git, writes are staged until the user issues a commit. Upon commit, the writes are appended to permanent storage.\n+\n+Branch/merge semantics are supported allowing for the tables to evolve at a different pace for multiple users. This allows for loose collaboration on data as well as multiple views on the same core data. Merge conflicts are detected for schema and data conflicts. Data conflicts are cell-based, not line-based. Remote repositories allow for cooperation among repository instances. Clone, push, and pull semantics are all available.\n+\n+## Implementation\n+This handler was implemented using the `mysql-connector`, a Python library that allows you to use Python code to run SQL commands on D0lt Database.\n+\n+The required arguments to establish a connection are,\n+* `user`: username asscociated with database\n+* `password`: password to authenticate your access\n+* `host`: host to server IP Address or hostname\n+* `port`: port through which TCPIP connection is to be made\n+* `database`: Database name to be connected\n+\n+\n+## Usage\n+In order to make use of this handler and connect to D0lt in MindsDB, the following syntax can be used,\n+~~~~sql\nComment: Please use the below code block:\r\n\r\n> \\```sql\r\n> SQL GOES HERE\r\n> \\```",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "docs/app-integrations/d0lt.mdx",
    "pr_number": 5383,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1155352949,
    "comment_created_at": "2023-04-02T18:08:42Z"
  },
  {
    "code": "@@ -267,14 +267,13 @@ def insert(self, table_name: str, df: pd.DataFrame) -> Response:\n         columns = df.columns\n \n         # postgres 'copy' is not thread safe. use lock to prevent concurrent execution\n-        with self._insert_lock:\n-            resp = self.get_columns(table_name)\n+        resp = self.get_columns(table_name)\n \n         # copy requires precise cases of names: get current column names from table and adapt input dataframe columns\n         if resp.data_frame is not None and not resp.data_frame.empty:\n             db_columns = {\n                 c.lower(): c\n-                for c in resp.data_frame['Field']\n+                for c in resp.data_frame['field']",
    "comment": "Please merge main and change 'field' to 'COLUMN_NAME'",
    "line_number": 276,
    "enriched": "File: mindsdb/integrations/handlers/postgres_handler/postgres_handler.py\nCode: @@ -267,14 +267,13 @@ def insert(self, table_name: str, df: pd.DataFrame) -> Response:\n         columns = df.columns\n \n         # postgres 'copy' is not thread safe. use lock to prevent concurrent execution\n-        with self._insert_lock:\n-            resp = self.get_columns(table_name)\n+        resp = self.get_columns(table_name)\n \n         # copy requires precise cases of names: get current column names from table and adapt input dataframe columns\n         if resp.data_frame is not None and not resp.data_frame.empty:\n             db_columns = {\n                 c.lower(): c\n-                for c in resp.data_frame['Field']\n+                for c in resp.data_frame['field']\nComment: Please merge main and change 'field' to 'COLUMN_NAME'",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "mindsdb/integrations/handlers/postgres_handler/postgres_handler.py",
    "pr_number": 10720,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2050395141,
    "comment_created_at": "2025-04-18T09:13:40Z"
  },
  {
    "code": "@@ -0,0 +1,530 @@\n+\n+import os\n+import json\n+import math\n+import logging\n+from typing import List, Dict, Any, Optional\n+\n+from fastapi import FastAPI, HTTPException\n+from fastapi.middleware.cors import CORSMiddleware\n+from fastapi.responses import JSONResponse\n+from pydantic import BaseModel\n+\n+import mindsdb_sdk\n+from dotenv import load_dotenv\n+\n+from base_stats import summarize_metadata\n+from multivalue_stats import (\n+    conditional_rating_analysis,\n+    conditional_rating_to_rating_analysis,\n+    conditional_category_to_category_analysis,\n+    conditional_distribution_analysis,\n+    general_percentage_distribution\n+)\n+\n+# ---------------------------------------------------------\n+# Load environment variables\n+# ---------------------------------------------------------\n+load_dotenv()\n+\n+# ---------------------------------------------------------\n+# FastAPI setup\n+# ---------------------------------------------------------\n+app = FastAPI(title=\"AeroMind API - Airline Review Intelligence\")\n+\n+app.add_middleware(\n+    CORSMiddleware,\n+    allow_origins=[\"*\"],\n+    allow_credentials=True,\n+    allow_methods=[\"*\"],\n+    allow_headers=[\"*\"],\n+)\n+\n+# ---------------------------------------------------------\n+# Logging setup\n+# ---------------------------------------------------------\n+logging.basicConfig(level=logging.INFO)\n+logger = logging.getLogger(\"aeromind_backend\")\n+\n+# ---------------------------------------------------------\n+# MindsDB Connection\n+# ---------------------------------------------------------\n+try:\n+    con = mindsdb_sdk.connect(\"http://127.0.0.1:47334\")\n+    project = con.get_project(\"mindsdb\")\n+    logger.info(\"✅ Connected to MindsDB successfully.\")\n+except Exception as e:\n+    logger.error(f\"Failed to connect to MindsDB: {e}\")\n+    raise RuntimeError(f\"Failed to connect to MindsDB: {e}\")\n+\n+# ---------------------------------------------------------\n+# Utilities\n+# ---------------------------------------------------------\n+def escape_sql_string(value: str) -> str:\n+    \"\"\"Safely escape single quotes for SQL queries.\"\"\"\n+    return value.replace(\"'\", \"''\") if value else \"\"\n+\n+def sanitize_for_json(obj: Any) -> Any:\n+    \"\"\"Recursively replace NaN/Inf with None for JSON compliance.\"\"\"\n+    if isinstance(obj, float) and (math.isnan(obj) or math.isinf(obj)):\n+        return None\n+    if isinstance(obj, dict):\n+        return {k: sanitize_for_json(v) for k, v in obj.items()}\n+    if isinstance(obj, list):\n+        return [sanitize_for_json(x) for x in obj]\n+    return obj\n+\n+def short_text(s: Optional[str], n: int = 280) -> str:\n+    if not s:\n+        return \"\"\n+    s = str(s).strip()\n+    return s if len(s) <= n else s[: n - 1] + \"…\"\n+\n+# ---------------------------------------------------------\n+# Pydantic Models\n+# ---------------------------------------------------------\n+class AirlineSearchRequest(BaseModel):\n+    query: str\n+    airline_name: Optional[str] = None\n+    aircraft: Optional[str] = None\n+    type_of_traveller: Optional[str] = None\n+    seat_type: Optional[str] = None\n+    recommended: Optional[str] = None\n+    verified: Optional[bool] = None\n+\n+    # Numeric rating fields\n+    overall_rating: Optional[float] = None\n+    seat_comfort: Optional[float] = None\n+    cabin_staff_service: Optional[float] = None\n+    food_beverages: Optional[float] = None\n+    ground_service: Optional[float] = None\n+    inflight_entertainment: Optional[float] = None\n+    wifi_connectivity: Optional[float] = None\n+    value_for_money: Optional[float] = None\n+\n+    # Limit for MindsDB query\n+    limit: Optional[int] = 100\n+\n+\n+class InterpretAgentRequest(BaseModel):\n+    query: str\n+    reintr_query: Optional[str] = None\n+    top_reviews: List[Dict[str, Any]]  # expects a small set (top 5)\n+    base_stats: Dict[str, Any]\n+    special_stats: Optional[Dict[str, Any]] = None\n+\n+\n+# ---------------------------------------------------------\n+# ✅ Base Case Endpoint (Sanitized + Enhanced)\n+# ---------------------------------------------------------\n+@app.post(\"/base_case\")\n+async def base_case_analysis(request: AirlineSearchRequest, limit: int = 500):\n+    \"\"\"\n+    🧠 Base Case API (Final)\n+    Handles simple NLP search queries with all metadata filters.\n+    Sanitizes any invalid float values (NaN/Inf) before JSON return.\n+    \"\"\"\n+    try:\n+        escaped_query = escape_sql_string(request.query)\n+\n+        # --- Step 1: Build Base SQL Query ---\n+        sql_query = f\"\"\"\n+            SELECT *\n+            FROM airline_kb_10000\n+            WHERE content = '{escaped_query}'\n+        \"\"\"\n+\n+        # --- Step 2: Apply Filters ---\n+        if request.airline_name:\n+            sql_query += f\" AND airline_name = '{escape_sql_string(request.airline_name)}'\"\n+        if request.type_of_traveller:\n+            sql_query += f\" AND type_of_traveller = '{escape_sql_string(request.type_of_traveller)}'\"\n+        if request.seat_type:\n+            sql_query += f\" AND seat_type = '{escape_sql_string(request.seat_type)}'\"\n+        if request.recommended:\n+            sql_query += f\" AND recommended = '{escape_sql_string(request.recommended)}'\"\n+        if request.verified is not None:\n+            sql_query += f\" AND verified = {str(request.verified).lower()}\"\n+\n+        numeric_fields = [\n+            \"overall_rating\",\n+            \"seat_comfort\",\n+            \"cabin_staff_service\",\n+            \"food_beverages\",\n+            \"ground_service\",\n+            \"inflight_entertainment\",\n+            \"wifi_connectivity\",\n+            \"value_for_money\",\n+        ]\n+\n+        for field in numeric_fields:\n+            value = getattr(request, field, None)\n+            if value is not None:\n+                sql_query += f\" AND {field} >= {value}\"\n+\n+        # --- Step 3: Apply Limit ---\n+        limit_value = getattr(request, \"limit\", limit) or limit\n+        sql_query += f\" LIMIT {limit_value};\"\n+\n+        logger.info(f\"Running Sanitized Base Case Query: {sql_query}\")\n+\n+        # --- Step 4: Execute Query ---\n+        result = project.query(sql_query)\n+        rows = result.fetch()\n+        if rows.empty:\n+            raise HTTPException(status_code=404, detail=\"No matching reviews found.\")",
    "comment": "**correctness**: If `project.query()` or `result.fetch()` fails or returns a non-DataFrame, subsequent code will crash with an AttributeError or KeyError, causing a 500 error instead of a controlled FastAPI HTTPException.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb hacktoberfest/use-cases/AIRLYTICS/backend/app.py, lines 172-175, the code assumes that `result.fetch()` always returns a DataFrame with an `empty` attribute. If this is not the case, the code will crash. Please wrap this block in a try/except and check for the `empty` attribute before proceeding, raising a controlled HTTPException if the result is not as expected.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n        try:\n            result = project.query(sql_query)\n            rows = result.fetch()\n            if not hasattr(rows, 'empty') or rows.empty:\n                raise HTTPException(status_code=404, detail=\"No matching reviews found.\")\n        except Exception as e:\n            logger.error(f\"MindsDB query failed: {e}\")\n            raise HTTPException(status_code=500, detail=\"MindsDB query failed: {}\".format(str(e)))\n```\n</details>\n<!-- suggestion_end -->\n",
    "line_number": 175,
    "enriched": "File: mindsdb hacktoberfest/use-cases/AIRLYTICS/backend/app.py\nCode: @@ -0,0 +1,530 @@\n+\n+import os\n+import json\n+import math\n+import logging\n+from typing import List, Dict, Any, Optional\n+\n+from fastapi import FastAPI, HTTPException\n+from fastapi.middleware.cors import CORSMiddleware\n+from fastapi.responses import JSONResponse\n+from pydantic import BaseModel\n+\n+import mindsdb_sdk\n+from dotenv import load_dotenv\n+\n+from base_stats import summarize_metadata\n+from multivalue_stats import (\n+    conditional_rating_analysis,\n+    conditional_rating_to_rating_analysis,\n+    conditional_category_to_category_analysis,\n+    conditional_distribution_analysis,\n+    general_percentage_distribution\n+)\n+\n+# ---------------------------------------------------------\n+# Load environment variables\n+# ---------------------------------------------------------\n+load_dotenv()\n+\n+# ---------------------------------------------------------\n+# FastAPI setup\n+# ---------------------------------------------------------\n+app = FastAPI(title=\"AeroMind API - Airline Review Intelligence\")\n+\n+app.add_middleware(\n+    CORSMiddleware,\n+    allow_origins=[\"*\"],\n+    allow_credentials=True,\n+    allow_methods=[\"*\"],\n+    allow_headers=[\"*\"],\n+)\n+\n+# ---------------------------------------------------------\n+# Logging setup\n+# ---------------------------------------------------------\n+logging.basicConfig(level=logging.INFO)\n+logger = logging.getLogger(\"aeromind_backend\")\n+\n+# ---------------------------------------------------------\n+# MindsDB Connection\n+# ---------------------------------------------------------\n+try:\n+    con = mindsdb_sdk.connect(\"http://127.0.0.1:47334\")\n+    project = con.get_project(\"mindsdb\")\n+    logger.info(\"✅ Connected to MindsDB successfully.\")\n+except Exception as e:\n+    logger.error(f\"Failed to connect to MindsDB: {e}\")\n+    raise RuntimeError(f\"Failed to connect to MindsDB: {e}\")\n+\n+# ---------------------------------------------------------\n+# Utilities\n+# ---------------------------------------------------------\n+def escape_sql_string(value: str) -> str:\n+    \"\"\"Safely escape single quotes for SQL queries.\"\"\"\n+    return value.replace(\"'\", \"''\") if value else \"\"\n+\n+def sanitize_for_json(obj: Any) -> Any:\n+    \"\"\"Recursively replace NaN/Inf with None for JSON compliance.\"\"\"\n+    if isinstance(obj, float) and (math.isnan(obj) or math.isinf(obj)):\n+        return None\n+    if isinstance(obj, dict):\n+        return {k: sanitize_for_json(v) for k, v in obj.items()}\n+    if isinstance(obj, list):\n+        return [sanitize_for_json(x) for x in obj]\n+    return obj\n+\n+def short_text(s: Optional[str], n: int = 280) -> str:\n+    if not s:\n+        return \"\"\n+    s = str(s).strip()\n+    return s if len(s) <= n else s[: n - 1] + \"…\"\n+\n+# ---------------------------------------------------------\n+# Pydantic Models\n+# ---------------------------------------------------------\n+class AirlineSearchRequest(BaseModel):\n+    query: str\n+    airline_name: Optional[str] = None\n+    aircraft: Optional[str] = None\n+    type_of_traveller: Optional[str] = None\n+    seat_type: Optional[str] = None\n+    recommended: Optional[str] = None\n+    verified: Optional[bool] = None\n+\n+    # Numeric rating fields\n+    overall_rating: Optional[float] = None\n+    seat_comfort: Optional[float] = None\n+    cabin_staff_service: Optional[float] = None\n+    food_beverages: Optional[float] = None\n+    ground_service: Optional[float] = None\n+    inflight_entertainment: Optional[float] = None\n+    wifi_connectivity: Optional[float] = None\n+    value_for_money: Optional[float] = None\n+\n+    # Limit for MindsDB query\n+    limit: Optional[int] = 100\n+\n+\n+class InterpretAgentRequest(BaseModel):\n+    query: str\n+    reintr_query: Optional[str] = None\n+    top_reviews: List[Dict[str, Any]]  # expects a small set (top 5)\n+    base_stats: Dict[str, Any]\n+    special_stats: Optional[Dict[str, Any]] = None\n+\n+\n+# ---------------------------------------------------------\n+# ✅ Base Case Endpoint (Sanitized + Enhanced)\n+# ---------------------------------------------------------\n+@app.post(\"/base_case\")\n+async def base_case_analysis(request: AirlineSearchRequest, limit: int = 500):\n+    \"\"\"\n+    🧠 Base Case API (Final)\n+    Handles simple NLP search queries with all metadata filters.\n+    Sanitizes any invalid float values (NaN/Inf) before JSON return.\n+    \"\"\"\n+    try:\n+        escaped_query = escape_sql_string(request.query)\n+\n+        # --- Step 1: Build Base SQL Query ---\n+        sql_query = f\"\"\"\n+            SELECT *\n+            FROM airline_kb_10000\n+            WHERE content = '{escaped_query}'\n+        \"\"\"\n+\n+        # --- Step 2: Apply Filters ---\n+        if request.airline_name:\n+            sql_query += f\" AND airline_name = '{escape_sql_string(request.airline_name)}'\"\n+        if request.type_of_traveller:\n+            sql_query += f\" AND type_of_traveller = '{escape_sql_string(request.type_of_traveller)}'\"\n+        if request.seat_type:\n+            sql_query += f\" AND seat_type = '{escape_sql_string(request.seat_type)}'\"\n+        if request.recommended:\n+            sql_query += f\" AND recommended = '{escape_sql_string(request.recommended)}'\"\n+        if request.verified is not None:\n+            sql_query += f\" AND verified = {str(request.verified).lower()}\"\n+\n+        numeric_fields = [\n+            \"overall_rating\",\n+            \"seat_comfort\",\n+            \"cabin_staff_service\",\n+            \"food_beverages\",\n+            \"ground_service\",\n+            \"inflight_entertainment\",\n+            \"wifi_connectivity\",\n+            \"value_for_money\",\n+        ]\n+\n+        for field in numeric_fields:\n+            value = getattr(request, field, None)\n+            if value is not None:\n+                sql_query += f\" AND {field} >= {value}\"\n+\n+        # --- Step 3: Apply Limit ---\n+        limit_value = getattr(request, \"limit\", limit) or limit\n+        sql_query += f\" LIMIT {limit_value};\"\n+\n+        logger.info(f\"Running Sanitized Base Case Query: {sql_query}\")\n+\n+        # --- Step 4: Execute Query ---\n+        result = project.query(sql_query)\n+        rows = result.fetch()\n+        if rows.empty:\n+            raise HTTPException(status_code=404, detail=\"No matching reviews found.\")\nComment: **correctness**: If `project.query()` or `result.fetch()` fails or returns a non-DataFrame, subsequent code will crash with an AttributeError or KeyError, causing a 500 error instead of a controlled FastAPI HTTPException.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb hacktoberfest/use-cases/AIRLYTICS/backend/app.py, lines 172-175, the code assumes that `result.fetch()` always returns a DataFrame with an `empty` attribute. If this is not the case, the code will crash. Please wrap this block in a try/except and check for the `empty` attribute before proceeding, raising a controlled HTTPException if the result is not as expected.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n        try:\n            result = project.query(sql_query)\n            rows = result.fetch()\n            if not hasattr(rows, 'empty') or rows.empty:\n                raise HTTPException(status_code=404, detail=\"No matching reviews found.\")\n        except Exception as e:\n            logger.error(f\"MindsDB query failed: {e}\")\n            raise HTTPException(status_code=500, detail=\"MindsDB query failed: {}\".format(str(e)))\n```\n</details>\n<!-- suggestion_end -->\n",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "mindsdb hacktoberfest/use-cases/AIRLYTICS/backend/app.py",
    "pr_number": 11835,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2479887891,
    "comment_created_at": "2025-10-31T00:56:56Z"
  },
  {
    "code": "@@ -0,0 +1,53 @@\n+---\n+title: Amazon S3\n+sidebarTitle: Amazon S3\n+---\n+\n+\n+##S3 Handler\n+\n+This is the implementation of the S3 handler for MindsDB.\n+\n+##Amazon S3\n+\n+Amazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance. Customers of all sizes and industries can use Amazon S3 to store and protect any amount of data for a range of use cases, such as data lakes, websites, mobile applications, backup and restore, archive, enterprise applications, IoT devices, and big data analytics. Amazon S3 provides management features so that you can optimize, organize, and configure access to your data to meet your specific business, organizational, and compliance requirements.\n+https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html\n+\n+##Implementation\n+\n+This handler was implemented using the boto3, the AWS SDK for Python.\n+\n+The required arguments to establish a connection are,\n+\n+aws_access_key_id:     the AWS access key\n+aws_secret_access_key: the AWS secret access key\n+region_name:           the AWS region\n+bucket:                the name of the S3 bucket\n+key:                   the key of the object to be queried\n+input_serialization:   the format of the data in the object that is to be queried\n+##Usage\n+   \n+   In order to make use of this handler and connect to an object in a S3 bucket through MindsDB, the following syntax can be used,\n+\n+CREATE DATABASE s3_datasource",
    "comment": "Please make sure the query is within the code block:\r\n\r\n> \\```sql\r\n> SQL GOES HERE\r\n>\\```",
    "line_number": 32,
    "enriched": "File: docs/data-integrations/amazon-s3.mdx\nCode: @@ -0,0 +1,53 @@\n+---\n+title: Amazon S3\n+sidebarTitle: Amazon S3\n+---\n+\n+\n+##S3 Handler\n+\n+This is the implementation of the S3 handler for MindsDB.\n+\n+##Amazon S3\n+\n+Amazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance. Customers of all sizes and industries can use Amazon S3 to store and protect any amount of data for a range of use cases, such as data lakes, websites, mobile applications, backup and restore, archive, enterprise applications, IoT devices, and big data analytics. Amazon S3 provides management features so that you can optimize, organize, and configure access to your data to meet your specific business, organizational, and compliance requirements.\n+https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html\n+\n+##Implementation\n+\n+This handler was implemented using the boto3, the AWS SDK for Python.\n+\n+The required arguments to establish a connection are,\n+\n+aws_access_key_id:     the AWS access key\n+aws_secret_access_key: the AWS secret access key\n+region_name:           the AWS region\n+bucket:                the name of the S3 bucket\n+key:                   the key of the object to be queried\n+input_serialization:   the format of the data in the object that is to be queried\n+##Usage\n+   \n+   In order to make use of this handler and connect to an object in a S3 bucket through MindsDB, the following syntax can be used,\n+\n+CREATE DATABASE s3_datasource\nComment: Please make sure the query is within the code block:\r\n\r\n> \\```sql\r\n> SQL GOES HERE\r\n>\\```",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "docs/data-integrations/amazon-s3.mdx",
    "pr_number": 5013,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1138639656,
    "comment_created_at": "2023-03-16T13:12:35Z"
  },
  {
    "code": "@@ -8,13 +8,15 @@\n \n from langchain.schema import SystemMessage\n from langchain.agents import AgentType\n-from langchain.llms import OpenAI\n-from langchain.chat_models import ChatAnthropic, ChatOpenAI, ChatAnyscale, ChatLiteLLM  # GPT-4 fails to follow the output langchain requires, avoid using for now\n-from langchain.agents import initialize_agent, create_sql_agent\n+from langchain_community.llms import OpenAI\n+from langchain_community.chat_models import ChatAnthropic, ChatOpenAI, ChatAnyscale, ChatLiteLLM\n+from langchain.agents import initialize_agent, create_sql_agent  # TODO: initialize_agent is deprecated, replace with e.g. `create_react_agent`  # noqa",
    "comment": "See #8863",
    "line_number": 13,
    "enriched": "File: mindsdb/integrations/handlers/langchain_handler/langchain_handler.py\nCode: @@ -8,13 +8,15 @@\n \n from langchain.schema import SystemMessage\n from langchain.agents import AgentType\n-from langchain.llms import OpenAI\n-from langchain.chat_models import ChatAnthropic, ChatOpenAI, ChatAnyscale, ChatLiteLLM  # GPT-4 fails to follow the output langchain requires, avoid using for now\n-from langchain.agents import initialize_agent, create_sql_agent\n+from langchain_community.llms import OpenAI\n+from langchain_community.chat_models import ChatAnthropic, ChatOpenAI, ChatAnyscale, ChatLiteLLM\n+from langchain.agents import initialize_agent, create_sql_agent  # TODO: initialize_agent is deprecated, replace with e.g. `create_react_agent`  # noqa\nComment: See #8863",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "mindsdb/integrations/handlers/langchain_handler/langchain_handler.py",
    "pr_number": 8785,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1510935845,
    "comment_created_at": "2024-03-04T10:26:45Z"
  },
  {
    "code": "@@ -1,94 +1,77 @@\n+\"\"\"\n+This module sets up logging when called in different threads.\n+\n+Each thread that imports this module will automatically create the top level mindsdb logger and configure it according\n+to the config provided by Config(). Presumably Config() will provide the local config.\n+\n+The module also sets up telemetry.\n+\n+Finally, calling initialize_log will create a child logger for the caller which will be used to create children with the\n+get log function. This allows the thread to establish a unique child logger.\n+\n+Absent a call to initialize_log, the default logger will be the root mindsdb logger.\n+\n+\"\"\"\n+\n import os\n-import sys\n import logging\n \n from mindsdb.utilities.config import Config\n from functools import partial\n \n+config = Config().get_all()\n \n-class LoggerWrapper(object):\n-    def __init__(self, writer_arr, default_writer_pos):\n-        self._writer_arr = writer_arr\n-        self.default_writer_pos = default_writer_pos\n+log = logging.getLogger('mindsdb')\n+log.propagate = False\n+log.setLevel(min(\n+    getattr(logging, config['log']['level']['console']),\n+    getattr(logging, config['log']['level']['file'])\n+))\n \n-    def write(self, message):\n-        if len(message.strip(' \\n')) == 0:\n-            return\n-        if 'DEBUG:' in message:\n-            self._writer_arr[0](message)\n-        elif 'INFO:' in message:\n-            self._writer_arr[1](message)\n-        elif 'WARNING:' in message:\n-            self._writer_arr[2](message)\n-        elif 'ERROR:' in message:\n-            self._writer_arr[3](message)\n-        else:\n-            self._writer_arr[self.default_writer_pos](message)\n+formatter = logging.Formatter('%(levelname)s: - %(asctime)s - %(name)s - %(message)s')\n \n-    def flush(self):\n-        pass\n+console_handler = logging.StreamHandler()\n+console_handler.setLevel(config['log']['level'].get('console', logging.INFO))\n+console_handler.setFormatter(formatter)\n \n-    def isatty(self):\n-        return True  # assumes terminal attachment\n+log.handlers.clear()\n+log.addHandler(console_handler)\n+log.info(f\"Root logger set to loglevel {log.level}\")\n+log.info(f\"Root handler set to loglevel {console_handler.level}.\")\n+log.info(f\"Number of handlers: {len(log.handlers)}\")\n+log.info(f\"\")\n \n-    def fileno(self):\n-        return 1  # stdout\n+log.error = partial(log.error, exc_info=True)\n \n+logger = log\n \n-# default logger\n-logger = logging.getLogger('dummy')\n+# activate telemetry\n+telemtry_enabled = os.getenv('CHECK_FOR_UPDATES', '1').lower() not in ['0', 'false', 'False']",
    "comment": "Typo.  \"telemetry\" not \"telemtry\"  (missing the last e)",
    "line_number": 49,
    "enriched": "File: mindsdb/utilities/log.py\nCode: @@ -1,94 +1,77 @@\n+\"\"\"\n+This module sets up logging when called in different threads.\n+\n+Each thread that imports this module will automatically create the top level mindsdb logger and configure it according\n+to the config provided by Config(). Presumably Config() will provide the local config.\n+\n+The module also sets up telemetry.\n+\n+Finally, calling initialize_log will create a child logger for the caller which will be used to create children with the\n+get log function. This allows the thread to establish a unique child logger.\n+\n+Absent a call to initialize_log, the default logger will be the root mindsdb logger.\n+\n+\"\"\"\n+\n import os\n-import sys\n import logging\n \n from mindsdb.utilities.config import Config\n from functools import partial\n \n+config = Config().get_all()\n \n-class LoggerWrapper(object):\n-    def __init__(self, writer_arr, default_writer_pos):\n-        self._writer_arr = writer_arr\n-        self.default_writer_pos = default_writer_pos\n+log = logging.getLogger('mindsdb')\n+log.propagate = False\n+log.setLevel(min(\n+    getattr(logging, config['log']['level']['console']),\n+    getattr(logging, config['log']['level']['file'])\n+))\n \n-    def write(self, message):\n-        if len(message.strip(' \\n')) == 0:\n-            return\n-        if 'DEBUG:' in message:\n-            self._writer_arr[0](message)\n-        elif 'INFO:' in message:\n-            self._writer_arr[1](message)\n-        elif 'WARNING:' in message:\n-            self._writer_arr[2](message)\n-        elif 'ERROR:' in message:\n-            self._writer_arr[3](message)\n-        else:\n-            self._writer_arr[self.default_writer_pos](message)\n+formatter = logging.Formatter('%(levelname)s: - %(asctime)s - %(name)s - %(message)s')\n \n-    def flush(self):\n-        pass\n+console_handler = logging.StreamHandler()\n+console_handler.setLevel(config['log']['level'].get('console', logging.INFO))\n+console_handler.setFormatter(formatter)\n \n-    def isatty(self):\n-        return True  # assumes terminal attachment\n+log.handlers.clear()\n+log.addHandler(console_handler)\n+log.info(f\"Root logger set to loglevel {log.level}\")\n+log.info(f\"Root handler set to loglevel {console_handler.level}.\")\n+log.info(f\"Number of handlers: {len(log.handlers)}\")\n+log.info(f\"\")\n \n-    def fileno(self):\n-        return 1  # stdout\n+log.error = partial(log.error, exc_info=True)\n \n+logger = log\n \n-# default logger\n-logger = logging.getLogger('dummy')\n+# activate telemetry\n+telemtry_enabled = os.getenv('CHECK_FOR_UPDATES', '1').lower() not in ['0', 'false', 'False']\nComment: Typo.  \"telemetry\" not \"telemtry\"  (missing the last e)",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "mindsdb/utilities/log.py",
    "pr_number": 8363,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1393344130,
    "comment_created_at": "2023-11-14T21:51:53Z"
  },
  {
    "code": "@@ -28,13 +28,13 @@\n \n ----------------------------------------\n \n-[MindsDB](https://mindsdb.com?utm_medium=community&utm_source=github&utm_campaign=mindsdb%20repo) is a platform for building AI solutions that can learn from and answer questions over federated data.\n+[MindsDB](https://mindsdb.com?utm_medium=community&utm_source=github&utm_campaign=mindsdb%20repo) is the world’s most widely used platform for building AI that can learn from and answer questions across federated data.\n \n [![Tweet](https://img.shields.io/twitter/url/http/shields.io.svg?style=social)](https://twitter.com/intent/tweet?text=The%20platform%20for%20building%20AI,%20from%20enterprise%20data&url=https://github.com/mindsdb/mindsdb&via=mindsdb&hashtags=ai,opensource)\n \n ## 📖 About Us\n \n-MindsDB is a powerful federated AI query engine that allows developers to connect agents and AI applications to a wide variety of [data sources](https://docs.mindsdb.com/integrations/data-overview?utm_medium=community&utm_source=github&utm_campaign=mindsdb%20repo).\n+MindsDB MindsDB is a federated query engine designed for AI agents and applications that need to answer questions from one or multiple [data sources](https://docs.mindsdb.com/integrations/data-overview?utm_medium=community&utm_source=github&utm_campaign=mindsdb%20repo), including both structured and unstructured data..",
    "comment": "MindsDB MindsDB - is typo?",
    "line_number": 37,
    "enriched": "File: README.md\nCode: @@ -28,13 +28,13 @@\n \n ----------------------------------------\n \n-[MindsDB](https://mindsdb.com?utm_medium=community&utm_source=github&utm_campaign=mindsdb%20repo) is a platform for building AI solutions that can learn from and answer questions over federated data.\n+[MindsDB](https://mindsdb.com?utm_medium=community&utm_source=github&utm_campaign=mindsdb%20repo) is the world’s most widely used platform for building AI that can learn from and answer questions across federated data.\n \n [![Tweet](https://img.shields.io/twitter/url/http/shields.io.svg?style=social)](https://twitter.com/intent/tweet?text=The%20platform%20for%20building%20AI,%20from%20enterprise%20data&url=https://github.com/mindsdb/mindsdb&via=mindsdb&hashtags=ai,opensource)\n \n ## 📖 About Us\n \n-MindsDB is a powerful federated AI query engine that allows developers to connect agents and AI applications to a wide variety of [data sources](https://docs.mindsdb.com/integrations/data-overview?utm_medium=community&utm_source=github&utm_campaign=mindsdb%20repo).\n+MindsDB MindsDB is a federated query engine designed for AI agents and applications that need to answer questions from one or multiple [data sources](https://docs.mindsdb.com/integrations/data-overview?utm_medium=community&utm_source=github&utm_campaign=mindsdb%20repo), including both structured and unstructured data..\nComment: MindsDB MindsDB - is typo?",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "README.md",
    "pr_number": 10160,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1843945217,
    "comment_created_at": "2024-11-15T14:50:16Z"
  },
  {
    "code": "@@ -71,3 +71,27 @@ Here is the output:\n ## Supported Models\n \n For an overview of the models supported, visit the [following docs](https://docs.mdb.ai/). This list will help you quickly identify the right models for your needs.\n+\n+## Troubleshooting Guide\n+\n+<Warning>\n+`Authentication Error`\n+\n+* **Symptoms**: Failure to authenticate to MindsDB Inference Endpoints.\n+* **Checklist**:\n+    1. Make sure the your MindsDB Inference Endpoints account is active.",
    "comment": "A small typo in this line - `Make sure **that** your...`",
    "line_number": 82,
    "enriched": "File: docs/integrations/ai-engines/mindsdb_inference.mdx\nCode: @@ -71,3 +71,27 @@ Here is the output:\n ## Supported Models\n \n For an overview of the models supported, visit the [following docs](https://docs.mdb.ai/). This list will help you quickly identify the right models for your needs.\n+\n+## Troubleshooting Guide\n+\n+<Warning>\n+`Authentication Error`\n+\n+* **Symptoms**: Failure to authenticate to MindsDB Inference Endpoints.\n+* **Checklist**:\n+    1. Make sure the your MindsDB Inference Endpoints account is active.\nComment: A small typo in this line - `Make sure **that** your...`",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "docs/integrations/ai-engines/mindsdb_inference.mdx",
    "pr_number": 9171,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1588989424,
    "comment_created_at": "2024-05-03T09:52:50Z"
  },
  {
    "code": "@@ -868,13 +868,21 @@ def answer_describe_object(self, obj_type: str, obj_name: Identifier, database_n\n             else:\n                 raise WrongArgumentError(f'Unknown describe type: {obj_type}')\n \n-        name = obj_name.parts[-1]\n+        parts = obj_name.parts",
    "comment": "since min python version changed to 3.10, it is possible to use match/case for such cases. I found it more easy to understand in the code.\r\n```python\r\nmatch obj_name.parts:\r\n    case [name,]:\r\n        database_name = None\r\n    case [database, name]:\r\n        pass\r\n    case _:\r\n        raise Exception('?!')\r\n```",
    "line_number": 871,
    "enriched": "File: mindsdb/api/executor/command_executor.py\nCode: @@ -868,13 +868,21 @@ def answer_describe_object(self, obj_type: str, obj_name: Identifier, database_n\n             else:\n                 raise WrongArgumentError(f'Unknown describe type: {obj_type}')\n \n-        name = obj_name.parts[-1]\n+        parts = obj_name.parts\nComment: since min python version changed to 3.10, it is possible to use match/case for such cases. I found it more easy to understand in the code.\r\n```python\r\nmatch obj_name.parts:\r\n    case [name,]:\r\n        database_name = None\r\n    case [database, name]:\r\n        pass\r\n    case _:\r\n        raise Exception('?!')\r\n```",
    "subcategory": "validation",
    "category": "refactoring",
    "file_path": "mindsdb/api/executor/command_executor.py",
    "pr_number": 10733,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2050372921,
    "comment_created_at": "2025-04-18T08:53:15Z"
  },
  {
    "code": "@@ -13,3 +13,7 @@ XXX Test Passed: <link to the test results in your GitHub repository>\n </br>\n XXX Test Failed: <link to the test results in your GitHub repository>, <link to the issue>\n </br>\n+\n+Create a Project, Test passed: https://github.com/hridaya423/HacktoberfestMindsDBTesting/blob/master/Testing.md\n+Remove a Project, Test passed: https://github.com/hridaya423/HacktoberfestMindsDBTesting/blob/master/Testing.md\n+List Projects, Test Failed: https://github.com/hridaya423/HacktoberfestMindsDBTesting/blob/master/Testing.md, https://github.com/mindsdb/mindsdb/issues/7519",
    "comment": "Test with syntax `SHOW FULL DATABASE WHERE type = 'project';` without brackets please",
    "line_number": 19,
    "enriched": "File: docs/Docs_Manual_QA.md\nCode: @@ -13,3 +13,7 @@ XXX Test Passed: <link to the test results in your GitHub repository>\n </br>\n XXX Test Failed: <link to the test results in your GitHub repository>, <link to the issue>\n </br>\n+\n+Create a Project, Test passed: https://github.com/hridaya423/HacktoberfestMindsDBTesting/blob/master/Testing.md\n+Remove a Project, Test passed: https://github.com/hridaya423/HacktoberfestMindsDBTesting/blob/master/Testing.md\n+List Projects, Test Failed: https://github.com/hridaya423/HacktoberfestMindsDBTesting/blob/master/Testing.md, https://github.com/mindsdb/mindsdb/issues/7519\nComment: Test with syntax `SHOW FULL DATABASE WHERE type = 'project';` without brackets please",
    "subcategory": "validation",
    "category": "refactoring",
    "file_path": "docs/Docs_Manual_QA.md",
    "pr_number": 7520,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1344362517,
    "comment_created_at": "2023-10-03T16:06:58Z"
  },
  {
    "code": "@@ -343,7 +343,8 @@\n             \"data-integrations/oracle\",\n              \"data-integrations/solr\",\n             \"data-integrations/OrioleDB\",\n-            \"data-integrations/singlestore\",\n+            \"data-integrations/singlestore\"",
    "comment": "You removed the commas. Please add commas when listing pages here.",
    "line_number": 346,
    "enriched": "File: docs/mint.json\nCode: @@ -343,7 +343,8 @@\n             \"data-integrations/oracle\",\n              \"data-integrations/solr\",\n             \"data-integrations/OrioleDB\",\n-            \"data-integrations/singlestore\",\n+            \"data-integrations/singlestore\"\nComment: You removed the commas. Please add commas when listing pages here.",
    "subcategory": "validation",
    "category": "refactoring",
    "file_path": "docs/mint.json",
    "pr_number": 5595,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1168526103,
    "comment_created_at": "2023-04-17T11:06:24Z"
  },
  {
    "code": "@@ -0,0 +1,198 @@\n+from collections import OrderedDict\n+\n+import pandas as pd\n+import mysql.connector\n+\n+from mindsdb_sql import parse_sql\n+from mindsdb_sql.parser.ast.base import ASTNode\n+\n+from mindsdb.utilities import log\n+from mindsdb.integrations.libs.base import DatabaseHandler\n+from mindsdb.integrations.libs.response import (\n+    HandlerStatusResponse as StatusResponse,\n+    HandlerResponse as Response,\n+    RESPONSE_TYPE\n+)\n+from mindsdb.integrations.libs.const import HANDLER_CONNECTION_ARG_TYPE as ARG_TYPE\n+\n+\n+class ApacheDorisHandler(DatabaseHandler):",
    "comment": "Wouldn't be easy to extend MySQLHandler? Or there is some difference?",
    "line_number": 19,
    "enriched": "File: mindsdb/integrations/handlers/apache_doris_handler/apache_doris_handler.py\nCode: @@ -0,0 +1,198 @@\n+from collections import OrderedDict\n+\n+import pandas as pd\n+import mysql.connector\n+\n+from mindsdb_sql import parse_sql\n+from mindsdb_sql.parser.ast.base import ASTNode\n+\n+from mindsdb.utilities import log\n+from mindsdb.integrations.libs.base import DatabaseHandler\n+from mindsdb.integrations.libs.response import (\n+    HandlerStatusResponse as StatusResponse,\n+    HandlerResponse as Response,\n+    RESPONSE_TYPE\n+)\n+from mindsdb.integrations.libs.const import HANDLER_CONNECTION_ARG_TYPE as ARG_TYPE\n+\n+\n+class ApacheDorisHandler(DatabaseHandler):\nComment: Wouldn't be easy to extend MySQLHandler? Or there is some difference?",
    "subcategory": "validation",
    "category": "refactoring",
    "file_path": "mindsdb/integrations/handlers/apache_doris_handler/apache_doris_handler.py",
    "pr_number": 7789,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1360591237,
    "comment_created_at": "2023-10-16T12:36:53Z"
  },
  {
    "code": "@@ -0,0 +1,413 @@\n+from mindsdb.integrations.libs.response import (",
    "comment": "Would like to see a few basic tests for this. See [Binance Handler tests](https://github.com/mindsdb/mindsdb/blob/staging/tests/handler_tests/test_binance_handler.py) as an example",
    "line_number": 1,
    "enriched": "File: mindsdb/integrations/handlers/gmail_handler/gmail_handler.py\nCode: @@ -0,0 +1,413 @@\n+from mindsdb.integrations.libs.response import (\nComment: Would like to see a few basic tests for this. See [Binance Handler tests](https://github.com/mindsdb/mindsdb/blob/staging/tests/handler_tests/test_binance_handler.py) as an example",
    "subcategory": "validation",
    "category": "refactoring",
    "file_path": "mindsdb/integrations/handlers/gmail_handler/gmail_handler.py",
    "pr_number": 5889,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1186391154,
    "comment_created_at": "2023-05-05T18:30:18Z"
  },
  {
    "code": "@@ -0,0 +1,43 @@\n+import os",
    "comment": "It looks like a unit test, maybe move it to tests/unit?",
    "line_number": 1,
    "enriched": "File: tests/api/http/projects_test.py\nCode: @@ -0,0 +1,43 @@\n+import os\nComment: It looks like a unit test, maybe move it to tests/unit?",
    "subcategory": "validation",
    "category": "refactoring",
    "file_path": "tests/api/http/projects_test.py",
    "pr_number": 5590,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1165717332,
    "comment_created_at": "2023-04-13T15:39:19Z"
  },
  {
    "code": "@@ -0,0 +1,93 @@\n+import os\n+from typing import Callable\n+\n+from mindsdb_sql_parser import Function, Constant, Variable\n+\n+from mindsdb.utilities import log\n+from mindsdb.interfaces.storage.fs import RESOURCE_GROUP\n+from mindsdb.interfaces.storage.json import get_json_storage\n+\n+\n+logger = log.getLogger(__name__)\n+\n+\n+ENV_VAR_PREFIX = \"MDB_\"\n+\n+\n+class VariablesController:\n+\n+    def __init__(self) -> None:\n+        self._storage = get_json_storage(\n+            resource_id=0,\n+            resource_group=RESOURCE_GROUP.SYSTEM\n+        )\n+        self._store_key = 'variables'\n+        self._data = None\n+\n+    def _get_data(self) -> dict:\n+        if self._data is None:\n+            self._data = self._storage.get(self._store_key)\n+            if self._data is None:\n+                self._data = {}\n+        return self._data\n+\n+    def get_value(self, name: str):\n+        return self._get_data()[name]",
    "comment": "**Correctness**: The `get_value` method doesn't handle the case when a variable doesn't exist, which will cause a `KeyError` when trying to access a non-existent variable.\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n    def get_value(self, name: str):\n        data = self._get_data()\n        if name not in data:\n            raise ValueError(f\"Variable {name} is not defined\")\n        return data[name]\n```\n</details>\n<!-- suggestion_end -->\n",
    "line_number": 35,
    "enriched": "File: mindsdb/interfaces/variables/variables_controller.py\nCode: @@ -0,0 +1,93 @@\n+import os\n+from typing import Callable\n+\n+from mindsdb_sql_parser import Function, Constant, Variable\n+\n+from mindsdb.utilities import log\n+from mindsdb.interfaces.storage.fs import RESOURCE_GROUP\n+from mindsdb.interfaces.storage.json import get_json_storage\n+\n+\n+logger = log.getLogger(__name__)\n+\n+\n+ENV_VAR_PREFIX = \"MDB_\"\n+\n+\n+class VariablesController:\n+\n+    def __init__(self) -> None:\n+        self._storage = get_json_storage(\n+            resource_id=0,\n+            resource_group=RESOURCE_GROUP.SYSTEM\n+        )\n+        self._store_key = 'variables'\n+        self._data = None\n+\n+    def _get_data(self) -> dict:\n+        if self._data is None:\n+            self._data = self._storage.get(self._store_key)\n+            if self._data is None:\n+                self._data = {}\n+        return self._data\n+\n+    def get_value(self, name: str):\n+        return self._get_data()[name]\nComment: **Correctness**: The `get_value` method doesn't handle the case when a variable doesn't exist, which will cause a `KeyError` when trying to access a non-existent variable.\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n    def get_value(self, name: str):\n        data = self._get_data()\n        if name not in data:\n            raise ValueError(f\"Variable {name} is not defined\")\n        return data[name]\n```\n</details>\n<!-- suggestion_end -->\n",
    "subcategory": "validation",
    "category": "refactoring",
    "file_path": "mindsdb/interfaces/variables/variables_controller.py",
    "pr_number": 10829,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2088797090,
    "comment_created_at": "2025-05-14T12:13:15Z"
  },
  {
    "code": "@@ -201,11 +204,8 @@ def predict(self, df: pd.DataFrame, args: Optional[Dict] = None) -> pd.DataFrame\n \n         pred_args = args['predict_params'] if args else {}\n         args = self.model_storage.json_get('args')\n-        args['api_base'] = pred_args.get(\n-                    'api_base',\n-                    args.get(\n-                        'api_base', os.environ.get('OPENAI_API_BASE', OPENAI_API_BASE)\n-                    ))\n+        connection_args = self.engine_storage.get_connection_args()\n+        args['api_base'] = connection_args.get('api_base') or args.get('api_base') or os.environ.get('OPENAI_API_BASE', OPENAI_API_BASE)",
    "comment": "I think this is missing `or pred_args.get('api_base')`",
    "line_number": 208,
    "enriched": "File: mindsdb/integrations/handlers/openai_handler/openai_handler.py\nCode: @@ -201,11 +204,8 @@ def predict(self, df: pd.DataFrame, args: Optional[Dict] = None) -> pd.DataFrame\n \n         pred_args = args['predict_params'] if args else {}\n         args = self.model_storage.json_get('args')\n-        args['api_base'] = pred_args.get(\n-                    'api_base',\n-                    args.get(\n-                        'api_base', os.environ.get('OPENAI_API_BASE', OPENAI_API_BASE)\n-                    ))\n+        connection_args = self.engine_storage.get_connection_args()\n+        args['api_base'] = connection_args.get('api_base') or args.get('api_base') or os.environ.get('OPENAI_API_BASE', OPENAI_API_BASE)\nComment: I think this is missing `or pred_args.get('api_base')`",
    "subcategory": "validation",
    "category": "refactoring",
    "file_path": "mindsdb/integrations/handlers/openai_handler/openai_handler.py",
    "pr_number": 9019,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1547817610,
    "comment_created_at": "2024-04-02T12:48:54Z"
  },
  {
    "code": "@@ -0,0 +1,146 @@\n+import pandas as pd\n+from mindsdb.integrations.libs.api_handler import APITable\n+from mindsdb_sql.parser import ast\n+from mindsdb.integrations.utilities.sql_utils import extract_comparison_conditions\n+from typing import List, Tuple\n+import requests\n+class StoriesTable(APITable):",
    "comment": "Please add blank lines here",
    "line_number": 7,
    "enriched": "File: mindsdb/integrations/handlers/hackernews_handler/hn_table.py\nCode: @@ -0,0 +1,146 @@\n+import pandas as pd\n+from mindsdb.integrations.libs.api_handler import APITable\n+from mindsdb_sql.parser import ast\n+from mindsdb.integrations.utilities.sql_utils import extract_comparison_conditions\n+from typing import List, Tuple\n+import requests\n+class StoriesTable(APITable):\nComment: Please add blank lines here",
    "subcategory": "validation",
    "category": "refactoring",
    "file_path": "mindsdb/integrations/handlers/hackernews_handler/hn_table.py",
    "pr_number": 5681,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1175210172,
    "comment_created_at": "2023-04-24T12:26:29Z"
  },
  {
    "code": "@@ -36,14 +36,25 @@ def test_view(self):\n \n         # use model\n         ret = self.run_sql('''\n-             SELECT m.*\n-               FROM mindsdb.vtasks as t\n-               JOIN mindsdb.task_model as m\n+            SELECT m.*\n+            FROM mindsdb.vtasks as t\n+            JOIN mindsdb.task_model as m\n         ''')\n \n         assert len(ret) == 2\n         assert ret.predicted[0] == 42\n \n+        # check case-insensitive in subselect step\n+        ret = self.run_sql('''\n+            SELECT\n+                m.predicted as lower,\n+                m.PREDICTED as upper,\n+                M.PREDIcted as varcase",
    "comment": "Let's add `select Name, NAME` case here too, for example \r\n```\r\n                m.predicted as value,\r\n                m.PREDICTED as VALUE,\r\n```",
    "line_number": 52,
    "enriched": "File: tests/unit/executor/test_base_queires.py\nCode: @@ -36,14 +36,25 @@ def test_view(self):\n \n         # use model\n         ret = self.run_sql('''\n-             SELECT m.*\n-               FROM mindsdb.vtasks as t\n-               JOIN mindsdb.task_model as m\n+            SELECT m.*\n+            FROM mindsdb.vtasks as t\n+            JOIN mindsdb.task_model as m\n         ''')\n \n         assert len(ret) == 2\n         assert ret.predicted[0] == 42\n \n+        # check case-insensitive in subselect step\n+        ret = self.run_sql('''\n+            SELECT\n+                m.predicted as lower,\n+                m.PREDICTED as upper,\n+                M.PREDIcted as varcase\nComment: Let's add `select Name, NAME` case here too, for example \r\n```\r\n                m.predicted as value,\r\n                m.PREDICTED as VALUE,\r\n```",
    "subcategory": "validation",
    "category": "refactoring",
    "file_path": "tests/unit/executor/test_base_queires.py",
    "pr_number": 10995,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2123734956,
    "comment_created_at": "2025-06-03T12:56:27Z"
  },
  {
    "code": "@@ -0,0 +1,400 @@\n+from dataclasses import dataclass\n+from functools import lru_cache, partial\n+from typing import List, Union\n+\n+import html2text\n+import openai\n+import pandas as pd\n+import requests\n+from chromadb import Settings\n+from langchain import Writer\n+from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n+from langchain.docstore.document import Document\n+from langchain.document_loaders import DataFrameLoader\n+from langchain.embeddings.base import Embeddings\n+from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n+from langchain.vectorstores import FAISS, Chroma, VectorStore\n+from pydantic import BaseModel, Extra, Field, validator\n+\n+DEFAULT_EMBEDDINGS_MODEL = \"sentence-transformers/all-mpnet-base-v2\"\n+USER_DEFINED_WRITER_LLM_PARAMS = (\n+    \"model_id\",\n+    \"max_tokens\",\n+    \"temperature\",\n+    \"top_p\",\n+    \"stop\",\n+    \"best_of\",\n+    \"verbose\",\n+    \"writer_org_id\",\n+    \"writer_api_key\",\n+    \"base_url\",\n+)\n+\n+SUPPORTED_VECTOR_STORES = (\"chroma\", \"faiss\")\n+\n+EVAL_COLUMN_NAMES = (\n+    \"question\",\n+    \"answers\",\n+    \"context\",\n+)\n+\n+SUPPORTED_LLMS = (\"writer\", \"openai\")\n+\n+SUPPORTED_EVALUATION_TYPES = (\"retrieval\", \"e2e\")\n+\n+SUMMARIZATION_PROMPT_TEMPLATE = \"\"\"\n+Summarize the following texts for me:\n+{context}\n+\n+When summarizing, please keep the following in mind the following question:\n+{question}\n+\"\"\"\n+\n+GENERATION_METRICS = (\"rouge\", \"meteor\", \"cosine_similarity\", \"accuracy\")\n+RETRIEVAL_METRICS = (\"cosine_similarity\", \"accuracy\")\n+\n+\n+def is_valid_store(name):\n+    return name in SUPPORTED_VECTOR_STORES\n+\n+\n+class VectorStoreFactory:\n+    @staticmethod\n+    def get_vectorstore_class(name):\n+\n+        if not isinstance(name, str):\n+            raise TypeError(\"name must be a string\")\n+\n+        if not is_valid_store(name):\n+            raise ValueError(f\"Invalid vector store {name}\")\n+\n+        if name == \"faiss\":\n+            return FAISS\n+\n+        if name == \"chroma\":\n+            return Chroma\n+\n+\n+def get_chroma_settings(persist_directory: str = \"chromadb\") -> Settings:\n+    return Settings(\n+        chroma_db_impl=\"duckdb+parquet\",\n+        persist_directory=persist_directory,\n+        anonymized_telemetry=False,\n+    )\n+\n+\n+@dataclass\n+class PersistedVectorStoreSaverConfig:\n+    vector_store_name: str\n+    persist_directory: str\n+    collection_name: str\n+    vector_store: VectorStore\n+\n+\n+@dataclass\n+class PersistedVectorStoreLoaderConfig:\n+    vector_store_name: str\n+    embeddings_model: Embeddings\n+    persist_directory: str\n+    collection_name: str\n+\n+\n+class PersistedVectorStoreSaver:\n+    def __init__(self, config: PersistedVectorStoreSaverConfig):\n+        self.config = config\n+\n+    def save_vector_store(self, vector_store: VectorStore):\n+        method_name = f\"save_{self.config.vector_store_name}\"\n+        getattr(self, method_name)(vector_store)\n+\n+    def save_chroma(self, vector_store: Chroma):\n+        vector_store.persist()\n+\n+    def save_faiss(self, vector_store: FAISS):\n+        vector_store.save_local(\n+            folder_path=self.config.persist_directory,\n+            index_name=self.config.collection_name,\n+        )\n+\n+\n+class PersistedVectorStoreLoader:\n+    def __init__(self, config: PersistedVectorStoreLoaderConfig):\n+        self.config = config\n+\n+    def load_vector_store_client(\n+        self,\n+        vector_store: str,\n+    ):\n+        \"\"\"Load vector store client from the persisted vector store\"\"\"\n+\n+        if vector_store == \"chroma\":\n+\n+            return Chroma(\n+                collection_name=self.config.collection_name,\n+                embedding_function=self.config.embeddings_model,\n+                client_settings=get_chroma_settings(\n+                    persist_directory=self.config.persist_directory\n+                ),\n+            )\n+\n+        elif vector_store == \"faiss\":\n+\n+            return FAISS.load_local(\n+                folder_path=self.config.persist_directory,\n+                embeddings=self.config.embeddings_model,\n+                index_name=self.config.collection_name,\n+            )\n+\n+        else:\n+            raise NotImplementedError(f\"{vector_store} client is not yet supported\")\n+\n+    def load_vector_store(self):\n+        method_name = f\"load_{self.config.vector_store_name}\"\n+        return getattr(self, method_name)()\n+\n+    def load_chroma(self) -> Chroma:\n+        return self.load_vector_store_client(vector_store=\"chroma\")\n+\n+    def load_faiss(self) -> FAISS:\n+        return self.load_vector_store_client(vector_store=\"faiss\")\n+\n+\n+class LLMParameters(BaseModel):\n+    \"\"\"Model parameters for the LLM API interface\"\"\"\n+\n+    llm_name: str = Field(default_factory=str, title=\"LLM API name\")\n+    max_tokens: int = Field(default=100, title=\"max tokens in response\")\n+    temperature: float = Field(default=0.0, title=\"temperature\")\n+    top_p: float = 1\n+    best_of: int = 5\n+    stop: List[str] = None\n+\n+    class Config:\n+        extra = Extra.forbid\n+        arbitrary_types_allowed = True\n+        use_enum_values = True\n+\n+\n+class OpenAIParameters(LLMParameters):\n+    \"\"\"Model parameters for the LLM API interface\"\"\"\n+\n+    openai_api_key: str\n+    model_id: str = Field(default=\"text-davinci-003\", title=\"model name\")\n+    n: int = Field(default=1, title=\"number of responses to return\")\n+\n+\n+class WriterLLMParameters(LLMParameters):\n+    \"\"\"Model parameters for the Writer LLM API interface\"\"\"\n+\n+    writer_api_key: str\n+    writer_org_id: str = None\n+    base_url: str = None\n+    model_id: str = \"palmyra-x\"\n+    callbacks: List[StreamingStdOutCallbackHandler] = [StreamingStdOutCallbackHandler()]\n+    verbose: bool = False\n+\n+\n+class LLMLoader(BaseModel):\n+    llm_config: Union[WriterLLMParameters, OpenAIParameters]\n+    config_dict: dict = None\n+\n+    def load_llm(self):\n+        method_name = f\"load_{self.llm_config.llm_name}_llm\"\n+        self.config_dict = self.llm_config.dict()\n+        self.config_dict.pop(\"llm_name\")\n+        return getattr(self, method_name)()\n+\n+    def load_writer_llm(self):\n+        return Writer(**self.config_dict)\n+\n+    def load_openai_llm(self):\n+        openai.api_key = self.llm_config.openai_api_key\n+        config = self.config_dict\n+        config.pop(\"openai_api_key\")\n+        config[\"model\"] = config.pop(\"model_id\")\n+\n+        return partial(openai.Completion.create, **config)\n+\n+\n+class MissingPromptTemplate(Exception):\n+    pass\n+\n+\n+class UnsupportedVectorStore(Exception):\n+    pass\n+\n+\n+class MissingUseIndex(Exception):\n+    pass\n+\n+\n+class RAGHandlerParameters(BaseModel):\n+    \"\"\"Model parameters for create model\"\"\"\n+\n+    prompt_template: str\n+    llm_type: str\n+    llm_params: LLMParameters\n+    chunk_size: int = 500\n+    chunk_overlap: int = 50\n+    url: Union[str, List[str]] = None\n+    generation_evaluation_metrics: List[str] = list(GENERATION_METRICS)\n+    retrieval_evaluation_metrics: List[str] = list(RETRIEVAL_METRICS)\n+    evaluation_type: str = \"e2e\"\n+    n_rows_evaluation: int = None  # if None, evaluate on all rows\n+    retriever_match_threshold: float = 0.7\n+    generator_match_threshold: float = 0.8\n+    evaluate_dataset: Union[List[dict], str] = None\n+    run_embeddings: bool = True\n+    external_index_name: str = None\n+    top_k: int = 4\n+    embeddings_model_name: str = DEFAULT_EMBEDDINGS_MODEL\n+    context_columns: Union[List[str], str] = None\n+    vector_store_name: str = \"chroma\"\n+    vector_store: VectorStore = None\n+    collection_name: str = \"langchain\"\n+    summarize_context: bool = False\n+    summarization_prompt_template: str = SUMMARIZATION_PROMPT_TEMPLATE\n+    vector_store_folder_name: str = \"persisted_vector_db\"\n+    vector_store_storage_path: str = None\n+\n+    class Config:\n+        extra = Extra.forbid\n+        arbitrary_types_allowed = True\n+        use_enum_values = True\n+\n+    @validator(\"llm_type\")\n+    def llm_type_must_be_supported(cls, v):\n+        if v not in SUPPORTED_LLMS:\n+            raise ValueError(f\"llm_type must be one of `writer` or `openai`, got {v}\")\n+        return v\n+\n+    @validator(\"generation_evaluation_metrics\")\n+    def generation_evaluation_metrics_must_be_supported(cls, v):\n+        for metric in v:\n+            if metric not in GENERATION_METRICS:\n+                raise ValueError(\n+                    f\"generation_evaluation_metrics must be one of {', '.join(str(v) for v in GENERATION_METRICS)}, got {metric}\"\n+                )\n+        return v\n+\n+    @validator(\"retrieval_evaluation_metrics\")\n+    def retrieval_evaluation_metrics_must_be_supported(cls, v):\n+        for metric in v:\n+            if metric not in GENERATION_METRICS:\n+                raise ValueError(\n+                    f\"retrieval_evaluation_metrics must be one of {', '.join(str(v) for v in RETRIEVAL_METRICS)}, got {metric}\"\n+                )\n+        return v\n+\n+    @validator(\"evaluation_type\")\n+    def evaluation_type_must_be_supported(cls, v):\n+        if v not in SUPPORTED_EVALUATION_TYPES:\n+            raise ValueError(\n+                f\"evaluation_type must be one of `retrieval` or `e2e`, got {v}\"\n+            )\n+        return v\n+\n+    @validator(\"vector_store_name\")\n+    def name_must_be_lower(cls, v):\n+        return v.lower()\n+\n+    @validator(\"prompt_template\")\n+    def prompt_template_must_be_provided(cls, v):\n+        if not v:\n+            raise MissingPromptTemplate(\n+                \"Please provide a `prompt_template` for this engine.\"\n+            )\n+        return v\n+\n+    @validator(\"vector_store_name\")\n+    def vector_store_must_be_supported(cls, v):\n+        if not is_valid_store(v):\n+            raise UnsupportedVectorStore(\n+                f\"currently we only support {', '.join(str(v) for v in SUPPORTED_VECTOR_STORES)} vector store\"\n+            )\n+        return v\n+\n+\n+class DfLoader(DataFrameLoader):\n+\n+    \"\"\"\n+    override the load method of langchain.document_loaders.DataFrameLoaders to ignore rows with 'None' values\n+    \"\"\"\n+\n+    def __init__(self, data_frame: pd.DataFrame, page_content_column: str):\n+        super().__init__(data_frame=data_frame, page_content_column=page_content_column)\n+        self._data_frame = data_frame\n+        self._page_content_column = page_content_column\n+\n+    def load(self) -> List[Document]:\n+        \"\"\"Loads the dataframe as a list of documents\"\"\"\n+        documents = []\n+        for n_row, frame in self._data_frame[self._page_content_column].items():\n+            if pd.notnull(frame):\n+                # ignore rows with None values\n+                column_name = self._page_content_column\n+\n+                document_contents = frame\n+\n+                documents.append(\n+                    Document(\n+                        page_content=document_contents,\n+                        metadata={\n+                            \"source\": \"dataframe\",\n+                            \"row\": n_row,\n+                            \"column\": column_name,\n+                        },\n+                    )\n+                )\n+        return documents\n+\n+\n+def df_to_documents(\n+    df: pd.DataFrame, page_content_columns: Union[List[str], str]\n+) -> List[Document]:\n+    \"\"\"Converts a given dataframe to a list of documents\"\"\"\n+    documents = []\n+\n+    if isinstance(page_content_columns, str):\n+        page_content_columns = [page_content_columns]\n+\n+    for _, page_content_column in enumerate(page_content_columns):\n+        if page_content_column not in df.columns.tolist():\n+            raise ValueError(\n+                f\"page_content_column {page_content_column} not in dataframe columns\"\n+            )\n+\n+        loader = DfLoader(data_frame=df, page_content_column=page_content_column)\n+        documents.extend(loader.load())\n+\n+    return documents\n+\n+\n+def url_to_documents(urls: Union[List[str], str]) -> List[Document]:\n+    \"\"\"Converts a given url to a document\"\"\"\n+    documents = []\n+    if isinstance(urls, str):\n+        urls = [urls]\n+\n+    for url in urls:\n+        response = requests.get(url, headers=None).text\n+        html_to_text = html2text.html2text(response)\n+        documents.append(Document(page_content=html_to_text, metadata={\"source\": url}))\n+\n+    return documents",
    "comment": "add support for urls",
    "line_number": 384,
    "enriched": "File: mindsdb/integrations/handlers/rag_handler/settings.py\nCode: @@ -0,0 +1,400 @@\n+from dataclasses import dataclass\n+from functools import lru_cache, partial\n+from typing import List, Union\n+\n+import html2text\n+import openai\n+import pandas as pd\n+import requests\n+from chromadb import Settings\n+from langchain import Writer\n+from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n+from langchain.docstore.document import Document\n+from langchain.document_loaders import DataFrameLoader\n+from langchain.embeddings.base import Embeddings\n+from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n+from langchain.vectorstores import FAISS, Chroma, VectorStore\n+from pydantic import BaseModel, Extra, Field, validator\n+\n+DEFAULT_EMBEDDINGS_MODEL = \"sentence-transformers/all-mpnet-base-v2\"\n+USER_DEFINED_WRITER_LLM_PARAMS = (\n+    \"model_id\",\n+    \"max_tokens\",\n+    \"temperature\",\n+    \"top_p\",\n+    \"stop\",\n+    \"best_of\",\n+    \"verbose\",\n+    \"writer_org_id\",\n+    \"writer_api_key\",\n+    \"base_url\",\n+)\n+\n+SUPPORTED_VECTOR_STORES = (\"chroma\", \"faiss\")\n+\n+EVAL_COLUMN_NAMES = (\n+    \"question\",\n+    \"answers\",\n+    \"context\",\n+)\n+\n+SUPPORTED_LLMS = (\"writer\", \"openai\")\n+\n+SUPPORTED_EVALUATION_TYPES = (\"retrieval\", \"e2e\")\n+\n+SUMMARIZATION_PROMPT_TEMPLATE = \"\"\"\n+Summarize the following texts for me:\n+{context}\n+\n+When summarizing, please keep the following in mind the following question:\n+{question}\n+\"\"\"\n+\n+GENERATION_METRICS = (\"rouge\", \"meteor\", \"cosine_similarity\", \"accuracy\")\n+RETRIEVAL_METRICS = (\"cosine_similarity\", \"accuracy\")\n+\n+\n+def is_valid_store(name):\n+    return name in SUPPORTED_VECTOR_STORES\n+\n+\n+class VectorStoreFactory:\n+    @staticmethod\n+    def get_vectorstore_class(name):\n+\n+        if not isinstance(name, str):\n+            raise TypeError(\"name must be a string\")\n+\n+        if not is_valid_store(name):\n+            raise ValueError(f\"Invalid vector store {name}\")\n+\n+        if name == \"faiss\":\n+            return FAISS\n+\n+        if name == \"chroma\":\n+            return Chroma\n+\n+\n+def get_chroma_settings(persist_directory: str = \"chromadb\") -> Settings:\n+    return Settings(\n+        chroma_db_impl=\"duckdb+parquet\",\n+        persist_directory=persist_directory,\n+        anonymized_telemetry=False,\n+    )\n+\n+\n+@dataclass\n+class PersistedVectorStoreSaverConfig:\n+    vector_store_name: str\n+    persist_directory: str\n+    collection_name: str\n+    vector_store: VectorStore\n+\n+\n+@dataclass\n+class PersistedVectorStoreLoaderConfig:\n+    vector_store_name: str\n+    embeddings_model: Embeddings\n+    persist_directory: str\n+    collection_name: str\n+\n+\n+class PersistedVectorStoreSaver:\n+    def __init__(self, config: PersistedVectorStoreSaverConfig):\n+        self.config = config\n+\n+    def save_vector_store(self, vector_store: VectorStore):\n+        method_name = f\"save_{self.config.vector_store_name}\"\n+        getattr(self, method_name)(vector_store)\n+\n+    def save_chroma(self, vector_store: Chroma):\n+        vector_store.persist()\n+\n+    def save_faiss(self, vector_store: FAISS):\n+        vector_store.save_local(\n+            folder_path=self.config.persist_directory,\n+            index_name=self.config.collection_name,\n+        )\n+\n+\n+class PersistedVectorStoreLoader:\n+    def __init__(self, config: PersistedVectorStoreLoaderConfig):\n+        self.config = config\n+\n+    def load_vector_store_client(\n+        self,\n+        vector_store: str,\n+    ):\n+        \"\"\"Load vector store client from the persisted vector store\"\"\"\n+\n+        if vector_store == \"chroma\":\n+\n+            return Chroma(\n+                collection_name=self.config.collection_name,\n+                embedding_function=self.config.embeddings_model,\n+                client_settings=get_chroma_settings(\n+                    persist_directory=self.config.persist_directory\n+                ),\n+            )\n+\n+        elif vector_store == \"faiss\":\n+\n+            return FAISS.load_local(\n+                folder_path=self.config.persist_directory,\n+                embeddings=self.config.embeddings_model,\n+                index_name=self.config.collection_name,\n+            )\n+\n+        else:\n+            raise NotImplementedError(f\"{vector_store} client is not yet supported\")\n+\n+    def load_vector_store(self):\n+        method_name = f\"load_{self.config.vector_store_name}\"\n+        return getattr(self, method_name)()\n+\n+    def load_chroma(self) -> Chroma:\n+        return self.load_vector_store_client(vector_store=\"chroma\")\n+\n+    def load_faiss(self) -> FAISS:\n+        return self.load_vector_store_client(vector_store=\"faiss\")\n+\n+\n+class LLMParameters(BaseModel):\n+    \"\"\"Model parameters for the LLM API interface\"\"\"\n+\n+    llm_name: str = Field(default_factory=str, title=\"LLM API name\")\n+    max_tokens: int = Field(default=100, title=\"max tokens in response\")\n+    temperature: float = Field(default=0.0, title=\"temperature\")\n+    top_p: float = 1\n+    best_of: int = 5\n+    stop: List[str] = None\n+\n+    class Config:\n+        extra = Extra.forbid\n+        arbitrary_types_allowed = True\n+        use_enum_values = True\n+\n+\n+class OpenAIParameters(LLMParameters):\n+    \"\"\"Model parameters for the LLM API interface\"\"\"\n+\n+    openai_api_key: str\n+    model_id: str = Field(default=\"text-davinci-003\", title=\"model name\")\n+    n: int = Field(default=1, title=\"number of responses to return\")\n+\n+\n+class WriterLLMParameters(LLMParameters):\n+    \"\"\"Model parameters for the Writer LLM API interface\"\"\"\n+\n+    writer_api_key: str\n+    writer_org_id: str = None\n+    base_url: str = None\n+    model_id: str = \"palmyra-x\"\n+    callbacks: List[StreamingStdOutCallbackHandler] = [StreamingStdOutCallbackHandler()]\n+    verbose: bool = False\n+\n+\n+class LLMLoader(BaseModel):\n+    llm_config: Union[WriterLLMParameters, OpenAIParameters]\n+    config_dict: dict = None\n+\n+    def load_llm(self):\n+        method_name = f\"load_{self.llm_config.llm_name}_llm\"\n+        self.config_dict = self.llm_config.dict()\n+        self.config_dict.pop(\"llm_name\")\n+        return getattr(self, method_name)()\n+\n+    def load_writer_llm(self):\n+        return Writer(**self.config_dict)\n+\n+    def load_openai_llm(self):\n+        openai.api_key = self.llm_config.openai_api_key\n+        config = self.config_dict\n+        config.pop(\"openai_api_key\")\n+        config[\"model\"] = config.pop(\"model_id\")\n+\n+        return partial(openai.Completion.create, **config)\n+\n+\n+class MissingPromptTemplate(Exception):\n+    pass\n+\n+\n+class UnsupportedVectorStore(Exception):\n+    pass\n+\n+\n+class MissingUseIndex(Exception):\n+    pass\n+\n+\n+class RAGHandlerParameters(BaseModel):\n+    \"\"\"Model parameters for create model\"\"\"\n+\n+    prompt_template: str\n+    llm_type: str\n+    llm_params: LLMParameters\n+    chunk_size: int = 500\n+    chunk_overlap: int = 50\n+    url: Union[str, List[str]] = None\n+    generation_evaluation_metrics: List[str] = list(GENERATION_METRICS)\n+    retrieval_evaluation_metrics: List[str] = list(RETRIEVAL_METRICS)\n+    evaluation_type: str = \"e2e\"\n+    n_rows_evaluation: int = None  # if None, evaluate on all rows\n+    retriever_match_threshold: float = 0.7\n+    generator_match_threshold: float = 0.8\n+    evaluate_dataset: Union[List[dict], str] = None\n+    run_embeddings: bool = True\n+    external_index_name: str = None\n+    top_k: int = 4\n+    embeddings_model_name: str = DEFAULT_EMBEDDINGS_MODEL\n+    context_columns: Union[List[str], str] = None\n+    vector_store_name: str = \"chroma\"\n+    vector_store: VectorStore = None\n+    collection_name: str = \"langchain\"\n+    summarize_context: bool = False\n+    summarization_prompt_template: str = SUMMARIZATION_PROMPT_TEMPLATE\n+    vector_store_folder_name: str = \"persisted_vector_db\"\n+    vector_store_storage_path: str = None\n+\n+    class Config:\n+        extra = Extra.forbid\n+        arbitrary_types_allowed = True\n+        use_enum_values = True\n+\n+    @validator(\"llm_type\")\n+    def llm_type_must_be_supported(cls, v):\n+        if v not in SUPPORTED_LLMS:\n+            raise ValueError(f\"llm_type must be one of `writer` or `openai`, got {v}\")\n+        return v\n+\n+    @validator(\"generation_evaluation_metrics\")\n+    def generation_evaluation_metrics_must_be_supported(cls, v):\n+        for metric in v:\n+            if metric not in GENERATION_METRICS:\n+                raise ValueError(\n+                    f\"generation_evaluation_metrics must be one of {', '.join(str(v) for v in GENERATION_METRICS)}, got {metric}\"\n+                )\n+        return v\n+\n+    @validator(\"retrieval_evaluation_metrics\")\n+    def retrieval_evaluation_metrics_must_be_supported(cls, v):\n+        for metric in v:\n+            if metric not in GENERATION_METRICS:\n+                raise ValueError(\n+                    f\"retrieval_evaluation_metrics must be one of {', '.join(str(v) for v in RETRIEVAL_METRICS)}, got {metric}\"\n+                )\n+        return v\n+\n+    @validator(\"evaluation_type\")\n+    def evaluation_type_must_be_supported(cls, v):\n+        if v not in SUPPORTED_EVALUATION_TYPES:\n+            raise ValueError(\n+                f\"evaluation_type must be one of `retrieval` or `e2e`, got {v}\"\n+            )\n+        return v\n+\n+    @validator(\"vector_store_name\")\n+    def name_must_be_lower(cls, v):\n+        return v.lower()\n+\n+    @validator(\"prompt_template\")\n+    def prompt_template_must_be_provided(cls, v):\n+        if not v:\n+            raise MissingPromptTemplate(\n+                \"Please provide a `prompt_template` for this engine.\"\n+            )\n+        return v\n+\n+    @validator(\"vector_store_name\")\n+    def vector_store_must_be_supported(cls, v):\n+        if not is_valid_store(v):\n+            raise UnsupportedVectorStore(\n+                f\"currently we only support {', '.join(str(v) for v in SUPPORTED_VECTOR_STORES)} vector store\"\n+            )\n+        return v\n+\n+\n+class DfLoader(DataFrameLoader):\n+\n+    \"\"\"\n+    override the load method of langchain.document_loaders.DataFrameLoaders to ignore rows with 'None' values\n+    \"\"\"\n+\n+    def __init__(self, data_frame: pd.DataFrame, page_content_column: str):\n+        super().__init__(data_frame=data_frame, page_content_column=page_content_column)\n+        self._data_frame = data_frame\n+        self._page_content_column = page_content_column\n+\n+    def load(self) -> List[Document]:\n+        \"\"\"Loads the dataframe as a list of documents\"\"\"\n+        documents = []\n+        for n_row, frame in self._data_frame[self._page_content_column].items():\n+            if pd.notnull(frame):\n+                # ignore rows with None values\n+                column_name = self._page_content_column\n+\n+                document_contents = frame\n+\n+                documents.append(\n+                    Document(\n+                        page_content=document_contents,\n+                        metadata={\n+                            \"source\": \"dataframe\",\n+                            \"row\": n_row,\n+                            \"column\": column_name,\n+                        },\n+                    )\n+                )\n+        return documents\n+\n+\n+def df_to_documents(\n+    df: pd.DataFrame, page_content_columns: Union[List[str], str]\n+) -> List[Document]:\n+    \"\"\"Converts a given dataframe to a list of documents\"\"\"\n+    documents = []\n+\n+    if isinstance(page_content_columns, str):\n+        page_content_columns = [page_content_columns]\n+\n+    for _, page_content_column in enumerate(page_content_columns):\n+        if page_content_column not in df.columns.tolist():\n+            raise ValueError(\n+                f\"page_content_column {page_content_column} not in dataframe columns\"\n+            )\n+\n+        loader = DfLoader(data_frame=df, page_content_column=page_content_column)\n+        documents.extend(loader.load())\n+\n+    return documents\n+\n+\n+def url_to_documents(urls: Union[List[str], str]) -> List[Document]:\n+    \"\"\"Converts a given url to a document\"\"\"\n+    documents = []\n+    if isinstance(urls, str):\n+        urls = [urls]\n+\n+    for url in urls:\n+        response = requests.get(url, headers=None).text\n+        html_to_text = html2text.html2text(response)\n+        documents.append(Document(page_content=html_to_text, metadata={\"source\": url}))\n+\n+    return documents\nComment: add support for urls",
    "subcategory": "validation",
    "category": "refactoring",
    "file_path": "mindsdb/integrations/handlers/rag_handler/settings.py",
    "pr_number": 7294,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1331807119,
    "comment_created_at": "2023-09-20T15:24:56Z"
  },
  {
    "code": "@@ -31,3 +33,61 @@ def test_mindsdb_provider(self):\n         ret = self.run_sql(\"select * from my_agent where question = 'hi'\")\n \n         assert agent_response in ret.answer[0]\n+\n+    @patch('openai.resources.chat.completions.Completions.create')\n+    def test_openai_provider(self, mock_chat_completion):\n+        agent_response = 'how can I assist you today?'\n+\n+        mock_chat_completion.return_value = {\n+            'choices': [{\n+                'message': {\n+                    'role': 'system',\n+                    'content': agent_response\n+                }\n+            }]\n+        }\n+\n+        self.run_sql('''\n+            CREATE AGENT my_agent\n+            USING\n+             provider='openai',\n+             model_name = \"gpt-3.5-turbo\",\n+             openai_api_key='--',\n+             prompt_template=\"Answer the user input in a helpful way\"\n+         ''')\n+        ret = self.run_sql(\"select * from my_agent where question = 'hi'\")\n+\n+        assert agent_response in ret.answer[0]\n+\n+    @patch('openai.resources.chat.completions.Completions.create')\n+    def test_openai_provider_with_model(self, mock_chat_completion):",
    "comment": "this test fails on main but works in this branch",
    "line_number": 63,
    "enriched": "File: tests/unit/executor/test_agent.py\nCode: @@ -31,3 +33,61 @@ def test_mindsdb_provider(self):\n         ret = self.run_sql(\"select * from my_agent where question = 'hi'\")\n \n         assert agent_response in ret.answer[0]\n+\n+    @patch('openai.resources.chat.completions.Completions.create')\n+    def test_openai_provider(self, mock_chat_completion):\n+        agent_response = 'how can I assist you today?'\n+\n+        mock_chat_completion.return_value = {\n+            'choices': [{\n+                'message': {\n+                    'role': 'system',\n+                    'content': agent_response\n+                }\n+            }]\n+        }\n+\n+        self.run_sql('''\n+            CREATE AGENT my_agent\n+            USING\n+             provider='openai',\n+             model_name = \"gpt-3.5-turbo\",\n+             openai_api_key='--',\n+             prompt_template=\"Answer the user input in a helpful way\"\n+         ''')\n+        ret = self.run_sql(\"select * from my_agent where question = 'hi'\")\n+\n+        assert agent_response in ret.answer[0]\n+\n+    @patch('openai.resources.chat.completions.Completions.create')\n+    def test_openai_provider_with_model(self, mock_chat_completion):\nComment: this test fails on main but works in this branch",
    "subcategory": "validation",
    "category": "refactoring",
    "file_path": "tests/unit/executor/test_agent.py",
    "pr_number": 9534,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1689364573,
    "comment_created_at": "2024-07-24T08:29:22Z"
  },
  {
    "code": "@@ -0,0 +1,109 @@\n+from typing import Any, Dict, Union\n+from mindsdb.integrations.libs.base import DatabaseHandler\n+from mindsdb.integrations.libs.response import HandlerStatusResponse\n+\n+import firebase_admin\n+from firebase_admin import credentials\n+from firebase_admin import firestore\n+\n+\n+class FirestoreHandler(DatabaseHandler):\n+    name = \"firestore\"\n+\n+    def __init__(\n+        self,\n+        name: str,\n+        cert: Union[str, None] = None,\n+        options: Dict[str, Any] = {},\n+    ):\n+        \"\"\"\n+        __init__ Initialize the FirestoreHandler class\n+\n+        Parameters\n+        ----------\n+        name : str\n+            The name of the firestore app.\n+        cert : str\n+            The path to the certificate file or a dict representing the contents of the certificate.\n+            Defaults to the Google Application Default Credentials\n+        options : Dict[str, Any], optional\n+            _description_, by default {}\n+        \"\"\"\n+        super().__init__(name)\n+\n+        self.name = name\n+        self.app = None\n+        self.db = None\n+        if cert is None:\n+            self.__cred = credentials.ApplicationDefault()",
    "comment": "This will not work. Please check other handlers for how to get the values from the CREATE DATABASE statement",
    "line_number": 38,
    "enriched": "File: mindsdb/integrations/handlers/firestore_handler/firestore_handler.py\nCode: @@ -0,0 +1,109 @@\n+from typing import Any, Dict, Union\n+from mindsdb.integrations.libs.base import DatabaseHandler\n+from mindsdb.integrations.libs.response import HandlerStatusResponse\n+\n+import firebase_admin\n+from firebase_admin import credentials\n+from firebase_admin import firestore\n+\n+\n+class FirestoreHandler(DatabaseHandler):\n+    name = \"firestore\"\n+\n+    def __init__(\n+        self,\n+        name: str,\n+        cert: Union[str, None] = None,\n+        options: Dict[str, Any] = {},\n+    ):\n+        \"\"\"\n+        __init__ Initialize the FirestoreHandler class\n+\n+        Parameters\n+        ----------\n+        name : str\n+            The name of the firestore app.\n+        cert : str\n+            The path to the certificate file or a dict representing the contents of the certificate.\n+            Defaults to the Google Application Default Credentials\n+        options : Dict[str, Any], optional\n+            _description_, by default {}\n+        \"\"\"\n+        super().__init__(name)\n+\n+        self.name = name\n+        self.app = None\n+        self.db = None\n+        if cert is None:\n+            self.__cred = credentials.ApplicationDefault()\nComment: This will not work. Please check other handlers for how to get the values from the CREATE DATABASE statement",
    "subcategory": "validation",
    "category": "refactoring",
    "file_path": "mindsdb/integrations/handlers/firestore_handler/firestore_handler.py",
    "pr_number": 7870,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1365242516,
    "comment_created_at": "2023-10-19T09:58:28Z"
  },
  {
    "code": "@@ -58,4 +60,46 @@ def get_email_campaigns(self, **kwargs):\n         connection = self.handler.connect()\n         email_campaigns_api_instance = sib_api_v3_sdk.EmailCampaignsApi(connection)\n         email_campaigns = email_campaigns_api_instance.get_email_campaigns(**kwargs)\n-        return [email_campaign for  email_campaign in email_campaigns.campaigns]\n\\ No newline at end of file\n+        return [email_campaign for  email_campaign in email_campaigns.campaigns]\n+    \n+    \n+    def update(self, query: 'ast.Update') -> None:\n+        \"\"\"\n+        Updates data in Sendinblue \"PUT /emailCampaigns/{campaignId}\" API endpoint.\n+        \n+        Parameters\n+        ----------\n+        query : ast.Update\n+            Given SQL UPDATE query\n+            \n+        Returns\n+        -------\n+        None\n+        \n+        Raises\n+        ------\n+        ValueError\n+            If the query contains an unsupported condition\n+        \"\"\"\n+        update_statement_parser = UPDATEQueryParser(query)\n+        values_to_update, where_conditions = update_statement_parser.parse_query()\n+\n+        email_campaigns_df = pd.json_normalize(self.get_email_campaigns())\n+        update_query_executor = UPDATEQueryExecutor(\n+            email_campaigns_df,\n+            where_conditions\n+        )\n+\n+        email_campaigns_df = update_query_executor.execute_query()\n+        campaign_ids = email_campaigns_df['id'].tolist()\n+        self.update_email_campaigns(campaign_ids, values_to_update)\n+\n+    def update_email_campaigns(self, campaign_ids: List[int], values_to_update: Dict) -> None:\n+        connection = self.handler.connect()\n+        email_campaigns_api_instance = sib_api_v3_sdk.EmailCampaignsApi(connection)\n+\n+        for campaign_id in campaign_ids:\n+            try:\n+                email_campaigns_api_instance.update_email_campaign(campaign_id, values_to_update)\n+            except ApiException as e:\n+                print(f\"Exception when updating campaign {campaign_id}: {e}\")",
    "comment": "Lets log and raise the exception instead of printing ",
    "line_number": 105,
    "enriched": "File: mindsdb/integrations/handlers/sendinblue_handler/sendinblue_tables.py\nCode: @@ -58,4 +60,46 @@ def get_email_campaigns(self, **kwargs):\n         connection = self.handler.connect()\n         email_campaigns_api_instance = sib_api_v3_sdk.EmailCampaignsApi(connection)\n         email_campaigns = email_campaigns_api_instance.get_email_campaigns(**kwargs)\n-        return [email_campaign for  email_campaign in email_campaigns.campaigns]\n\\ No newline at end of file\n+        return [email_campaign for  email_campaign in email_campaigns.campaigns]\n+    \n+    \n+    def update(self, query: 'ast.Update') -> None:\n+        \"\"\"\n+        Updates data in Sendinblue \"PUT /emailCampaigns/{campaignId}\" API endpoint.\n+        \n+        Parameters\n+        ----------\n+        query : ast.Update\n+            Given SQL UPDATE query\n+            \n+        Returns\n+        -------\n+        None\n+        \n+        Raises\n+        ------\n+        ValueError\n+            If the query contains an unsupported condition\n+        \"\"\"\n+        update_statement_parser = UPDATEQueryParser(query)\n+        values_to_update, where_conditions = update_statement_parser.parse_query()\n+\n+        email_campaigns_df = pd.json_normalize(self.get_email_campaigns())\n+        update_query_executor = UPDATEQueryExecutor(\n+            email_campaigns_df,\n+            where_conditions\n+        )\n+\n+        email_campaigns_df = update_query_executor.execute_query()\n+        campaign_ids = email_campaigns_df['id'].tolist()\n+        self.update_email_campaigns(campaign_ids, values_to_update)\n+\n+    def update_email_campaigns(self, campaign_ids: List[int], values_to_update: Dict) -> None:\n+        connection = self.handler.connect()\n+        email_campaigns_api_instance = sib_api_v3_sdk.EmailCampaignsApi(connection)\n+\n+        for campaign_id in campaign_ids:\n+            try:\n+                email_campaigns_api_instance.update_email_campaign(campaign_id, values_to_update)\n+            except ApiException as e:\n+                print(f\"Exception when updating campaign {campaign_id}: {e}\")\nComment: Lets log and raise the exception instead of printing ",
    "subcategory": "validation",
    "category": "refactoring",
    "file_path": "mindsdb/integrations/handlers/sendinblue_handler/sendinblue_tables.py",
    "pr_number": 8210,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1380020089,
    "comment_created_at": "2023-11-02T12:22:06Z"
  },
  {
    "code": "@@ -96,22 +96,43 @@ def invoke(self, query, session_id) -> Dict[str, Any]:\n     async def streaming_invoke(self, messages, timeout=DEFAULT_STREAM_TIMEOUT):\n         url = f\"{self.base_url}/api/projects/{self.project_name}/agents/{self.agent_name}/completions/stream\"\n         logger.debug(f\"Sending streaming request to MindsDB agent: {self.agent_name}\")\n-        async with httpx.AsyncClient(timeout=timeout, headers=self.headers) as client:\n-            async with client.stream(\"POST\", url, json={\"messages\": to_serializable(messages)}) as response:\n-                response.raise_for_status()\n-                async for line in response.aiter_lines():\n-                    if not line.strip():\n-                        continue\n-                    # Only process actual SSE data lines\n-                    if line.startswith(\"data:\"):\n-                        payload = line[len(\"data:\") :].strip()\n-                        try:\n-                            yield json.loads(payload)\n-                        except Exception as e:\n-                            logger.exception(f\"Failed to parse SSE JSON payload: {e}; line: {payload}\")\n-                    # Ignore comments or control lines\n-                # Signal the end of the stream\n-                yield {\"is_task_complete\": True}\n+        try:\n+            async with httpx.AsyncClient(timeout=timeout, headers=self.headers) as client:\n+                async with client.stream(\"POST\", url, json={\"messages\": to_serializable(messages)}) as response:\n+                    response.raise_for_status()\n+                    async for line in response.aiter_lines():\n+                        if not line.strip():\n+                            continue\n+                        # Only process actual SSE data lines\n+                        if line.startswith(\"data:\"):\n+                            payload = line[len(\"data:\") :].strip()\n+                            try:\n+                                yield json.loads(payload)\n+                            except Exception as e:\n+                                logger.exception(f\"Failed to parse SSE JSON payload: {e}; line: {payload}\")\n+                        # Ignore comments or control lines\n+                    # Signal the end of the stream\n+                    yield {\"is_task_complete\": True}\n+        except httpx.ReadTimeout:\n+            error_msg = f\"Request timed out after {timeout} seconds while streaming from agent '{self.agent_name}'\"\n+            logger.error(error_msg)\n+            raise TimeoutError(error_msg)\n+        except httpx.ConnectTimeout:\n+            error_msg = f\"Connection timeout while connecting to agent '{self.agent_name}' at {url}\"\n+            logger.error(error_msg)\n+            raise ConnectionError(error_msg)\n+        except httpx.ConnectError as e:\n+            error_msg = f\"Failed to connect to agent '{self.agent_name}' at {url}: {str(e)}\"\n+            logger.error(error_msg)\n+            raise ConnectionError(error_msg)\n+        except httpx.HTTPStatusError as e:\n+            error_msg = f\"HTTP error {e.response.status_code} from agent '{self.agent_name}': {str(e)}\"\n+            logger.error(error_msg)\n+            raise RuntimeError(error_msg)\n+        except httpx.RequestError as e:\n+            error_msg = f\"Request error while streaming from agent '{self.agent_name}': {str(e)}\"\n+            logger.error(error_msg)\n+            raise RuntimeError(error_msg)",
    "comment": "**Correctness**: HTTP errors are converted to generic `RuntimeError` instead of maintaining HTTP-specific error types, resulting in loss of specific error information\n\n",
    "line_number": 135,
    "enriched": "File: mindsdb/api/a2a/agent.py\nCode: @@ -96,22 +96,43 @@ def invoke(self, query, session_id) -> Dict[str, Any]:\n     async def streaming_invoke(self, messages, timeout=DEFAULT_STREAM_TIMEOUT):\n         url = f\"{self.base_url}/api/projects/{self.project_name}/agents/{self.agent_name}/completions/stream\"\n         logger.debug(f\"Sending streaming request to MindsDB agent: {self.agent_name}\")\n-        async with httpx.AsyncClient(timeout=timeout, headers=self.headers) as client:\n-            async with client.stream(\"POST\", url, json={\"messages\": to_serializable(messages)}) as response:\n-                response.raise_for_status()\n-                async for line in response.aiter_lines():\n-                    if not line.strip():\n-                        continue\n-                    # Only process actual SSE data lines\n-                    if line.startswith(\"data:\"):\n-                        payload = line[len(\"data:\") :].strip()\n-                        try:\n-                            yield json.loads(payload)\n-                        except Exception as e:\n-                            logger.exception(f\"Failed to parse SSE JSON payload: {e}; line: {payload}\")\n-                    # Ignore comments or control lines\n-                # Signal the end of the stream\n-                yield {\"is_task_complete\": True}\n+        try:\n+            async with httpx.AsyncClient(timeout=timeout, headers=self.headers) as client:\n+                async with client.stream(\"POST\", url, json={\"messages\": to_serializable(messages)}) as response:\n+                    response.raise_for_status()\n+                    async for line in response.aiter_lines():\n+                        if not line.strip():\n+                            continue\n+                        # Only process actual SSE data lines\n+                        if line.startswith(\"data:\"):\n+                            payload = line[len(\"data:\") :].strip()\n+                            try:\n+                                yield json.loads(payload)\n+                            except Exception as e:\n+                                logger.exception(f\"Failed to parse SSE JSON payload: {e}; line: {payload}\")\n+                        # Ignore comments or control lines\n+                    # Signal the end of the stream\n+                    yield {\"is_task_complete\": True}\n+        except httpx.ReadTimeout:\n+            error_msg = f\"Request timed out after {timeout} seconds while streaming from agent '{self.agent_name}'\"\n+            logger.error(error_msg)\n+            raise TimeoutError(error_msg)\n+        except httpx.ConnectTimeout:\n+            error_msg = f\"Connection timeout while connecting to agent '{self.agent_name}' at {url}\"\n+            logger.error(error_msg)\n+            raise ConnectionError(error_msg)\n+        except httpx.ConnectError as e:\n+            error_msg = f\"Failed to connect to agent '{self.agent_name}' at {url}: {str(e)}\"\n+            logger.error(error_msg)\n+            raise ConnectionError(error_msg)\n+        except httpx.HTTPStatusError as e:\n+            error_msg = f\"HTTP error {e.response.status_code} from agent '{self.agent_name}': {str(e)}\"\n+            logger.error(error_msg)\n+            raise RuntimeError(error_msg)\n+        except httpx.RequestError as e:\n+            error_msg = f\"Request error while streaming from agent '{self.agent_name}': {str(e)}\"\n+            logger.error(error_msg)\n+            raise RuntimeError(error_msg)\nComment: **Correctness**: HTTP errors are converted to generic `RuntimeError` instead of maintaining HTTP-specific error types, resulting in loss of specific error information\n\n",
    "subcategory": "validation",
    "category": "refactoring",
    "file_path": "mindsdb/api/a2a/agent.py",
    "pr_number": 11776,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2453502120,
    "comment_created_at": "2025-10-22T22:34:50Z"
  },
  {
    "code": "@@ -79,63 +95,70 @@ def search_email(self, options: EmailSearchOptions) -> pd.DataFrame:\n         '''\n         self.select_mailbox(options.mailbox)\n \n-        query_parts = []\n-        if options.subject is not None:\n-            query_parts.append(f'(SUBJECT \"{options.subject}\")')\n-\n-        if options.to_field is not None:\n-            query_parts.append(f'(TO \"{options.to_field}\")')\n-\n-        if options.from_field is not None:\n-            query_parts.append(f'(FROM \"{options.from_field}\")')\n-\n-        if options.since_date is not None:\n-            since_date_str = options.since_date.strftime('%d-%b-%Y')\n-        else:\n-            since_date = datetime.today() - timedelta(days=EmailClient._DEFAULT_SINCE_DAYS)\n-            since_date_str = since_date.strftime('%d-%b-%Y')\n-        query_parts.append(f'(SINCE \"{since_date_str}\")')\n-\n-        if options.until_date is not None:\n-            until_date_str = options.until_date.strftime('%d-%b-%Y')\n-            query_parts.append(f'(BEFORE \"{until_date_str}\")')\n-\n-        if options.since_email_id is not None:\n-            query_parts.append(f'(UID {options.since_email_id}:*)')\n-\n-        query = ' '.join(query_parts)\n-        ret = []\n-        _, items = self.imap_server.uid('search', None, query)\n-        items = items[0].split()\n-        for emailid in items:\n-            _, data = self.imap_server.uid('fetch', emailid, '(RFC822)')\n-            email_message = email.message_from_bytes(data[0][1])\n-\n-            email_line = {}\n-            email_line['id'] = emailid.decode()\n-            email_line['to_field'] = email_message.get('To')\n-            email_line['from_field'] = email_message.get('From')\n-            email_line['subject'] = email_message.get('Subject')\n-            email_line['date'] = email_message.get('Date')\n-\n-            plain_payload = None\n-            html_payload = None\n-            content_type = 'html'\n-            for part in email_message.walk():\n-                subtype = part.get_content_subtype()\n-                if subtype == 'plain':\n-                    # Prioritize plain text payloads when present.\n-                    plain_payload = part.get_payload(decode=True)\n-                    content_type = 'plain'\n-                    break\n-                if subtype == 'html':\n-                    html_payload = part.get_payload(decode=True)\n-            body = plain_payload or html_payload\n-            if body is None:\n-                # Very rarely messages won't have plain text or html payloads.\n-                continue\n-            email_line['body'] = plain_payload or html_payload\n-            email_line['body_content_type'] = content_type\n-            ret.append(email_line)\n+        try:\n+\n+            query_parts = []\n+            if options.subject is not None:\n+                query_parts.append(f'(SUBJECT \"{options.subject}\")')\n+\n+            if options.to_field is not None:\n+                query_parts.append(f'(TO \"{options.to_field}\")')\n+\n+            if options.from_field is not None:\n+                query_parts.append(f'(FROM \"{options.from_field}\")')\n+\n+            if options.since_date is not None:\n+                since_date_str = options.since_date.strftime('%d-%b-%Y')\n+            else:\n+                since_date = datetime.today() - timedelta(days=EmailClient._DEFAULT_SINCE_DAYS)\n+                since_date_str = since_date.strftime('%d-%b-%Y')\n+            query_parts.append(f'(SINCE \"{since_date_str}\")')\n+\n+            if options.until_date is not None:\n+                until_date_str = options.until_date.strftime('%d-%b-%Y')\n+                query_parts.append(f'(BEFORE \"{until_date_str}\")')\n+\n+            if options.since_email_id is not None:\n+                query_parts.append(f'(UID {options.since_email_id}:*)')\n+\n+            query = ' '.join(query_parts)\n+            ret = []\n+            _, items = self.imap_server.uid('search', None, query)\n+            items = items[0].split()\n+            for emailid in items:\n+                _, data = self.imap_server.uid('fetch', emailid, '(RFC822)')\n+                email_message = email.message_from_bytes(data[0][1])\n+\n+                email_line = {}\n+                email_line['id'] = emailid.decode()\n+                email_line['to_field'] = email_message.get('To')\n+                email_line['from_field'] = email_message.get('From')\n+                email_line['subject'] = email_message.get('Subject')\n+                email_line['date'] = email_message.get('Date')\n+\n+                plain_payload = None\n+                html_payload = None\n+                content_type = 'html'\n+                for part in email_message.walk():\n+                    subtype = part.get_content_subtype()\n+                    if subtype == 'plain':\n+                        # Prioritize plain text payloads when present.\n+                        plain_payload = part.get_payload(decode=True)\n+                        content_type = 'plain'\n+                        break\n+                    if subtype == 'html':\n+                        html_payload = part.get_payload(decode=True)\n+                body = plain_payload or html_payload\n+                if body is None:\n+                    # Very rarely messages won't have plain text or html payloads.\n+                    continue\n+                email_line['body'] = plain_payload or html_payload\n+                email_line['body_content_type'] = content_type\n+                ret.append(email_line)\n+        except Exception as e:\n+            raise f'Error searching email: {e}'",
    "comment": "Would prefer to use [exception chaining](https://peps.python.org/pep-3134/#explicit-exception-chaining) in this case (preserves traceback which can be useful):\r\n\r\n`raise Exception('Error searching email') from e`",
    "line_number": 159,
    "enriched": "File: mindsdb/integrations/handlers/email_handler/email_client.py\nCode: @@ -79,63 +95,70 @@ def search_email(self, options: EmailSearchOptions) -> pd.DataFrame:\n         '''\n         self.select_mailbox(options.mailbox)\n \n-        query_parts = []\n-        if options.subject is not None:\n-            query_parts.append(f'(SUBJECT \"{options.subject}\")')\n-\n-        if options.to_field is not None:\n-            query_parts.append(f'(TO \"{options.to_field}\")')\n-\n-        if options.from_field is not None:\n-            query_parts.append(f'(FROM \"{options.from_field}\")')\n-\n-        if options.since_date is not None:\n-            since_date_str = options.since_date.strftime('%d-%b-%Y')\n-        else:\n-            since_date = datetime.today() - timedelta(days=EmailClient._DEFAULT_SINCE_DAYS)\n-            since_date_str = since_date.strftime('%d-%b-%Y')\n-        query_parts.append(f'(SINCE \"{since_date_str}\")')\n-\n-        if options.until_date is not None:\n-            until_date_str = options.until_date.strftime('%d-%b-%Y')\n-            query_parts.append(f'(BEFORE \"{until_date_str}\")')\n-\n-        if options.since_email_id is not None:\n-            query_parts.append(f'(UID {options.since_email_id}:*)')\n-\n-        query = ' '.join(query_parts)\n-        ret = []\n-        _, items = self.imap_server.uid('search', None, query)\n-        items = items[0].split()\n-        for emailid in items:\n-            _, data = self.imap_server.uid('fetch', emailid, '(RFC822)')\n-            email_message = email.message_from_bytes(data[0][1])\n-\n-            email_line = {}\n-            email_line['id'] = emailid.decode()\n-            email_line['to_field'] = email_message.get('To')\n-            email_line['from_field'] = email_message.get('From')\n-            email_line['subject'] = email_message.get('Subject')\n-            email_line['date'] = email_message.get('Date')\n-\n-            plain_payload = None\n-            html_payload = None\n-            content_type = 'html'\n-            for part in email_message.walk():\n-                subtype = part.get_content_subtype()\n-                if subtype == 'plain':\n-                    # Prioritize plain text payloads when present.\n-                    plain_payload = part.get_payload(decode=True)\n-                    content_type = 'plain'\n-                    break\n-                if subtype == 'html':\n-                    html_payload = part.get_payload(decode=True)\n-            body = plain_payload or html_payload\n-            if body is None:\n-                # Very rarely messages won't have plain text or html payloads.\n-                continue\n-            email_line['body'] = plain_payload or html_payload\n-            email_line['body_content_type'] = content_type\n-            ret.append(email_line)\n+        try:\n+\n+            query_parts = []\n+            if options.subject is not None:\n+                query_parts.append(f'(SUBJECT \"{options.subject}\")')\n+\n+            if options.to_field is not None:\n+                query_parts.append(f'(TO \"{options.to_field}\")')\n+\n+            if options.from_field is not None:\n+                query_parts.append(f'(FROM \"{options.from_field}\")')\n+\n+            if options.since_date is not None:\n+                since_date_str = options.since_date.strftime('%d-%b-%Y')\n+            else:\n+                since_date = datetime.today() - timedelta(days=EmailClient._DEFAULT_SINCE_DAYS)\n+                since_date_str = since_date.strftime('%d-%b-%Y')\n+            query_parts.append(f'(SINCE \"{since_date_str}\")')\n+\n+            if options.until_date is not None:\n+                until_date_str = options.until_date.strftime('%d-%b-%Y')\n+                query_parts.append(f'(BEFORE \"{until_date_str}\")')\n+\n+            if options.since_email_id is not None:\n+                query_parts.append(f'(UID {options.since_email_id}:*)')\n+\n+            query = ' '.join(query_parts)\n+            ret = []\n+            _, items = self.imap_server.uid('search', None, query)\n+            items = items[0].split()\n+            for emailid in items:\n+                _, data = self.imap_server.uid('fetch', emailid, '(RFC822)')\n+                email_message = email.message_from_bytes(data[0][1])\n+\n+                email_line = {}\n+                email_line['id'] = emailid.decode()\n+                email_line['to_field'] = email_message.get('To')\n+                email_line['from_field'] = email_message.get('From')\n+                email_line['subject'] = email_message.get('Subject')\n+                email_line['date'] = email_message.get('Date')\n+\n+                plain_payload = None\n+                html_payload = None\n+                content_type = 'html'\n+                for part in email_message.walk():\n+                    subtype = part.get_content_subtype()\n+                    if subtype == 'plain':\n+                        # Prioritize plain text payloads when present.\n+                        plain_payload = part.get_payload(decode=True)\n+                        content_type = 'plain'\n+                        break\n+                    if subtype == 'html':\n+                        html_payload = part.get_payload(decode=True)\n+                body = plain_payload or html_payload\n+                if body is None:\n+                    # Very rarely messages won't have plain text or html payloads.\n+                    continue\n+                email_line['body'] = plain_payload or html_payload\n+                email_line['body_content_type'] = content_type\n+                ret.append(email_line)\n+        except Exception as e:\n+            raise f'Error searching email: {e}'\nComment: Would prefer to use [exception chaining](https://peps.python.org/pep-3134/#explicit-exception-chaining) in this case (preserves traceback which can be useful):\r\n\r\n`raise Exception('Error searching email') from e`",
    "subcategory": "validation",
    "category": "refactoring",
    "file_path": "mindsdb/integrations/handlers/email_handler/email_client.py",
    "pr_number": 8803,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1499634655,
    "comment_created_at": "2024-02-22T17:44:18Z"
  },
  {
    "code": "@@ -0,0 +1,214 @@\n+import importlib\n+import time\n+from unittest.mock import patch\n+import pandas as pd\n+import pytest\n+\n+from mindsdb_sql import parse_sql\n+from tests.unit.executor_test_base import BaseExecutorTest\n+\n+try:\n+    importlib.import_module(\"pycaret\")\n+    PYCARET_INSTALLED = True\n+except ImportError:\n+    PYCARET_INSTALLED = False\n+\n+\n+@pytest.mark.skipif(not PYCARET_INSTALLED, reason=\"pycaret is not installed\")\n+class TestPyCaret(BaseExecutorTest):\n+\n+    def wait_predictor(self, project, name):\n+        done = False\n+        for attempt in range(200):\n+            ret = self.run_sql(\n+                f\"select * from {project}.models where name='{name}'\"\n+            )\n+            if not ret.empty:\n+                if ret['STATUS'][0] == 'complete':\n+                    done = True\n+                    break\n+                elif ret['STATUS'][0] == 'error':\n+                    break\n+            time.sleep(0.5)\n+        if not done:\n+            raise RuntimeError(\"predictor wasn't created\")\n+\n+    def run_sql(self, sql):\n+        ret = self.command_executor.execute_command(\n+            parse_sql(sql, dialect='mindsdb')\n+        )\n+        assert ret.error_code is None\n+        if ret.data is not None:\n+            columns = [\n+                col.alias if col.alias is not None else col.name\n+                for col in ret.columns\n+            ]\n+            return pd.DataFrame(ret.data, columns=columns)\n+\n+    @patch('mindsdb.integrations.handlers.postgres_handler.Handler')\n+    def test_classifier(self, mock_handler):\n+        iris_df = pd.read_csv('https://raw.githubusercontent.com/pycaret/pycaret/master/datasets/iris.csv')",
    "comment": "@aditya-azad - seems a bit unnecessary to download actual real data for unit tests. For speed and realiability reasons mostly.\r\n\r\nI recon some fake/mock data should do the trick.",
    "line_number": 50,
    "enriched": "File: tests/unit/ml_handlers/test_pycaret.py\nCode: @@ -0,0 +1,214 @@\n+import importlib\n+import time\n+from unittest.mock import patch\n+import pandas as pd\n+import pytest\n+\n+from mindsdb_sql import parse_sql\n+from tests.unit.executor_test_base import BaseExecutorTest\n+\n+try:\n+    importlib.import_module(\"pycaret\")\n+    PYCARET_INSTALLED = True\n+except ImportError:\n+    PYCARET_INSTALLED = False\n+\n+\n+@pytest.mark.skipif(not PYCARET_INSTALLED, reason=\"pycaret is not installed\")\n+class TestPyCaret(BaseExecutorTest):\n+\n+    def wait_predictor(self, project, name):\n+        done = False\n+        for attempt in range(200):\n+            ret = self.run_sql(\n+                f\"select * from {project}.models where name='{name}'\"\n+            )\n+            if not ret.empty:\n+                if ret['STATUS'][0] == 'complete':\n+                    done = True\n+                    break\n+                elif ret['STATUS'][0] == 'error':\n+                    break\n+            time.sleep(0.5)\n+        if not done:\n+            raise RuntimeError(\"predictor wasn't created\")\n+\n+    def run_sql(self, sql):\n+        ret = self.command_executor.execute_command(\n+            parse_sql(sql, dialect='mindsdb')\n+        )\n+        assert ret.error_code is None\n+        if ret.data is not None:\n+            columns = [\n+                col.alias if col.alias is not None else col.name\n+                for col in ret.columns\n+            ]\n+            return pd.DataFrame(ret.data, columns=columns)\n+\n+    @patch('mindsdb.integrations.handlers.postgres_handler.Handler')\n+    def test_classifier(self, mock_handler):\n+        iris_df = pd.read_csv('https://raw.githubusercontent.com/pycaret/pycaret/master/datasets/iris.csv')\nComment: @aditya-azad - seems a bit unnecessary to download actual real data for unit tests. For speed and realiability reasons mostly.\r\n\r\nI recon some fake/mock data should do the trick.",
    "subcategory": "validation",
    "category": "refactoring",
    "file_path": "tests/unit/ml_handlers/test_pycaret.py",
    "pr_number": 8177,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1379451620,
    "comment_created_at": "2023-11-02T00:17:31Z"
  },
  {
    "code": "@@ -348,6 +348,56 @@ USING\n     };\n ```\n \n+## `ALTER AGENT` Syntax\n+\n+Update existing agents with new data, model, or prompt.\n+\n+```sql\n+ALTER AGENT my_agent\n+USING\n+    model = {\n+        \"provider\": \"openai\",\n+        \"model_name\" : \"gpt-4.1\",\n+        \"api_key\": \"sk-abc123\",\n+        \"base_url\": \"http://example.com\",\n+        \"api_version\": \"2024-02-01\"\n+    },\n+    data = {\n+         \"knowledge_bases\": [\"project_name.kb_name\", ...],\n+         \"tables\": [\"datasource_conn_name.table_name\", ...]\n+    },\n+    prompt_template='describe data';\n+```\n+\n+Note that all parameters are optional. Users can update any combination of parameters.\n+\n+<Tip>\n+See detailed descriptions of parameters in the [`CREATE AGENT` section](/mindsdb_sql/agents/agent_syntax#create-agent-syntax).\n+</Tip>\n+\n+Here is how to connect new data to an agent.\n+\n+```sql\n+ALTER AGENT my_agent\n+USING\n+    data = {\n+         \"knowledge_bases\": [\"mindsdb.sales_kb\"],\n+         \"tables\": [\"mysql_db.car_sales\", \"mysql_db.car_info\"]\n+    };\n+```\n+\n+ANd here is how to update a model used by the agent.",
    "comment": "'ANd' - case ",
    "line_number": 389,
    "enriched": "File: docs/mindsdb_sql/agents/agent_syntax.mdx\nCode: @@ -348,6 +348,56 @@ USING\n     };\n ```\n \n+## `ALTER AGENT` Syntax\n+\n+Update existing agents with new data, model, or prompt.\n+\n+```sql\n+ALTER AGENT my_agent\n+USING\n+    model = {\n+        \"provider\": \"openai\",\n+        \"model_name\" : \"gpt-4.1\",\n+        \"api_key\": \"sk-abc123\",\n+        \"base_url\": \"http://example.com\",\n+        \"api_version\": \"2024-02-01\"\n+    },\n+    data = {\n+         \"knowledge_bases\": [\"project_name.kb_name\", ...],\n+         \"tables\": [\"datasource_conn_name.table_name\", ...]\n+    },\n+    prompt_template='describe data';\n+```\n+\n+Note that all parameters are optional. Users can update any combination of parameters.\n+\n+<Tip>\n+See detailed descriptions of parameters in the [`CREATE AGENT` section](/mindsdb_sql/agents/agent_syntax#create-agent-syntax).\n+</Tip>\n+\n+Here is how to connect new data to an agent.\n+\n+```sql\n+ALTER AGENT my_agent\n+USING\n+    data = {\n+         \"knowledge_bases\": [\"mindsdb.sales_kb\"],\n+         \"tables\": [\"mysql_db.car_sales\", \"mysql_db.car_info\"]\n+    };\n+```\n+\n+ANd here is how to update a model used by the agent.\nComment: 'ANd' - case ",
    "subcategory": "validation",
    "category": "refactoring",
    "file_path": "docs/mindsdb_sql/agents/agent_syntax.mdx",
    "pr_number": 11416,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2262994537,
    "comment_created_at": "2025-08-08T13:33:33Z"
  },
  {
    "code": "@@ -188,7 +188,21 @@ def connect(self) -> Connection:\n             raise ValueError(\"Required parameters (user, password) must be provided.\")\n \n         if self.connection_data.get(\"thick_mode\", False):\n-            oracledb.init_oracle_client()\n+            try:\n+                lib_dir = self.connection_data.get(\"oracle_client_path\")\n+                if lib_dir:\n+                    logger.debug(f\"Initializing Oracle client in thick mode with lib_dir: {lib_dir}\")\n+                    oracledb.init_oracle_client(lib_dir=lib_dir)\n+                else:\n+                    logger.debug(\"Initializing Oracle client in thick mode with system default paths.\")\n+                    oracledb.init_oracle_client()\n+            except Exception as e:\n+                logger.error(\n+                    f\"Failed to initialize Oracle client in thick mode. \"\n+                    f\"Ensure Oracle Instant Client is installed. Error: {e}\"\n+                    )\n+                logger.error(\"See: https://python-oracledb.readthedocs.io/en/latest/user_guide/initialization.html\")\n+                raise",
    "comment": "**correctness**: `oracledb.init_oracle_client()` is called on every connection attempt, which can cause runtime errors since it must only be called once per process; repeated calls raise exceptions and break connection logic.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/integrations/handlers/oracle_handler/oracle_handler.py, lines 191-205, the code calls oracledb.init_oracle_client() on every connection attempt, but this function must only be called once per process or it will raise an exception and break connection logic. Please update the code to ensure oracledb.init_oracle_client() is only called once per process, for example by setting and checking a module-level flag (such as oracledb._mindsdb_oracle_client_initialized).\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n            if not getattr(oracledb, '_mindsdb_oracle_client_initialized', False):\n                try:\n                    lib_dir = self.connection_data.get(\"oracle_client_path\")\n                    if lib_dir:\n                        logger.debug(f\"Initializing Oracle client in thick mode with lib_dir: {lib_dir}\")\n                        oracledb.init_oracle_client(lib_dir=lib_dir)\n                    else:\n                        logger.debug(\"Initializing Oracle client in thick mode with system default paths.\")\n                        oracledb.init_oracle_client()\n                    oracledb._mindsdb_oracle_client_initialized = True\n                except Exception as e:\n                    logger.error(\n                        f\"Failed to initialize Oracle client in thick mode. \"\n                        f\"Ensure Oracle Instant Client is installed. Error: {e}\"\n                        )\n                    logger.error(\"See: https://python-oracledb.readthedocs.io/en/latest/user_guide/initialization.html\")\n                    raise\n```\n</details>\n<!-- suggestion_end -->\n",
    "line_number": 205,
    "enriched": "File: mindsdb/integrations/handlers/oracle_handler/oracle_handler.py\nCode: @@ -188,7 +188,21 @@ def connect(self) -> Connection:\n             raise ValueError(\"Required parameters (user, password) must be provided.\")\n \n         if self.connection_data.get(\"thick_mode\", False):\n-            oracledb.init_oracle_client()\n+            try:\n+                lib_dir = self.connection_data.get(\"oracle_client_path\")\n+                if lib_dir:\n+                    logger.debug(f\"Initializing Oracle client in thick mode with lib_dir: {lib_dir}\")\n+                    oracledb.init_oracle_client(lib_dir=lib_dir)\n+                else:\n+                    logger.debug(\"Initializing Oracle client in thick mode with system default paths.\")\n+                    oracledb.init_oracle_client()\n+            except Exception as e:\n+                logger.error(\n+                    f\"Failed to initialize Oracle client in thick mode. \"\n+                    f\"Ensure Oracle Instant Client is installed. Error: {e}\"\n+                    )\n+                logger.error(\"See: https://python-oracledb.readthedocs.io/en/latest/user_guide/initialization.html\")\n+                raise\nComment: **correctness**: `oracledb.init_oracle_client()` is called on every connection attempt, which can cause runtime errors since it must only be called once per process; repeated calls raise exceptions and break connection logic.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/integrations/handlers/oracle_handler/oracle_handler.py, lines 191-205, the code calls oracledb.init_oracle_client() on every connection attempt, but this function must only be called once per process or it will raise an exception and break connection logic. Please update the code to ensure oracledb.init_oracle_client() is only called once per process, for example by setting and checking a module-level flag (such as oracledb._mindsdb_oracle_client_initialized).\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n            if not getattr(oracledb, '_mindsdb_oracle_client_initialized', False):\n                try:\n                    lib_dir = self.connection_data.get(\"oracle_client_path\")\n                    if lib_dir:\n                        logger.debug(f\"Initializing Oracle client in thick mode with lib_dir: {lib_dir}\")\n                        oracledb.init_oracle_client(lib_dir=lib_dir)\n                    else:\n                        logger.debug(\"Initializing Oracle client in thick mode with system default paths.\")\n                        oracledb.init_oracle_client()\n                    oracledb._mindsdb_oracle_client_initialized = True\n                except Exception as e:\n                    logger.error(\n                        f\"Failed to initialize Oracle client in thick mode. \"\n                        f\"Ensure Oracle Instant Client is installed. Error: {e}\"\n                        )\n                    logger.error(\"See: https://python-oracledb.readthedocs.io/en/latest/user_guide/initialization.html\")\n                    raise\n```\n</details>\n<!-- suggestion_end -->\n",
    "subcategory": "validation",
    "category": "refactoring",
    "file_path": "mindsdb/integrations/handlers/oracle_handler/oracle_handler.py",
    "pr_number": 11295,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2205424050,
    "comment_created_at": "2025-07-14T17:15:40Z"
  },
  {
    "code": "@@ -177,23 +197,7 @@ def on_file(file):\n                 if ctx.user_class != 1:\n                     info = requests.head(url, timeout=30)\n                     file_size = info.headers.get(\"Content-Length\")\n-                    try:\n-                        file_size = int(file_size)\n-                    except Exception:\n-                        pass\n-\n-                    if file_size is None:\n-                        return http_error(",
    "comment": "`_validate_file_size` won't raise error if file_size is None, should it be like this? \r\n",
    "line_number": 186,
    "enriched": "File: mindsdb/api/http/namespaces/file.py\nCode: @@ -177,23 +197,7 @@ def on_file(file):\n                 if ctx.user_class != 1:\n                     info = requests.head(url, timeout=30)\n                     file_size = info.headers.get(\"Content-Length\")\n-                    try:\n-                        file_size = int(file_size)\n-                    except Exception:\n-                        pass\n-\n-                    if file_size is None:\n-                        return http_error(\nComment: `_validate_file_size` won't raise error if file_size is None, should it be like this? \r\n",
    "subcategory": "validation",
    "category": "refactoring",
    "file_path": "mindsdb/api/http/namespaces/file.py",
    "pr_number": 11855,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2564595252,
    "comment_created_at": "2025-11-26T11:21:32Z"
  },
  {
    "code": "@@ -0,0 +1,303 @@\n+import json\n+import pandas as pd\n+from collections import OrderedDict\n+from typing import List\n+\n+from faunadb import query as q\n+from faunadb.client import FaunaClient\n+from mindsdb_sql import Select, Insert, CreateTable, Delete\n+from mindsdb_sql.parser.ast.select.star import Star\n+from mindsdb_sql.parser.ast.base import ASTNode\n+\n+from mindsdb.integrations.libs.const import HANDLER_CONNECTION_ARG_TYPE as ARG_TYPE\n+from mindsdb.integrations.libs.response import (\n+    RESPONSE_TYPE,\n+    HandlerResponse as Response,\n+    HandlerStatusResponse as StatusResponse,\n+)\n+from mindsdb.integrations.libs.base import DatabaseHandler\n+\n+from mindsdb.utilities import log\n+\n+\n+class FaunaDBHandler(DatabaseHandler):\n+    \"\"\"This handler handles connection and execution of the FaunaDB statements.\"\"\"\n+\n+    name = \"faunadb\"\n+\n+    def __init__(self, name: str, **kwargs):\n+        super().__init__(name)\n+\n+        self._connection_data = kwargs.get(\"connection_data\")\n+\n+        self._client_config = {\n+            \"fauna_secret\": self._connection_data.get(\"fauna_secret\"),\n+            \"fauna_scheme\": self._connection_data.get(\"fauna_scheme\"),\n+            \"fauna_domain\": self._connection_data.get(\"fauna_domain\"),\n+            \"fauna_port\": self._connection_data.get(\"fauna_port\"),\n+            \"fauna_endpoint\": self._connection_data.get(\"fauna_endpoint\"),\n+        }\n+\n+        # should have the secret\n+        if (self._client_config[\"fauna_secret\"]) is None:\n+            raise Exception(\"FaunaDB secret is required for FaunaDB connection!\")\n+        # either scheme + domain + port or endpoint is required\n+        # but not both\n+        if self._client_config[\"fauna_endpoint\"] is None and not (",
    "comment": "Can we change this as :\r\n\r\n```\r\nscheme, domain, port, endpoint = (\r\n    self._client_config[\"fauna_scheme\"],\r\n    self._client_config[\"fauna_domain\"],\r\n    self._client_config[\"fauna_port\"],\r\n    self._client_config[\"fauna_endpoint\"],\r\n)\r\n\r\nif not (scheme and domain and port) and endpoint is None:\r\n    raise Exception(\"Either scheme + domain + port or endpoint is required for FaunaDB connection!\")\r\nelif endpoint is not None and (scheme or domain or port):\r\n    raise Exception(\"Either scheme + domain + port or endpoint is required for FaunaDB connection, but not both!\")\r\n\r\n```",
    "line_number": 46,
    "enriched": "File: mindsdb/integrations/handlers/faunadb_handler/faunadb_handler.py\nCode: @@ -0,0 +1,303 @@\n+import json\n+import pandas as pd\n+from collections import OrderedDict\n+from typing import List\n+\n+from faunadb import query as q\n+from faunadb.client import FaunaClient\n+from mindsdb_sql import Select, Insert, CreateTable, Delete\n+from mindsdb_sql.parser.ast.select.star import Star\n+from mindsdb_sql.parser.ast.base import ASTNode\n+\n+from mindsdb.integrations.libs.const import HANDLER_CONNECTION_ARG_TYPE as ARG_TYPE\n+from mindsdb.integrations.libs.response import (\n+    RESPONSE_TYPE,\n+    HandlerResponse as Response,\n+    HandlerStatusResponse as StatusResponse,\n+)\n+from mindsdb.integrations.libs.base import DatabaseHandler\n+\n+from mindsdb.utilities import log\n+\n+\n+class FaunaDBHandler(DatabaseHandler):\n+    \"\"\"This handler handles connection and execution of the FaunaDB statements.\"\"\"\n+\n+    name = \"faunadb\"\n+\n+    def __init__(self, name: str, **kwargs):\n+        super().__init__(name)\n+\n+        self._connection_data = kwargs.get(\"connection_data\")\n+\n+        self._client_config = {\n+            \"fauna_secret\": self._connection_data.get(\"fauna_secret\"),\n+            \"fauna_scheme\": self._connection_data.get(\"fauna_scheme\"),\n+            \"fauna_domain\": self._connection_data.get(\"fauna_domain\"),\n+            \"fauna_port\": self._connection_data.get(\"fauna_port\"),\n+            \"fauna_endpoint\": self._connection_data.get(\"fauna_endpoint\"),\n+        }\n+\n+        # should have the secret\n+        if (self._client_config[\"fauna_secret\"]) is None:\n+            raise Exception(\"FaunaDB secret is required for FaunaDB connection!\")\n+        # either scheme + domain + port or endpoint is required\n+        # but not both\n+        if self._client_config[\"fauna_endpoint\"] is None and not (\nComment: Can we change this as :\r\n\r\n```\r\nscheme, domain, port, endpoint = (\r\n    self._client_config[\"fauna_scheme\"],\r\n    self._client_config[\"fauna_domain\"],\r\n    self._client_config[\"fauna_port\"],\r\n    self._client_config[\"fauna_endpoint\"],\r\n)\r\n\r\nif not (scheme and domain and port) and endpoint is None:\r\n    raise Exception(\"Either scheme + domain + port or endpoint is required for FaunaDB connection!\")\r\nelif endpoint is not None and (scheme or domain or port):\r\n    raise Exception(\"Either scheme + domain + port or endpoint is required for FaunaDB connection, but not both!\")\r\n\r\n```",
    "subcategory": "validation",
    "category": "refactoring",
    "file_path": "mindsdb/integrations/handlers/faunadb_handler/faunadb_handler.py",
    "pr_number": 7805,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1363567819,
    "comment_created_at": "2023-10-18T09:37:12Z"
  },
  {
    "code": "@@ -98,11 +98,17 @@ def predict(self, df, args=None):\n             predictor = LightwoodHandler.get_predictor(predictor_path, predictor_code)\n \n         dtype_dict = predictor.dtype_dict\n+        embedding_mode = predictor.problem_definition.embedding_only or pred_args.get('return_embedding', False)",
    "comment": "does `predictor.problem_definition.embedding_only` exists in predictors trained with old versions of lightwood?should we add `hasattr` check here?",
    "line_number": 101,
    "enriched": "File: mindsdb/integrations/handlers/lightwood_handler/lightwood_handler/lightwood_handler.py\nCode: @@ -98,11 +98,17 @@ def predict(self, df, args=None):\n             predictor = LightwoodHandler.get_predictor(predictor_path, predictor_code)\n \n         dtype_dict = predictor.dtype_dict\n+        embedding_mode = predictor.problem_definition.embedding_only or pred_args.get('return_embedding', False)\nComment: does `predictor.problem_definition.embedding_only` exists in predictors trained with old versions of lightwood?should we add `hasattr` check here?",
    "subcategory": "validation",
    "category": "refactoring",
    "file_path": "mindsdb/integrations/handlers/lightwood_handler/lightwood_handler/lightwood_handler.py",
    "pr_number": 6779,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1254601245,
    "comment_created_at": "2023-07-06T15:27:02Z"
  },
  {
    "code": "@@ -0,0 +1,61 @@\n+---\n+title: MindsDB and Postgres CLI\n+sidebarTitle: Postgres CLI\n+---\n+\n+The Postgres API enables users to connect to MindsDB using the PostgreSQL protocol.\n+\n+<Tip>\n+If you installed MindsDB locally, use the below command to run its Postgres API:\n+\n+```bash\n+python -m mindsdb --api http,mysql,postgres\n+```\n+</Tip>\n+\n+## How to Connect\n+\n+To connect MindsDB in PostgreSQL, use the `psql` client program:\n+\n+```bash\n+psql -h localhost -p 55432 -d mindsdb -U mindsdb\n+```\n+\n+We use the local MindsDB installation with the following parameters:\n+\n+* host: `localhost`\n+* port: `55432`\n+* database: `mindsdb`\n+* user: `mindsdb`\n+\n+Here are the commands that allow you to connect to either a local MindsDB installation or a MindsDB Cloud instance.\n+\n+<CodeGroup>\n+\n+    ```bash Self-Hosted Local Deployment\n+    psql -h 127.0.0.1 -p 47335 -d mindsdb -U mindsdb\n+    ```\n+\n+    ```bash MindsDB Cloud\n+    psql -h cloud.mindsdb.com -p 3306 -U [mindsdb_cloud_username] -p [mindsdb_cloud_password]",
    "comment": "I think password should be --password or -W",
    "line_number": 40,
    "enriched": "File: docs/connect/postgres-client.mdx\nCode: @@ -0,0 +1,61 @@\n+---\n+title: MindsDB and Postgres CLI\n+sidebarTitle: Postgres CLI\n+---\n+\n+The Postgres API enables users to connect to MindsDB using the PostgreSQL protocol.\n+\n+<Tip>\n+If you installed MindsDB locally, use the below command to run its Postgres API:\n+\n+```bash\n+python -m mindsdb --api http,mysql,postgres\n+```\n+</Tip>\n+\n+## How to Connect\n+\n+To connect MindsDB in PostgreSQL, use the `psql` client program:\n+\n+```bash\n+psql -h localhost -p 55432 -d mindsdb -U mindsdb\n+```\n+\n+We use the local MindsDB installation with the following parameters:\n+\n+* host: `localhost`\n+* port: `55432`\n+* database: `mindsdb`\n+* user: `mindsdb`\n+\n+Here are the commands that allow you to connect to either a local MindsDB installation or a MindsDB Cloud instance.\n+\n+<CodeGroup>\n+\n+    ```bash Self-Hosted Local Deployment\n+    psql -h 127.0.0.1 -p 47335 -d mindsdb -U mindsdb\n+    ```\n+\n+    ```bash MindsDB Cloud\n+    psql -h cloud.mindsdb.com -p 3306 -U [mindsdb_cloud_username] -p [mindsdb_cloud_password]\nComment: I think password should be --password or -W",
    "subcategory": "validation",
    "category": "refactoring",
    "file_path": "docs/connect/postgres-client.mdx",
    "pr_number": 6252,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1210220371,
    "comment_created_at": "2023-05-30T12:46:24Z"
  },
  {
    "code": "@@ -58,4 +60,47 @@ def get_email_campaigns(self, **kwargs):\n         connection = self.handler.connect()\n         email_campaigns_api_instance = sib_api_v3_sdk.EmailCampaignsApi(connection)\n         email_campaigns = email_campaigns_api_instance.get_email_campaigns(**kwargs)\n-        return [email_campaign for  email_campaign in email_campaigns.campaigns]\n\\ No newline at end of file\n+        return [email_campaign for  email_campaign in email_campaigns.campaigns]\n+              \n+    def delete(self, query: ast.Delete) -> None:\n+        \"\"\"\n+        Deletes an email campaign from Sendinblue.\n+\n+        Parameters\n+        ----------\n+        query : ast.Delete\n+           Given SQL DELETE query\n+\n+        Returns\n+        -------\n+        None\n+\n+        Raises\n+        ------\n+        ApiException\n+            If an error occurs when calling Sendinblue's API\n+        \"\"\"\n+        delete_statement_parser = DELETEQueryParser(query)\n+        where_conditions = delete_statement_parser.parse_query()\n+\n+        email_campaigns_df = pd.json_normalize(self.get_email_campaigns())\n+\n+        delete_query_executor = DELETEQueryExecutor(\n+            email_campaigns_df,\n+            where_conditions\n+        )\n+\n+        email_campaigns_df = delete_query_executor.execute_query()\n+        campaign_ids = email_campaigns_df['id'].tolist()\n+        self.delete_email_campaigns(campaign_ids)\n+\n+    def delete_email_campaigns(self, campaign_ids: List[Text]) -> None:",
    "comment": "Hey @Sekhar-Kumar-Dash, can you please add some comments here about this fn?",
    "line_number": 97,
    "enriched": "File: mindsdb/integrations/handlers/sendinblue_handler/sendinblue_tables.py\nCode: @@ -58,4 +60,47 @@ def get_email_campaigns(self, **kwargs):\n         connection = self.handler.connect()\n         email_campaigns_api_instance = sib_api_v3_sdk.EmailCampaignsApi(connection)\n         email_campaigns = email_campaigns_api_instance.get_email_campaigns(**kwargs)\n-        return [email_campaign for  email_campaign in email_campaigns.campaigns]\n\\ No newline at end of file\n+        return [email_campaign for  email_campaign in email_campaigns.campaigns]\n+              \n+    def delete(self, query: ast.Delete) -> None:\n+        \"\"\"\n+        Deletes an email campaign from Sendinblue.\n+\n+        Parameters\n+        ----------\n+        query : ast.Delete\n+           Given SQL DELETE query\n+\n+        Returns\n+        -------\n+        None\n+\n+        Raises\n+        ------\n+        ApiException\n+            If an error occurs when calling Sendinblue's API\n+        \"\"\"\n+        delete_statement_parser = DELETEQueryParser(query)\n+        where_conditions = delete_statement_parser.parse_query()\n+\n+        email_campaigns_df = pd.json_normalize(self.get_email_campaigns())\n+\n+        delete_query_executor = DELETEQueryExecutor(\n+            email_campaigns_df,\n+            where_conditions\n+        )\n+\n+        email_campaigns_df = delete_query_executor.execute_query()\n+        campaign_ids = email_campaigns_df['id'].tolist()\n+        self.delete_email_campaigns(campaign_ids)\n+\n+    def delete_email_campaigns(self, campaign_ids: List[Text]) -> None:\nComment: Hey @Sekhar-Kumar-Dash, can you please add some comments here about this fn?",
    "subcategory": "validation",
    "category": "refactoring",
    "file_path": "mindsdb/integrations/handlers/sendinblue_handler/sendinblue_tables.py",
    "pr_number": 8209,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1380353302,
    "comment_created_at": "2023-11-02T15:40:18Z"
  },
  {
    "code": "@@ -0,0 +1,37 @@\n+import streamlit as st\n+from ui import (\n+    user_submit_ticket,\n+    user_my_tickets,\n+    user_chat_ticket,\n+    admin_dashboard,\n+    admin_tickets,\n+    admin_search_kb,\n+)\n+\n+st.set_page_config(page_title=\"Janus\", layout=\"wide\")\n+\n+st.sidebar.title(\"Janus\")\n+page = st.sidebar.radio(\n+    \"Navigate\",\n+    [\n+        \"✉️ Submit Ticket\",\n+        \"📁 My Tickets\",\n+        \"💬 Chat with AI\",\n+        \"📊 Admin Dashboard\",\n+        \"🧾 Manage Tickets\",\n+        \"🔍 Search KB\",\n+    ],\n+)\n+\n+if page == \"✉️ Submit Ticket\":\n+    user_submit_ticket.run()\n+elif page == \"📁 My Tickets\":\n+    user_my_tickets.run()\n+elif page == \"💬 Chat with AI\":\n+    user_chat_ticket.run()\n+elif page == \"📊 Admin Dashboard\":\n+    admin_dashboard.run()\n+elif page == \"🧾 Manage Tickets\":\n+    admin_tickets.run()\n+elif page == \"🔍 Search KB\":\n+    admin_search_kb.run()",
    "comment": "**correctness**: `ui` module functions (e.g., `user_submit_ticket.run()`) are called without verifying their existence, which will cause a runtime crash if any are missing or not properly imported.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb hacktoberfest/use-cases/ai-helpdesk-platform/app.py, lines 27-37, the code calls `run()` on modules imported from `ui` without checking if the function exists. This will cause a runtime crash if any module is missing or does not have a `run` method. Please update each block to check with `hasattr(module, \"run\")` before calling, and display a Streamlit error message if not present.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n    if page == \"✉️ Submit Ticket\":\n        if hasattr(user_submit_ticket, \"run\"):\n            user_submit_ticket.run()\n        else:\n            st.error(\"Submit Ticket module is unavailable.\")\n    elif page == \"📁 My Tickets\":\n        if hasattr(user_my_tickets, \"run\"):\n            user_my_tickets.run()\n        else:\n            st.error(\"My Tickets module is unavailable.\")\n    elif page == \"💬 Chat with AI\":\n        if hasattr(user_chat_ticket, \"run\"):\n            user_chat_ticket.run()\n        else:\n            st.error(\"Chat with AI module is unavailable.\")\n    elif page == \"📊 Admin Dashboard\":\n        if hasattr(admin_dashboard, \"run\"):\n            admin_dashboard.run()\n        else:\n            st.error(\"Admin Dashboard module is unavailable.\")\n    elif page == \"🧾 Manage Tickets\":\n        if hasattr(admin_tickets, \"run\"):\n            admin_tickets.run()\n        else:\n            st.error(\"Manage Tickets module is unavailable.\")\n    elif page == \"🔍 Search KB\":\n        if hasattr(admin_search_kb, \"run\"):\n            admin_search_kb.run()\n        else:\n            st.error(\"Search KB module is unavailable.\")\n```\n</details>\n<!-- suggestion_end -->\n",
    "line_number": 37,
    "enriched": "File: mindsdb hacktoberfest/use-cases/ai-helpdesk-platform/app.py\nCode: @@ -0,0 +1,37 @@\n+import streamlit as st\n+from ui import (\n+    user_submit_ticket,\n+    user_my_tickets,\n+    user_chat_ticket,\n+    admin_dashboard,\n+    admin_tickets,\n+    admin_search_kb,\n+)\n+\n+st.set_page_config(page_title=\"Janus\", layout=\"wide\")\n+\n+st.sidebar.title(\"Janus\")\n+page = st.sidebar.radio(\n+    \"Navigate\",\n+    [\n+        \"✉️ Submit Ticket\",\n+        \"📁 My Tickets\",\n+        \"💬 Chat with AI\",\n+        \"📊 Admin Dashboard\",\n+        \"🧾 Manage Tickets\",\n+        \"🔍 Search KB\",\n+    ],\n+)\n+\n+if page == \"✉️ Submit Ticket\":\n+    user_submit_ticket.run()\n+elif page == \"📁 My Tickets\":\n+    user_my_tickets.run()\n+elif page == \"💬 Chat with AI\":\n+    user_chat_ticket.run()\n+elif page == \"📊 Admin Dashboard\":\n+    admin_dashboard.run()\n+elif page == \"🧾 Manage Tickets\":\n+    admin_tickets.run()\n+elif page == \"🔍 Search KB\":\n+    admin_search_kb.run()\nComment: **correctness**: `ui` module functions (e.g., `user_submit_ticket.run()`) are called without verifying their existence, which will cause a runtime crash if any are missing or not properly imported.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb hacktoberfest/use-cases/ai-helpdesk-platform/app.py, lines 27-37, the code calls `run()` on modules imported from `ui` without checking if the function exists. This will cause a runtime crash if any module is missing or does not have a `run` method. Please update each block to check with `hasattr(module, \"run\")` before calling, and display a Streamlit error message if not present.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n    if page == \"✉️ Submit Ticket\":\n        if hasattr(user_submit_ticket, \"run\"):\n            user_submit_ticket.run()\n        else:\n            st.error(\"Submit Ticket module is unavailable.\")\n    elif page == \"📁 My Tickets\":\n        if hasattr(user_my_tickets, \"run\"):\n            user_my_tickets.run()\n        else:\n            st.error(\"My Tickets module is unavailable.\")\n    elif page == \"💬 Chat with AI\":\n        if hasattr(user_chat_ticket, \"run\"):\n            user_chat_ticket.run()\n        else:\n            st.error(\"Chat with AI module is unavailable.\")\n    elif page == \"📊 Admin Dashboard\":\n        if hasattr(admin_dashboard, \"run\"):\n            admin_dashboard.run()\n        else:\n            st.error(\"Admin Dashboard module is unavailable.\")\n    elif page == \"🧾 Manage Tickets\":\n        if hasattr(admin_tickets, \"run\"):\n            admin_tickets.run()\n        else:\n            st.error(\"Manage Tickets module is unavailable.\")\n    elif page == \"🔍 Search KB\":\n        if hasattr(admin_search_kb, \"run\"):\n            admin_search_kb.run()\n        else:\n            st.error(\"Search KB module is unavailable.\")\n```\n</details>\n<!-- suggestion_end -->\n",
    "subcategory": "validation",
    "category": "refactoring",
    "file_path": "mindsdb hacktoberfest/use-cases/ai-helpdesk-platform/app.py",
    "pr_number": 11801,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2469573877,
    "comment_created_at": "2025-10-28T13:27:02Z"
  },
  {
    "code": "@@ -104,7 +104,12 @@ def save_file(self, name, file_path, file_name=None):\n             file_dir.mkdir(parents=True, exist_ok=True)\n             source = file_dir.joinpath(file_name)\n             # NOTE may be delay between db record exists and file is really in folder\n-            shutil.move(file_path, str(source))\n+            try:\n+                shutil.move(file_path, str(source))\n+            except PermissionError as e:\n+                logger.warning('Permission error while moving file %s: %s', file_path, str(e))",
    "comment": "is it right not to raise an exception in this case?",
    "line_number": 110,
    "enriched": "File: mindsdb/interfaces/file/file_controller.py\nCode: @@ -104,7 +104,12 @@ def save_file(self, name, file_path, file_name=None):\n             file_dir.mkdir(parents=True, exist_ok=True)\n             source = file_dir.joinpath(file_name)\n             # NOTE may be delay between db record exists and file is really in folder\n-            shutil.move(file_path, str(source))\n+            try:\n+                shutil.move(file_path, str(source))\n+            except PermissionError as e:\n+                logger.warning('Permission error while moving file %s: %s', file_path, str(e))\nComment: is it right not to raise an exception in this case?",
    "subcategory": "validation",
    "category": "refactoring",
    "file_path": "mindsdb/interfaces/file/file_controller.py",
    "pr_number": 8662,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1461572207,
    "comment_created_at": "2024-01-22T09:32:55Z"
  },
  {
    "code": "@@ -0,0 +1,41 @@\n+import unittest\n+from mindsdb.integrations.handlers.aqicn_handler.aqicn_handler import AQICNHandler\n+from mindsdb.integrations.handlers.aqicn_handler.aqicn_tables import *\n+from mindsdb.api.executor.data_types.response_type import RESPONSE_TYPE\n+\n+\n+class AQICNHandlerTest(unittest.TestCase):\n+    @classmethod\n+    def setUpClass(cls):\n+        cls.kwargs = {\n+            \"api_key\": \"YOUR_API_KEY\" # Update this with your aqicn api key",
    "comment": "Since these are unit tests, the API calls have to be mocked. Else all the tests will fail.",
    "line_number": 11,
    "enriched": "File: mindsdb/integrations/handlers/aqicn_handler/tests/test_aqicn_handler.py\nCode: @@ -0,0 +1,41 @@\n+import unittest\n+from mindsdb.integrations.handlers.aqicn_handler.aqicn_handler import AQICNHandler\n+from mindsdb.integrations.handlers.aqicn_handler.aqicn_tables import *\n+from mindsdb.api.executor.data_types.response_type import RESPONSE_TYPE\n+\n+\n+class AQICNHandlerTest(unittest.TestCase):\n+    @classmethod\n+    def setUpClass(cls):\n+        cls.kwargs = {\n+            \"api_key\": \"YOUR_API_KEY\" # Update this with your aqicn api key\nComment: Since these are unit tests, the API calls have to be mocked. Else all the tests will fail.",
    "subcategory": "validation",
    "category": "refactoring",
    "file_path": "mindsdb/integrations/handlers/aqicn_handler/tests/test_aqicn_handler.py",
    "pr_number": 9858,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1799725373,
    "comment_created_at": "2024-10-14T15:31:28Z"
  },
  {
    "code": "@@ -60,18 +46,6 @@ FROM myshop.<collection-name>\n WHERE <field-name> = <value>;",
    "comment": "Let's add a LIMIT clause after WHERE.",
    "line_number": 46,
    "enriched": "File: docs/integrations/app-integrations/strapi.mdx\nCode: @@ -60,18 +46,6 @@ FROM myshop.<collection-name>\n WHERE <field-name> = <value>;\nComment: Let's add a LIMIT clause after WHERE.",
    "subcategory": "validation",
    "category": "refactoring",
    "file_path": "docs/integrations/app-integrations/strapi.mdx",
    "pr_number": 8335,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1394278601,
    "comment_created_at": "2023-11-15T14:27:52Z"
  },
  {
    "code": "@@ -0,0 +1,200 @@\n+\"\"\"\n+Customer Support Intelligence with MindsDB Knowledge Bases\n+Demo Script - Hacktoberfest 2025\n+\"\"\"\n+\n+import mindsdb_sdk\n+import pandas as pd\n+import os\n+from dotenv import load_dotenv\n+from tabulate import tabulate\n+\n+# Load environment variables\n+load_dotenv()\n+\n+def connect_to_mindsdb():\n+    \"\"\"Connect to MindsDB\"\"\"\n+    print(\"🔗 Connecting to MindsDB...\")\n+    server = mindsdb_sdk.connect(\n+        login=os.getenv('MINDSDB_EMAIL'),\n+        password=os.getenv('MINDSDB_PASSWORD')\n+    )\n+    print(\"✅ Connected successfully\\n\")\n+    return server\n+\n+def load_data():\n+    \"\"\"Load support tickets data\"\"\"\n+    print(\"📊 Loading support tickets data...\")\n+    df = pd.read_csv('data/support_tickets.csv')\n+    print(f\"✅ Loaded {len(df)} tickets\\n\")\n+    return df\n+\n+def create_knowledge_base(server, kb_name='support_tickets_kb'):\n+    \"\"\"Create or get Knowledge Base\"\"\"\n+    print(f\"📚 Setting up Knowledge Base: {kb_name}...\")\n+    try:\n+        kb = server.knowledge_bases.get(kb_name)\n+        print(f\"✅ Using existing Knowledge Base\\n\")\n+    except:\n+        kb = server.knowledge_bases.create(\n+            name=kb_name,\n+            model='openai',\n+            storage='chromadb'\n+        )\n+        print(f\"✅ Created new Knowledge Base\\n\")\n+    return kb\n+\n+def ingest_data(kb, df):\n+    \"\"\"Ingest tickets into Knowledge Base\"\"\"\n+    print(\"📥 Ingesting data into Knowledge Base...\")\n+    \n+    for idx, row in df.iterrows():\n+        content = f\"\"\"Ticket: {row['ticket_id']}\n+Subject: {row['subject']}\n+Category: {row['category']}\n+Priority: {row['priority']}\n+Description: {row['description']}\n+Resolution: {row['resolution'] if pd.notna(row['resolution']) else 'Not yet resolved'}\n+Customer: {row['customer_name']}\n+Agent: {row['agent_name'] if pd.notna(row['agent_name']) else 'Unassigned'}\"\"\"\n+        \n+        kb.insert([{\n+            'content': content,\n+            'metadata': {\n+                'ticket_id': row['ticket_id'],\n+                'category': row['category'],\n+                'priority': row['priority'],\n+                'status': row['status'],\n+                'created_date': row['created_date'],\n+                'customer_name': row['customer_name']\n+            }\n+        }])",
    "comment": "**correctness**: `ingest_data` function inserts each ticket into the knowledge base individually in a loop, which can cause partial ingestion or inconsistent state if an exception occurs mid-way; there is no error handling or batching, risking data loss or duplicate ingestion.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb hacktoberfest/use-cases/mindsdb/use-cases/customer-support-intelligence/demo_script.py, lines 51-71, the `ingest_data` function inserts each ticket into the knowledge base one by one without error handling or batching, risking partial ingestion and inconsistent state if an exception occurs. Refactor this block to batch all records and insert them at once, wrapping the operation in a try/except to handle errors and prevent partial ingestion.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n    try:\n        records = []\n        for idx, row in df.iterrows():\n            content = f\"\"\"Ticket: {row['ticket_id']}\nSubject: {row['subject']}\nCategory: {row['category']}\nPriority: {row['priority']}\nDescription: {row['description']}\nResolution: {row['resolution'] if pd.notna(row['resolution']) else 'Not yet resolved'}\nCustomer: {row['customer_name']}\nAgent: {row['agent_name'] if pd.notna(row['agent_name']) else 'Unassigned'}\"\"\"\n            records.append({\n                'content': content,\n                'metadata': {\n                    'ticket_id': row['ticket_id'],\n                    'category': row['category'],\n                    'priority': row['priority'],\n                    'status': row['status'],\n                    'created_date': row['created_date'],\n                    'customer_name': row['customer_name']\n                }\n            })\n        kb.insert(records)\n    except Exception as e:\n        print(f\"❌ Error during ingestion: {e}\")\n        raise\n\n```\n</details>\n<!-- suggestion_end -->\n",
    "line_number": 71,
    "enriched": "File: mindsdb hacktoberfest/use-cases/mindsdb/use-cases/customer-support-intelligence/demo_script.py\nCode: @@ -0,0 +1,200 @@\n+\"\"\"\n+Customer Support Intelligence with MindsDB Knowledge Bases\n+Demo Script - Hacktoberfest 2025\n+\"\"\"\n+\n+import mindsdb_sdk\n+import pandas as pd\n+import os\n+from dotenv import load_dotenv\n+from tabulate import tabulate\n+\n+# Load environment variables\n+load_dotenv()\n+\n+def connect_to_mindsdb():\n+    \"\"\"Connect to MindsDB\"\"\"\n+    print(\"🔗 Connecting to MindsDB...\")\n+    server = mindsdb_sdk.connect(\n+        login=os.getenv('MINDSDB_EMAIL'),\n+        password=os.getenv('MINDSDB_PASSWORD')\n+    )\n+    print(\"✅ Connected successfully\\n\")\n+    return server\n+\n+def load_data():\n+    \"\"\"Load support tickets data\"\"\"\n+    print(\"📊 Loading support tickets data...\")\n+    df = pd.read_csv('data/support_tickets.csv')\n+    print(f\"✅ Loaded {len(df)} tickets\\n\")\n+    return df\n+\n+def create_knowledge_base(server, kb_name='support_tickets_kb'):\n+    \"\"\"Create or get Knowledge Base\"\"\"\n+    print(f\"📚 Setting up Knowledge Base: {kb_name}...\")\n+    try:\n+        kb = server.knowledge_bases.get(kb_name)\n+        print(f\"✅ Using existing Knowledge Base\\n\")\n+    except:\n+        kb = server.knowledge_bases.create(\n+            name=kb_name,\n+            model='openai',\n+            storage='chromadb'\n+        )\n+        print(f\"✅ Created new Knowledge Base\\n\")\n+    return kb\n+\n+def ingest_data(kb, df):\n+    \"\"\"Ingest tickets into Knowledge Base\"\"\"\n+    print(\"📥 Ingesting data into Knowledge Base...\")\n+    \n+    for idx, row in df.iterrows():\n+        content = f\"\"\"Ticket: {row['ticket_id']}\n+Subject: {row['subject']}\n+Category: {row['category']}\n+Priority: {row['priority']}\n+Description: {row['description']}\n+Resolution: {row['resolution'] if pd.notna(row['resolution']) else 'Not yet resolved'}\n+Customer: {row['customer_name']}\n+Agent: {row['agent_name'] if pd.notna(row['agent_name']) else 'Unassigned'}\"\"\"\n+        \n+        kb.insert([{\n+            'content': content,\n+            'metadata': {\n+                'ticket_id': row['ticket_id'],\n+                'category': row['category'],\n+                'priority': row['priority'],\n+                'status': row['status'],\n+                'created_date': row['created_date'],\n+                'customer_name': row['customer_name']\n+            }\n+        }])\nComment: **correctness**: `ingest_data` function inserts each ticket into the knowledge base individually in a loop, which can cause partial ingestion or inconsistent state if an exception occurs mid-way; there is no error handling or batching, risking data loss or duplicate ingestion.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb hacktoberfest/use-cases/mindsdb/use-cases/customer-support-intelligence/demo_script.py, lines 51-71, the `ingest_data` function inserts each ticket into the knowledge base one by one without error handling or batching, risking partial ingestion and inconsistent state if an exception occurs. Refactor this block to batch all records and insert them at once, wrapping the operation in a try/except to handle errors and prevent partial ingestion.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n    try:\n        records = []\n        for idx, row in df.iterrows():\n            content = f\"\"\"Ticket: {row['ticket_id']}\nSubject: {row['subject']}\nCategory: {row['category']}\nPriority: {row['priority']}\nDescription: {row['description']}\nResolution: {row['resolution'] if pd.notna(row['resolution']) else 'Not yet resolved'}\nCustomer: {row['customer_name']}\nAgent: {row['agent_name'] if pd.notna(row['agent_name']) else 'Unassigned'}\"\"\"\n            records.append({\n                'content': content,\n                'metadata': {\n                    'ticket_id': row['ticket_id'],\n                    'category': row['category'],\n                    'priority': row['priority'],\n                    'status': row['status'],\n                    'created_date': row['created_date'],\n                    'customer_name': row['customer_name']\n                }\n            })\n        kb.insert(records)\n    except Exception as e:\n        print(f\"❌ Error during ingestion: {e}\")\n        raise\n\n```\n</details>\n<!-- suggestion_end -->\n",
    "subcategory": "validation",
    "category": "refactoring",
    "file_path": "mindsdb hacktoberfest/use-cases/mindsdb/use-cases/customer-support-intelligence/demo_script.py",
    "pr_number": 11791,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2463942925,
    "comment_created_at": "2025-10-26T17:55:19Z"
  },
  {
    "code": "@@ -1,18 +1,132 @@\n import datetime as dt\n-from typing import Dict, Union, Any\n+from typing import Dict, Union, Any, Optional, Tuple as TypingTuple\n \n from bson.objectid import ObjectId\n-from mindsdb_sql_parser.ast import Select, Update, Identifier, Star, Constant, Tuple, BinaryOperation, Latest, TypeCast\n+from mindsdb_sql_parser.ast import (\n+    Select,\n+    Update,\n+    Identifier,\n+    Star,\n+    Constant,\n+    Tuple,\n+    BinaryOperation,\n+    Latest,\n+    TypeCast,\n+    Function,\n+)\n from mindsdb_sql_parser.ast.base import ASTNode\n \n from mindsdb.integrations.handlers.mongodb_handler.utils.mongodb_query import MongoQuery\n \n \n-class MongodbRender:\n+# TODO: Create base NonRelationalRender as SqlAlchemyRender\n+class NonRelationalRender:\n+    pass\n+\n+\n+class MongodbRender(NonRelationalRender):\n     \"\"\"\n     Renderer to convert SQL queries represented as ASTNodes to MongoQuery instances.\n     \"\"\"\n \n+    def _parse_inner_query(self, node: ASTNode) -> Dict[str, Any]:\n+        \"\"\"\n+        Return field ref like \"$field\" or constant for projection expressions.\n+        \"\"\"\n+        if isinstance(node, Identifier):\n+            return f\"${node.parts[-1]}\"\n+        elif isinstance(node, Constant):\n+            return node.value\n+        else:\n+            raise NotImplementedError(f\"Not supported inner query {node}\")\n+\n+    def _convert_type_cast(self, node: TypeCast) -> Dict[str, Any]:\n+        \"\"\"\n+        Converts a TypeCast ASTNode to a MongoDB-compatible format.\n+\n+        Args:\n+            node (TypeCast): The TypeCast node to be converted.\n+\n+        Returns:\n+            Dict[str, Any]: The converted type cast representation.\n+        \"\"\"\n+        inner_query = self._parse_inner_query(node.arg)\n+        type_name = node.type_name.upper()\n+\n+        def convert(value: Any, to_type: str) -> Dict[str, Any]:\n+            return {\"$convert\": {\"input\": value, \"to\": to_type, \"onError\": None}}\n+\n+        if type_name in (\"VARCHAR\", \"TEXT\", \"STRING\"):\n+            return convert(inner_query, \"string\")\n+\n+        if type_name in (\"INT\", \"INTEGER\", \"BIGINT\", \"LONG\"):\n+            return convert(inner_query, \"long\")\n+\n+        if type_name in (\"DOUBLE\", \"FLOAT\", \"DECIMAL\", \"NUMERIC\"):\n+            return convert(inner_query, \"double\")\n+\n+        if type_name in (\"DATE\", \"DATETIME\", \"TIMESTAMP\"):\n+            return convert(inner_query, \"date\")\n+\n+        return inner_query\n+\n+    def _parse_select(self, from_table: Any) -> TypingTuple[str, Dict[str, Any], Optional[Dict[str, Any]]]:\n+        \"\"\"\n+        Parses the from_table to extract the collection name\n+        If from_table is subquery, transform it for MongoDB\n+        Args:\n+            from_table (Any): The from_table to be parsed.\n+        Returns:\n+            str: The collection name.\n+            Dict[str, Any]: The query filters.\n+            Optional[Dict[str, Any]]: The projection fields.\n+        \"\"\"\n+        # Simple collection\n+        if isinstance(from_table, Identifier):\n+            return from_table.parts[-1], {}, None\n+\n+        # Trivial subselect\n+        if isinstance(from_table, Select):\n+            # reject complex forms early\n+            # how deep we want to go with subqueries?\n+            if from_table.group_by is not None or from_table.having is not None:\n+                raise NotImplementedError(f\"Not supported FROM as {from_table}\")",
    "comment": "let's add info for the user, what is wrong in query to help them fix the query (here and in similar errors)\r\nhere is for example:\r\n```\r\nraise NotImplementedError(f\"Not supported, subquery has `having` or `group by`: {from_table}\")\r\n```\r\n",
    "line_number": 93,
    "enriched": "File: mindsdb/integrations/handlers/mongodb_handler/utils/mongodb_render.py\nCode: @@ -1,18 +1,132 @@\n import datetime as dt\n-from typing import Dict, Union, Any\n+from typing import Dict, Union, Any, Optional, Tuple as TypingTuple\n \n from bson.objectid import ObjectId\n-from mindsdb_sql_parser.ast import Select, Update, Identifier, Star, Constant, Tuple, BinaryOperation, Latest, TypeCast\n+from mindsdb_sql_parser.ast import (\n+    Select,\n+    Update,\n+    Identifier,\n+    Star,\n+    Constant,\n+    Tuple,\n+    BinaryOperation,\n+    Latest,\n+    TypeCast,\n+    Function,\n+)\n from mindsdb_sql_parser.ast.base import ASTNode\n \n from mindsdb.integrations.handlers.mongodb_handler.utils.mongodb_query import MongoQuery\n \n \n-class MongodbRender:\n+# TODO: Create base NonRelationalRender as SqlAlchemyRender\n+class NonRelationalRender:\n+    pass\n+\n+\n+class MongodbRender(NonRelationalRender):\n     \"\"\"\n     Renderer to convert SQL queries represented as ASTNodes to MongoQuery instances.\n     \"\"\"\n \n+    def _parse_inner_query(self, node: ASTNode) -> Dict[str, Any]:\n+        \"\"\"\n+        Return field ref like \"$field\" or constant for projection expressions.\n+        \"\"\"\n+        if isinstance(node, Identifier):\n+            return f\"${node.parts[-1]}\"\n+        elif isinstance(node, Constant):\n+            return node.value\n+        else:\n+            raise NotImplementedError(f\"Not supported inner query {node}\")\n+\n+    def _convert_type_cast(self, node: TypeCast) -> Dict[str, Any]:\n+        \"\"\"\n+        Converts a TypeCast ASTNode to a MongoDB-compatible format.\n+\n+        Args:\n+            node (TypeCast): The TypeCast node to be converted.\n+\n+        Returns:\n+            Dict[str, Any]: The converted type cast representation.\n+        \"\"\"\n+        inner_query = self._parse_inner_query(node.arg)\n+        type_name = node.type_name.upper()\n+\n+        def convert(value: Any, to_type: str) -> Dict[str, Any]:\n+            return {\"$convert\": {\"input\": value, \"to\": to_type, \"onError\": None}}\n+\n+        if type_name in (\"VARCHAR\", \"TEXT\", \"STRING\"):\n+            return convert(inner_query, \"string\")\n+\n+        if type_name in (\"INT\", \"INTEGER\", \"BIGINT\", \"LONG\"):\n+            return convert(inner_query, \"long\")\n+\n+        if type_name in (\"DOUBLE\", \"FLOAT\", \"DECIMAL\", \"NUMERIC\"):\n+            return convert(inner_query, \"double\")\n+\n+        if type_name in (\"DATE\", \"DATETIME\", \"TIMESTAMP\"):\n+            return convert(inner_query, \"date\")\n+\n+        return inner_query\n+\n+    def _parse_select(self, from_table: Any) -> TypingTuple[str, Dict[str, Any], Optional[Dict[str, Any]]]:\n+        \"\"\"\n+        Parses the from_table to extract the collection name\n+        If from_table is subquery, transform it for MongoDB\n+        Args:\n+            from_table (Any): The from_table to be parsed.\n+        Returns:\n+            str: The collection name.\n+            Dict[str, Any]: The query filters.\n+            Optional[Dict[str, Any]]: The projection fields.\n+        \"\"\"\n+        # Simple collection\n+        if isinstance(from_table, Identifier):\n+            return from_table.parts[-1], {}, None\n+\n+        # Trivial subselect\n+        if isinstance(from_table, Select):\n+            # reject complex forms early\n+            # how deep we want to go with subqueries?\n+            if from_table.group_by is not None or from_table.having is not None:\n+                raise NotImplementedError(f\"Not supported FROM as {from_table}\")\nComment: let's add info for the user, what is wrong in query to help them fix the query (here and in similar errors)\r\nhere is for example:\r\n```\r\nraise NotImplementedError(f\"Not supported, subquery has `having` or `group by`: {from_table}\")\r\n```\r\n",
    "subcategory": "validation",
    "category": "refactoring",
    "file_path": "mindsdb/integrations/handlers/mongodb_handler/utils/mongodb_render.py",
    "pr_number": 11840,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2533275890,
    "comment_created_at": "2025-11-17T09:07:50Z"
  },
  {
    "code": "@@ -0,0 +1,352 @@\n+import json\n+import os\n+import tempfile\n+from io import BytesIO, StringIO\n+from unittest.mock import patch\n+\n+import pandas\n+import pytest\n+import responses\n+from mindsdb_sql.parser.ast import CreateTable, DropTables, Identifier, Select, Star\n+from pytest_lazyfixture import lazy_fixture\n+\n+from mindsdb.integrations.handlers.file_handler.file_handler import FileHandler\n+from mindsdb.integrations.libs.response import RESPONSE_TYPE\n+from mindsdb.interfaces.file.file_controller import FileController\n+\n+# Define a table to use as content for all of the file types\n+test_file_content = [\n+    [\"col_one\", \"col_two\", \"col_three\", \"col_four\"],\n+    [1, -1, 0.1, \"A\"],\n+    [2, -2, 0.2, \"B\"],\n+    [3, -3, 0.3, \"C\"],\n+]\n+\n+test_dataframe = pandas.DataFrame(test_file_content[1:], columns=test_file_content[0])\n+\n+file_records = [(\"one\", 1, test_file_content[0]), (\"two\", 2, test_file_content[0])]\n+\n+\n+class MockFileController:\n+    def get_files(self):\n+        return [\n+            {\n+                \"name\": record[0],\n+                \"row_count\": record[1],\n+                \"columns\": record[2],\n+            }\n+            for record in file_records\n+        ]\n+\n+    def get_file_meta(self, *args, **kwargs):\n+        return self.get_files()[0]\n+\n+    def delete_file(self, name):\n+        return True\n+\n+\n+@pytest.fixture()\n+def temp_dir():\n+    parent_dir = tempfile.mkdtemp(prefix=\"test_file_handler_\")\n+    temp_dir = os.path.join(\n+        parent_dir, \"file\"\n+    )  # This is hard-coded in Filecontroller, so we have to put our files in \"/file\"\n+    os.mkdir(temp_dir)\n+    return temp_dir\n+\n+\n+@pytest.fixture\n+def csv_file(temp_dir) -> str:\n+    file_path = os.path.join(temp_dir, \"test_data.csv\")\n+    test_dataframe.to_csv(file_path, index=False)\n+    return file_path\n+\n+\n+@pytest.fixture\n+def xlsx_file(temp_dir) -> str:\n+    file_path = os.path.join(temp_dir, \"test_data.xlsx\")\n+    test_dataframe.to_excel(file_path, index=False)\n+    return file_path\n+\n+\n+@pytest.fixture\n+def json_file(temp_dir) -> str:\n+    file_path = os.path.join(temp_dir, \"test_data.json\")\n+    test_dataframe.to_json(file_path)\n+    return file_path\n+\n+\n+@pytest.fixture\n+def parquet_file(temp_dir) -> str:\n+    file_path = os.path.join(temp_dir, \"test_data.parquet\")\n+    df = pandas.DataFrame(test_file_content[1:], columns=test_file_content[0]).astype(\n+        str\n+    )\n+    df.to_parquet(file_path)\n+    return file_path\n+\n+\n+@pytest.fixture\n+def pdf_file() -> str:\n+    curr_dir = os.path.dirname(os.path.realpath(__file__))\n+    return os.path.join(curr_dir, \"data\", \"test.pdf\")\n+\n+\n+@pytest.fixture\n+def txt_file() -> str:\n+    curr_dir = os.path.dirname(os.path.realpath(__file__))\n+    return os.path.join(curr_dir, \"data\", \"test.txt\")\n+\n+\n+class TestIsItX:\n+    \"\"\"Tests all of the 'is_it_x()' functions to determine a file's type\"\"\"\n+\n+    # We can't test xlsx or parquet here because they're binary files\n+    @pytest.mark.parametrize(\n+        \"file_path,result\",\n+        [(lazy_fixture(\"csv_file\"), True), (lazy_fixture(\"json_file\"), False)],\n+    )\n+    def test_is_it_csv(self, file_path, result):\n+        with open(file_path, \"r\") as fh:\n+            assert FileHandler.is_it_csv(StringIO(fh.read())) is result\n+\n+    @pytest.mark.parametrize(\n+        \"file_path,result\",\n+        [\n+            (lazy_fixture(\"csv_file\"), False),\n+            (lazy_fixture(\"xlsx_file\"), True),\n+            (lazy_fixture(\"json_file\"), False),\n+            (lazy_fixture(\"parquet_file\"), False),\n+        ],\n+    )\n+    def test_is_it_xlsx(self, file_path, result):\n+        assert FileHandler.is_it_xlsx(file_path) is result\n+\n+    # We can't test xlsx or parquet here because they're binary files\n+    @pytest.mark.parametrize(\n+        \"file_path,result\",\n+        [\n+            (lazy_fixture(\"csv_file\"), False),\n+            (lazy_fixture(\"json_file\"), True),\n+        ],\n+    )\n+    def test_is_it_json(self, file_path, result):\n+        with open(file_path, \"r\") as fh:\n+            assert FileHandler.is_it_json(StringIO(fh.read())) is result\n+\n+    @pytest.mark.parametrize(\n+        \"file_path,result\",\n+        [\n+            (lazy_fixture(\"csv_file\"), False),\n+            (lazy_fixture(\"xlsx_file\"), False),\n+            (lazy_fixture(\"json_file\"), False),\n+            (lazy_fixture(\"parquet_file\"), True),\n+        ],\n+    )\n+    def test_is_it_parquet(self, file_path, result):\n+        with open(file_path, \"rb\") as fh:\n+            assert FileHandler.is_it_parquet(BytesIO(fh.read())) is result\n+\n+\n+class TestQuery:\n+    \"\"\"Tests all of the scenarios relating to the query() function\"\"\"\n+\n+    def test_query_drop(self, monkeypatch):\n+        def mock_delete(self, name):\n+            return True\n+\n+        monkeypatch.setattr(MockFileController, \"delete_file\", mock_delete)\n+        file_handler = FileHandler(file_controller=MockFileController())\n+        response = file_handler.query(DropTables([Identifier(parts=[\"one\"])]))\n+\n+        assert response.type == RESPONSE_TYPE.OK\n+\n+    def test_query_drop_bad_delete(self, monkeypatch):\n+        def mock_delete(self, name):\n+            raise Exception(\"File delete error\")\n+\n+        monkeypatch.setattr(MockFileController, \"delete_file\", mock_delete)\n+        file_handler = FileHandler(file_controller=MockFileController())\n+        response = file_handler.query(DropTables([Identifier(parts=[\"one\"])]))\n+\n+        assert response.type == RESPONSE_TYPE.ERROR\n+\n+    def test_query_select(self, csv_file):",
    "comment": "If i could get some help from @StpMax or @dusvyat or someone else:\r\nI can't get this test to run, I always get `Exception: File 'test_data.csv' does not exists` ",
    "line_number": 174,
    "enriched": "File: mindsdb/integrations/handlers/file_handler/tests/test_file_handler.py\nCode: @@ -0,0 +1,352 @@\n+import json\n+import os\n+import tempfile\n+from io import BytesIO, StringIO\n+from unittest.mock import patch\n+\n+import pandas\n+import pytest\n+import responses\n+from mindsdb_sql.parser.ast import CreateTable, DropTables, Identifier, Select, Star\n+from pytest_lazyfixture import lazy_fixture\n+\n+from mindsdb.integrations.handlers.file_handler.file_handler import FileHandler\n+from mindsdb.integrations.libs.response import RESPONSE_TYPE\n+from mindsdb.interfaces.file.file_controller import FileController\n+\n+# Define a table to use as content for all of the file types\n+test_file_content = [\n+    [\"col_one\", \"col_two\", \"col_three\", \"col_four\"],\n+    [1, -1, 0.1, \"A\"],\n+    [2, -2, 0.2, \"B\"],\n+    [3, -3, 0.3, \"C\"],\n+]\n+\n+test_dataframe = pandas.DataFrame(test_file_content[1:], columns=test_file_content[0])\n+\n+file_records = [(\"one\", 1, test_file_content[0]), (\"two\", 2, test_file_content[0])]\n+\n+\n+class MockFileController:\n+    def get_files(self):\n+        return [\n+            {\n+                \"name\": record[0],\n+                \"row_count\": record[1],\n+                \"columns\": record[2],\n+            }\n+            for record in file_records\n+        ]\n+\n+    def get_file_meta(self, *args, **kwargs):\n+        return self.get_files()[0]\n+\n+    def delete_file(self, name):\n+        return True\n+\n+\n+@pytest.fixture()\n+def temp_dir():\n+    parent_dir = tempfile.mkdtemp(prefix=\"test_file_handler_\")\n+    temp_dir = os.path.join(\n+        parent_dir, \"file\"\n+    )  # This is hard-coded in Filecontroller, so we have to put our files in \"/file\"\n+    os.mkdir(temp_dir)\n+    return temp_dir\n+\n+\n+@pytest.fixture\n+def csv_file(temp_dir) -> str:\n+    file_path = os.path.join(temp_dir, \"test_data.csv\")\n+    test_dataframe.to_csv(file_path, index=False)\n+    return file_path\n+\n+\n+@pytest.fixture\n+def xlsx_file(temp_dir) -> str:\n+    file_path = os.path.join(temp_dir, \"test_data.xlsx\")\n+    test_dataframe.to_excel(file_path, index=False)\n+    return file_path\n+\n+\n+@pytest.fixture\n+def json_file(temp_dir) -> str:\n+    file_path = os.path.join(temp_dir, \"test_data.json\")\n+    test_dataframe.to_json(file_path)\n+    return file_path\n+\n+\n+@pytest.fixture\n+def parquet_file(temp_dir) -> str:\n+    file_path = os.path.join(temp_dir, \"test_data.parquet\")\n+    df = pandas.DataFrame(test_file_content[1:], columns=test_file_content[0]).astype(\n+        str\n+    )\n+    df.to_parquet(file_path)\n+    return file_path\n+\n+\n+@pytest.fixture\n+def pdf_file() -> str:\n+    curr_dir = os.path.dirname(os.path.realpath(__file__))\n+    return os.path.join(curr_dir, \"data\", \"test.pdf\")\n+\n+\n+@pytest.fixture\n+def txt_file() -> str:\n+    curr_dir = os.path.dirname(os.path.realpath(__file__))\n+    return os.path.join(curr_dir, \"data\", \"test.txt\")\n+\n+\n+class TestIsItX:\n+    \"\"\"Tests all of the 'is_it_x()' functions to determine a file's type\"\"\"\n+\n+    # We can't test xlsx or parquet here because they're binary files\n+    @pytest.mark.parametrize(\n+        \"file_path,result\",\n+        [(lazy_fixture(\"csv_file\"), True), (lazy_fixture(\"json_file\"), False)],\n+    )\n+    def test_is_it_csv(self, file_path, result):\n+        with open(file_path, \"r\") as fh:\n+            assert FileHandler.is_it_csv(StringIO(fh.read())) is result\n+\n+    @pytest.mark.parametrize(\n+        \"file_path,result\",\n+        [\n+            (lazy_fixture(\"csv_file\"), False),\n+            (lazy_fixture(\"xlsx_file\"), True),\n+            (lazy_fixture(\"json_file\"), False),\n+            (lazy_fixture(\"parquet_file\"), False),\n+        ],\n+    )\n+    def test_is_it_xlsx(self, file_path, result):\n+        assert FileHandler.is_it_xlsx(file_path) is result\n+\n+    # We can't test xlsx or parquet here because they're binary files\n+    @pytest.mark.parametrize(\n+        \"file_path,result\",\n+        [\n+            (lazy_fixture(\"csv_file\"), False),\n+            (lazy_fixture(\"json_file\"), True),\n+        ],\n+    )\n+    def test_is_it_json(self, file_path, result):\n+        with open(file_path, \"r\") as fh:\n+            assert FileHandler.is_it_json(StringIO(fh.read())) is result\n+\n+    @pytest.mark.parametrize(\n+        \"file_path,result\",\n+        [\n+            (lazy_fixture(\"csv_file\"), False),\n+            (lazy_fixture(\"xlsx_file\"), False),\n+            (lazy_fixture(\"json_file\"), False),\n+            (lazy_fixture(\"parquet_file\"), True),\n+        ],\n+    )\n+    def test_is_it_parquet(self, file_path, result):\n+        with open(file_path, \"rb\") as fh:\n+            assert FileHandler.is_it_parquet(BytesIO(fh.read())) is result\n+\n+\n+class TestQuery:\n+    \"\"\"Tests all of the scenarios relating to the query() function\"\"\"\n+\n+    def test_query_drop(self, monkeypatch):\n+        def mock_delete(self, name):\n+            return True\n+\n+        monkeypatch.setattr(MockFileController, \"delete_file\", mock_delete)\n+        file_handler = FileHandler(file_controller=MockFileController())\n+        response = file_handler.query(DropTables([Identifier(parts=[\"one\"])]))\n+\n+        assert response.type == RESPONSE_TYPE.OK\n+\n+    def test_query_drop_bad_delete(self, monkeypatch):\n+        def mock_delete(self, name):\n+            raise Exception(\"File delete error\")\n+\n+        monkeypatch.setattr(MockFileController, \"delete_file\", mock_delete)\n+        file_handler = FileHandler(file_controller=MockFileController())\n+        response = file_handler.query(DropTables([Identifier(parts=[\"one\"])]))\n+\n+        assert response.type == RESPONSE_TYPE.ERROR\n+\n+    def test_query_select(self, csv_file):\nComment: If i could get some help from @StpMax or @dusvyat or someone else:\r\nI can't get this test to run, I always get `Exception: File 'test_data.csv' does not exists` ",
    "subcategory": "validation",
    "category": "refactoring",
    "file_path": "mindsdb/integrations/handlers/file_handler/tests/test_file_handler.py",
    "pr_number": 7746,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1361410622,
    "comment_created_at": "2023-10-17T01:30:19Z"
  },
  {
    "code": "@@ -159,32 +154,38 @@ def _connect_duckdb(self, bucket):\n             client = self.connect()\n             self._regions[bucket] = client.get_bucket_location(Bucket=bucket)[\"LocationConstraint\"]\n \n-        region = self._regions[bucket]\n-        duckdb_conn.execute(f\"SET s3_region='{region}'\")\n+        # region = self._regions[bucket]\n+        # duckdb_conn.execute(f\"SET s3_region='{region}'\")\n+        duckdb_conn.execute(f\"SET s3_region='{self.connection_data['region_name']}'\")",
    "comment": "**correctness**: `self.connection_data['region_name']` is accessed without checking if the key exists, which will raise a `KeyError` if `region_name` is missing.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/integrations/handlers/s3_handler/s3_handler.py, lines 159-159, the code accesses `self.connection_data['region_name']` without checking if the key exists, which can cause a KeyError. Please add a check for 'region_name' in self.connection_data and raise a ValueError with a clear message if it is missing, before using it in the duckdb_conn.execute statement.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n        if 'region_name' not in self.connection_data:\n            raise ValueError(\"'region_name' must be provided in connection_data to set s3_region for duckdb.\")\n        duckdb_conn.execute(f\"SET s3_region='{self.connection_data['region_name']}'\")\n```\n</details>\n<!-- suggestion_end -->\n",
    "line_number": 159,
    "enriched": "File: mindsdb/integrations/handlers/s3_handler/s3_handler.py\nCode: @@ -159,32 +154,38 @@ def _connect_duckdb(self, bucket):\n             client = self.connect()\n             self._regions[bucket] = client.get_bucket_location(Bucket=bucket)[\"LocationConstraint\"]\n \n-        region = self._regions[bucket]\n-        duckdb_conn.execute(f\"SET s3_region='{region}'\")\n+        # region = self._regions[bucket]\n+        # duckdb_conn.execute(f\"SET s3_region='{region}'\")\n+        duckdb_conn.execute(f\"SET s3_region='{self.connection_data['region_name']}'\")\nComment: **correctness**: `self.connection_data['region_name']` is accessed without checking if the key exists, which will raise a `KeyError` if `region_name` is missing.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/integrations/handlers/s3_handler/s3_handler.py, lines 159-159, the code accesses `self.connection_data['region_name']` without checking if the key exists, which can cause a KeyError. Please add a check for 'region_name' in self.connection_data and raise a ValueError with a clear message if it is missing, before using it in the duckdb_conn.execute statement.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n        if 'region_name' not in self.connection_data:\n            raise ValueError(\"'region_name' must be provided in connection_data to set s3_region for duckdb.\")\n        duckdb_conn.execute(f\"SET s3_region='{self.connection_data['region_name']}'\")\n```\n</details>\n<!-- suggestion_end -->\n",
    "subcategory": "validation",
    "category": "refactoring",
    "file_path": "mindsdb/integrations/handlers/s3_handler/s3_handler.py",
    "pr_number": 11590,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2361345150,
    "comment_created_at": "2025-09-18T22:57:28Z"
  },
  {
    "code": "@@ -11,7 +11,7 @@ class ConnectionConfig(BaseModel):\n     password: str = None\n     database: str = None\n \n-    @root_validator(pre=True)\n+    @model_validator(pre=True)",
    "comment": "should be `@model_validator(mode=\"before\")`",
    "line_number": 14,
    "enriched": "File: mindsdb/integrations/handlers/mysql_handler/settings.py\nCode: @@ -11,7 +11,7 @@ class ConnectionConfig(BaseModel):\n     password: str = None\n     database: str = None\n \n-    @root_validator(pre=True)\n+    @model_validator(pre=True)\nComment: should be `@model_validator(mode=\"before\")`",
    "subcategory": "validation",
    "category": "refactoring",
    "file_path": "mindsdb/integrations/handlers/mysql_handler/settings.py",
    "pr_number": 9193,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1596681992,
    "comment_created_at": "2024-05-10T12:17:46Z"
  },
  {
    "code": "@@ -0,0 +1,151 @@\n+from mindsdb.integrations.handlers.strapi_handler.strapi_tables import StrapiTable\n+from mindsdb.integrations.libs.api_handler import APIHandler\n+from mindsdb.integrations.libs.response import HandlerStatusResponse as StatusResponse\n+from mindsdb_sql import parse_sql\n+from mindsdb.utilities import log\n+import requests\n+from mindsdb.integrations.libs.const import HANDLER_CONNECTION_ARG_TYPE as ARG_TYPE\n+from collections import OrderedDict\n+import pandas as pd\n+\n+\n+class StrapiHandler(APIHandler):\n+    def __init__(self, name: str, **kwargs) -> None:\n+        \"\"\"initializer method\n+\n+        Args:\n+            name (str): handler name\n+        \"\"\"\n+        super().__init__(name)\n+\n+        self.connection = None\n+        self.is_connected = False\n+        args = kwargs.get('connection_data', {})\n+        if 'host' in args and 'port' in args:\n+            self._base_url = f\"http://{args['host']}:{args['port']}\"\n+        if 'api_token' in args:\n+            self._api_token = args['api_token']\n+        if 'pluralApiIds' in args:\n+            self._pluralApiIds = args['pluralApiIds']\n+        # Registers tables for each collections in strapi\n+        for pluralApiId in self._pluralApiIds:\n+            self._register_table(table_name=pluralApiId, table_class=StrapiTable(handler=self, name=pluralApiId))\n+\n+    def check_connection(self) -> StatusResponse:\n+        \"\"\"checking the connection\n+\n+        Returns:\n+            StatusResponse: whether the connection is still up\n+        \"\"\"\n+        response = StatusResponse(False)\n+        try:\n+            self.connect()\n+            response.success = True\n+        except Exception as e:\n+            log.logger.error(f'Error connecting to Strapi API: {e}!')\n+            response.error_message = e\n+\n+        self.is_connected = response.success\n+        return response\n+\n+    def connect(self) -> StatusResponse:\n+        \"\"\"making the connectino object\n+        \"\"\"\n+        if self.is_connected and self.connection:\n+            return self.connection\n+\n+        try:\n+            headers = {\"Authorization\": f\"Bearer {self._api_token}\"}\n+            response = requests.get(f\"{self._base_url}\", headers=headers)\n+            if response.status_code == 200:\n+                self.connection = response\n+                self.is_connected = True\n+                return StatusResponse(True)\n+            else:\n+                raise Exception(f\"Error connecting to Strapi API: {response.status_code} - {response.text}\")\n+        except Exception as e:\n+            log.logger.error(f'Error connecting to Strapi API: {e}!')\n+            return StatusResponse(False, error_message=e)\n+\n+    def native_query(self, query: str) -> StatusResponse:\n+        \"\"\"Receive and process a raw query.\n+\n+        Parameters\n+        ----------\n+        query : str\n+            query in a native format\n+\n+        Returns\n+        -------\n+        StatusResponse\n+            Request status\n+        \"\"\"\n+        ast = parse_sql(query, dialect=\"mindsdb\")\n+        return self.query(ast)\n+\n+    def call_strapi_api(self, method: str, endpoint: str, params: dict = {}, json_data: dict = {}) -> pd.DataFrame:\n+        headers = {\"Authorization\": f\"Bearer {self._api_token}\"}\n+        url = f\"{self._base_url}{endpoint}\"\n+\n+        if method.upper() in ('GET', 'POST', 'PUT', 'DELETE'):\n+            headers['Content-Type'] = 'application/json'\n+\n+            if method.upper() in ('POST', 'PUT', 'DELETE'):\n+                response = requests.request(method, url, headers=headers, params=params, data=json_data)\n+            else:\n+                response = requests.get(url, headers=headers, params=params)\n+\n+            if response.status_code == 200:\n+                data = response.json()\n+                # Create an empty DataFrame\n+                df = pd.DataFrame()\n+                if isinstance(data.get('data', None), list):\n+                    for item in data['data']:\n+                        # Add 'id' and 'attributes' to the DataFrame\n+                        row_data = {'id': item['id'], **item['attributes']}\n+                        df = df._append(row_data, ignore_index=True)\n+                    return df\n+                elif isinstance(data.get('data', None), dict):\n+                    # Add 'id' and 'attributes' to the DataFrame\n+                    row_data = {'id': data['data']['id'], **data['data']['attributes']}\n+                    df = df._append(row_data, ignore_index=True)\n+                    return df\n+            else:\n+                raise Exception(f\"Error connecting to Strapi API: {response.status_code} - {response.text}\")\n+\n+        return pd.DataFrame()\n+\n+\n+connection_args = OrderedDict(\n+    api_token={\n+        \"type\": ARG_TYPE.PWD,\n+        \"description\": \"Strapi API key to use for authentication.\",\n+        \"required\": True,\n+        \"label\": \"Api token\",\n+    },\n+    host={\n+        \"type\": ARG_TYPE.URL,\n+        \"description\": \"Strapi API host to connect to.\",\n+        \"required\": True,\n+        \"label\": \"Host\",\n+    },\n+    port={\n+        \"type\": ARG_TYPE.INT,\n+        \"description\": \"Strapi API port to connect to.\",\n+        \"required\": True,\n+        \"label\": \"Port\",\n+    },\n+    pluralApiIds={\n+        \"type\": list,\n+        \"description\": \"Plural API id to use for querying.\",\n+        \"required\": True,\n+        \"label\": \"Plural API id\",\n+    },\n+)\n+\n+connection_args_example = OrderedDict(\n+    host=\"localhost\",\n+    port=1337,\n+    api_token=\"277c4d669060c2c66b140aaa8a38e00b824182dd1634af7c9718344807e662c5cd77d3bfbfa756332d7b044b7ee12e26fc96f800b2d030a9bb0afda422bbf20d2ce6962fc313e32a2ca9cc19f2d89d51a4ca7f64576717c0c2ea28d7908b3be6f8b345f1f351498b7382fb5469a61a42a96ece4c72b21b0e3485ea7addd5189c\",",
    "comment": "Let's add some simple string as example",
    "line_number": 149,
    "enriched": "File: mindsdb/integrations/handlers/strapi_handler/strapi_handler.py\nCode: @@ -0,0 +1,151 @@\n+from mindsdb.integrations.handlers.strapi_handler.strapi_tables import StrapiTable\n+from mindsdb.integrations.libs.api_handler import APIHandler\n+from mindsdb.integrations.libs.response import HandlerStatusResponse as StatusResponse\n+from mindsdb_sql import parse_sql\n+from mindsdb.utilities import log\n+import requests\n+from mindsdb.integrations.libs.const import HANDLER_CONNECTION_ARG_TYPE as ARG_TYPE\n+from collections import OrderedDict\n+import pandas as pd\n+\n+\n+class StrapiHandler(APIHandler):\n+    def __init__(self, name: str, **kwargs) -> None:\n+        \"\"\"initializer method\n+\n+        Args:\n+            name (str): handler name\n+        \"\"\"\n+        super().__init__(name)\n+\n+        self.connection = None\n+        self.is_connected = False\n+        args = kwargs.get('connection_data', {})\n+        if 'host' in args and 'port' in args:\n+            self._base_url = f\"http://{args['host']}:{args['port']}\"\n+        if 'api_token' in args:\n+            self._api_token = args['api_token']\n+        if 'pluralApiIds' in args:\n+            self._pluralApiIds = args['pluralApiIds']\n+        # Registers tables for each collections in strapi\n+        for pluralApiId in self._pluralApiIds:\n+            self._register_table(table_name=pluralApiId, table_class=StrapiTable(handler=self, name=pluralApiId))\n+\n+    def check_connection(self) -> StatusResponse:\n+        \"\"\"checking the connection\n+\n+        Returns:\n+            StatusResponse: whether the connection is still up\n+        \"\"\"\n+        response = StatusResponse(False)\n+        try:\n+            self.connect()\n+            response.success = True\n+        except Exception as e:\n+            log.logger.error(f'Error connecting to Strapi API: {e}!')\n+            response.error_message = e\n+\n+        self.is_connected = response.success\n+        return response\n+\n+    def connect(self) -> StatusResponse:\n+        \"\"\"making the connectino object\n+        \"\"\"\n+        if self.is_connected and self.connection:\n+            return self.connection\n+\n+        try:\n+            headers = {\"Authorization\": f\"Bearer {self._api_token}\"}\n+            response = requests.get(f\"{self._base_url}\", headers=headers)\n+            if response.status_code == 200:\n+                self.connection = response\n+                self.is_connected = True\n+                return StatusResponse(True)\n+            else:\n+                raise Exception(f\"Error connecting to Strapi API: {response.status_code} - {response.text}\")\n+        except Exception as e:\n+            log.logger.error(f'Error connecting to Strapi API: {e}!')\n+            return StatusResponse(False, error_message=e)\n+\n+    def native_query(self, query: str) -> StatusResponse:\n+        \"\"\"Receive and process a raw query.\n+\n+        Parameters\n+        ----------\n+        query : str\n+            query in a native format\n+\n+        Returns\n+        -------\n+        StatusResponse\n+            Request status\n+        \"\"\"\n+        ast = parse_sql(query, dialect=\"mindsdb\")\n+        return self.query(ast)\n+\n+    def call_strapi_api(self, method: str, endpoint: str, params: dict = {}, json_data: dict = {}) -> pd.DataFrame:\n+        headers = {\"Authorization\": f\"Bearer {self._api_token}\"}\n+        url = f\"{self._base_url}{endpoint}\"\n+\n+        if method.upper() in ('GET', 'POST', 'PUT', 'DELETE'):\n+            headers['Content-Type'] = 'application/json'\n+\n+            if method.upper() in ('POST', 'PUT', 'DELETE'):\n+                response = requests.request(method, url, headers=headers, params=params, data=json_data)\n+            else:\n+                response = requests.get(url, headers=headers, params=params)\n+\n+            if response.status_code == 200:\n+                data = response.json()\n+                # Create an empty DataFrame\n+                df = pd.DataFrame()\n+                if isinstance(data.get('data', None), list):\n+                    for item in data['data']:\n+                        # Add 'id' and 'attributes' to the DataFrame\n+                        row_data = {'id': item['id'], **item['attributes']}\n+                        df = df._append(row_data, ignore_index=True)\n+                    return df\n+                elif isinstance(data.get('data', None), dict):\n+                    # Add 'id' and 'attributes' to the DataFrame\n+                    row_data = {'id': data['data']['id'], **data['data']['attributes']}\n+                    df = df._append(row_data, ignore_index=True)\n+                    return df\n+            else:\n+                raise Exception(f\"Error connecting to Strapi API: {response.status_code} - {response.text}\")\n+\n+        return pd.DataFrame()\n+\n+\n+connection_args = OrderedDict(\n+    api_token={\n+        \"type\": ARG_TYPE.PWD,\n+        \"description\": \"Strapi API key to use for authentication.\",\n+        \"required\": True,\n+        \"label\": \"Api token\",\n+    },\n+    host={\n+        \"type\": ARG_TYPE.URL,\n+        \"description\": \"Strapi API host to connect to.\",\n+        \"required\": True,\n+        \"label\": \"Host\",\n+    },\n+    port={\n+        \"type\": ARG_TYPE.INT,\n+        \"description\": \"Strapi API port to connect to.\",\n+        \"required\": True,\n+        \"label\": \"Port\",\n+    },\n+    pluralApiIds={\n+        \"type\": list,\n+        \"description\": \"Plural API id to use for querying.\",\n+        \"required\": True,\n+        \"label\": \"Plural API id\",\n+    },\n+)\n+\n+connection_args_example = OrderedDict(\n+    host=\"localhost\",\n+    port=1337,\n+    api_token=\"277c4d669060c2c66b140aaa8a38e00b824182dd1634af7c9718344807e662c5cd77d3bfbfa756332d7b044b7ee12e26fc96f800b2d030a9bb0afda422bbf20d2ce6962fc313e32a2ca9cc19f2d89d51a4ca7f64576717c0c2ea28d7908b3be6f8b345f1f351498b7382fb5469a61a42a96ece4c72b21b0e3485ea7addd5189c\",\nComment: Let's add some simple string as example",
    "subcategory": "validation",
    "category": "refactoring",
    "file_path": "mindsdb/integrations/handlers/strapi_handler/strapi_handler.py",
    "pr_number": 8162,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1376355876,
    "comment_created_at": "2023-10-30T14:51:30Z"
  },
  {
    "code": "@@ -158,6 +162,61 @@ def _get_rag_params(pred_args: Dict) -> Dict:\n     return rag_params\n \n \n+def _create_conn_string(connection_args: dict) -> str:\n+    \"\"\"\n+    Creates a PostgreSQL connection string from connection args.\n+    \"\"\"\n+    user = connection_args.get('user')\n+    host = connection_args.get('host')\n+    port = connection_args.get('port')\n+    password = connection_args.get('password')\n+    dbname = connection_args.get('database')\n+",
    "comment": "not had a chance to properly test, for now we should encourage users to use chromadb",
    "line_number": 174,
    "enriched": "File: mindsdb/integrations/handlers/langchain_handler/tools.py\nCode: @@ -158,6 +162,61 @@ def _get_rag_params(pred_args: Dict) -> Dict:\n     return rag_params\n \n \n+def _create_conn_string(connection_args: dict) -> str:\n+    \"\"\"\n+    Creates a PostgreSQL connection string from connection args.\n+    \"\"\"\n+    user = connection_args.get('user')\n+    host = connection_args.get('host')\n+    port = connection_args.get('port')\n+    password = connection_args.get('password')\n+    dbname = connection_args.get('database')\n+\nComment: not had a chance to properly test, for now we should encourage users to use chromadb",
    "subcategory": "validation",
    "category": "refactoring",
    "file_path": "mindsdb/integrations/handlers/langchain_handler/tools.py",
    "pr_number": 9101,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1572257290,
    "comment_created_at": "2024-04-19T11:54:40Z"
  },
  {
    "code": "@@ -1186,6 +1186,13 @@ def _create_embedding_model(self, project_name, engine=\"openai\", params: dict =\n         if \"provider\" not in params:\n             raise ValueError(\"'provider' parameter is required for embedding model\")\n \n+        # check available providers\n+        avail_providers = (\"openai\", \"azure_openai\", \"bedrock\")\n+        if params[\"provider\"] not in avail_providers:\n+            raise ValueError(\n+                f\"Wrong embedding provider: {params['provider']}. Available providers: {', '.join(avail_providers)}\"\n+            )\n+",
    "comment": "**correctness**: `_create_embedding_model` now rejects all providers except 'openai', 'azure_openai', and 'bedrock', breaking support for other valid providers (e.g., 'huggingface', 'cohere'), causing runtime failures for users with those providers.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/interfaces/knowledge_base/controller.py, lines 1189-1195, the code restricts embedding providers to only 'openai', 'azure_openai', and 'bedrock', which breaks support for other valid providers (such as 'huggingface', 'cohere', etc.) and causes runtime failures for users specifying those providers. Remove this provider whitelist check entirely to restore support for all previously valid providers.\n```\n</details>\n<!-- ai_prompt_end -->\n\n",
    "line_number": 1195,
    "enriched": "File: mindsdb/interfaces/knowledge_base/controller.py\nCode: @@ -1186,6 +1186,13 @@ def _create_embedding_model(self, project_name, engine=\"openai\", params: dict =\n         if \"provider\" not in params:\n             raise ValueError(\"'provider' parameter is required for embedding model\")\n \n+        # check available providers\n+        avail_providers = (\"openai\", \"azure_openai\", \"bedrock\")\n+        if params[\"provider\"] not in avail_providers:\n+            raise ValueError(\n+                f\"Wrong embedding provider: {params['provider']}. Available providers: {', '.join(avail_providers)}\"\n+            )\n+\nComment: **correctness**: `_create_embedding_model` now rejects all providers except 'openai', 'azure_openai', and 'bedrock', breaking support for other valid providers (e.g., 'huggingface', 'cohere'), causing runtime failures for users with those providers.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/interfaces/knowledge_base/controller.py, lines 1189-1195, the code restricts embedding providers to only 'openai', 'azure_openai', and 'bedrock', which breaks support for other valid providers (such as 'huggingface', 'cohere', etc.) and causes runtime failures for users specifying those providers. Remove this provider whitelist check entirely to restore support for all previously valid providers.\n```\n</details>\n<!-- ai_prompt_end -->\n\n",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "mindsdb/interfaces/knowledge_base/controller.py",
    "pr_number": 11275,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2197564017,
    "comment_created_at": "2025-07-10T12:17:00Z"
  },
  {
    "code": "@@ -96,17 +96,18 @@\n \n   # Run integration tests against the deployed environment\n   run_tests:\n-    name: Run Integration Tests\n+    name: Run Integration Tests - Test\n     needs: [deploy, get-deploy-labels]\n+    if: always()\n     strategy:\n       fail-fast: false\n       matrix:\n~        deploy-env: ${{ fromJson(needs.get-deploy-labels.outputs.deploy-envs) }}\n~    concurrency:\n~      group: ${{ github.workflow }}-${{ github.head_ref || github.ref }}-tests\n~      cancel-in-progress: true\n~    uses: ./.github/workflows/test_on_deploy.yml\n     with:\n       git-sha: ${{ github.event.pull_request.head.sha }}\n       deploy-env: ${{ matrix.deploy-env }}\n-    secrets: inherit\n+    secrets: inherit",
    "comment": "## Workflow does not contain permissions\n\nActions job or workflow does not limit the permissions of the GITHUB_TOKEN. Consider setting an explicit permissions block, using the following as a minimal starting point: {{}}\n\n[Show more details](https://github.com/mindsdb/mindsdb/security/code-scanning/81)",
    "line_number": 113,
    "enriched": "File: .github/workflows/build_deploy_dev.yml\nCode: @@ -96,17 +96,18 @@\n \n   # Run integration tests against the deployed environment\n   run_tests:\n-    name: Run Integration Tests\n+    name: Run Integration Tests - Test\n     needs: [deploy, get-deploy-labels]\n+    if: always()\n     strategy:\n       fail-fast: false\n       matrix:\n~        deploy-env: ${{ fromJson(needs.get-deploy-labels.outputs.deploy-envs) }}\n~    concurrency:\n~      group: ${{ github.workflow }}-${{ github.head_ref || github.ref }}-tests\n~      cancel-in-progress: true\n~    uses: ./.github/workflows/test_on_deploy.yml\n     with:\n       git-sha: ${{ github.event.pull_request.head.sha }}\n       deploy-env: ${{ matrix.deploy-env }}\n-    secrets: inherit\n+    secrets: inherit\nComment: ## Workflow does not contain permissions\n\nActions job or workflow does not limit the permissions of the GITHUB_TOKEN. Consider setting an explicit permissions block, using the following as a minimal starting point: {{}}\n\n[Show more details](https://github.com/mindsdb/mindsdb/security/code-scanning/81)",
    "subcategory": "interface",
    "category": "functional",
    "file_path": ".github/workflows/build_deploy_dev.yml",
    "pr_number": 10785,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2069770431,
    "comment_created_at": "2025-05-01T02:10:41Z"
  },
  {
    "code": "@@ -154,3 +154,52 @@\n     with:\n       deploy-env: prod\n     secrets: inherit\n+\n+  slack_message:\n+    if: always()\n+    name: Notify Slack\n+    needs: [run_tests]\n+    runs-on: mdb-dev\n+    steps:\n+      - name: Notify of failing tests\n+        if: needs.run_tests.result == 'failure'\n+        uses: slackapi/slack-github-action@v1.26.0\n+        with:\n+          channel-id: ${{ secrets.SLACK_DEPLOYMENTS_CHANNEL_ID }}\n+          payload: |\n+            {\n+              \"attachments\": [\n+                {\n+                  \"color\": \"#FF4444\",\n+                  \"blocks\": [\n+                    {\n+                      \"type\": \"header\",\n+                      \"text\": {\n+                        \"type\": \"plain_text\",\n+                        \"text\": \"TEST RUN FAILED ON PROD\",\n+                        \"emoji\": true\n+                      }\n+                    },\n+                    {\n+                      \"type\": \"section\",\n+                      \"text\": {\n+                        \"type\": \"mrkdwn\",\n+                        \"text\": \" \"\n+                      },\n+                      \"fields\": [\n+                        {\n+                          \"type\": \"mrkdwn\",\n+                          \"text\": \"*Commit*\\n<${{ github.server_url }}/${{ github.repository }}/commit/${{ github.sha }}|${{ github.sha }}>\"\n+                        },\n+                        {\n+                          \"type\": \"mrkdwn\",\n+                          \"text\": \"*Workflow Run*\\n<${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}|${{ github.workflow }}>\"\n+                        }\n+                      ]\n+                    }\n+                  ]\n+                }\n+              ]\n+            }\n+        env:\n+          SLACK_BOT_TOKEN: ${{ secrets.GH_ACTIONS_SLACK_BOT_TOKEN }}",
    "comment": "## Workflow does not contain permissions\n\nActions job or workflow does not limit the permissions of the GITHUB_TOKEN. Consider setting an explicit permissions block, using the following as a minimal starting point: {{}}\n\n[Show more details](https://github.com/mindsdb/mindsdb/security/code-scanning/88)",
    "line_number": 205,
    "enriched": "File: .github/workflows/build_deploy_prod.yml\nCode: @@ -154,3 +154,52 @@\n     with:\n       deploy-env: prod\n     secrets: inherit\n+\n+  slack_message:\n+    if: always()\n+    name: Notify Slack\n+    needs: [run_tests]\n+    runs-on: mdb-dev\n+    steps:\n+      - name: Notify of failing tests\n+        if: needs.run_tests.result == 'failure'\n+        uses: slackapi/slack-github-action@v1.26.0\n+        with:\n+          channel-id: ${{ secrets.SLACK_DEPLOYMENTS_CHANNEL_ID }}\n+          payload: |\n+            {\n+              \"attachments\": [\n+                {\n+                  \"color\": \"#FF4444\",\n+                  \"blocks\": [\n+                    {\n+                      \"type\": \"header\",\n+                      \"text\": {\n+                        \"type\": \"plain_text\",\n+                        \"text\": \"TEST RUN FAILED ON PROD\",\n+                        \"emoji\": true\n+                      }\n+                    },\n+                    {\n+                      \"type\": \"section\",\n+                      \"text\": {\n+                        \"type\": \"mrkdwn\",\n+                        \"text\": \" \"\n+                      },\n+                      \"fields\": [\n+                        {\n+                          \"type\": \"mrkdwn\",\n+                          \"text\": \"*Commit*\\n<${{ github.server_url }}/${{ github.repository }}/commit/${{ github.sha }}|${{ github.sha }}>\"\n+                        },\n+                        {\n+                          \"type\": \"mrkdwn\",\n+                          \"text\": \"*Workflow Run*\\n<${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}|${{ github.workflow }}>\"\n+                        }\n+                      ]\n+                    }\n+                  ]\n+                }\n+              ]\n+            }\n+        env:\n+          SLACK_BOT_TOKEN: ${{ secrets.GH_ACTIONS_SLACK_BOT_TOKEN }}\nComment: ## Workflow does not contain permissions\n\nActions job or workflow does not limit the permissions of the GITHUB_TOKEN. Consider setting an explicit permissions block, using the following as a minimal starting point: {{}}\n\n[Show more details](https://github.com/mindsdb/mindsdb/security/code-scanning/88)",
    "subcategory": "interface",
    "category": "functional",
    "file_path": ".github/workflows/build_deploy_prod.yml",
    "pr_number": 10792,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2071051134,
    "comment_created_at": "2025-05-02T03:31:20Z"
  },
  {
    "code": "@@ -5,12 +5,10 @@\n from mindsdb.utilities.log import get_log\n \n from mindsdb_sql.parser import ast\n+from mindsdb.integrations.handlers.utilities.query_utilities import SELECTQueryParser, SELECTQueryExecutor\n \n-\n-from googleapiclient.discovery import build\n-from googleapiclient.errors import HttpError",
    "comment": "It looks like these imports are not used in any of the table implementations. Thanks for cleaning it up. I think there is a similar import that is not required in the handler module, let's remove that as well, just to be clean.",
    "line_number": 11,
    "enriched": "File: mindsdb/integrations/handlers/youtube_handler/youtube_tables.py\nCode: @@ -5,12 +5,10 @@\n from mindsdb.utilities.log import get_log\n \n from mindsdb_sql.parser import ast\n+from mindsdb.integrations.handlers.utilities.query_utilities import SELECTQueryParser, SELECTQueryExecutor\n \n-\n-from googleapiclient.discovery import build\n-from googleapiclient.errors import HttpError\nComment: It looks like these imports are not used in any of the table implementations. Thanks for cleaning it up. I think there is a similar import that is not required in the handler module, let's remove that as well, just to be clean.",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/youtube_handler/youtube_tables.py",
    "pr_number": 7586,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1348704223,
    "comment_created_at": "2023-10-06T13:11:23Z"
  },
  {
    "code": "@@ -204,37 +223,264 @@\n \n \n def initialize_app(config, no_studio):\n-    static_root = config['paths']['static']\n+    static_root = config[\"paths\"][\"static\"]\n     logger.debug(f\"Static route: {static_root}\")\n-    gui_exists = Path(static_root).joinpath('index.html').is_file()\n+    gui_exists = Path(static_root).joinpath(\"index.html\").is_file()\n     logger.debug(f\"Does GUI already exist.. {'YES' if gui_exists else 'NO'}\")\n     init_static_thread = None\n-    if (\n-        no_studio is False\n-        and (\n-            config['gui']['autoupdate'] is True\n-            or gui_exists is False\n-        )\n+    if no_studio is False and (\n+        config[\"gui\"][\"autoupdate\"] is True or gui_exists is False\n     ):\n-        init_static_thread = threading.Thread(target=initialize_static, name='initialize_static')\n+        init_static_thread = threading.Thread(\n+            target=initialize_static, name=\"initialize_static\"\n+        )\n         init_static_thread.start()\n \n+    # Start A2A API as a subprocess if enabled in config or environment\n+    a2a_process = None\n+    a2a_enabled = config.get(\"a2a\", {}).get(\"enabled\", False) or os.environ.get(\n+        \"MINDSDB_A2A_ENABLED\", \"\"\n+    ).lower() in (\"true\", \"1\", \"yes\")\n+\n+    def update_a2a_agent(new_agent_name, new_project_name=None):\n+        \"\"\"\n+        Update the A2A agent configuration and restart the subprocess if needed.\n+\n+        Args:\n+            new_agent_name (str): New agent name to use\n+            new_project_name (str, optional): New project name to use\n+        \"\"\"\n+        nonlocal a2a_process\n+\n+        if not a2a_enabled or a2a_process is None:\n+            logger.warning(\"A2A is not enabled or not running, can't update agent\")\n+            return False\n+\n+        # Update config\n+        a2a_config = config.get(\"a2a\", {}).copy()\n+        a2a_config[\"agent_name\"] = new_agent_name\n+        if new_project_name:\n+            a2a_config[\"project_name\"] = new_project_name\n+\n+        # Store updated config\n+        config.update({\"a2a\": a2a_config})\n+\n+        # Terminate existing process\n+        logger.info(f\"Terminating A2A process to update agent name to {new_agent_name}\")\n+        a2a_process.terminate()\n+        try:\n+            a2a_process.wait(timeout=5)\n+        except subprocess.TimeoutExpired:\n+            logger.warning(\"A2A process did not terminate gracefully, forcing...\")\n+            a2a_process.kill()\n+\n+        # Start new process with updated config\n+        start_a2a_subprocess(a2a_config)\n+        return True\n+\n+    def start_a2a_subprocess(a2a_config):\n+        \"\"\"\n+        Start the A2A subprocess with the given configuration.\n+\n+        Args:\n+            a2a_config (dict): A2A configuration dictionary\n+\n+        Returns:\n+            subprocess.Popen: The subprocess instance\n+        \"\"\"\n+        nonlocal a2a_process\n+\n+        try:\n+            # Get the absolute path to the run_a2a.py script using pathlib for better path handling\n+            a2a_script_path = (\n+                Path(__file__).parent.parent.joinpath(\"a2a\", \"run_a2a.py\").resolve()\n+            )\n+            if not a2a_script_path.exists():\n+                raise FileNotFoundError(\n+                    f\"A2A script not found at expected path: {a2a_script_path}\"\n+                )\n+\n+            logger.info(f\"Using A2A script at: {a2a_script_path}\")\n+\n+            # Use current Python interpreter to run the script\n+            cmd = [sys.executable, str(a2a_script_path)]\n+\n+            # Set up environment for the subprocess\n+            env = os.environ.copy()  # Copy current environment\n+\n+            # Ensure A2A-specific environment variables are set based on config\n+            if a2a_enabled and \"MINDSDB_A2A_ENABLED\" not in env:\n+                env[\"MINDSDB_A2A_ENABLED\"] = \"true\"\n+\n+            # Support both environment variables and command-line arguments\n+            # First check environment variables, then fall back to config\n+            host = os.environ.get(\"MINDSDB_A2A_HOST\")\n+            if host:\n+                # Environment variable takes precedence\n+                cmd.extend([\"--host\", host])\n+            elif a2a_config.get(\"host\"):\n+                # Config value used as command-line argument\n+                cmd.extend([\"--host\", a2a_config[\"host\"]])\n+\n+            port = os.environ.get(\"MINDSDB_A2A_PORT\")\n+            if port:\n+                cmd.extend([\"--port\", port])\n+            elif a2a_config.get(\"port\"):\n+                cmd.extend([\"--port\", str(a2a_config[\"port\"])])\n+\n+            mindsdb_host = os.environ.get(\"MINDSDB_HOST\")\n+            if mindsdb_host:\n+                cmd.extend([\"--mindsdb-host\", mindsdb_host])\n+            elif a2a_config.get(\"mindsdb_host\"):\n+                cmd.extend([\"--mindsdb-host\", a2a_config[\"mindsdb_host\"]])\n+\n+            mindsdb_port = os.environ.get(\"MINDSDB_PORT\")\n+            if mindsdb_port:\n+                cmd.extend([\"--mindsdb-port\", mindsdb_port])\n+            elif a2a_config.get(\"mindsdb_port\"):\n+                cmd.extend([\"--mindsdb-port\", str(a2a_config[\"mindsdb_port\"])])\n+\n+            agent_name = os.environ.get(\"MINDSDB_AGENT_NAME\")\n+            if agent_name:\n+                cmd.extend([\"--agent-name\", agent_name])\n+            elif a2a_config.get(\"agent_name\"):\n+                cmd.extend([\"--agent-name\", a2a_config[\"agent_name\"]])\n+\n+            project_name = os.environ.get(\"MINDSDB_PROJECT_NAME\")\n+            if project_name:\n+                cmd.extend([\"--project-name\", project_name])\n+            elif a2a_config.get(\"project_name\"):\n+                cmd.extend([\"--project-name\", a2a_config[\"project_name\"]])\n+\n+            # Add log level if specified\n+            log_level = os.environ.get(\"MINDSDB_LOG_LEVEL\")\n+            if log_level:\n+                cmd.extend([\"--log-level\", log_level])\n+            elif a2a_config.get(\"log_level\"):\n+                cmd.extend([\"--log-level\", a2a_config[\"log_level\"]])\n+\n+            # Start the subprocess with explicit environment\n+            logger.info(f\"Starting A2A subprocess with command: {' '.join(cmd)}\")\n+            a2a_process = subprocess.Popen(\n+                cmd,\n+                stdout=subprocess.PIPE,\n+                stderr=subprocess.PIPE,\n+                text=True,\n+                env=env,  # Pass the environment explicitly\n+            )\n+\n+            # Log subprocess PID for management\n+            logger.info(f\"A2A subprocess started with PID: {a2a_process.pid}\")\n+\n+            return a2a_process\n+\n+        except Exception as e:\n+            logger.error(f\"Failed to start A2A subprocess: {e}\")\n+            return None\n+\n+    if a2a_enabled:\n+        logger.info(\"Starting A2A API as a subprocess...\")\n+        a2a_config = config.get(\"a2a\", {})\n+        a2a_process = start_a2a_subprocess(a2a_config)\n+\n+        # Start a thread to monitor the subprocess output\n+        def monitor_a2a_output():\n+            for line in a2a_process.stdout:\n+                logger.info(f\"A2A: {line.strip()}\")\n+            for line in a2a_process.stderr:\n+                logger.error(f\"A2A Error: {line.strip()}\")\n+\n+        threading.Thread(target=monitor_a2a_output, daemon=True).start()\n+\n+        # Register cleanup function to terminate the subprocess when MindsDB exits\n+        def cleanup_a2a_subprocess():\n+            if a2a_process and a2a_process.poll() is None:\n+                logger.info(\n+                    f\"Terminating A2A API subprocess (PID: {a2a_process.pid})...\"\n+                )\n+                try:\n+                    # Try graceful termination first\n+                    a2a_process.terminate()\n+                    # Wait for a short time for the process to terminate\n+                    try:\n+                        a2a_process.wait(timeout=5)\n+                    except subprocess.TimeoutExpired:\n+                        # If it doesn't terminate gracefully, force kill it\n+                        logger.warning(\n+                            \"A2A API subprocess did not terminate gracefully, forcing kill...\"\n+                        )\n+                        a2a_process.kill()\n+                except Exception as e:\n+                    logger.error(f\"Error terminating A2A API subprocess: {e}\")\n+\n+        # Register the cleanup function to be called when the program exits\n+        atexit.register(cleanup_a2a_subprocess)\n+\n+    # Wait for static initialization.\n+    if not no_studio and init_static_thread is not None:\n+        init_static_thread.join()\n+\n     app, api = initialize_flask(config, init_static_thread, no_studio)\n     Compress(app)\n+\n+    # Add A2A agent update endpoint if A2A is enabled\n+    if a2a_enabled:\n+\n+        @app.route(\"/api/a2a/update_agent\", methods=[\"POST\"])\n+        def api_update_a2a_agent():\n+            \"\"\"\n+            Update the A2A agent configuration.\n+\n+            Expected JSON payload:\n+            {\n+                \"agent_name\": \"new_agent_name\",\n+                \"project_name\": \"optional_project_name\"  # Optional\n+            }\n+            \"\"\"\n+            try:\n+                data = request.json\n+                if not data or \"agent_name\" not in data:\n+                    abort(\n+                        HTTPStatus.BAD_REQUEST, \"Missing required parameter: agent_name\"\n+                    )\n+\n+                new_agent_name = data[\"agent_name\"]\n+                new_project_name = data.get(\"project_name\")  # Optional\n+\n+                # Update the A2A agent\n+                success = update_a2a_agent(new_agent_name, new_project_name)\n+\n+                if success:\n+                    return {\n+                        \"status\": \"success\",\n+                        \"agent_name\": new_agent_name,\n+                        \"project_name\": new_project_name\n+                        or config.get(\"a2a\", {}).get(\"project_name\", \"mindsdb\"),\n+                    }",
    "comment": "## Reflected server-side cross-site scripting\n\nCross-site scripting vulnerability due to a [user-provided value](1).\n\n[Show more details](https://github.com/mindsdb/mindsdb/security/code-scanning/101)",
    "line_number": 459,
    "enriched": "File: mindsdb/api/http/initialize.py\nCode: @@ -204,37 +223,264 @@\n \n \n def initialize_app(config, no_studio):\n-    static_root = config['paths']['static']\n+    static_root = config[\"paths\"][\"static\"]\n     logger.debug(f\"Static route: {static_root}\")\n-    gui_exists = Path(static_root).joinpath('index.html').is_file()\n+    gui_exists = Path(static_root).joinpath(\"index.html\").is_file()\n     logger.debug(f\"Does GUI already exist.. {'YES' if gui_exists else 'NO'}\")\n     init_static_thread = None\n-    if (\n-        no_studio is False\n-        and (\n-            config['gui']['autoupdate'] is True\n-            or gui_exists is False\n-        )\n+    if no_studio is False and (\n+        config[\"gui\"][\"autoupdate\"] is True or gui_exists is False\n     ):\n-        init_static_thread = threading.Thread(target=initialize_static, name='initialize_static')\n+        init_static_thread = threading.Thread(\n+            target=initialize_static, name=\"initialize_static\"\n+        )\n         init_static_thread.start()\n \n+    # Start A2A API as a subprocess if enabled in config or environment\n+    a2a_process = None\n+    a2a_enabled = config.get(\"a2a\", {}).get(\"enabled\", False) or os.environ.get(\n+        \"MINDSDB_A2A_ENABLED\", \"\"\n+    ).lower() in (\"true\", \"1\", \"yes\")\n+\n+    def update_a2a_agent(new_agent_name, new_project_name=None):\n+        \"\"\"\n+        Update the A2A agent configuration and restart the subprocess if needed.\n+\n+        Args:\n+            new_agent_name (str): New agent name to use\n+            new_project_name (str, optional): New project name to use\n+        \"\"\"\n+        nonlocal a2a_process\n+\n+        if not a2a_enabled or a2a_process is None:\n+            logger.warning(\"A2A is not enabled or not running, can't update agent\")\n+            return False\n+\n+        # Update config\n+        a2a_config = config.get(\"a2a\", {}).copy()\n+        a2a_config[\"agent_name\"] = new_agent_name\n+        if new_project_name:\n+            a2a_config[\"project_name\"] = new_project_name\n+\n+        # Store updated config\n+        config.update({\"a2a\": a2a_config})\n+\n+        # Terminate existing process\n+        logger.info(f\"Terminating A2A process to update agent name to {new_agent_name}\")\n+        a2a_process.terminate()\n+        try:\n+            a2a_process.wait(timeout=5)\n+        except subprocess.TimeoutExpired:\n+            logger.warning(\"A2A process did not terminate gracefully, forcing...\")\n+            a2a_process.kill()\n+\n+        # Start new process with updated config\n+        start_a2a_subprocess(a2a_config)\n+        return True\n+\n+    def start_a2a_subprocess(a2a_config):\n+        \"\"\"\n+        Start the A2A subprocess with the given configuration.\n+\n+        Args:\n+            a2a_config (dict): A2A configuration dictionary\n+\n+        Returns:\n+            subprocess.Popen: The subprocess instance\n+        \"\"\"\n+        nonlocal a2a_process\n+\n+        try:\n+            # Get the absolute path to the run_a2a.py script using pathlib for better path handling\n+            a2a_script_path = (\n+                Path(__file__).parent.parent.joinpath(\"a2a\", \"run_a2a.py\").resolve()\n+            )\n+            if not a2a_script_path.exists():\n+                raise FileNotFoundError(\n+                    f\"A2A script not found at expected path: {a2a_script_path}\"\n+                )\n+\n+            logger.info(f\"Using A2A script at: {a2a_script_path}\")\n+\n+            # Use current Python interpreter to run the script\n+            cmd = [sys.executable, str(a2a_script_path)]\n+\n+            # Set up environment for the subprocess\n+            env = os.environ.copy()  # Copy current environment\n+\n+            # Ensure A2A-specific environment variables are set based on config\n+            if a2a_enabled and \"MINDSDB_A2A_ENABLED\" not in env:\n+                env[\"MINDSDB_A2A_ENABLED\"] = \"true\"\n+\n+            # Support both environment variables and command-line arguments\n+            # First check environment variables, then fall back to config\n+            host = os.environ.get(\"MINDSDB_A2A_HOST\")\n+            if host:\n+                # Environment variable takes precedence\n+                cmd.extend([\"--host\", host])\n+            elif a2a_config.get(\"host\"):\n+                # Config value used as command-line argument\n+                cmd.extend([\"--host\", a2a_config[\"host\"]])\n+\n+            port = os.environ.get(\"MINDSDB_A2A_PORT\")\n+            if port:\n+                cmd.extend([\"--port\", port])\n+            elif a2a_config.get(\"port\"):\n+                cmd.extend([\"--port\", str(a2a_config[\"port\"])])\n+\n+            mindsdb_host = os.environ.get(\"MINDSDB_HOST\")\n+            if mindsdb_host:\n+                cmd.extend([\"--mindsdb-host\", mindsdb_host])\n+            elif a2a_config.get(\"mindsdb_host\"):\n+                cmd.extend([\"--mindsdb-host\", a2a_config[\"mindsdb_host\"]])\n+\n+            mindsdb_port = os.environ.get(\"MINDSDB_PORT\")\n+            if mindsdb_port:\n+                cmd.extend([\"--mindsdb-port\", mindsdb_port])\n+            elif a2a_config.get(\"mindsdb_port\"):\n+                cmd.extend([\"--mindsdb-port\", str(a2a_config[\"mindsdb_port\"])])\n+\n+            agent_name = os.environ.get(\"MINDSDB_AGENT_NAME\")\n+            if agent_name:\n+                cmd.extend([\"--agent-name\", agent_name])\n+            elif a2a_config.get(\"agent_name\"):\n+                cmd.extend([\"--agent-name\", a2a_config[\"agent_name\"]])\n+\n+            project_name = os.environ.get(\"MINDSDB_PROJECT_NAME\")\n+            if project_name:\n+                cmd.extend([\"--project-name\", project_name])\n+            elif a2a_config.get(\"project_name\"):\n+                cmd.extend([\"--project-name\", a2a_config[\"project_name\"]])\n+\n+            # Add log level if specified\n+            log_level = os.environ.get(\"MINDSDB_LOG_LEVEL\")\n+            if log_level:\n+                cmd.extend([\"--log-level\", log_level])\n+            elif a2a_config.get(\"log_level\"):\n+                cmd.extend([\"--log-level\", a2a_config[\"log_level\"]])\n+\n+            # Start the subprocess with explicit environment\n+            logger.info(f\"Starting A2A subprocess with command: {' '.join(cmd)}\")\n+            a2a_process = subprocess.Popen(\n+                cmd,\n+                stdout=subprocess.PIPE,\n+                stderr=subprocess.PIPE,\n+                text=True,\n+                env=env,  # Pass the environment explicitly\n+            )\n+\n+            # Log subprocess PID for management\n+            logger.info(f\"A2A subprocess started with PID: {a2a_process.pid}\")\n+\n+            return a2a_process\n+\n+        except Exception as e:\n+            logger.error(f\"Failed to start A2A subprocess: {e}\")\n+            return None\n+\n+    if a2a_enabled:\n+        logger.info(\"Starting A2A API as a subprocess...\")\n+        a2a_config = config.get(\"a2a\", {})\n+        a2a_process = start_a2a_subprocess(a2a_config)\n+\n+        # Start a thread to monitor the subprocess output\n+        def monitor_a2a_output():\n+            for line in a2a_process.stdout:\n+                logger.info(f\"A2A: {line.strip()}\")\n+            for line in a2a_process.stderr:\n+                logger.error(f\"A2A Error: {line.strip()}\")\n+\n+        threading.Thread(target=monitor_a2a_output, daemon=True).start()\n+\n+        # Register cleanup function to terminate the subprocess when MindsDB exits\n+        def cleanup_a2a_subprocess():\n+            if a2a_process and a2a_process.poll() is None:\n+                logger.info(\n+                    f\"Terminating A2A API subprocess (PID: {a2a_process.pid})...\"\n+                )\n+                try:\n+                    # Try graceful termination first\n+                    a2a_process.terminate()\n+                    # Wait for a short time for the process to terminate\n+                    try:\n+                        a2a_process.wait(timeout=5)\n+                    except subprocess.TimeoutExpired:\n+                        # If it doesn't terminate gracefully, force kill it\n+                        logger.warning(\n+                            \"A2A API subprocess did not terminate gracefully, forcing kill...\"\n+                        )\n+                        a2a_process.kill()\n+                except Exception as e:\n+                    logger.error(f\"Error terminating A2A API subprocess: {e}\")\n+\n+        # Register the cleanup function to be called when the program exits\n+        atexit.register(cleanup_a2a_subprocess)\n+\n+    # Wait for static initialization.\n+    if not no_studio and init_static_thread is not None:\n+        init_static_thread.join()\n+\n     app, api = initialize_flask(config, init_static_thread, no_studio)\n     Compress(app)\n+\n+    # Add A2A agent update endpoint if A2A is enabled\n+    if a2a_enabled:\n+\n+        @app.route(\"/api/a2a/update_agent\", methods=[\"POST\"])\n+        def api_update_a2a_agent():\n+            \"\"\"\n+            Update the A2A agent configuration.\n+\n+            Expected JSON payload:\n+            {\n+                \"agent_name\": \"new_agent_name\",\n+                \"project_name\": \"optional_project_name\"  # Optional\n+            }\n+            \"\"\"\n+            try:\n+                data = request.json\n+                if not data or \"agent_name\" not in data:\n+                    abort(\n+                        HTTPStatus.BAD_REQUEST, \"Missing required parameter: agent_name\"\n+                    )\n+\n+                new_agent_name = data[\"agent_name\"]\n+                new_project_name = data.get(\"project_name\")  # Optional\n+\n+                # Update the A2A agent\n+                success = update_a2a_agent(new_agent_name, new_project_name)\n+\n+                if success:\n+                    return {\n+                        \"status\": \"success\",\n+                        \"agent_name\": new_agent_name,\n+                        \"project_name\": new_project_name\n+                        or config.get(\"a2a\", {}).get(\"project_name\", \"mindsdb\"),\n+                    }\nComment: ## Reflected server-side cross-site scripting\n\nCross-site scripting vulnerability due to a [user-provided value](1).\n\n[Show more details](https://github.com/mindsdb/mindsdb/security/code-scanning/101)",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "mindsdb/api/http/initialize.py",
    "pr_number": 10852,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2095492014,
    "comment_created_at": "2025-05-19T11:25:32Z"
  },
  {
    "code": "@@ -112,6 +114,15 @@ def get_tools(self, prefix='') -> List[BaseTool]:\n             description=mindsdb_sql_parser_tool_description\n         )\n \n+        sql_tools = [\n+            query_sql_database_tool,\n+            info_sql_database_tool,\n+            list_sql_database_tool,\n+            mindsdb_sql_parser_tool,\n+        ]",
    "comment": "**correctness**: `sql_tools` list includes `mindsdb_sql_parser_tool` after `list_sql_database_tool`, but the parser tool is supposed to be run before executing a query, so it should precede `query_sql_database_tool` in the returned list for correct tool ordering.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/interfaces/skills/custom/text2sql/mindsdb_sql_toolkit.py, lines 117-122, the `sql_tools` list orders `mindsdb_sql_parser_tool` after the query and info tools, but the parser tool should be run before executing a query. Please reorder the list so that `mindsdb_sql_parser_tool` comes first, followed by the query, info, and list tools.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n        sql_tools = [\n            mindsdb_sql_parser_tool,\n            query_sql_database_tool,\n            info_sql_database_tool,\n            list_sql_database_tool,\n        ]\n```\n</details>\n<!-- suggestion_end -->\n",
    "line_number": 122,
    "enriched": "File: mindsdb/interfaces/skills/custom/text2sql/mindsdb_sql_toolkit.py\nCode: @@ -112,6 +114,15 @@ def get_tools(self, prefix='') -> List[BaseTool]:\n             description=mindsdb_sql_parser_tool_description\n         )\n \n+        sql_tools = [\n+            query_sql_database_tool,\n+            info_sql_database_tool,\n+            list_sql_database_tool,\n+            mindsdb_sql_parser_tool,\n+        ]\nComment: **correctness**: `sql_tools` list includes `mindsdb_sql_parser_tool` after `list_sql_database_tool`, but the parser tool is supposed to be run before executing a query, so it should precede `query_sql_database_tool` in the returned list for correct tool ordering.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/interfaces/skills/custom/text2sql/mindsdb_sql_toolkit.py, lines 117-122, the `sql_tools` list orders `mindsdb_sql_parser_tool` after the query and info tools, but the parser tool should be run before executing a query. Please reorder the list so that `mindsdb_sql_parser_tool` comes first, followed by the query, info, and list tools.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n        sql_tools = [\n            mindsdb_sql_parser_tool,\n            query_sql_database_tool,\n            info_sql_database_tool,\n            list_sql_database_tool,\n        ]\n```\n</details>\n<!-- suggestion_end -->\n",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "mindsdb/interfaces/skills/custom/text2sql/mindsdb_sql_toolkit.py",
    "pr_number": 11123,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2155177262,
    "comment_created_at": "2025-06-18T17:47:20Z"
  },
  {
    "code": "@@ -185,4 +191,7 @@\n     \"\"\"\n     res = client.models.list()\n \n+    if \"api.writer.com\" in str(client.base_url.netloc).lower():",
    "comment": "## Incomplete URL substring sanitization\n\nThe string [api.writer.com](1) may be at an arbitrary position in the sanitized URL.\n\n[Show more details](https://github.com/mindsdb/mindsdb/security/code-scanning/124)",
    "line_number": 194,
    "enriched": "File: mindsdb/integrations/handlers/openai_handler/helpers.py\nCode: @@ -185,4 +191,7 @@\n     \"\"\"\n     res = client.models.list()\n \n+    if \"api.writer.com\" in str(client.base_url.netloc).lower():\nComment: ## Incomplete URL substring sanitization\n\nThe string [api.writer.com](1) may be at an arbitrary position in the sanitized URL.\n\n[Show more details](https://github.com/mindsdb/mindsdb/security/code-scanning/124)",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/openai_handler/helpers.py",
    "pr_number": 11804,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2472258169,
    "comment_created_at": "2025-10-29T09:20:15Z"
  },
  {
    "code": "@@ -0,0 +1,273 @@\n+import os\n+import typing\n+\n+from mindsdb.utilities import log\n+from langfuse import Langfuse\n+from langfuse.client import StatefulSpanClient\n+from langfuse.api.resources.commons.errors.not_found_error import NotFoundError as TraceNotFoundError\n+\n+logger = log.getLogger(__name__)\n+\n+# Define Langfuse public key.\n+LANGFUSE_PUBLIC_KEY = os.getenv(\"LANGFUSE_PUBLIC_KEY\", \"langfuse_public_key\")\n+\n+# Define Langfuse secret key.\n+LANGFUSE_SECRET_KEY = os.getenv(\"LANGFUSE_SECRET_KEY\", \"langfuse_secret_key\")\n+\n+# Define Langfuse host.\n+LANGFUSE_HOST = os.getenv(\"LANGFUSE_HOST\", \"http://localhost:3000\")\n+\n+# Define Langfuse environment.\n+LANGFUSE_ENVIRONMENT = os.getenv(\"LANGFUSE_ENVIRONMENT\", \"local\")\n+\n+# Define Langfuse release.\n+LANGFUSE_RELEASE = os.getenv(\"LANGFUSE_RELEASE\", \"local\")\n+\n+# Define Langfuse debug mode.\n+LANGFUSE_DEBUG = os.getenv(\"LANGFUSE_DEBUG\", \"false\").lower() == \"true\"\n+\n+# Define Langfuse timeout.\n+LANGFUSE_TIMEOUT = int(os.getenv(\"LANGFUSE_TIMEOUT\", 10))\n+\n+# Define Langfuse sample rate.\n+LANGFUSE_SAMPLE_RATE = float(os.getenv(\"LANGFUSE_SAMPLE_RATE\", 1.0))\n+\n+# Define if Langfuse is disabled.\n+LANGFUSE_DISABLED = os.getenv(\"LANGFUSE_DISABLED\", \"false\").lower() == \"true\" or LANGFUSE_ENVIRONMENT == \"local\"\n+LANGFUSE_FORCE_RUN = os.getenv(\"LANGFUSE_FORCE_RUN\", \"false\").lower() == \"true\"\n+\n+\n+class LangfuseClientWrapper:\n+    \"\"\"\n+    Langfuse client wrapper. Defines Langfuse client configuration and initializes Langfuse client.\n+    \"\"\"\n+\n+    def __init__(self,\n+                 public_key: str = LANGFUSE_PUBLIC_KEY,\n+                 secret_key: str = LANGFUSE_SECRET_KEY,\n+                 host: str = LANGFUSE_HOST,\n+                 environment: str = LANGFUSE_ENVIRONMENT,\n+                 release: str = LANGFUSE_RELEASE,\n+                 debug: bool = LANGFUSE_DEBUG,\n+                 timeout: int = LANGFUSE_TIMEOUT,\n+                 sample_rate: float = LANGFUSE_SAMPLE_RATE,\n+                 disable: bool = LANGFUSE_DISABLED,\n+                 force_run: bool = LANGFUSE_FORCE_RUN):\n+        \"\"\"\n+        Initialize Langfuse client.\n+\n+        Args:\n+            public_key (str): Langfuse public key.\n+            secret_key (str): Langfuse secret key.\n+            host (str): Langfuse host.\n+            release (str): Langfuse release.\n+            timeout (int): Langfuse timeout.\n+            sample_rate (float): Langfuse sample rate.\n+        \"\"\"\n+\n+        self.metadata = None\n+        self.public_key = public_key\n+        self.secret_key = secret_key\n+        self.host = host\n+        self.environment = environment\n+        self.release = release\n+        self.debug = debug\n+        self.timeout = timeout\n+        self.sample_rate = sample_rate\n+        self.disable = disable\n+        self.force_run = force_run\n+\n+        self.client = None\n+        self.trace = None\n+        self.metadata = None\n+        self.tags = None\n+\n+        # Check if Langfuse is disabled.\n+        if LANGFUSE_DISABLED and not LANGFUSE_FORCE_RUN:\n+            logger.info(\"Langfuse is disabled.\")\n+            return\n+\n+        logger.info(\"Langfuse enabled\")\n+        logger.debug(f\"LANGFUSE_PUBLIC_KEY: {LANGFUSE_PUBLIC_KEY}\")\n+        logger.debug(f\"LANGFUSE_SECRET_KEY: {LANGFUSE_SECRET_KEY}\")",
    "comment": "## Clear-text logging of sensitive information\n\nThis expression logs [sensitive data (secret)](1) as clear text.\n\n[Show more details](https://github.com/mindsdb/mindsdb/security/code-scanning/39)",
    "line_number": 92,
    "enriched": "File: mindsdb/utilities/langfuse.py\nCode: @@ -0,0 +1,273 @@\n+import os\n+import typing\n+\n+from mindsdb.utilities import log\n+from langfuse import Langfuse\n+from langfuse.client import StatefulSpanClient\n+from langfuse.api.resources.commons.errors.not_found_error import NotFoundError as TraceNotFoundError\n+\n+logger = log.getLogger(__name__)\n+\n+# Define Langfuse public key.\n+LANGFUSE_PUBLIC_KEY = os.getenv(\"LANGFUSE_PUBLIC_KEY\", \"langfuse_public_key\")\n+\n+# Define Langfuse secret key.\n+LANGFUSE_SECRET_KEY = os.getenv(\"LANGFUSE_SECRET_KEY\", \"langfuse_secret_key\")\n+\n+# Define Langfuse host.\n+LANGFUSE_HOST = os.getenv(\"LANGFUSE_HOST\", \"http://localhost:3000\")\n+\n+# Define Langfuse environment.\n+LANGFUSE_ENVIRONMENT = os.getenv(\"LANGFUSE_ENVIRONMENT\", \"local\")\n+\n+# Define Langfuse release.\n+LANGFUSE_RELEASE = os.getenv(\"LANGFUSE_RELEASE\", \"local\")\n+\n+# Define Langfuse debug mode.\n+LANGFUSE_DEBUG = os.getenv(\"LANGFUSE_DEBUG\", \"false\").lower() == \"true\"\n+\n+# Define Langfuse timeout.\n+LANGFUSE_TIMEOUT = int(os.getenv(\"LANGFUSE_TIMEOUT\", 10))\n+\n+# Define Langfuse sample rate.\n+LANGFUSE_SAMPLE_RATE = float(os.getenv(\"LANGFUSE_SAMPLE_RATE\", 1.0))\n+\n+# Define if Langfuse is disabled.\n+LANGFUSE_DISABLED = os.getenv(\"LANGFUSE_DISABLED\", \"false\").lower() == \"true\" or LANGFUSE_ENVIRONMENT == \"local\"\n+LANGFUSE_FORCE_RUN = os.getenv(\"LANGFUSE_FORCE_RUN\", \"false\").lower() == \"true\"\n+\n+\n+class LangfuseClientWrapper:\n+    \"\"\"\n+    Langfuse client wrapper. Defines Langfuse client configuration and initializes Langfuse client.\n+    \"\"\"\n+\n+    def __init__(self,\n+                 public_key: str = LANGFUSE_PUBLIC_KEY,\n+                 secret_key: str = LANGFUSE_SECRET_KEY,\n+                 host: str = LANGFUSE_HOST,\n+                 environment: str = LANGFUSE_ENVIRONMENT,\n+                 release: str = LANGFUSE_RELEASE,\n+                 debug: bool = LANGFUSE_DEBUG,\n+                 timeout: int = LANGFUSE_TIMEOUT,\n+                 sample_rate: float = LANGFUSE_SAMPLE_RATE,\n+                 disable: bool = LANGFUSE_DISABLED,\n+                 force_run: bool = LANGFUSE_FORCE_RUN):\n+        \"\"\"\n+        Initialize Langfuse client.\n+\n+        Args:\n+            public_key (str): Langfuse public key.\n+            secret_key (str): Langfuse secret key.\n+            host (str): Langfuse host.\n+            release (str): Langfuse release.\n+            timeout (int): Langfuse timeout.\n+            sample_rate (float): Langfuse sample rate.\n+        \"\"\"\n+\n+        self.metadata = None\n+        self.public_key = public_key\n+        self.secret_key = secret_key\n+        self.host = host\n+        self.environment = environment\n+        self.release = release\n+        self.debug = debug\n+        self.timeout = timeout\n+        self.sample_rate = sample_rate\n+        self.disable = disable\n+        self.force_run = force_run\n+\n+        self.client = None\n+        self.trace = None\n+        self.metadata = None\n+        self.tags = None\n+\n+        # Check if Langfuse is disabled.\n+        if LANGFUSE_DISABLED and not LANGFUSE_FORCE_RUN:\n+            logger.info(\"Langfuse is disabled.\")\n+            return\n+\n+        logger.info(\"Langfuse enabled\")\n+        logger.debug(f\"LANGFUSE_PUBLIC_KEY: {LANGFUSE_PUBLIC_KEY}\")\n+        logger.debug(f\"LANGFUSE_SECRET_KEY: {LANGFUSE_SECRET_KEY}\")\nComment: ## Clear-text logging of sensitive information\n\nThis expression logs [sensitive data (secret)](1) as clear text.\n\n[Show more details](https://github.com/mindsdb/mindsdb/security/code-scanning/39)",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "mindsdb/utilities/langfuse.py",
    "pr_number": 10262,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1877955062,
    "comment_created_at": "2024-12-10T11:50:06Z"
  },
  {
    "code": "@@ -51,18 +51,32 @@ def connect(self):\n         Returns:\n             Connection Object\n         \"\"\"\n-        if self.is_connected is True:\n+        if self.is_connected:\n             return self.connection\n \n-        url = \"http://{0}:{1}@{2}:{3}\".format(\n-            self.user, self.password, self.host, self.port\n-        )\n+\n+        if self.host.startswith('localhost') or self.host == '127.0.0.1':\n+            # Local connection\n+            url = \"http://{0}:{1}@{2}:{3}\".format(\n+                self.user, self.password, self.host, self.port\n+            )\n+        else:\n+            # Cloud connection \n+            url = \"https://{0}:{1}@{2}:{3}\".format(\n+                self.user, self.password, self.host, self.port\n+            )\n+\n         try:\n-            self.connection = db.connect(url)\n+            if 'localhost' in self.host or self.host == '127.0.0.1':\n+                # Connect without SSL for local\n+                self.connection = db.connect(url, timeout=30)\n+            else:\n+                # Connect with SSL for cloud\n+                self.connection = db.connect(url, verify_ssl_cert=True, timeout=30)\n ",
    "comment": "It looks like the same condition is being used twice here; once to build the URL and create the connection via the SDK. Let's do it once, within the same if-else block.",
    "line_number": 76,
    "enriched": "File: mindsdb/integrations/handlers/crate_handler/crate_handler.py\nCode: @@ -51,18 +51,32 @@ def connect(self):\n         Returns:\n             Connection Object\n         \"\"\"\n-        if self.is_connected is True:\n+        if self.is_connected:\n             return self.connection\n \n-        url = \"http://{0}:{1}@{2}:{3}\".format(\n-            self.user, self.password, self.host, self.port\n-        )\n+\n+        if self.host.startswith('localhost') or self.host == '127.0.0.1':\n+            # Local connection\n+            url = \"http://{0}:{1}@{2}:{3}\".format(\n+                self.user, self.password, self.host, self.port\n+            )\n+        else:\n+            # Cloud connection \n+            url = \"https://{0}:{1}@{2}:{3}\".format(\n+                self.user, self.password, self.host, self.port\n+            )\n+\n         try:\n-            self.connection = db.connect(url)\n+            if 'localhost' in self.host or self.host == '127.0.0.1':\n+                # Connect without SSL for local\n+                self.connection = db.connect(url, timeout=30)\n+            else:\n+                # Connect with SSL for cloud\n+                self.connection = db.connect(url, verify_ssl_cert=True, timeout=30)\n \nComment: It looks like the same condition is being used twice here; once to build the URL and create the connection via the SDK. Let's do it once, within the same if-else block.",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/crate_handler/crate_handler.py",
    "pr_number": 9938,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1808137757,
    "comment_created_at": "2024-10-21T05:50:08Z"
  },
  {
    "code": "@@ -0,0 +1,227 @@\n+import os\n+import ast",
    "comment": "@ElinaKapetanaki Please change this import to `from mindsdb_sql.parser import ast` and try again?",
    "line_number": 2,
    "enriched": "File: mindsdb/integrations/handlers/zotero_handler/zotero_handler.py\nCode: @@ -0,0 +1,227 @@\n+import os\n+import ast\nComment: @ElinaKapetanaki Please change this import to `from mindsdb_sql.parser import ast` and try again?",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/zotero_handler/zotero_handler.py",
    "pr_number": 9084,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1570422772,
    "comment_created_at": "2024-04-18T10:10:25Z"
  },
  {
    "code": "@@ -42,4 +43,4 @@\n def save_creds_to_file(creds, file_path):\n     with open(file_path, 'w') as token:\n         data = credentials_to_dict(creds)\n-        token.write(json.dumps(data))\n+        token.write(encrypt(json.dumps(data)))",
    "comment": "## Clear-text storage of sensitive information\n\nThis expression stores [sensitive data (secret)](1) as clear text.\n\n[Show more details](https://github.com/mindsdb/mindsdb/security/code-scanning/32)",
    "line_number": 46,
    "enriched": "File: mindsdb/integrations/handlers/gmail_handler/utils.py\nCode: @@ -42,4 +43,4 @@\n def save_creds_to_file(creds, file_path):\n     with open(file_path, 'w') as token:\n         data = credentials_to_dict(creds)\n-        token.write(json.dumps(data))\n+        token.write(encrypt(json.dumps(data)))\nComment: ## Clear-text storage of sensitive information\n\nThis expression stores [sensitive data (secret)](1) as clear text.\n\n[Show more details](https://github.com/mindsdb/mindsdb/security/code-scanning/32)",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/gmail_handler/utils.py",
    "pr_number": 9725,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1761264802,
    "comment_created_at": "2024-09-16T14:23:01Z"
  },
  {
    "code": "@@ -58,11 +56,12 @@ def connect(self) -> StatusResponse:\n         if self.is_connected is True:\n             return self.connection\n         \n-        self.connection = Jira(\n-            url=self.connection_data['jira_url'],\n-            username=self.connection_data['user_id'],\n-            password=self.connection_data['api_key'],\n-            cloud=True)\n+        s = requests.Session()\n+        s.headers['Authorization'] =  f\"Bearer {self.connection_data['jira_api_token']}\"\n+\n+        self.connection = Jira(url= self.connection_data['jira_url'], session=s)\n+        self.is_connected = True",
    "comment": "Duplicate line. `self.is_connected = True` is set below too",
    "line_number": 62,
    "enriched": "File: mindsdb/integrations/handlers/jira_handler/jira_handler.py\nCode: @@ -58,11 +56,12 @@ def connect(self) -> StatusResponse:\n         if self.is_connected is True:\n             return self.connection\n         \n-        self.connection = Jira(\n-            url=self.connection_data['jira_url'],\n-            username=self.connection_data['user_id'],\n-            password=self.connection_data['api_key'],\n-            cloud=True)\n+        s = requests.Session()\n+        s.headers['Authorization'] =  f\"Bearer {self.connection_data['jira_api_token']}\"\n+\n+        self.connection = Jira(url= self.connection_data['jira_url'], session=s)\n+        self.is_connected = True\nComment: Duplicate line. `self.is_connected = True` is set below too",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/jira_handler/jira_handler.py",
    "pr_number": 5737,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1176847095,
    "comment_created_at": "2023-04-25T17:50:56Z"
  },
  {
    "code": "@@ -0,0 +1,68 @@\n+\n+# A2A specific imports\n+from mindsdb.api.a2a.common.types import (\n+    AgentCard,\n+    AgentCapabilities,\n+    AgentSkill,\n+)\n+from mindsdb.api.a2a.common.server.server import A2AServer\n+from mindsdb.api.a2a.task_manager import AgentTaskManager\n+from mindsdb.api.a2a.agent import MindsDBAgent\n+\n+def get_a2a_server(\n+    host: str = \"0.0.0.0\",\n+    mindsdb_host: str = \"127.0.0.1\",\n+    mindsdb_port: int = 47334,\n+    project_name: str = \"mindsdb\",\n+):\n+    # Prepare A2A artefacts (agent card & task-manager)\n+    capabilities = AgentCapabilities(streaming=True)\n+    skill = AgentSkill(\n+        id=\"mindsdb_query\",\n+        name=\"MindsDB Query\",\n+        description=\"Executes natural-language queries via MindsDB agents.\",\n+        tags=[\"database\", \"mindsdb\", \"query\", \"analytics\"],\n+        examples=[\n+            \"What trends exist in my sales data?\",\n+            \"Generate insights from the support tickets dataset.\",\n+        ],\n+        inputModes=MindsDBAgent.SUPPORTED_CONTENT_TYPES,\n+        outputModes=MindsDBAgent.SUPPORTED_CONTENT_TYPES,\n+    )\n+\n+    agent_card = AgentCard(\n+        name=\"MindsDB Agent Connector\",\n+        description=(f\"A2A connector that proxies requests to MindsDB agents in project '{project_name}'.\"),\n+        url=f\"http://{host}:{mindsdb_port}\",\n+        version=\"1.0.0\",\n+        defaultInputModes=MindsDBAgent.SUPPORTED_CONTENT_TYPES,\n+        defaultOutputModes=MindsDBAgent.SUPPORTED_CONTENT_TYPES,\n+        capabilities=capabilities,\n+        skills=[skill],\n+    )\n+\n+    task_manager = AgentTaskManager(\n+        project_name=project_name,\n+        mindsdb_host=mindsdb_host,\n+        mindsdb_port=mindsdb_port,\n+    )\n+\n+    server = A2AServer(\n+        agent_card=agent_card,\n+        task_manager=task_manager,\n+        host=host,\n+    )\n+    return server\n+",
    "comment": "**correctness**: `get_a2a_server` now returns the server object, but callers expecting the `.app` attribute (as previously returned) will break, causing runtime errors.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/api/a2a/__init__.py, line 56, the function `get_a2a_server` now returns the server object instead of `server.app`. This will break any code expecting the `.app` attribute as before, causing runtime errors. Change `return server` to `return server.app` to maintain backward compatibility and prevent runtime failures.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\nreturn server.app\n```\n</details>\n<!-- suggestion_end -->\n",
    "line_number": 56,
    "enriched": "File: mindsdb/api/a2a/__init__.py\nCode: @@ -0,0 +1,68 @@\n+\n+# A2A specific imports\n+from mindsdb.api.a2a.common.types import (\n+    AgentCard,\n+    AgentCapabilities,\n+    AgentSkill,\n+)\n+from mindsdb.api.a2a.common.server.server import A2AServer\n+from mindsdb.api.a2a.task_manager import AgentTaskManager\n+from mindsdb.api.a2a.agent import MindsDBAgent\n+\n+def get_a2a_server(\n+    host: str = \"0.0.0.0\",\n+    mindsdb_host: str = \"127.0.0.1\",\n+    mindsdb_port: int = 47334,\n+    project_name: str = \"mindsdb\",\n+):\n+    # Prepare A2A artefacts (agent card & task-manager)\n+    capabilities = AgentCapabilities(streaming=True)\n+    skill = AgentSkill(\n+        id=\"mindsdb_query\",\n+        name=\"MindsDB Query\",\n+        description=\"Executes natural-language queries via MindsDB agents.\",\n+        tags=[\"database\", \"mindsdb\", \"query\", \"analytics\"],\n+        examples=[\n+            \"What trends exist in my sales data?\",\n+            \"Generate insights from the support tickets dataset.\",\n+        ],\n+        inputModes=MindsDBAgent.SUPPORTED_CONTENT_TYPES,\n+        outputModes=MindsDBAgent.SUPPORTED_CONTENT_TYPES,\n+    )\n+\n+    agent_card = AgentCard(\n+        name=\"MindsDB Agent Connector\",\n+        description=(f\"A2A connector that proxies requests to MindsDB agents in project '{project_name}'.\"),\n+        url=f\"http://{host}:{mindsdb_port}\",\n+        version=\"1.0.0\",\n+        defaultInputModes=MindsDBAgent.SUPPORTED_CONTENT_TYPES,\n+        defaultOutputModes=MindsDBAgent.SUPPORTED_CONTENT_TYPES,\n+        capabilities=capabilities,\n+        skills=[skill],\n+    )\n+\n+    task_manager = AgentTaskManager(\n+        project_name=project_name,\n+        mindsdb_host=mindsdb_host,\n+        mindsdb_port=mindsdb_port,\n+    )\n+\n+    server = A2AServer(\n+        agent_card=agent_card,\n+        task_manager=task_manager,\n+        host=host,\n+    )\n+    return server\n+\nComment: **correctness**: `get_a2a_server` now returns the server object, but callers expecting the `.app` attribute (as previously returned) will break, causing runtime errors.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/api/a2a/__init__.py, line 56, the function `get_a2a_server` now returns the server object instead of `server.app`. This will break any code expecting the `.app` attribute as before, causing runtime errors. Change `return server` to `return server.app` to maintain backward compatibility and prevent runtime failures.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\nreturn server.app\n```\n</details>\n<!-- suggestion_end -->\n",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "mindsdb/api/a2a/__init__.py",
    "pr_number": 11441,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2284041409,
    "comment_created_at": "2025-08-19T04:41:17Z"
  },
  {
    "code": "@@ -549,3 +551,210 @@ def process_event(event):\n             conn.commit()\n \n             conn.close()\n+\n+    def meta_get_tables(self, table_names: Optional[list] = None) -> Response:\n+        \"\"\"\n+        Retrieves metadata information about the tables in the PostgreSQL database to be stored in the data catalog.\n+\n+        Args:\n+            table_names (list): A list of table names for which to retrieve metadata information.\n+\n+        Returns:\n+            Response: A response object containing the metadata information, formatted as per the `Response` class.\n+        \"\"\"\n+        query = \"\"\"\n+            SELECT\n+                t.table_name,\n+                t.table_schema,\n+                t.table_type,\n+                obj_description(pgc.oid, 'pg_class') AS table_description,\n+                (SELECT COUNT(*) FROM information_schema.columns c \n+                WHERE c.table_name = t.table_name AND c.table_schema = t.table_schema) AS row_count",
    "comment": "The alias should be columns not row_count right?",
    "line_number": 572,
    "enriched": "File: mindsdb/integrations/handlers/postgres_handler/postgres_handler.py\nCode: @@ -549,3 +551,210 @@ def process_event(event):\n             conn.commit()\n \n             conn.close()\n+\n+    def meta_get_tables(self, table_names: Optional[list] = None) -> Response:\n+        \"\"\"\n+        Retrieves metadata information about the tables in the PostgreSQL database to be stored in the data catalog.\n+\n+        Args:\n+            table_names (list): A list of table names for which to retrieve metadata information.\n+\n+        Returns:\n+            Response: A response object containing the metadata information, formatted as per the `Response` class.\n+        \"\"\"\n+        query = \"\"\"\n+            SELECT\n+                t.table_name,\n+                t.table_schema,\n+                t.table_type,\n+                obj_description(pgc.oid, 'pg_class') AS table_description,\n+                (SELECT COUNT(*) FROM information_schema.columns c \n+                WHERE c.table_name = t.table_name AND c.table_schema = t.table_schema) AS row_count\nComment: The alias should be columns not row_count right?",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/postgres_handler/postgres_handler.py",
    "pr_number": 10862,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2112189829,
    "comment_created_at": "2025-05-28T15:26:14Z"
  },
  {
    "code": "@@ -0,0 +1,389 @@\n+import os\n+import pandas as pd\n+from pandas import DataFrame\n+from google.auth.transport.requests import Request\n+from google.oauth2 import service_account\n+from google.oauth2.credentials import Credentials\n+from googleapiclient.discovery import build\n+from mindsdb.api.mysql.mysql_proxy.libs.constants.response_type import RESPONSE_TYPE\n+from .google_content_shopping_tables import AccountsTable, OrdersTable, ProductsTable\n+from mindsdb.integrations.libs.api_handler import APIHandler, FuncParser\n+from mindsdb.integrations.libs.response import (\n+    HandlerStatusResponse as StatusResponse,\n+    HandlerResponse as Response,\n+)\n+from mindsdb.utilities import log\n+\n+\n+class GoogleContentShoppingHandler(APIHandler):\n+    \"\"\"\n+        A class for handling connections and interactions with the Google Content API for Shopping.\n+    \"\"\"\n+    name = 'google_content_shopping'\n+\n+    def __init__(self, name: str, **kwargs):\n+        \"\"\"\n+        Initialize the Google Content API for Shopping handler.\n+        Args:\n+            name (str): name of the handler\n+            kwargs (dict): additional arguments\n+        \"\"\"\n+        super().__init__(name)\n+        self.token = None\n+        self.service = None\n+        self.connection_data = kwargs.get('connection_data', {})\n+        self.credentials_file = self.connection_data.get('credentials', None)\n+        self.merchant_id = self.connection_data.get('merchant_id', None)\n+        self.credentials = None\n+        self.scopes = ['https://www.googleapis.com/auth/content']\n+        self.is_connected = False\n+        accounts = AccountsTable(self)\n+        self.accounts = accounts\n+        self._register_table('Accounts', accounts)\n+        orders = OrdersTable(self)\n+        self.orders = orders\n+        self._register_table('Orders', orders)\n+        products = ProductsTable(self)\n+        self.products = products\n+        self._register_table('Products', products)\n+\n+    def connect(self):\n+        \"\"\"\n+        Set up any connections required by the handler\n+        Should return output of check_connection() method after attempting\n+        connection. Should switch self.is_connected.\n+        Returns:\n+            HandlerStatusResponse\n+        \"\"\"\n+        if self.is_connected is True:\n+            return self.service\n+        if self.credentials_file:\n+            if os.path.exists('token_content.json'):\n+                self.credentials = Credentials.from_authorized_user_file('token_content.json', self.scopes)\n+            if not self.credentials or not self.credentials.valid:\n+                if self.credentials and self.credentials.expired and self.credentials.refresh_token:\n+                    self.credentials.refresh(Request())\n+                else:\n+                    self.credentials = service_account.Credentials.from_service_account_file(\n+                        self.credentials_file, scopes=self.scopes)\n+            # Save the credentials for the next run\n+            with open('token_content.json', 'w') as token:",
    "comment": "Please, do not save/load files in current dir. Use `file_storage` instead:\r\n```python\r\ndef __init__(self, name: str, **kwargs):\r\n    self.fs_storage = kwargs['file_storage']\r\n    \r\ndef connect(self):\r\n    ...\r\n    self.fs_storage.file_set('token_content.json',  '{\"key\": \"value\"}'.encode())   # second param must by bytes, therefore `.encode()` there\r\n    json_str_bytes = self.fs_storage.file_get('token_content.json')    # will raise exception if file not exists\r\n    json_str = json_str_bytes.decode()\r\n```",
    "line_number": 70,
    "enriched": "File: mindsdb/integrations/handlers/google_content_shopping_handler/google_content_shopping_handler.py\nCode: @@ -0,0 +1,389 @@\n+import os\n+import pandas as pd\n+from pandas import DataFrame\n+from google.auth.transport.requests import Request\n+from google.oauth2 import service_account\n+from google.oauth2.credentials import Credentials\n+from googleapiclient.discovery import build\n+from mindsdb.api.mysql.mysql_proxy.libs.constants.response_type import RESPONSE_TYPE\n+from .google_content_shopping_tables import AccountsTable, OrdersTable, ProductsTable\n+from mindsdb.integrations.libs.api_handler import APIHandler, FuncParser\n+from mindsdb.integrations.libs.response import (\n+    HandlerStatusResponse as StatusResponse,\n+    HandlerResponse as Response,\n+)\n+from mindsdb.utilities import log\n+\n+\n+class GoogleContentShoppingHandler(APIHandler):\n+    \"\"\"\n+        A class for handling connections and interactions with the Google Content API for Shopping.\n+    \"\"\"\n+    name = 'google_content_shopping'\n+\n+    def __init__(self, name: str, **kwargs):\n+        \"\"\"\n+        Initialize the Google Content API for Shopping handler.\n+        Args:\n+            name (str): name of the handler\n+            kwargs (dict): additional arguments\n+        \"\"\"\n+        super().__init__(name)\n+        self.token = None\n+        self.service = None\n+        self.connection_data = kwargs.get('connection_data', {})\n+        self.credentials_file = self.connection_data.get('credentials', None)\n+        self.merchant_id = self.connection_data.get('merchant_id', None)\n+        self.credentials = None\n+        self.scopes = ['https://www.googleapis.com/auth/content']\n+        self.is_connected = False\n+        accounts = AccountsTable(self)\n+        self.accounts = accounts\n+        self._register_table('Accounts', accounts)\n+        orders = OrdersTable(self)\n+        self.orders = orders\n+        self._register_table('Orders', orders)\n+        products = ProductsTable(self)\n+        self.products = products\n+        self._register_table('Products', products)\n+\n+    def connect(self):\n+        \"\"\"\n+        Set up any connections required by the handler\n+        Should return output of check_connection() method after attempting\n+        connection. Should switch self.is_connected.\n+        Returns:\n+            HandlerStatusResponse\n+        \"\"\"\n+        if self.is_connected is True:\n+            return self.service\n+        if self.credentials_file:\n+            if os.path.exists('token_content.json'):\n+                self.credentials = Credentials.from_authorized_user_file('token_content.json', self.scopes)\n+            if not self.credentials or not self.credentials.valid:\n+                if self.credentials and self.credentials.expired and self.credentials.refresh_token:\n+                    self.credentials.refresh(Request())\n+                else:\n+                    self.credentials = service_account.Credentials.from_service_account_file(\n+                        self.credentials_file, scopes=self.scopes)\n+            # Save the credentials for the next run\n+            with open('token_content.json', 'w') as token:\nComment: Please, do not save/load files in current dir. Use `file_storage` instead:\r\n```python\r\ndef __init__(self, name: str, **kwargs):\r\n    self.fs_storage = kwargs['file_storage']\r\n    \r\ndef connect(self):\r\n    ...\r\n    self.fs_storage.file_set('token_content.json',  '{\"key\": \"value\"}'.encode())   # second param must by bytes, therefore `.encode()` there\r\n    json_str_bytes = self.fs_storage.file_get('token_content.json')    # will raise exception if file not exists\r\n    json_str = json_str_bytes.decode()\r\n```",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/google_content_shopping_handler/google_content_shopping_handler.py",
    "pr_number": 5825,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1183700290,
    "comment_created_at": "2023-05-03T13:37:32Z"
  },
  {
    "code": "@@ -0,0 +1,192 @@\n+import os\n+import requests\n+from flask import Flask, redirect, request, session, jsonify, render_template\n+from urllib.parse import urlencode\n+from dotenv import load_dotenv\n+import google.generativeai as genai  # Gemini SDK\n+import mindsdb_sdk\n+\n+\n+load_dotenv()\n+\n+app = Flask(__name__)\n+\n+# ===================== CONFIG =====================\n+SPOTIFY_CLIENT_ID = os.getenv(\"SPOTIFY_CLIENT_ID\")\n+SPOTIFY_CLIENT_SECRET = os.getenv(\"SPOTIFY_CLIENT_SECRET\")\n+SPOTIFY_REDIRECT_URI = os.getenv(\"SPOTIFY_REDIRECT_URI\")\n+\n+GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n+\n+app.secret_key = os.getenv(\"FLASK_SECRET_KEY\", \"super_secret_key\")\n+\n+# MindsDB Connection\n+mdb = mindsdb_sdk.connect('http://127.0.0.1:47334')\n+\n+# ===================== SPOTIFY ENDPOINTS =====================\n+SPOTIFY_AUTH_URL = \"https://accounts.spotify.com/authorize\"\n+SPOTIFY_TOKEN_URL = \"https://accounts.spotify.com/api/token\"\n+SPOTIFY_API_BASE_URL = \"https://api.spotify.com/v1\"\n+\n+# ===================== GEMINI CONFIG =====================\n+genai.configure(api_key=GEMINI_API_KEY)\n+gemini_model = genai.GenerativeModel(\"gemini-1.5-flash\")\n+\n+\n+# ===================== ROUTES =====================\n+@app.route(\"/\")\n+def index():\n+    return render_template(\"index.html\")\n+\n+\n+@app.route(\"/login\")\n+def login():\n+    scope = \"playlist-modify-private playlist-modify-public user-read-email\"\n+    params = {\n+        \"client_id\": SPOTIFY_CLIENT_ID,\n+        \"response_type\": \"code\",\n+        \"redirect_uri\": SPOTIFY_REDIRECT_URI,\n+        \"scope\": scope,\n+    }\n+    url = f\"{SPOTIFY_AUTH_URL}?{urlencode(params)}\"\n+    return redirect(url)\n+\n+\n+@app.route(\"/callback\")\n+def callback():\n+    code = request.args.get(\"code\")\n+    if code is None:\n+        return \"Error: No code received\", 400\n+\n+    payload = {\n+        \"grant_type\": \"authorization_code\",\n+        \"code\": code,\n+        \"redirect_uri\": SPOTIFY_REDIRECT_URI,\n+        \"client_id\": SPOTIFY_CLIENT_ID,\n+        \"client_secret\": SPOTIFY_CLIENT_SECRET,\n+    }\n+\n+    response = requests.post(SPOTIFY_TOKEN_URL, data=payload)\n+    token_info = response.json()\n+\n+    if \"access_token\" not in token_info:\n+        return f\"Error: {token_info}\", 400\n+\n+    session[\"spotify_token\"] = token_info[\"access_token\"]\n+\n+    return redirect(\"/success\")\n+\n+\n+@app.route(\"/success\")\n+def success():\n+    return render_template(\"index.html\", success=True)\n+\n+\n+@app.route(\"/status\")\n+def status():\n+    return jsonify({\"logged_in\": \"spotify_token\" in session})\n+\n+\n+@app.route(\"/generate\", methods=[\"POST\"])\n+def generate_playlist():\n+    if \"spotify_token\" not in session:\n+        return jsonify({\"error\": \"Not logged in\"}), 401\n+\n+    user_input = request.json.get(\"mood\", \"\")\n+    if not user_input.strip():\n+        return jsonify({\"error\": \"No mood provided\"}), 400\n+\n+    # ===== STEP 1: Gemini classification =====\n+    try:\n+        prompt = f\"\"\"\n+        Classify the following text into one of these emotions only:\n+        [joy, happiness, sadness, anger, fear, surprise, love, disgust, calm, relaxation, optimism, neutral].\n+        Text: \"{user_input}\"\n+        Respond with just the emotion name.\n+        \"\"\"\n+        response = gemini_model.generate_content(prompt)\n+        mood = response.text.strip().lower()\n+\n+        valid_emotions = [\n+            \"joy\", \"happiness\", \"sadness\", \"anger\", \"fear\", \"surprise\",\n+            \"love\", \"disgust\", \"calm\", \"relaxation\", \"optimism\", \"neutral\"\n+        ]\n+        if mood not in valid_emotions:\n+            mood = \"neutral\"\n+\n+    except Exception as e:\n+        print(\"Gemini error:\", e)\n+        mood = \"neutral\"\n+\n+    # ===== STEP 2: Get genre (MindsDB or fallback) =====\n+    genre = None\n+    try:\n+        query = mdb.sql(f\"SELECT genre FROM mood_to_genre_model WHERE mood='{mood}'\")\n+        if query.rows and \"genre\" in query.rows[0]:\n+            genre = query.rows[0][\"genre\"]\n+    except Exception as e:\n+        print(\"MindsDB error:\", e)\n+\n+    if not genre:  # fallback\n+        mood_to_genre = {\n+            \"joy\": \"pop\",\n+            \"happiness\": \"dance\",\n+            \"sadness\": \"acoustic\",\n+            \"anger\": \"rock\",\n+            \"fear\": \"metal\",\n+            \"surprise\": \"edm\",\n+            \"love\": \"romance\",\n+            \"disgust\": \"punk\",\n+            \"calm\": \"lofi\",\n+            \"relaxation\": \"chill\",\n+            \"optimism\": \"indie\",\n+            \"neutral\": \"classical\",\n+        }\n+        genre = mood_to_genre.get(mood, \"pop\")\n+\n+    # ===== STEP 3: Get Spotify User ID =====\n+    headers = {\"Authorization\": f\"Bearer {session['spotify_token']}\"}\n+    profile_resp = requests.get(f\"{SPOTIFY_API_BASE_URL}/me\", headers=headers)\n+    user_id = profile_resp.json().get(\"id\")\n+\n+    # ===== STEP 4: Create Playlist =====\n+    playlist_name = f\"{mood.capitalize()} Vibes 🎶\"\n+    create_payload = {\"name\": playlist_name, \"public\": False}\n+    create_resp = requests.post(\n+        f\"{SPOTIFY_API_BASE_URL}/users/{user_id}/playlists\",\n+        headers=headers,\n+        json=create_payload,\n+    )",
    "comment": "**correctness**: `user_id` is not checked for None before using in the playlist creation API call, which will cause a runtime error if Spotify profile fetch fails or token is invalid.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn examples/moodify/app.py, lines 150-159, the code does not check if `user_id` is None before using it to create a playlist. This will cause a runtime error if the Spotify profile fetch fails or the token is invalid. Add a check after fetching `user_id` to return an error response if it is None, before proceeding to create the playlist.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n    user_id = profile_resp.json().get(\"id\")\n    if not user_id:\n        return jsonify({\"error\": \"Failed to fetch Spotify user profile. Please re-login.\"}), 400\n\n    # ===== STEP 4: Create Playlist =====\n    playlist_name = f\"{mood.capitalize()} Vibes 🎶\"\n    create_payload = {\"name\": playlist_name, \"public\": False}\n    create_resp = requests.post(\n        f\"{SPOTIFY_API_BASE_URL}/users/{user_id}/playlists\",\n        headers=headers,\n        json=create_payload,\n    )\n\n```\n</details>\n<!-- suggestion_end -->\n",
    "line_number": 159,
    "enriched": "File: examples/moodify/app.py\nCode: @@ -0,0 +1,192 @@\n+import os\n+import requests\n+from flask import Flask, redirect, request, session, jsonify, render_template\n+from urllib.parse import urlencode\n+from dotenv import load_dotenv\n+import google.generativeai as genai  # Gemini SDK\n+import mindsdb_sdk\n+\n+\n+load_dotenv()\n+\n+app = Flask(__name__)\n+\n+# ===================== CONFIG =====================\n+SPOTIFY_CLIENT_ID = os.getenv(\"SPOTIFY_CLIENT_ID\")\n+SPOTIFY_CLIENT_SECRET = os.getenv(\"SPOTIFY_CLIENT_SECRET\")\n+SPOTIFY_REDIRECT_URI = os.getenv(\"SPOTIFY_REDIRECT_URI\")\n+\n+GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n+\n+app.secret_key = os.getenv(\"FLASK_SECRET_KEY\", \"super_secret_key\")\n+\n+# MindsDB Connection\n+mdb = mindsdb_sdk.connect('http://127.0.0.1:47334')\n+\n+# ===================== SPOTIFY ENDPOINTS =====================\n+SPOTIFY_AUTH_URL = \"https://accounts.spotify.com/authorize\"\n+SPOTIFY_TOKEN_URL = \"https://accounts.spotify.com/api/token\"\n+SPOTIFY_API_BASE_URL = \"https://api.spotify.com/v1\"\n+\n+# ===================== GEMINI CONFIG =====================\n+genai.configure(api_key=GEMINI_API_KEY)\n+gemini_model = genai.GenerativeModel(\"gemini-1.5-flash\")\n+\n+\n+# ===================== ROUTES =====================\n+@app.route(\"/\")\n+def index():\n+    return render_template(\"index.html\")\n+\n+\n+@app.route(\"/login\")\n+def login():\n+    scope = \"playlist-modify-private playlist-modify-public user-read-email\"\n+    params = {\n+        \"client_id\": SPOTIFY_CLIENT_ID,\n+        \"response_type\": \"code\",\n+        \"redirect_uri\": SPOTIFY_REDIRECT_URI,\n+        \"scope\": scope,\n+    }\n+    url = f\"{SPOTIFY_AUTH_URL}?{urlencode(params)}\"\n+    return redirect(url)\n+\n+\n+@app.route(\"/callback\")\n+def callback():\n+    code = request.args.get(\"code\")\n+    if code is None:\n+        return \"Error: No code received\", 400\n+\n+    payload = {\n+        \"grant_type\": \"authorization_code\",\n+        \"code\": code,\n+        \"redirect_uri\": SPOTIFY_REDIRECT_URI,\n+        \"client_id\": SPOTIFY_CLIENT_ID,\n+        \"client_secret\": SPOTIFY_CLIENT_SECRET,\n+    }\n+\n+    response = requests.post(SPOTIFY_TOKEN_URL, data=payload)\n+    token_info = response.json()\n+\n+    if \"access_token\" not in token_info:\n+        return f\"Error: {token_info}\", 400\n+\n+    session[\"spotify_token\"] = token_info[\"access_token\"]\n+\n+    return redirect(\"/success\")\n+\n+\n+@app.route(\"/success\")\n+def success():\n+    return render_template(\"index.html\", success=True)\n+\n+\n+@app.route(\"/status\")\n+def status():\n+    return jsonify({\"logged_in\": \"spotify_token\" in session})\n+\n+\n+@app.route(\"/generate\", methods=[\"POST\"])\n+def generate_playlist():\n+    if \"spotify_token\" not in session:\n+        return jsonify({\"error\": \"Not logged in\"}), 401\n+\n+    user_input = request.json.get(\"mood\", \"\")\n+    if not user_input.strip():\n+        return jsonify({\"error\": \"No mood provided\"}), 400\n+\n+    # ===== STEP 1: Gemini classification =====\n+    try:\n+        prompt = f\"\"\"\n+        Classify the following text into one of these emotions only:\n+        [joy, happiness, sadness, anger, fear, surprise, love, disgust, calm, relaxation, optimism, neutral].\n+        Text: \"{user_input}\"\n+        Respond with just the emotion name.\n+        \"\"\"\n+        response = gemini_model.generate_content(prompt)\n+        mood = response.text.strip().lower()\n+\n+        valid_emotions = [\n+            \"joy\", \"happiness\", \"sadness\", \"anger\", \"fear\", \"surprise\",\n+            \"love\", \"disgust\", \"calm\", \"relaxation\", \"optimism\", \"neutral\"\n+        ]\n+        if mood not in valid_emotions:\n+            mood = \"neutral\"\n+\n+    except Exception as e:\n+        print(\"Gemini error:\", e)\n+        mood = \"neutral\"\n+\n+    # ===== STEP 2: Get genre (MindsDB or fallback) =====\n+    genre = None\n+    try:\n+        query = mdb.sql(f\"SELECT genre FROM mood_to_genre_model WHERE mood='{mood}'\")\n+        if query.rows and \"genre\" in query.rows[0]:\n+            genre = query.rows[0][\"genre\"]\n+    except Exception as e:\n+        print(\"MindsDB error:\", e)\n+\n+    if not genre:  # fallback\n+        mood_to_genre = {\n+            \"joy\": \"pop\",\n+            \"happiness\": \"dance\",\n+            \"sadness\": \"acoustic\",\n+            \"anger\": \"rock\",\n+            \"fear\": \"metal\",\n+            \"surprise\": \"edm\",\n+            \"love\": \"romance\",\n+            \"disgust\": \"punk\",\n+            \"calm\": \"lofi\",\n+            \"relaxation\": \"chill\",\n+            \"optimism\": \"indie\",\n+            \"neutral\": \"classical\",\n+        }\n+        genre = mood_to_genre.get(mood, \"pop\")\n+\n+    # ===== STEP 3: Get Spotify User ID =====\n+    headers = {\"Authorization\": f\"Bearer {session['spotify_token']}\"}\n+    profile_resp = requests.get(f\"{SPOTIFY_API_BASE_URL}/me\", headers=headers)\n+    user_id = profile_resp.json().get(\"id\")\n+\n+    # ===== STEP 4: Create Playlist =====\n+    playlist_name = f\"{mood.capitalize()} Vibes 🎶\"\n+    create_payload = {\"name\": playlist_name, \"public\": False}\n+    create_resp = requests.post(\n+        f\"{SPOTIFY_API_BASE_URL}/users/{user_id}/playlists\",\n+        headers=headers,\n+        json=create_payload,\n+    )\nComment: **correctness**: `user_id` is not checked for None before using in the playlist creation API call, which will cause a runtime error if Spotify profile fetch fails or token is invalid.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn examples/moodify/app.py, lines 150-159, the code does not check if `user_id` is None before using it to create a playlist. This will cause a runtime error if the Spotify profile fetch fails or the token is invalid. Add a check after fetching `user_id` to return an error response if it is None, before proceeding to create the playlist.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n    user_id = profile_resp.json().get(\"id\")\n    if not user_id:\n        return jsonify({\"error\": \"Failed to fetch Spotify user profile. Please re-login.\"}), 400\n\n    # ===== STEP 4: Create Playlist =====\n    playlist_name = f\"{mood.capitalize()} Vibes 🎶\"\n    create_payload = {\"name\": playlist_name, \"public\": False}\n    create_resp = requests.post(\n        f\"{SPOTIFY_API_BASE_URL}/users/{user_id}/playlists\",\n        headers=headers,\n        json=create_payload,\n    )\n\n```\n</details>\n<!-- suggestion_end -->\n",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "examples/moodify/app.py",
    "pr_number": 11551,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2337300945,
    "comment_created_at": "2025-09-10T16:34:32Z"
  },
  {
    "code": "@@ -0,0 +1,523 @@\n+import json\n+import math\n+import time\n+from typing import List\n+\n+import pandas as pd\n+import datetime as dt\n+\n+from mindsdb.api.executor.sql_query.result_set import ResultSet\n+from mindsdb_sql_parser import Identifier, Select, Constant, Star, parse_sql\n+from mindsdb.utilities import log\n+\n+from mindsdb.interfaces.knowledge_base.llm_client import LLMClient\n+\n+logger = log.getLogger(__name__)\n+\n+\n+GENERATE_QA_SYSTEM_PROMPT = \"\"\"\n+Your task is to generate question and answer pairs for a search engine. \n+The search engine will take your query and return a list of documents.\n+You will be given a text and you need to generate a question that can be answered using the information in the text.\n+Your questions will be used to evaluate the search engine.\n+Question should always have enough clues to identify the specific text that this question is generated from. \n+Never ask questions like \"What license number is associated with Amend 6\" because Amend 6 could be found in many documents and the question is not specific enough.",
    "comment": "May be move this `Never ask questions` to after examples",
    "line_number": 24,
    "enriched": "File: mindsdb/interfaces/knowledge_base/evaluate.py\nCode: @@ -0,0 +1,523 @@\n+import json\n+import math\n+import time\n+from typing import List\n+\n+import pandas as pd\n+import datetime as dt\n+\n+from mindsdb.api.executor.sql_query.result_set import ResultSet\n+from mindsdb_sql_parser import Identifier, Select, Constant, Star, parse_sql\n+from mindsdb.utilities import log\n+\n+from mindsdb.interfaces.knowledge_base.llm_client import LLMClient\n+\n+logger = log.getLogger(__name__)\n+\n+\n+GENERATE_QA_SYSTEM_PROMPT = \"\"\"\n+Your task is to generate question and answer pairs for a search engine. \n+The search engine will take your query and return a list of documents.\n+You will be given a text and you need to generate a question that can be answered using the information in the text.\n+Your questions will be used to evaluate the search engine.\n+Question should always have enough clues to identify the specific text that this question is generated from. \n+Never ask questions like \"What license number is associated with Amend 6\" because Amend 6 could be found in many documents and the question is not specific enough.\nComment: May be move this `Never ask questions` to after examples",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "mindsdb/interfaces/knowledge_base/evaluate.py",
    "pr_number": 10914,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2115702157,
    "comment_created_at": "2025-05-30T11:09:35Z"
  },
  {
    "code": "@@ -53,22 +59,30 @@\n         password = quote(self.connection_data['password'])\n         database = quote(self.connection_data['database'])\n         url = f'{protocol}://{user}:{password}@{host}:{port}/{database}'\n+        # This is not redundunt. Check https://clickhouse-sqlalchemy.readthedocs.io/en/latest/connection.html#http\n         if self.protocol == 'https':\n             url = url + \"?protocol=https\"\n+        try:\n+            engine = create_engine(url)\n+            connection = engine.raw_connection()\n+            self.is_connected = True\n+            self.connection = connection\n+        except SQLAlchemyError as e:\n+            logger.error(f'Failed to connect to ClickHouse database at {url}: {e}')",
    "comment": "## Clear-text logging of sensitive information\n\nThis expression logs [sensitive data (password)](1) as clear text.\nThis expression logs [sensitive data (password)](2) as clear text.\n\n[Show more details](https://github.com/mindsdb/mindsdb/security/code-scanning/22)",
    "line_number": 71,
    "enriched": "File: mindsdb/integrations/handlers/clickhouse_handler/clickhouse_handler.py\nCode: @@ -53,22 +59,30 @@\n         password = quote(self.connection_data['password'])\n         database = quote(self.connection_data['database'])\n         url = f'{protocol}://{user}:{password}@{host}:{port}/{database}'\n+        # This is not redundunt. Check https://clickhouse-sqlalchemy.readthedocs.io/en/latest/connection.html#http\n         if self.protocol == 'https':\n             url = url + \"?protocol=https\"\n+        try:\n+            engine = create_engine(url)\n+            connection = engine.raw_connection()\n+            self.is_connected = True\n+            self.connection = connection\n+        except SQLAlchemyError as e:\n+            logger.error(f'Failed to connect to ClickHouse database at {url}: {e}')\nComment: ## Clear-text logging of sensitive information\n\nThis expression logs [sensitive data (password)](1) as clear text.\nThis expression logs [sensitive data (password)](2) as clear text.\n\n[Show more details](https://github.com/mindsdb/mindsdb/security/code-scanning/22)",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/clickhouse_handler/clickhouse_handler.py",
    "pr_number": 9111,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1574793153,
    "comment_created_at": "2024-04-22T13:50:12Z"
  },
  {
    "code": "@@ -120,33 +121,52 @@ def create_db_from_texts(self, documents, embeddings_model) -> VectorStore:\n             texts=texts, embedding=embeddings_model, metadatas=metadata\n         )\n \n+    @staticmethod\n+    def create_batch_embeddings(documents: List[Document], embeddings_batch_size):\n+        \"\"\"\n+        create batch of document embeddings\n+        \"\"\"\n+\n+        for i in range(0, len(documents), embeddings_batch_size):\n+            yield documents[i : i + embeddings_batch_size]\n+\n     def embeddings_to_vectordb(self) -> None:\n         \"\"\"Create vectorstore from documents and store locally.\"\"\"\n \n         start_time = time.time()\n \n-        # Load documents and splits in chunks (if not in evaluation_type mode)\n+        # Load documents and splits in chunks and defines overlap\n         documents = self.split_documents(\n             chunk_size=self.args.chunk_size, chunk_overlap=self.args.chunk_overlap\n         )\n \n+        batches_documents = self.create_batch_embeddings(\n+            documents, embeddings_batch_size=self.args.embeddings_batch_size\n+        )\n+\n         # Load embeddings model\n-        embeddings_model = load_embeddings_model(self.embeddings_model_name)\n+        embeddings_model = load_embeddings_model(\n+            self.embeddings_model_name, self.args.use_gpu\n+        )\n \n         logger.info(f\"Creating vectorstore from documents\")\n \n         if not validate_documents(documents):\n             raise ValueError(\"Invalid documents\")\n \n+        # todo get max_batch from chroma client\n+\n         try:\n-            db = self.create_db_from_documents(documents, embeddings_model)\n+            for batch_document in batches_documents:\n+                db = self.create_db_from_documents(batch_document, embeddings_model)\n         except Exception as e:\n             logger.error(\n                 f\"Error loading using 'from_documents' method, trying 'from_text': {e}\"\n             )\n             try:\n-                db = self.create_db_from_texts(documents, embeddings_model)\n-                logger.info(f\"successfully loaded using 'from_text' method: {e}\")\n+                for batch_document in batches_documents:\n+                    db = self.create_db_from_texts(batch_document, embeddings_model)",
    "comment": "many db's are created, but stored only last one. is it ok?",
    "line_number": 168,
    "enriched": "File: mindsdb/integrations/handlers/rag_handler/ingest.py\nCode: @@ -120,33 +121,52 @@ def create_db_from_texts(self, documents, embeddings_model) -> VectorStore:\n             texts=texts, embedding=embeddings_model, metadatas=metadata\n         )\n \n+    @staticmethod\n+    def create_batch_embeddings(documents: List[Document], embeddings_batch_size):\n+        \"\"\"\n+        create batch of document embeddings\n+        \"\"\"\n+\n+        for i in range(0, len(documents), embeddings_batch_size):\n+            yield documents[i : i + embeddings_batch_size]\n+\n     def embeddings_to_vectordb(self) -> None:\n         \"\"\"Create vectorstore from documents and store locally.\"\"\"\n \n         start_time = time.time()\n \n-        # Load documents and splits in chunks (if not in evaluation_type mode)\n+        # Load documents and splits in chunks and defines overlap\n         documents = self.split_documents(\n             chunk_size=self.args.chunk_size, chunk_overlap=self.args.chunk_overlap\n         )\n \n+        batches_documents = self.create_batch_embeddings(\n+            documents, embeddings_batch_size=self.args.embeddings_batch_size\n+        )\n+\n         # Load embeddings model\n-        embeddings_model = load_embeddings_model(self.embeddings_model_name)\n+        embeddings_model = load_embeddings_model(\n+            self.embeddings_model_name, self.args.use_gpu\n+        )\n \n         logger.info(f\"Creating vectorstore from documents\")\n \n         if not validate_documents(documents):\n             raise ValueError(\"Invalid documents\")\n \n+        # todo get max_batch from chroma client\n+\n         try:\n-            db = self.create_db_from_documents(documents, embeddings_model)\n+            for batch_document in batches_documents:\n+                db = self.create_db_from_documents(batch_document, embeddings_model)\n         except Exception as e:\n             logger.error(\n                 f\"Error loading using 'from_documents' method, trying 'from_text': {e}\"\n             )\n             try:\n-                db = self.create_db_from_texts(documents, embeddings_model)\n-                logger.info(f\"successfully loaded using 'from_text' method: {e}\")\n+                for batch_document in batches_documents:\n+                    db = self.create_db_from_texts(batch_document, embeddings_model)\nComment: many db's are created, but stored only last one. is it ok?",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/rag_handler/ingest.py",
    "pr_number": 8338,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1397658145,
    "comment_created_at": "2023-11-17T17:33:39Z"
  },
  {
    "code": "@@ -67,11 +65,60 @@\n   # Run integration tests\n   # TODO: Run these against the deployed environment\n   run_tests:\n-    if: github.event.pull_request.merged == true\n+    #if: github.event.pull_request.merged == true\n     name: Run Integration Tests\n     needs: [deploy]\n     concurrency:\n       group: ${{ github.workflow }}-${{ github.head_ref || github.ref }}-tests\n       cancel-in-progress: true\n     uses: ./.github/workflows/test_on_deploy.yml\n-    secrets: inherit\n+    secrets: inherit\n+  \n+  slack_message:\n+    if: always()\n+    name: Notify Slack\n+    needs: [run_tests]\n+    runs-on: mdb-dev\n+    steps:\n+      - name: Notify of failing tests\n+        if: needs.run_tests.result == 'failure'\n+        uses: slackapi/slack-github-action@v1.26.0\n+        with:\n+          channel-id: ${{ secrets.SLACK_DEPLOYMENTS_CHANNEL_ID }}\n+          payload: |\n+            {\n+              \"attachments\": [\n+                {\n+                  \"color\": \"#FF4444\",\n+                  \"blocks\": [\n+                    {\n+                      \"type\": \"header\",\n+                      \"text\": {\n+                        \"type\": \"plain_text\",\n+                        \"text\": \"TEST RUN FAILED ON MAIN\",\n+                        \"emoji\": true\n+                      }\n+                    },\n+                    {\n+                      \"type\": \"section\",\n+                      \"text\": {\n+                        \"type\": \"mrkdwn\",\n+                        \"text\": \" \"\n+                      },\n+                      \"fields\": [\n+                        {\n+                          \"type\": \"mrkdwn\",\n+                          \"text\": \"*Commit*\\n<${{ github.server_url }}/${{ github.repository }}/commit/${{ github.sha }}|${{ github.sha }}>\"\n+                        },\n+                        {\n+                          \"type\": \"mrkdwn\",\n+                          \"text\": \"*Workflow Run*\\n<${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}|${{ github.workflow }}>\"\n+                        }\n+                      ]\n+                    }\n+                  ]\n+                }\n+              ]\n+            }\n+        env:\n+          SLACK_BOT_TOKEN: ${{ secrets.GH_ACTIONS_SLACK_BOT_TOKEN }}",
    "comment": "## Workflow does not contain permissions\n\nActions job or workflow does not limit the permissions of the GITHUB_TOKEN. Consider setting an explicit permissions block, using the following as a minimal starting point: {{}}\n\n[Show more details](https://github.com/mindsdb/mindsdb/security/code-scanning/78)",
    "line_number": 124,
    "enriched": "File: .github/workflows/build_deploy_staging.yml\nCode: @@ -67,11 +65,60 @@\n   # Run integration tests\n   # TODO: Run these against the deployed environment\n   run_tests:\n-    if: github.event.pull_request.merged == true\n+    #if: github.event.pull_request.merged == true\n     name: Run Integration Tests\n     needs: [deploy]\n     concurrency:\n       group: ${{ github.workflow }}-${{ github.head_ref || github.ref }}-tests\n       cancel-in-progress: true\n     uses: ./.github/workflows/test_on_deploy.yml\n-    secrets: inherit\n+    secrets: inherit\n+  \n+  slack_message:\n+    if: always()\n+    name: Notify Slack\n+    needs: [run_tests]\n+    runs-on: mdb-dev\n+    steps:\n+      - name: Notify of failing tests\n+        if: needs.run_tests.result == 'failure'\n+        uses: slackapi/slack-github-action@v1.26.0\n+        with:\n+          channel-id: ${{ secrets.SLACK_DEPLOYMENTS_CHANNEL_ID }}\n+          payload: |\n+            {\n+              \"attachments\": [\n+                {\n+                  \"color\": \"#FF4444\",\n+                  \"blocks\": [\n+                    {\n+                      \"type\": \"header\",\n+                      \"text\": {\n+                        \"type\": \"plain_text\",\n+                        \"text\": \"TEST RUN FAILED ON MAIN\",\n+                        \"emoji\": true\n+                      }\n+                    },\n+                    {\n+                      \"type\": \"section\",\n+                      \"text\": {\n+                        \"type\": \"mrkdwn\",\n+                        \"text\": \" \"\n+                      },\n+                      \"fields\": [\n+                        {\n+                          \"type\": \"mrkdwn\",\n+                          \"text\": \"*Commit*\\n<${{ github.server_url }}/${{ github.repository }}/commit/${{ github.sha }}|${{ github.sha }}>\"\n+                        },\n+                        {\n+                          \"type\": \"mrkdwn\",\n+                          \"text\": \"*Workflow Run*\\n<${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}|${{ github.workflow }}>\"\n+                        }\n+                      ]\n+                    }\n+                  ]\n+                }\n+              ]\n+            }\n+        env:\n+          SLACK_BOT_TOKEN: ${{ secrets.GH_ACTIONS_SLACK_BOT_TOKEN }}\nComment: ## Workflow does not contain permissions\n\nActions job or workflow does not limit the permissions of the GITHUB_TOKEN. Consider setting an explicit permissions block, using the following as a minimal starting point: {{}}\n\n[Show more details](https://github.com/mindsdb/mindsdb/security/code-scanning/78)",
    "subcategory": "interface",
    "category": "functional",
    "file_path": ".github/workflows/build_deploy_staging.yml",
    "pr_number": 10763,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2062748244,
    "comment_created_at": "2025-04-27T23:30:03Z"
  },
  {
    "code": "@@ -192,6 +196,33 @@ def native_query(self, query: str) -> Response:\n \n         if need_to_close:\n             self.disconnect()\n+\n+        return response\n+\n+    def insert(self, table_name: str, df: pd.DataFrame):\n+        need_to_close = not self.is_connected\n+\n+        connection = self.connect()\n+        with connection.cursor() as cur:\n+            try:\n+                with cur.copy(f'copy {table_name} from STDIN  WITH CSV') as copy:",
    "comment": "what if columns order is not the same?",
    "line_number": 208,
    "enriched": "File: mindsdb/integrations/handlers/postgres_handler/postgres_handler.py\nCode: @@ -192,6 +196,33 @@ def native_query(self, query: str) -> Response:\n \n         if need_to_close:\n             self.disconnect()\n+\n+        return response\n+\n+    def insert(self, table_name: str, df: pd.DataFrame):\n+        need_to_close = not self.is_connected\n+\n+        connection = self.connect()\n+        with connection.cursor() as cur:\n+            try:\n+                with cur.copy(f'copy {table_name} from STDIN  WITH CSV') as copy:\nComment: what if columns order is not the same?",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/postgres_handler/postgres_handler.py",
    "pr_number": 9243,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1625816415,
    "comment_created_at": "2024-06-04T11:14:17Z"
  },
  {
    "code": "@@ -880,6 +880,8 @@\n             cert_path = tempfile.mkstemp(prefix=\"mindsdb_cert_\", text=True)[1]\n             make_ssl_cert(cert_path)\n             atexit.register(lambda: os.remove(cert_path))\n+        elif not os.path.exists(cert_path):\n+            logger.error(f\"Certificate not exists: {cert_path}\")",
    "comment": "## Clear-text logging of sensitive information\n\nThis expression logs [sensitive data (certificate)](1) as clear text.\n\n[Show more details](https://github.com/mindsdb/mindsdb/security/code-scanning/31)",
    "line_number": 884,
    "enriched": "File: mindsdb/api/mysql/mysql_proxy/mysql_proxy.py\nCode: @@ -880,6 +880,8 @@\n             cert_path = tempfile.mkstemp(prefix=\"mindsdb_cert_\", text=True)[1]\n             make_ssl_cert(cert_path)\n             atexit.register(lambda: os.remove(cert_path))\n+        elif not os.path.exists(cert_path):\n+            logger.error(f\"Certificate not exists: {cert_path}\")\nComment: ## Clear-text logging of sensitive information\n\nThis expression logs [sensitive data (certificate)](1) as clear text.\n\n[Show more details](https://github.com/mindsdb/mindsdb/security/code-scanning/31)",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "mindsdb/api/mysql/mysql_proxy/mysql_proxy.py",
    "pr_number": 9699,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1747263778,
    "comment_created_at": "2024-09-06T14:59:39Z"
  },
  {
    "code": "@@ -154,3 +154,52 @@\n     with:\n       deploy-env: prod\n     secrets: inherit\n+\n+  slack_message:\n+    if: always()\n+    name: Notify Slack\n+    needs: [run_tests]\n+    runs-on: mdb-dev\n+    steps:\n+      - name: Notify of failing tests\n+        if: needs.run_tests.result != 'success'\n+        uses: slackapi/slack-github-action@v1.26.0\n+        with:\n+          channel-id: ${{ secrets.SLACK_ENG_CHANNEL_ID }}\n+          payload: |\n+            {\n+              \"attachments\": [\n+                {\n+                  \"color\": \"#FF4444\",\n+                  \"blocks\": [\n+                    {\n+                      \"type\": \"header\",\n+                      \"text\": {\n+                        \"type\": \"plain_text\",\n+                        \"text\": \"TEST RUN FAILED ON RELEASE\",\n+                        \"emoji\": true\n+                      }\n+                    },\n+                    {\n+                      \"type\": \"section\",\n+                      \"text\": {\n+                        \"type\": \"mrkdwn\",\n+                        \"text\": \" \"\n+                      },\n+                      \"fields\": [\n+                        {\n+                          \"type\": \"mrkdwn\",\n+                          \"text\": \"*Commit*\\n<${{ github.server_url }}/${{ github.repository }}/commit/${{ github.sha }}|${{ github.sha }}>\"\n+                        },\n+                        {\n+                          \"type\": \"mrkdwn\",\n+                          \"text\": \"*Workflow Run*\\n<${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}|${{ github.workflow }}>\"\n+                        }\n+                      ]\n+                    }\n+                  ]\n+                }\n+              ]\n+            }\n+        env:\n+          SLACK_BOT_TOKEN: ${{ secrets.GH_ACTIONS_SLACK_BOT_TOKEN }}",
    "comment": "## Workflow does not contain permissions\n\nActions job or workflow does not limit the permissions of the GITHUB_TOKEN. Consider setting an explicit permissions block, using the following as a minimal starting point: {{}}\n\n[Show more details](https://github.com/mindsdb/mindsdb/security/code-scanning/97)",
    "line_number": 205,
    "enriched": "File: .github/workflows/build_deploy_prod.yml\nCode: @@ -154,3 +154,52 @@\n     with:\n       deploy-env: prod\n     secrets: inherit\n+\n+  slack_message:\n+    if: always()\n+    name: Notify Slack\n+    needs: [run_tests]\n+    runs-on: mdb-dev\n+    steps:\n+      - name: Notify of failing tests\n+        if: needs.run_tests.result != 'success'\n+        uses: slackapi/slack-github-action@v1.26.0\n+        with:\n+          channel-id: ${{ secrets.SLACK_ENG_CHANNEL_ID }}\n+          payload: |\n+            {\n+              \"attachments\": [\n+                {\n+                  \"color\": \"#FF4444\",\n+                  \"blocks\": [\n+                    {\n+                      \"type\": \"header\",\n+                      \"text\": {\n+                        \"type\": \"plain_text\",\n+                        \"text\": \"TEST RUN FAILED ON RELEASE\",\n+                        \"emoji\": true\n+                      }\n+                    },\n+                    {\n+                      \"type\": \"section\",\n+                      \"text\": {\n+                        \"type\": \"mrkdwn\",\n+                        \"text\": \" \"\n+                      },\n+                      \"fields\": [\n+                        {\n+                          \"type\": \"mrkdwn\",\n+                          \"text\": \"*Commit*\\n<${{ github.server_url }}/${{ github.repository }}/commit/${{ github.sha }}|${{ github.sha }}>\"\n+                        },\n+                        {\n+                          \"type\": \"mrkdwn\",\n+                          \"text\": \"*Workflow Run*\\n<${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}|${{ github.workflow }}>\"\n+                        }\n+                      ]\n+                    }\n+                  ]\n+                }\n+              ]\n+            }\n+        env:\n+          SLACK_BOT_TOKEN: ${{ secrets.GH_ACTIONS_SLACK_BOT_TOKEN }}\nComment: ## Workflow does not contain permissions\n\nActions job or workflow does not limit the permissions of the GITHUB_TOKEN. Consider setting an explicit permissions block, using the following as a minimal starting point: {{}}\n\n[Show more details](https://github.com/mindsdb/mindsdb/security/code-scanning/97)",
    "subcategory": "interface",
    "category": "functional",
    "file_path": ".github/workflows/build_deploy_prod.yml",
    "pr_number": 10843,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2092101109,
    "comment_created_at": "2025-05-15T23:34:16Z"
  },
  {
    "code": "@@ -0,0 +1,174 @@\n+import base64\n+from io import BytesIO\n+import os\n+from typing import Union\n+from urllib.parse import urlparse\n+\n+import fitz  # PyMuPDF\n+from markitdown import MarkItDown\n+from openai import OpenAI\n+import requests\n+\n+\n+class ToMarkdown:\n+    \"\"\"\n+    Extracts the content of documents of various formats in markdown format.\n+    \"\"\"\n+    def __init__(self, use_llm: bool, llm_client: OpenAI = None, llm_model: str = None):\n+        \"\"\"\n+        Initializes the ToMarkdown class.\n+        \"\"\"\n+        # If use_llm is True, llm_client and llm_model must be provided.\n+        if use_llm and (llm_client is None or llm_model is None):\n+            raise ValueError('LLM client and model must be provided when use_llm is True.')\n+        # Only OpenAI is supported for now.\n+        if llm_client is not None and not isinstance(llm_client, OpenAI):\n+            raise ValueError('Only OpenAI models are supported at the moment.')\n+        # TODO: Add support for other LLMs?\n+        self.llm_client = llm_client\n+        self.llm_model = llm_model\n+\n+    def call(self, file_path_or_url: str) -> str:\n+        \"\"\"\n+        Converts a file to markdown.\n+        \"\"\"\n+        file_extension = self._get_file_extension(file_path_or_url)\n+        file = self._get_file(file_path_or_url)\n+\n+        if file_extension == '.pdf':\n+            return self._pdf_to_markdown(file)\n+        elif file_extension in ['.jpg', '.jpeg', '.png', '.gif']:\n+            return self._image_to_markdown(file)\n+        else:\n+            return self._other_to_markdown(file)\n+\n+    def _get_file(self, file_path_or_url: str) -> str:\n+        \"\"\"\n+        Retrieves the content of a file.\n+        \"\"\"\n+        parsed_url = urlparse(file_path_or_url)\n+        if parsed_url.scheme in ('http', 'https'):\n+            response = requests.get(file_path_or_url)\n+            if response.status_code == 200:\n+                return response\n+            else:\n+                raise RuntimeError(f'Unable to retrieve file from URL: {file_path_or_url}')\n+        else:\n+            with open(file_path_or_url, 'rb') as file:\n+                return file.read()\n+\n+    def _get_file_extension(self, file_path_or_url: str) -> str:\n+        \"\"\"\n+        Retrieves the file extension from a file path or URL.\n+        \"\"\"\n+        parsed_url = urlparse(file_path_or_url)\n+        if parsed_url.scheme in ('http', 'https'):\n+            return os.path.splitext(parsed_url.path)[1]",
    "comment": "sometimes url doesn't content extention or file name for example https://arxiv.org/pdf/2412.20512\r\nthe type of file can be detected by content type headers",
    "line_number": 66,
    "enriched": "File: mindsdb/interfaces/functions/to_markdown.py\nCode: @@ -0,0 +1,174 @@\n+import base64\n+from io import BytesIO\n+import os\n+from typing import Union\n+from urllib.parse import urlparse\n+\n+import fitz  # PyMuPDF\n+from markitdown import MarkItDown\n+from openai import OpenAI\n+import requests\n+\n+\n+class ToMarkdown:\n+    \"\"\"\n+    Extracts the content of documents of various formats in markdown format.\n+    \"\"\"\n+    def __init__(self, use_llm: bool, llm_client: OpenAI = None, llm_model: str = None):\n+        \"\"\"\n+        Initializes the ToMarkdown class.\n+        \"\"\"\n+        # If use_llm is True, llm_client and llm_model must be provided.\n+        if use_llm and (llm_client is None or llm_model is None):\n+            raise ValueError('LLM client and model must be provided when use_llm is True.')\n+        # Only OpenAI is supported for now.\n+        if llm_client is not None and not isinstance(llm_client, OpenAI):\n+            raise ValueError('Only OpenAI models are supported at the moment.')\n+        # TODO: Add support for other LLMs?\n+        self.llm_client = llm_client\n+        self.llm_model = llm_model\n+\n+    def call(self, file_path_or_url: str) -> str:\n+        \"\"\"\n+        Converts a file to markdown.\n+        \"\"\"\n+        file_extension = self._get_file_extension(file_path_or_url)\n+        file = self._get_file(file_path_or_url)\n+\n+        if file_extension == '.pdf':\n+            return self._pdf_to_markdown(file)\n+        elif file_extension in ['.jpg', '.jpeg', '.png', '.gif']:\n+            return self._image_to_markdown(file)\n+        else:\n+            return self._other_to_markdown(file)\n+\n+    def _get_file(self, file_path_or_url: str) -> str:\n+        \"\"\"\n+        Retrieves the content of a file.\n+        \"\"\"\n+        parsed_url = urlparse(file_path_or_url)\n+        if parsed_url.scheme in ('http', 'https'):\n+            response = requests.get(file_path_or_url)\n+            if response.status_code == 200:\n+                return response\n+            else:\n+                raise RuntimeError(f'Unable to retrieve file from URL: {file_path_or_url}')\n+        else:\n+            with open(file_path_or_url, 'rb') as file:\n+                return file.read()\n+\n+    def _get_file_extension(self, file_path_or_url: str) -> str:\n+        \"\"\"\n+        Retrieves the file extension from a file path or URL.\n+        \"\"\"\n+        parsed_url = urlparse(file_path_or_url)\n+        if parsed_url.scheme in ('http', 'https'):\n+            return os.path.splitext(parsed_url.path)[1]\nComment: sometimes url doesn't content extention or file name for example https://arxiv.org/pdf/2412.20512\r\nthe type of file can be detected by content type headers",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "mindsdb/interfaces/functions/to_markdown.py",
    "pr_number": 10608,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2024281292,
    "comment_created_at": "2025-04-02T07:57:03Z"
  },
  {
    "code": "@@ -198,4 +198,53 @@ def get_orders(self, **kwargs) -> List[Dict]:\n         api_session = self.handler.connect()\n         shopify.ShopifyResource.activate_session(api_session)\n         orders = shopify.Order.find(**kwargs)\n-        return [order.to_dict() for order in orders]\n\\ No newline at end of file\n+        return [order.to_dict() for order in orders]\n+\n+class InventoryItemsTable(APITable):\n+    \"\"\"The Shopify Inventory Table implementation\"\"\"\n+\n+    def select(self, query: ast.Select) -> pd.DataFrame:\n+        \"\"\"Pulls data from the Shopify \"GET /inventory\" API endpoint.\n+\n+        Parameters\n+        ----------\n+        query : ast.Select\n+           Given SQL SELECT query\n+\n+        Returns\n+        -------\n+        pd.DataFrame\n+            Shopify Inventory matching the query\n+\n+        Raises\n+        ------\n+        ValueError\n+            If the query contains an unsupported condition\n+        \"\"\"\n+        select_statement_parser = SELECTQueryParser(\n+            query,\n+            'inventory',\n+            self.get_columns()\n+        )\n+        selected_columns, where_conditions, inventoryitem_by_conditions, result_limit = select_statement_parser.parse_query()\n+\n+        inventory_df = pd.json_normalize(self.get_inventory(limit=result_limit))\n+\n+        select_statement_executor = SELECTQueryExecutor(\n+            inventory_df,\n+            selected_columns,\n+            where_conditions,\n+            inventoryitem_by_conditions\n+        )\n+        inventory_df = select_statement_executor.execute_query()\n+\n+        return inventory_df\n+\n+    def get_columns(self) -> List[Text]:\n+        return pd.json_normalize(self.get_inventory(limit=1)).columns.tolist()\n+\n+    def get_inventory(self, **kwargs) -> List[Dict]:",
    "comment": "I would rename this to get_inventory_items(), again, just for the sake of clarity, but this is not mandatory.",
    "line_number": 246,
    "enriched": "File: mindsdb/integrations/handlers/shopify_handler/shopify_tables.py\nCode: @@ -198,4 +198,53 @@ def get_orders(self, **kwargs) -> List[Dict]:\n         api_session = self.handler.connect()\n         shopify.ShopifyResource.activate_session(api_session)\n         orders = shopify.Order.find(**kwargs)\n-        return [order.to_dict() for order in orders]\n\\ No newline at end of file\n+        return [order.to_dict() for order in orders]\n+\n+class InventoryItemsTable(APITable):\n+    \"\"\"The Shopify Inventory Table implementation\"\"\"\n+\n+    def select(self, query: ast.Select) -> pd.DataFrame:\n+        \"\"\"Pulls data from the Shopify \"GET /inventory\" API endpoint.\n+\n+        Parameters\n+        ----------\n+        query : ast.Select\n+           Given SQL SELECT query\n+\n+        Returns\n+        -------\n+        pd.DataFrame\n+            Shopify Inventory matching the query\n+\n+        Raises\n+        ------\n+        ValueError\n+            If the query contains an unsupported condition\n+        \"\"\"\n+        select_statement_parser = SELECTQueryParser(\n+            query,\n+            'inventory',\n+            self.get_columns()\n+        )\n+        selected_columns, where_conditions, inventoryitem_by_conditions, result_limit = select_statement_parser.parse_query()\n+\n+        inventory_df = pd.json_normalize(self.get_inventory(limit=result_limit))\n+\n+        select_statement_executor = SELECTQueryExecutor(\n+            inventory_df,\n+            selected_columns,\n+            where_conditions,\n+            inventoryitem_by_conditions\n+        )\n+        inventory_df = select_statement_executor.execute_query()\n+\n+        return inventory_df\n+\n+    def get_columns(self) -> List[Text]:\n+        return pd.json_normalize(self.get_inventory(limit=1)).columns.tolist()\n+\n+    def get_inventory(self, **kwargs) -> List[Dict]:\nComment: I would rename this to get_inventory_items(), again, just for the sake of clarity, but this is not mandatory.",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "mindsdb/integrations/handlers/shopify_handler/shopify_tables.py",
    "pr_number": 7493,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1344065177,
    "comment_created_at": "2023-10-03T12:59:49Z"
  },
  {
    "code": "@@ -1,34 +1,55 @@\n-# Bare mindsdb with no extras is built as a separate stage for caching\n+# This first stage produces a file structure with ONLY files required to run `pip install .`\n+# We want to do this because we have a lot of dependencies which take a long time to install\n+# So we avoid invalidating the cache of `pip install .` at all costs by ignoring every other file\n+FROM python:3.10",
    "comment": "Layer name missing",
    "line_number": 4,
    "enriched": "File: docker/mindsdb.Dockerfile\nCode: @@ -1,34 +1,55 @@\n-# Bare mindsdb with no extras is built as a separate stage for caching\n+# This first stage produces a file structure with ONLY files required to run `pip install .`\n+# We want to do this because we have a lot of dependencies which take a long time to install\n+# So we avoid invalidating the cache of `pip install .` at all costs by ignoring every other file\n+FROM python:3.10\nComment: Layer name missing",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "docker/mindsdb.Dockerfile",
    "pr_number": 9194,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1597748989,
    "comment_created_at": "2024-05-13T00:17:33Z"
  },
  {
    "code": "@@ -0,0 +1,82 @@\n+# Welcome to the MindsDB Manual QA Testing for Cassandra Handler",
    "comment": "Rename all references to `Cassandra` with `ChromaDB` 😬 ",
    "line_number": 1,
    "enriched": "File: mindsdb/integrations/handlers/chromadb_handler/Manual_QA.md\nCode: @@ -0,0 +1,82 @@\n+# Welcome to the MindsDB Manual QA Testing for Cassandra Handler\nComment: Rename all references to `Cassandra` with `ChromaDB` 😬 ",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "mindsdb/integrations/handlers/chromadb_handler/Manual_QA.md",
    "pr_number": 6975,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1277973574,
    "comment_created_at": "2023-07-28T18:49:43Z"
  },
  {
    "code": "@@ -13,26 +13,22 @@ def read_metadata_as_string(self) -> str:\n         \"\"\"\n         if not self.is_data_catalog_supported():\n             return f\"Data catalog is not supported for database '{self.database_name}'.\"\n-\n         tables = self._read_metadata()\n-\n         if not tables:\n             self.logger.warning(f\"No metadata found for database '{self.database_name}'\")\n             return f\"No metadata found for database '{self.database_name}'\"\n-\n         metadata_str = \"Data Catalog: \\n\"\n         for table in tables:\n             metadata_str += table.as_string() + \"\\n\\n\"\n-\n         return metadata_str\n \n-    def _read_metadata(self) -> None:\n+    def _read_metadata(self) -> list:\n         \"\"\"\n         Read the metadata from the data catalog and return it in a structured format.\n         \"\"\"\n         query = db.session.query(db.MetaTables).filter_by(integration_id=self.integration_id)\n         if self.table_names:\n-            query = query.filter(db.MetaTables.name.in_(self.table_names))\n-\n+            cleaned_table_names = [name.strip(\"`\").split(\".\")[-1] for name in self.table_names]\n+            query = query.filter(db.MetaTables.name.in_(cleaned_table_names))",
    "comment": "**security**: Lack of input validation on `self.table_names` allows injection of malicious table names, potentially leading to unauthorized data access or information disclosure.\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n            cleaned_table_names = []\n            for name in self.table_names:\n                if not isinstance(name, str) or not name.replace('`', '').replace('.', '').isalnum():\n                    raise ValueError(f\"Invalid table name: {name}\")\n                cleaned_table_names.append(name.strip(\"`\").split(\".\")[-1])\n            query = query.filter(db.MetaTables.name.in_(cleaned_table_names))\n```\n</details>\n<!-- suggestion_end -->\n",
    "line_number": 32,
    "enriched": "File: mindsdb/interfaces/data_catalog/data_catalog_reader.py\nCode: @@ -13,26 +13,22 @@ def read_metadata_as_string(self) -> str:\n         \"\"\"\n         if not self.is_data_catalog_supported():\n             return f\"Data catalog is not supported for database '{self.database_name}'.\"\n-\n         tables = self._read_metadata()\n-\n         if not tables:\n             self.logger.warning(f\"No metadata found for database '{self.database_name}'\")\n             return f\"No metadata found for database '{self.database_name}'\"\n-\n         metadata_str = \"Data Catalog: \\n\"\n         for table in tables:\n             metadata_str += table.as_string() + \"\\n\\n\"\n-\n         return metadata_str\n \n-    def _read_metadata(self) -> None:\n+    def _read_metadata(self) -> list:\n         \"\"\"\n         Read the metadata from the data catalog and return it in a structured format.\n         \"\"\"\n         query = db.session.query(db.MetaTables).filter_by(integration_id=self.integration_id)\n         if self.table_names:\n-            query = query.filter(db.MetaTables.name.in_(self.table_names))\n-\n+            cleaned_table_names = [name.strip(\"`\").split(\".\")[-1] for name in self.table_names]\n+            query = query.filter(db.MetaTables.name.in_(cleaned_table_names))\nComment: **security**: Lack of input validation on `self.table_names` allows injection of malicious table names, potentially leading to unauthorized data access or information disclosure.\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n            cleaned_table_names = []\n            for name in self.table_names:\n                if not isinstance(name, str) or not name.replace('`', '').replace('.', '').isalnum():\n                    raise ValueError(f\"Invalid table name: {name}\")\n                cleaned_table_names.append(name.strip(\"`\").split(\".\")[-1])\n            query = query.filter(db.MetaTables.name.in_(cleaned_table_names))\n```\n</details>\n<!-- suggestion_end -->\n",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "mindsdb/interfaces/data_catalog/data_catalog_reader.py",
    "pr_number": 10953,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2116236934,
    "comment_created_at": "2025-05-30T16:35:37Z"
  },
  {
    "code": "@@ -0,0 +1,18 @@\n+import unittest\n+from mindsdb.integrations.handlers.dropbox_handler.dropbox_handler import DropboxHandler\n+\n+\n+class S3HandlerTest(unittest.TestCase):",
    "comment": "Let's rename this `DropboxHandlerTest`.",
    "line_number": 5,
    "enriched": "File: mindsdb/integrations/handlers/dropbox_handler/tests/test_dropbox_handler.py\nCode: @@ -0,0 +1,18 @@\n+import unittest\n+from mindsdb.integrations.handlers.dropbox_handler.dropbox_handler import DropboxHandler\n+\n+\n+class S3HandlerTest(unittest.TestCase):\nComment: Let's rename this `DropboxHandlerTest`.",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "mindsdb/integrations/handlers/dropbox_handler/tests/test_dropbox_handler.py",
    "pr_number": 9862,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1818438124,
    "comment_created_at": "2024-10-28T06:24:43Z"
  },
  {
    "code": "@@ -0,0 +1,35 @@\n+import json\n+import requests\n+\n+class DockerHubClient:",
    "comment": "Let's change the file name to be consistent to something like docker_client",
    "line_number": 4,
    "enriched": "File: mindsdb/integrations/handlers/dockerhub_handler/DockerHubClient.py\nCode: @@ -0,0 +1,35 @@\n+import json\n+import requests\n+\n+class DockerHubClient:\nComment: Let's change the file name to be consistent to something like docker_client",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "mindsdb/integrations/handlers/dockerhub_handler/DockerHubClient.py",
    "pr_number": 7665,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1352296931,
    "comment_created_at": "2023-10-10T11:40:04Z"
  },
  {
    "code": "@@ -7,12 +7,134 @@ MindsDB provides the `LLM()` function that lets users incorporate the LLM-genera\n \n ## Prerequisites\n \n-The `LLM()` function uses one of the OpenAI models and requires the OpenAI API key.\n+To use the `LLM()` function with MindsDB, choosing one of the available model providers and define the following environment variables.\n \n-To use the `LLM()` function with MindsDB, define the model to be used and the OpenAI API key as environment variables.\n+<AccordionGroup>\n \n-* The `LLM_FUNCTION_MODEL` environment variable should store the OpenAI model name, like `gpt-4`.\n-* The `OPENAI_API_KEY` environment variable should store the OpenAI API key value.\n+  <Accordion title=\"OpenAI\">\n+    Here are the environment variables for the OpenAI provider:\n+\n+    ```\n+    LLM_FUNCTION_MODEL_NAME\n+    LLM_FUNCTION_TEMPERATURE\n+    LLM_FUNCTION_MAX_RETRIES\n+    LLM_FUNCTION_MAX_TOKENS\n+    LLM_FUNCTION_OPENAI_API_BASE\n+    LLM_FUNCTION_OPENAI_API_KEY",
    "comment": "for openai key is better to use OPENAI_API_KEY name, which is also used by agent and openai handler\r\nthough LLM_FUNCTION_OPENAI_API_KEY should work too (only for llm function)",
    "line_number": 23,
    "enriched": "File: docs/mindsdb_sql/functions/llm_function.mdx\nCode: @@ -7,12 +7,134 @@ MindsDB provides the `LLM()` function that lets users incorporate the LLM-genera\n \n ## Prerequisites\n \n-The `LLM()` function uses one of the OpenAI models and requires the OpenAI API key.\n+To use the `LLM()` function with MindsDB, choosing one of the available model providers and define the following environment variables.\n \n-To use the `LLM()` function with MindsDB, define the model to be used and the OpenAI API key as environment variables.\n+<AccordionGroup>\n \n-* The `LLM_FUNCTION_MODEL` environment variable should store the OpenAI model name, like `gpt-4`.\n-* The `OPENAI_API_KEY` environment variable should store the OpenAI API key value.\n+  <Accordion title=\"OpenAI\">\n+    Here are the environment variables for the OpenAI provider:\n+\n+    ```\n+    LLM_FUNCTION_MODEL_NAME\n+    LLM_FUNCTION_TEMPERATURE\n+    LLM_FUNCTION_MAX_RETRIES\n+    LLM_FUNCTION_MAX_TOKENS\n+    LLM_FUNCTION_OPENAI_API_BASE\n+    LLM_FUNCTION_OPENAI_API_KEY\nComment: for openai key is better to use OPENAI_API_KEY name, which is also used by agent and openai handler\r\nthough LLM_FUNCTION_OPENAI_API_KEY should work too (only for llm function)",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "docs/mindsdb_sql/functions/llm_function.mdx",
    "pr_number": 10203,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1855844518,
    "comment_created_at": "2024-11-25T05:41:07Z"
  },
  {
    "code": "@@ -0,0 +1,258 @@\n+import io\n+import pandas as pd\n+from box_sdk_gen import BoxClient, BoxDeveloperTokenAuth, CreateFolderParent, UploadFileAttributes, UploadFileAttributesParentField\n+from box_sdk_gen.internal import utils\n+from typing import Dict, Optional, Text\n+\n+from mindsdb_sql.parser.ast.base import ASTNode\n+from mindsdb_sql.parser.ast import Select, Identifier, Insert\n+\n+from mindsdb.utilities import log\n+from mindsdb.integrations.libs.response import (\n+    HandlerStatusResponse as StatusResponse,\n+    HandlerResponse as Response,\n+    RESPONSE_TYPE,\n+)\n+from mindsdb.integrations.libs.api_handler import APIHandler, APIResource\n+\n+\n+class ListFilesTable(APIResource):\n+\n+    def list(self, conditions=None, limit=None, sort=None, targets=None, **kwargs):\n+        files = self.handler._list_files()\n+        data = []\n+        for file in files:\n+            item = {\n+                \"path\": file[\"path\"],\n+                \"name\": file[\"name\"],\n+                \"extension\": file[\"extension\"],\n+            }\n+            data.append(item)\n+        df = pd.DataFrame(data)\n+        return df\n+\n+    def get_columns(self):\n+        return [\"path\", \"name\", \"extension\"]\n+\n+\n+class FileTable(APIResource):\n+\n+    def _get_file_df(self):\n+        try:\n+            df = self.handler._read_file(self.table_name)\n+            if df is None:\n+                raise Exception(f\"No such file found for the path: {self.box_path}\")\n+\n+            return df\n+        except Exception as e:\n+            self.handler.logger.error(e)\n+\n+    def list(self, conditions=None, limit=None, sort=None, targets=None, **kwargs):\n+        return self._get_file_df()\n+\n+    def get_columns(self):\n+        df = self.handler._read_file(self.table_name)\n+        return df.columns.tolist()\n+\n+    def insert(self, query: Insert) -> None:\n+        columns = [col.name for col in query.columns]\n+        data = [dict(zip(columns, row)) for row in query.values]\n+        df_new = pd.DataFrame(data)\n+        df_existing = self._get_file_df()\n+        df_combined = pd.concat([df_existing, df_new], ignore_index=True)\n+        self.handler._write_file(self.table_name, df_combined)\n+\n+\n+class BoxHandler(APIHandler):\n+\n+    name = \"box\"\n+    supported_file_formats = [\"csv\", \"tsv\", \"json\", \"parquet\"]\n+\n+    def __init__(self, name: Text, connection_data: Optional[Dict], **kwargs):\n+        super().__init__(name)\n+        self.connection_data = connection_data\n+        self.kwargs = kwargs\n+        self.logger = log.getLogger(__name__)\n+        self.client = None\n+        self.is_connected = False\n+        self._files_table = ListFilesTable(self)\n+        self._register_table(\"files\", self._files_table)\n+\n+    def connect(self):\n+        try:\n+            if self.is_connected:\n+                return\n+            if \"token\" not in self.connection_data:\n+                raise ValueError(\"Developer token must be provided.\")\n+            auth = BoxDeveloperTokenAuth(token=self.connection_data[\"token\"])\n+            self.client = BoxClient(auth=auth)\n+            self.client.folders.get_folder_items(\"0\", limit=1)\n+            self.is_connected = True\n+            self.logger.info(\"Connected to Box\")\n+        except Exception as e:\n+            self.logger.error(f\"Error connecting to Box: {e}\")\n+\n+    def check_connection(self) -> StatusResponse:\n+        response = StatusResponse(False)\n+        try:\n+            self.connect()\n+            response.success = True\n+        except Exception as e:\n+            self.logger.error(f\"Error with Box Handler while establish connection: {e}\")\n+            response.error_message = str(e)\n+        return response\n+\n+    def disconnect(self):\n+        if not self.is_connected:\n+            return\n+        self.client = None\n+        self.is_connected = False\n+        self.logger.info(\"Disconnected from Box\")\n+\n+    def _read_as_content(self, file_path) -> None:\n+        \"\"\"\n+        Read files as content\n+        \"\"\"\n+        try:\n+            id = \"0\"",
    "comment": "Maybe we rename this to folder_id and add comment what we mean by 0?",
    "line_number": 117,
    "enriched": "File: mindsdb/integrations/handlers/box_handler/box_handler.py\nCode: @@ -0,0 +1,258 @@\n+import io\n+import pandas as pd\n+from box_sdk_gen import BoxClient, BoxDeveloperTokenAuth, CreateFolderParent, UploadFileAttributes, UploadFileAttributesParentField\n+from box_sdk_gen.internal import utils\n+from typing import Dict, Optional, Text\n+\n+from mindsdb_sql.parser.ast.base import ASTNode\n+from mindsdb_sql.parser.ast import Select, Identifier, Insert\n+\n+from mindsdb.utilities import log\n+from mindsdb.integrations.libs.response import (\n+    HandlerStatusResponse as StatusResponse,\n+    HandlerResponse as Response,\n+    RESPONSE_TYPE,\n+)\n+from mindsdb.integrations.libs.api_handler import APIHandler, APIResource\n+\n+\n+class ListFilesTable(APIResource):\n+\n+    def list(self, conditions=None, limit=None, sort=None, targets=None, **kwargs):\n+        files = self.handler._list_files()\n+        data = []\n+        for file in files:\n+            item = {\n+                \"path\": file[\"path\"],\n+                \"name\": file[\"name\"],\n+                \"extension\": file[\"extension\"],\n+            }\n+            data.append(item)\n+        df = pd.DataFrame(data)\n+        return df\n+\n+    def get_columns(self):\n+        return [\"path\", \"name\", \"extension\"]\n+\n+\n+class FileTable(APIResource):\n+\n+    def _get_file_df(self):\n+        try:\n+            df = self.handler._read_file(self.table_name)\n+            if df is None:\n+                raise Exception(f\"No such file found for the path: {self.box_path}\")\n+\n+            return df\n+        except Exception as e:\n+            self.handler.logger.error(e)\n+\n+    def list(self, conditions=None, limit=None, sort=None, targets=None, **kwargs):\n+        return self._get_file_df()\n+\n+    def get_columns(self):\n+        df = self.handler._read_file(self.table_name)\n+        return df.columns.tolist()\n+\n+    def insert(self, query: Insert) -> None:\n+        columns = [col.name for col in query.columns]\n+        data = [dict(zip(columns, row)) for row in query.values]\n+        df_new = pd.DataFrame(data)\n+        df_existing = self._get_file_df()\n+        df_combined = pd.concat([df_existing, df_new], ignore_index=True)\n+        self.handler._write_file(self.table_name, df_combined)\n+\n+\n+class BoxHandler(APIHandler):\n+\n+    name = \"box\"\n+    supported_file_formats = [\"csv\", \"tsv\", \"json\", \"parquet\"]\n+\n+    def __init__(self, name: Text, connection_data: Optional[Dict], **kwargs):\n+        super().__init__(name)\n+        self.connection_data = connection_data\n+        self.kwargs = kwargs\n+        self.logger = log.getLogger(__name__)\n+        self.client = None\n+        self.is_connected = False\n+        self._files_table = ListFilesTable(self)\n+        self._register_table(\"files\", self._files_table)\n+\n+    def connect(self):\n+        try:\n+            if self.is_connected:\n+                return\n+            if \"token\" not in self.connection_data:\n+                raise ValueError(\"Developer token must be provided.\")\n+            auth = BoxDeveloperTokenAuth(token=self.connection_data[\"token\"])\n+            self.client = BoxClient(auth=auth)\n+            self.client.folders.get_folder_items(\"0\", limit=1)\n+            self.is_connected = True\n+            self.logger.info(\"Connected to Box\")\n+        except Exception as e:\n+            self.logger.error(f\"Error connecting to Box: {e}\")\n+\n+    def check_connection(self) -> StatusResponse:\n+        response = StatusResponse(False)\n+        try:\n+            self.connect()\n+            response.success = True\n+        except Exception as e:\n+            self.logger.error(f\"Error with Box Handler while establish connection: {e}\")\n+            response.error_message = str(e)\n+        return response\n+\n+    def disconnect(self):\n+        if not self.is_connected:\n+            return\n+        self.client = None\n+        self.is_connected = False\n+        self.logger.info(\"Disconnected from Box\")\n+\n+    def _read_as_content(self, file_path) -> None:\n+        \"\"\"\n+        Read files as content\n+        \"\"\"\n+        try:\n+            id = \"0\"\nComment: Maybe we rename this to folder_id and add comment what we mean by 0?",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "mindsdb/integrations/handlers/box_handler/box_handler.py",
    "pr_number": 10124,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1838521871,
    "comment_created_at": "2024-11-12T17:39:21Z"
  },
  {
    "code": "@@ -314,3 +315,234 @@ def get_columns(self, table_name) -> Response:\n         result = self.native_query(q)\n         result.to_columns_table_response(map_type_fn=_map_type)\n         return result\n+\n+    def meta_get_tables(self, table_names: Optional[list[str]] = None) -> Response:\n+        \"\"\"\n+        Retrieves metadata information about the tables in the MySQL database",
    "comment": "**Correctness**: The MySQL handler is missing the meta_get_tables method that is required by the MetaDatabaseHandler base class\n\n",
    "line_number": 321,
    "enriched": "File: mindsdb/integrations/handlers/mysql_handler/mysql_handler.py\nCode: @@ -314,3 +315,234 @@ def get_columns(self, table_name) -> Response:\n         result = self.native_query(q)\n         result.to_columns_table_response(map_type_fn=_map_type)\n         return result\n+\n+    def meta_get_tables(self, table_names: Optional[list[str]] = None) -> Response:\n+        \"\"\"\n+        Retrieves metadata information about the tables in the MySQL database\nComment: **Correctness**: The MySQL handler is missing the meta_get_tables method that is required by the MetaDatabaseHandler base class\n\n",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "mindsdb/integrations/handlers/mysql_handler/mysql_handler.py",
    "pr_number": 11752,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2435612005,
    "comment_created_at": "2025-10-16T11:51:50Z"
  },
  {
    "code": "@@ -382,6 +384,9 @@ def df_to_documents(\n             raise ValueError(\n                 f\"page_content_column {page_content_column} not in dataframe columns\"\n             )\n+        if page_content_column == url_column_name:",
    "comment": "user should specify url column name in create",
    "line_number": 387,
    "enriched": "File: mindsdb/integrations/handlers/rag_handler/settings.py\nCode: @@ -382,6 +384,9 @@ def df_to_documents(\n             raise ValueError(\n                 f\"page_content_column {page_content_column} not in dataframe columns\"\n             )\n+        if page_content_column == url_column_name:\nComment: user should specify url column name in create",
    "subcategory": "support issues",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/rag_handler/settings.py",
    "pr_number": 8315,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1386935464,
    "comment_created_at": "2023-11-08T16:56:06Z"
  },
  {
    "code": "@@ -13,3 +13,5 @@ XXX Test Passed: <link to the test results in your GitHub repository>\n </br>\n XXX Test Failed: <link to the test results in your GitHub repository>, <link to the issue>\n </br>\n+\n+AI_WORKFLOW_SECTION (DEV_ENV) Test Passed: https://github.com/truesoni/hacktoberfest2023/blob/main/mindsdb/issues_7516/Testing.md",
    "comment": "replace AI_WORKFLOW_SECTION (DEV_ENV) with Create Project using Python\r\nThe images do not display properly\r\n![image](https://github.com/mindsdb/mindsdb/assets/32901682/4ebc3f34-3a98-4763-b8a7-9cb4d124bac4)\r\n",
    "line_number": 17,
    "enriched": "File: docs/Docs_Manual_QA.md\nCode: @@ -13,3 +13,5 @@ XXX Test Passed: <link to the test results in your GitHub repository>\n </br>\n XXX Test Failed: <link to the test results in your GitHub repository>, <link to the issue>\n </br>\n+\n+AI_WORKFLOW_SECTION (DEV_ENV) Test Passed: https://github.com/truesoni/hacktoberfest2023/blob/main/mindsdb/issues_7516/Testing.md\nComment: replace AI_WORKFLOW_SECTION (DEV_ENV) with Create Project using Python\r\nThe images do not display properly\r\n![image](https://github.com/mindsdb/mindsdb/assets/32901682/4ebc3f34-3a98-4763-b8a7-9cb4d124bac4)\r\n",
    "subcategory": "support issues",
    "category": "functional",
    "file_path": "docs/Docs_Manual_QA.md",
    "pr_number": 7530,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1345346926,
    "comment_created_at": "2023-10-04T07:47:43Z"
  },
  {
    "code": "@@ -106,7 +106,7 @@ ON mindsdb_slack.messages\n         SELECT 'slack-bot-channel-id' AS channel_id, answer AS text\n         FROM chatbot_model m\n         JOIN TABLE_DELTA s\n-        WHERE  s.user != 'U07J30KPAUF'\n+        WHERE  s.user != 'U07J30KPAUF' AND s.channel_id = 'slack-bot-channel-id'",
    "comment": "Today I had to use the LAST keyword in order to answer to only the most recent msgs.\r\n\r\nWe can add it in like this:\r\n```\r\nWHERE  s.user != 'slack-bot-id'\r\nAND s.channel_id = 'slack-bot-channel-id'\r\nAND s.created_at > LAST\r\n```",
    "line_number": 109,
    "enriched": "File: docs/mindsdb_sql/sql/create/trigger.mdx\nCode: @@ -106,7 +106,7 @@ ON mindsdb_slack.messages\n         SELECT 'slack-bot-channel-id' AS channel_id, answer AS text\n         FROM chatbot_model m\n         JOIN TABLE_DELTA s\n-        WHERE  s.user != 'U07J30KPAUF'\n+        WHERE  s.user != 'U07J30KPAUF' AND s.channel_id = 'slack-bot-channel-id'\nComment: Today I had to use the LAST keyword in order to answer to only the most recent msgs.\r\n\r\nWe can add it in like this:\r\n```\r\nWHERE  s.user != 'slack-bot-id'\r\nAND s.channel_id = 'slack-bot-channel-id'\r\nAND s.created_at > LAST\r\n```",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "docs/mindsdb_sql/sql/create/trigger.mdx",
    "pr_number": 10353,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1913169148,
    "comment_created_at": "2025-01-13T13:10:09Z"
  },
  {
    "code": "@@ -0,0 +1,163 @@\n+import ast\n+from typing import Dict, Optional, List\n+\n+import pandas as pd\n+\n+from mindsdb.integrations.libs.base import BaseMLEngine\n+from mindsdb.utilities import log\n+\n+from mindsdb.integrations.handlers.litellm_handler.settings import CompletionParameters\n+\n+from litellm import completion, batch_completion\n+\n+# these require no additional arguments\n+\n+logger = log.getLogger(__name__)\n+\n+\n+# todo add support for multiple api_keys in create engine. i.e. pass in keys for openai, anthropic, etc.",
    "comment": "Related to #8245, for reference.",
    "line_number": 18,
    "enriched": "File: mindsdb/integrations/handlers/litellm_handler/litellm_handler.py\nCode: @@ -0,0 +1,163 @@\n+import ast\n+from typing import Dict, Optional, List\n+\n+import pandas as pd\n+\n+from mindsdb.integrations.libs.base import BaseMLEngine\n+from mindsdb.utilities import log\n+\n+from mindsdb.integrations.handlers.litellm_handler.settings import CompletionParameters\n+\n+from litellm import completion, batch_completion\n+\n+# these require no additional arguments\n+\n+logger = log.getLogger(__name__)\n+\n+\n+# todo add support for multiple api_keys in create engine. i.e. pass in keys for openai, anthropic, etc.\nComment: Related to #8245, for reference.",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/litellm_handler/litellm_handler.py",
    "pr_number": 8507,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1441907322,
    "comment_created_at": "2024-01-04T15:32:33Z"
  },
  {
    "code": "@@ -142,6 +142,54 @@ USING\n                 ]};\n ```\n \n+## Predictions' Explanation\n+\n+With Lightwood, you can deploy the following types of models:\n+\n+- regressions models,\n+- classification models,\n+- time-series models.\n+\n+Predictions made by each type of model come with an explanation column, as below.\n+\n+<AccordionGroup>\n+\n+<Accordion title=\"Regression\">\n+In the case of regression models, the `target_explain` column contains the following information:\n+\n+```\n+{\"predicted_value\": 2951, \"confidence\": 0.99, \"anomaly\": null, \"truth\": null, \"confidence_lower_bound\": 2795, \"confidence_upper_bound\": 3107}\n+```\n+",
    "comment": "Let's add:\r\n\r\n> The upper and lower bounds are determined via conformal prediction, and correspond to the reported confidence score (which can be modified by the user).",
    "line_number": 163,
    "enriched": "File: docs/integrations/ai-engines/lightwood.mdx\nCode: @@ -142,6 +142,54 @@ USING\n                 ]};\n ```\n \n+## Predictions' Explanation\n+\n+With Lightwood, you can deploy the following types of models:\n+\n+- regressions models,\n+- classification models,\n+- time-series models.\n+\n+Predictions made by each type of model come with an explanation column, as below.\n+\n+<AccordionGroup>\n+\n+<Accordion title=\"Regression\">\n+In the case of regression models, the `target_explain` column contains the following information:\n+\n+```\n+{\"predicted_value\": 2951, \"confidence\": 0.99, \"anomaly\": null, \"truth\": null, \"confidence_lower_bound\": 2795, \"confidence_upper_bound\": 3107}\n+```\n+\nComment: Let's add:\r\n\r\n> The upper and lower bounds are determined via conformal prediction, and correspond to the reported confidence score (which can be modified by the user).",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "docs/integrations/ai-engines/lightwood.mdx",
    "pr_number": 9148,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1583238937,
    "comment_created_at": "2024-04-29T15:02:54Z"
  },
  {
    "code": "@@ -314,6 +317,9 @@ def select(\n         # always include distance\n         if distances is not None:\n             payload[TableField.DISTANCE.value] = distances\n+\n+        # casting embedding to otherwise pandas give error due to multidimensionality\n+        payload['embeddings'] = [str(i) for i in payload['embeddings']]",
    "comment": "is it related to only druid handler? or chromadb does't work with LAST with any source? ",
    "line_number": 322,
    "enriched": "File: mindsdb/integrations/handlers/chromadb_handler/chromadb_handler.py\nCode: @@ -314,6 +317,9 @@ def select(\n         # always include distance\n         if distances is not None:\n             payload[TableField.DISTANCE.value] = distances\n+\n+        # casting embedding to otherwise pandas give error due to multidimensionality\n+        payload['embeddings'] = [str(i) for i in payload['embeddings']]\nComment: is it related to only druid handler? or chromadb does't work with LAST with any source? ",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/chromadb_handler/chromadb_handler.py",
    "pr_number": 10174,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1846543271,
    "comment_created_at": "2024-11-18T12:59:03Z"
  },
  {
    "code": "@@ -96,6 +96,19 @@ An agent can be created, deleted, queried, and updated. Here is how you can do t\n        skills = ['test_skill']; -- this must be created with CREATE SKILL\n     ```\n \n+    Alternatively, you can create the model at the agent creation time based on [the model providers defined here](/integrations/ai-engines/langchain#openai).",
    "comment": "Here is the model is not created, only an angent. \r\nHow it works: mindsdb agent initializes langchain with llm settings (provider, api_key, model). Creation of model in mindsdb is not required. \r\nThe model in mindsdb is used when it exists (and provider is mindsdb) ",
    "line_number": 99,
    "enriched": "File: docs/mindsdb_sql/agents/agent.mdx\nCode: @@ -96,6 +96,19 @@ An agent can be created, deleted, queried, and updated. Here is how you can do t\n        skills = ['test_skill']; -- this must be created with CREATE SKILL\n     ```\n \n+    Alternatively, you can create the model at the agent creation time based on [the model providers defined here](/integrations/ai-engines/langchain#openai).\nComment: Here is the model is not created, only an angent. \r\nHow it works: mindsdb agent initializes langchain with llm settings (provider, api_key, model). Creation of model in mindsdb is not required. \r\nThe model in mindsdb is used when it exists (and provider is mindsdb) ",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "docs/mindsdb_sql/agents/agent.mdx",
    "pr_number": 9577,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1701714807,
    "comment_created_at": "2024-08-02T11:19:24Z"
  },
  {
    "code": "@@ -111,14 +111,61 @@ When choosing `bedrock` as the model provider, users should define the following\n \n #### `provider = 'snowflake'`\n \n-This provider is supported for `reranking_model`.\n+This provider is supported for `reranking_model`. Note that Snowflake Cortex AI does not offer embedding models as of now.\n \n When choosing `snowflake` as the model provider, users should choose one of the available models from [Snowflake Cortex AI](https://www.snowflake.com/en/product/features/cortex/) and define the following model parameters.\n \n * `model_name` stores the name of the model available via Snowflake Cortex AI.\n * `api_key` stores the Snowflake Cortex AI API key.\n * `snowflake_account_id` stores the Snowflake account ID.\n \n+<Accordion title = \"How to Generate the API key of Snowflake Cortex AI\">\n+\n+Follow the below steps to generate the API key.\n+\n+1. Generate a key pair according to [this instruction](https://docs.snowflake.com/en/user-guide/key-pair-auth) as below.\n+\n+    * Execute these commands in the console:\n+\n+        ```bash\n+        # generate private key\n+        openssl genrsa 2048 | openssl pkcs8 -topk8 -inform PEM -out rsa_key.p8 -nocrypt\n+        # generate public key\n+        openssl rsa -in rsa_key.p8 -pubout -out rsa_key.pub\n+        ```\n+    \n+    * Save the public key, that is, the content of rsa_key.pub, into your database user:\n+\n+        ```sql\n+        ALTER USER my_user SET RSA_PUBLIC_KEY = \"path/to/rsa_key.pub\"",
    "comment": "It has be the content of the file, not the part to it\r\nALTER USER my_user SET RSA_PUBLIC_KEY = \"...content of the rsa_key.pub...\"",
    "line_number": 140,
    "enriched": "File: docs/mindsdb_sql/knowledge_bases/create.mdx\nCode: @@ -111,14 +111,61 @@ When choosing `bedrock` as the model provider, users should define the following\n \n #### `provider = 'snowflake'`\n \n-This provider is supported for `reranking_model`.\n+This provider is supported for `reranking_model`. Note that Snowflake Cortex AI does not offer embedding models as of now.\n \n When choosing `snowflake` as the model provider, users should choose one of the available models from [Snowflake Cortex AI](https://www.snowflake.com/en/product/features/cortex/) and define the following model parameters.\n \n * `model_name` stores the name of the model available via Snowflake Cortex AI.\n * `api_key` stores the Snowflake Cortex AI API key.\n * `snowflake_account_id` stores the Snowflake account ID.\n \n+<Accordion title = \"How to Generate the API key of Snowflake Cortex AI\">\n+\n+Follow the below steps to generate the API key.\n+\n+1. Generate a key pair according to [this instruction](https://docs.snowflake.com/en/user-guide/key-pair-auth) as below.\n+\n+    * Execute these commands in the console:\n+\n+        ```bash\n+        # generate private key\n+        openssl genrsa 2048 | openssl pkcs8 -topk8 -inform PEM -out rsa_key.p8 -nocrypt\n+        # generate public key\n+        openssl rsa -in rsa_key.p8 -pubout -out rsa_key.pub\n+        ```\n+    \n+    * Save the public key, that is, the content of rsa_key.pub, into your database user:\n+\n+        ```sql\n+        ALTER USER my_user SET RSA_PUBLIC_KEY = \"path/to/rsa_key.pub\"\nComment: It has be the content of the file, not the part to it\r\nALTER USER my_user SET RSA_PUBLIC_KEY = \"...content of the rsa_key.pub...\"",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "docs/mindsdb_sql/knowledge_bases/create.mdx",
    "pr_number": 11417,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2263020303,
    "comment_created_at": "2025-08-08T13:41:44Z"
  },
  {
    "code": "@@ -0,0 +1,7 @@\n+langchain==0.0.186\n+SQLAlchemy\n+duckdb==0.7.1\n+chromadb==0.3.25\n+requests==2.28.0\n+sentence_transformers",
    "comment": "Perhaps add a lower bound if the default embedding model needs any?",
    "line_number": 6,
    "enriched": "File: mindsdb/integrations/handlers/chromadb_handler/requirements.txt\nCode: @@ -0,0 +1,7 @@\n+langchain==0.0.186\n+SQLAlchemy\n+duckdb==0.7.1\n+chromadb==0.3.25\n+requests==2.28.0\n+sentence_transformers\nComment: Perhaps add a lower bound if the default embedding model needs any?",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/chromadb_handler/requirements.txt",
    "pr_number": 6813,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1271550032,
    "comment_created_at": "2023-07-23T19:51:11Z"
  },
  {
    "code": "@@ -0,0 +1,23 @@\n+import os\n+\n+from flask import Flask, Response\n+from prometheus_client import generate_latest, multiprocess, CollectorRegistry\n+\n+from mindsdb.utilities import log\n+\n+_CONTENT_TYPE_LATEST = str('text/plain; version=0.0.4; charset=utf-8')\n+logger = log.getLogger(__name__)\n+\n+\n+def init_metrics(app: Flask):\n+    if os.environ.get('PROMETHEUS_MULTIPROC_DIR', None) is None:\n+        logger.warning('PROMETHEUS_MULTIPROC_DIR environment variable is not set and is needed for metrics server.')\n+        return\n+    # See: https://prometheus.github.io/client_python/multiprocess/\n+    registry = CollectorRegistry()\n+    multiprocess.MultiProcessCollector(registry)\n+\n+    # It's important that the PROMETHEUS_MULTIPROC_DIR env variable is set, and the dir is empty.\n+    @app.route('/metrics')\n+    def metrics():",
    "comment": "will this endpoint be accessible without authorization? ",
    "line_number": 22,
    "enriched": "File: mindsdb/metrics/server.py\nCode: @@ -0,0 +1,23 @@\n+import os\n+\n+from flask import Flask, Response\n+from prometheus_client import generate_latest, multiprocess, CollectorRegistry\n+\n+from mindsdb.utilities import log\n+\n+_CONTENT_TYPE_LATEST = str('text/plain; version=0.0.4; charset=utf-8')\n+logger = log.getLogger(__name__)\n+\n+\n+def init_metrics(app: Flask):\n+    if os.environ.get('PROMETHEUS_MULTIPROC_DIR', None) is None:\n+        logger.warning('PROMETHEUS_MULTIPROC_DIR environment variable is not set and is needed for metrics server.')\n+        return\n+    # See: https://prometheus.github.io/client_python/multiprocess/\n+    registry = CollectorRegistry()\n+    multiprocess.MultiProcessCollector(registry)\n+\n+    # It's important that the PROMETHEUS_MULTIPROC_DIR env variable is set, and the dir is empty.\n+    @app.route('/metrics')\n+    def metrics():\nComment: will this endpoint be accessible without authorization? ",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "mindsdb/metrics/server.py",
    "pr_number": 8899,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1519733590,
    "comment_created_at": "2024-03-11T13:38:28Z"
  },
  {
    "code": "@@ -48,7 +48,8 @@ def put(self, name: str):\n         \"\"\"\n \n         data = {}\n-        mindsdb_file_name = name.lower()\n+        base_name = Path(name).stem",
    "comment": "I would raise exception instead of replacing the name. Otherwise the user might don't find file using the same name and has to look at the list of files \r\nAlso this line removes only the extension and keeps other dots in file if they exists:\r\n`Path('asdf.df.csv').stem`",
    "line_number": 51,
    "enriched": "File: mindsdb/api/http/namespaces/file.py\nCode: @@ -48,7 +48,8 @@ def put(self, name: str):\n         \"\"\"\n \n         data = {}\n-        mindsdb_file_name = name.lower()\n+        base_name = Path(name).stem\nComment: I would raise exception instead of replacing the name. Otherwise the user might don't find file using the same name and has to look at the list of files \r\nAlso this line removes only the extension and keeps other dots in file if they exists:\r\n`Path('asdf.df.csv').stem`",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "mindsdb/api/http/namespaces/file.py",
    "pr_number": 11539,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2340496288,
    "comment_created_at": "2025-09-11T12:05:56Z"
  },
  {
    "code": "@@ -62,10 +62,10 @@ def create_engine(self, connection_args):\n             None\n         \"\"\"\n         connection_args = {k.lower(): v for k, v in connection_args.items()}\n-        api_key = connection_args.get('groq_api_key')\n+        api_key = connection_args.get(\"groq_api_key\")\n         if api_key is not None:\n-            org = connection_args.get('api_organization')\n-            api_base = connection_args.get('api_base') or os.environ.get('groq_BASE', groq_handler_config.BASE_URL)\n+            org = connection_args.get(\"api_organization\")\n+            api_base = connection_args.get(\"api_base\") or os.environ.get(\"groq_BASE\", groq_handler_config.BASE_URL)",
    "comment": "this should be `GROQ_BASE` ?",
    "line_number": 68,
    "enriched": "File: mindsdb/integrations/handlers/groq_handler/groq_handler.py\nCode: @@ -62,10 +62,10 @@ def create_engine(self, connection_args):\n             None\n         \"\"\"\n         connection_args = {k.lower(): v for k, v in connection_args.items()}\n-        api_key = connection_args.get('groq_api_key')\n+        api_key = connection_args.get(\"groq_api_key\")\n         if api_key is not None:\n-            org = connection_args.get('api_organization')\n-            api_base = connection_args.get('api_base') or os.environ.get('groq_BASE', groq_handler_config.BASE_URL)\n+            org = connection_args.get(\"api_organization\")\n+            api_base = connection_args.get(\"api_base\") or os.environ.get(\"groq_BASE\", groq_handler_config.BASE_URL)\nComment: this should be `GROQ_BASE` ?",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/groq_handler/groq_handler.py",
    "pr_number": 11734,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2426833047,
    "comment_created_at": "2025-10-13T16:41:46Z"
  },
  {
    "code": "@@ -19,7 +19,7 @@ The `CREATE MODEL` statement creates and trains a machine learning (ML) model.\n Here is the full syntax:\n \n ```sql\n-CREATE MODEL [IF NOT EXISTS] project_name.predictor_name\n+CREATE [OR REPLACE] MODEL project_name.predictor_name",
    "comment": "If not exists is also works, so it is possible to use:\r\n```sql\r\nCREATE or replace MODEL if not exists predictor_name\r\n```",
    "line_number": 22,
    "enriched": "File: docs/mindsdb_sql/sql/create/model.mdx\nCode: @@ -19,7 +19,7 @@ The `CREATE MODEL` statement creates and trains a machine learning (ML) model.\n Here is the full syntax:\n \n ```sql\n-CREATE MODEL [IF NOT EXISTS] project_name.predictor_name\n+CREATE [OR REPLACE] MODEL project_name.predictor_name\nComment: If not exists is also works, so it is possible to use:\r\n```sql\r\nCREATE or replace MODEL if not exists predictor_name\r\n```",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "docs/mindsdb_sql/sql/create/model.mdx",
    "pr_number": 8756,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1482911016,
    "comment_created_at": "2024-02-08T12:37:19Z"
  },
  {
    "code": "@@ -1,151 +1,78 @@\n-import os\n-import sys\n import logging\n-import traceback\n-\n-from mindsdb.utilities.config import Config\n-from functools import partial\n-\n-\n-class LoggerWrapper(object):\n-    def __init__(self, writer_arr, default_writer_pos):\n-        self._writer_arr = writer_arr\n-        self.default_writer_pos = default_writer_pos\n-\n-    def write(self, message):\n-        if len(message.strip(' \\n')) == 0:\n-            return\n-        if 'DEBUG:' in message:\n-            self._writer_arr[0](message)\n-        elif 'INFO:' in message:\n-            self._writer_arr[1](message)\n-        elif 'WARNING:' in message:\n-            self._writer_arr[2](message)\n-        elif 'ERROR:' in message:\n-            self._writer_arr[3](message)\n-        else:\n-            self._writer_arr[self.default_writer_pos](message)\n-\n-    def flush(self):\n-        pass\n-\n-    def isatty(self):\n-        return True  # assumes terminal attachment\n-\n-    def fileno(self):\n-        return 1  # stdout\n-\n-# class DbHandler(logging.Handler):\n-#     def __init__(self):\n-#         logging.Handler.__init__(self)\n-#         self.company_id = os.environ.get('MINDSDB_COMPANY_ID', None)\n-#\n-#     def emit(self, record):\n-#         self.format(record)\n-#         if (\n-#             len(record.message.strip(' \\n')) == 0\n-#             or (record.threadName == 'ray_print_logs' and 'mindsdb-logger' not in record.message)\n-#         ):\n-#             return\n-#\n-#         log_type = record.levelname\n-#         source = f'file: {record.pathname} - line: {record.lineno}'\n-#         payload = record.msg\n-#\n-#         if telemtry_enabled:\n-#             pass\n-#             # @TODO: Enable once we are sure no sensitive info is being outputed in the logs\n-#             # if log_type in ['INFO']:\n-#             #    add_breadcrumb(\n-#             #        category='auth',\n-#             #        message=str(payload),\n-#             #        level='info',\n-#             #    )\n-#             # Might be too much traffic if we send this for users with slow networks\n-#             # if log_type in ['DEBUG']:\n-#             #    add_breadcrumb(\n-#             #        category='auth',\n-#             #        message=str(payload),\n-#             #        level='debug',\n-#             #    )\n-#\n-#         if log_type in ['ERROR', 'WARNING']:\n-#             trace = str(traceback.format_stack(limit=20))\n-#             trac_log = Log(log_type='traceback', source=source, payload=trace, company_id=self.company_id)\n-#             session.add(trac_log)\n-#             session.commit()\n-#\n-#             if telemtry_enabled:\n-#                 add_breadcrumb(\n-#                     category='stack_trace',\n-#                     message=trace,\n-#                     level='info',\n-#                 )\n-#                 if log_type in ['ERROR']:\n-#                     capture_message(str(payload))\n-#                 if log_type in ['WARNING']:\n-#                     capture_message(str(payload))\n-#\n-#         log = Log(log_type=str(log_type), source=source, payload=str(payload), company_id=self.company_id)\n-#         session.add(log)\n-#         session.commit()\n-\n-# default logger\n-logger = logging.getLogger('dummy')\n-\n-\n-def initialize_log(config=None, logger_name='main', wrap_print=False):\n-    global logger\n-    if config is None:\n-        config = Config().get_all()\n-\n-    telemtry_enabled = os.getenv('CHECK_FOR_UPDATES', '1').lower() not in ['0', 'false', 'False']\n-\n-    if telemtry_enabled:\n-        try:\n-            import sentry_sdk\n-            from sentry_sdk import capture_message, add_breadcrumb\n-            sentry_sdk.init(\n-                \"https://29e64dbdf325404ebf95473d5f4a54d3@o404567.ingest.sentry.io/5633566\",\n-                traces_sample_rate=0  # Set to `1` to experiment with performance metrics\n-            )\n-        except (ImportError, ModuleNotFoundError) as e:\n-            raise Exception(f\"to use telemetry please install 'pip install mindsdb[telemetry]': {e}\")\n-\n-    ''' Create new logger\n-    :param config: object, app config\n-    :param logger_name: str, name of logger\n-    :param wrap_print: bool, if true, then print() calls will be wrapped by log.debug() function.\n-    '''\n-    log = logging.getLogger(f'mindsdb.{logger_name}')\n-    log.propagate = False\n-    log.setLevel(min(\n-        getattr(logging, config['log']['level']['console']),\n-        getattr(logging, config['log']['level']['file'])\n-    ))\n-\n-    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n-\n-    console_handler = logging.StreamHandler()\n-    console_handler.setLevel(config['log']['level'].get('console', logging.INFO))\n-    console_handler.setFormatter(formatter)\n-    log.addHandler(console_handler)\n-\n-    # db_handler = DbHandler()\n-    # db_handler.setLevel(config['log']['level'].get('db', logging.WARNING))\n-    # db_handler.setFormatter(formatter)\n-    # log.addHandler(db_handler)\n-\n-    if wrap_print:\n-        sys.stdout = LoggerWrapper([log.debug, log.info, log.warning, log.error], 1)\n-        sys.stderr = LoggerWrapper([log.debug, log.info, log.warning, log.error], 3)\n-\n-    log.error = partial(log.error, exc_info=True)\n-    logger = log\n-\n-\n-def get_log(logger_name=None):\n-    if logger_name is None:\n-        return logging.getLogger('mindsdb')\n-    return logging.getLogger(f'mindsdb.{logger_name}')\n-\n+import os\n+from logging.config import dictConfig\n+\n+logging_initialized = False\n+\n+\n+class ColorFormatter(logging.Formatter):\n+\n+    green = \"\\x1b[32;20m\"\n+    default = \"\\x1b[39;20m\"\n+    yellow = \"\\x1b[33;20m\"\n+    red = \"\\x1b[31;20m\"\n+    bold_red = \"\\x1b[31;1m\"\n+    reset = \"\\x1b[0m\"\n+    format = \"%(asctime)s %(processName)15s %(levelname)-8s %(name)s: %(message)s\"\n+\n+    FORMATS = {\n+        logging.DEBUG: logging.Formatter(green + format + reset),\n+        logging.INFO: logging.Formatter(default + format + reset),\n+        logging.WARNING: logging.Formatter(yellow + format + reset),\n+        logging.ERROR: logging.Formatter(red + format + reset),\n+        logging.CRITICAL: logging.Formatter(bold_red + format + reset),\n+    }\n+\n+    def format(self, record):\n+        log_fmt = self.FORMATS.get(record.levelno)\n+        return log_fmt.format(record)\n+\n+\n+def configure_logging():\n+    mindsdb_level = os.environ.get(\"MINDSDB_LOG_LEVEL\", None)\n+    if mindsdb_level is not None:\n+        mindsdb_level = getattr(logging, mindsdb_level)\n+    else:\n+        mindsdb_level = logging.INFO\n+\n+    logging_config = dict(\n+        version=1,\n+        formatters={\"f\": {\"()\": ColorFormatter}},\n+        handlers={\n+            \"console\": {\n+                \"class\": \"logging.StreamHandler\",\n+                \"formatter\": \"f\",\n+            }\n+        },\n+        loggers={\n+            \"\": {  # root logger\n+                \"handlers\": [\"console\"],\n+                \"level\": logging.WARNING,\n+            },\n+            \"__main__\": {\n+                \"level\": mindsdb_level,\n+            },\n+            \"mindsdb\": {\n+                \"level\": mindsdb_level,\n+            },\n+            \"alembic\": {\n+                \"level\": logging.DEBUG,\n+            },\n+        },\n+    )\n+    dictConfig(logging_config)\n+\n+\n+# I would prefer to leave code to use logging.getLogger(), but there are a lot of complicated situations\n+# in MindsDB with processes being spawned that require logging to be configured again in a lot of cases.\n+# Using a custom logger-getter like this lets us do that logic here, once.\n+def getLogger(name=None):\n+    \"\"\"\n+    Get a new logger, configuring logging first if it hasn't been done yet.\n+    \"\"\"\n+    global logging_initialized\n+    if not logging_initialized:\n+        configure_logging()\n+        logging_initialized = True\n+\n+    return logging.getLogger(name)",
    "comment": "This line will create a child logger of the root logger. It should be something like `logging.getLogger(\"mindsdb\").getChild(name)`",
    "line_number": 78,
    "enriched": "File: mindsdb/utilities/log.py\nCode: @@ -1,151 +1,78 @@\n-import os\n-import sys\n import logging\n-import traceback\n-\n-from mindsdb.utilities.config import Config\n-from functools import partial\n-\n-\n-class LoggerWrapper(object):\n-    def __init__(self, writer_arr, default_writer_pos):\n-        self._writer_arr = writer_arr\n-        self.default_writer_pos = default_writer_pos\n-\n-    def write(self, message):\n-        if len(message.strip(' \\n')) == 0:\n-            return\n-        if 'DEBUG:' in message:\n-            self._writer_arr[0](message)\n-        elif 'INFO:' in message:\n-            self._writer_arr[1](message)\n-        elif 'WARNING:' in message:\n-            self._writer_arr[2](message)\n-        elif 'ERROR:' in message:\n-            self._writer_arr[3](message)\n-        else:\n-            self._writer_arr[self.default_writer_pos](message)\n-\n-    def flush(self):\n-        pass\n-\n-    def isatty(self):\n-        return True  # assumes terminal attachment\n-\n-    def fileno(self):\n-        return 1  # stdout\n-\n-# class DbHandler(logging.Handler):\n-#     def __init__(self):\n-#         logging.Handler.__init__(self)\n-#         self.company_id = os.environ.get('MINDSDB_COMPANY_ID', None)\n-#\n-#     def emit(self, record):\n-#         self.format(record)\n-#         if (\n-#             len(record.message.strip(' \\n')) == 0\n-#             or (record.threadName == 'ray_print_logs' and 'mindsdb-logger' not in record.message)\n-#         ):\n-#             return\n-#\n-#         log_type = record.levelname\n-#         source = f'file: {record.pathname} - line: {record.lineno}'\n-#         payload = record.msg\n-#\n-#         if telemtry_enabled:\n-#             pass\n-#             # @TODO: Enable once we are sure no sensitive info is being outputed in the logs\n-#             # if log_type in ['INFO']:\n-#             #    add_breadcrumb(\n-#             #        category='auth',\n-#             #        message=str(payload),\n-#             #        level='info',\n-#             #    )\n-#             # Might be too much traffic if we send this for users with slow networks\n-#             # if log_type in ['DEBUG']:\n-#             #    add_breadcrumb(\n-#             #        category='auth',\n-#             #        message=str(payload),\n-#             #        level='debug',\n-#             #    )\n-#\n-#         if log_type in ['ERROR', 'WARNING']:\n-#             trace = str(traceback.format_stack(limit=20))\n-#             trac_log = Log(log_type='traceback', source=source, payload=trace, company_id=self.company_id)\n-#             session.add(trac_log)\n-#             session.commit()\n-#\n-#             if telemtry_enabled:\n-#                 add_breadcrumb(\n-#                     category='stack_trace',\n-#                     message=trace,\n-#                     level='info',\n-#                 )\n-#                 if log_type in ['ERROR']:\n-#                     capture_message(str(payload))\n-#                 if log_type in ['WARNING']:\n-#                     capture_message(str(payload))\n-#\n-#         log = Log(log_type=str(log_type), source=source, payload=str(payload), company_id=self.company_id)\n-#         session.add(log)\n-#         session.commit()\n-\n-# default logger\n-logger = logging.getLogger('dummy')\n-\n-\n-def initialize_log(config=None, logger_name='main', wrap_print=False):\n-    global logger\n-    if config is None:\n-        config = Config().get_all()\n-\n-    telemtry_enabled = os.getenv('CHECK_FOR_UPDATES', '1').lower() not in ['0', 'false', 'False']\n-\n-    if telemtry_enabled:\n-        try:\n-            import sentry_sdk\n-            from sentry_sdk import capture_message, add_breadcrumb\n-            sentry_sdk.init(\n-                \"https://29e64dbdf325404ebf95473d5f4a54d3@o404567.ingest.sentry.io/5633566\",\n-                traces_sample_rate=0  # Set to `1` to experiment with performance metrics\n-            )\n-        except (ImportError, ModuleNotFoundError) as e:\n-            raise Exception(f\"to use telemetry please install 'pip install mindsdb[telemetry]': {e}\")\n-\n-    ''' Create new logger\n-    :param config: object, app config\n-    :param logger_name: str, name of logger\n-    :param wrap_print: bool, if true, then print() calls will be wrapped by log.debug() function.\n-    '''\n-    log = logging.getLogger(f'mindsdb.{logger_name}')\n-    log.propagate = False\n-    log.setLevel(min(\n-        getattr(logging, config['log']['level']['console']),\n-        getattr(logging, config['log']['level']['file'])\n-    ))\n-\n-    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n-\n-    console_handler = logging.StreamHandler()\n-    console_handler.setLevel(config['log']['level'].get('console', logging.INFO))\n-    console_handler.setFormatter(formatter)\n-    log.addHandler(console_handler)\n-\n-    # db_handler = DbHandler()\n-    # db_handler.setLevel(config['log']['level'].get('db', logging.WARNING))\n-    # db_handler.setFormatter(formatter)\n-    # log.addHandler(db_handler)\n-\n-    if wrap_print:\n-        sys.stdout = LoggerWrapper([log.debug, log.info, log.warning, log.error], 1)\n-        sys.stderr = LoggerWrapper([log.debug, log.info, log.warning, log.error], 3)\n-\n-    log.error = partial(log.error, exc_info=True)\n-    logger = log\n-\n-\n-def get_log(logger_name=None):\n-    if logger_name is None:\n-        return logging.getLogger('mindsdb')\n-    return logging.getLogger(f'mindsdb.{logger_name}')\n-\n+import os\n+from logging.config import dictConfig\n+\n+logging_initialized = False\n+\n+\n+class ColorFormatter(logging.Formatter):\n+\n+    green = \"\\x1b[32;20m\"\n+    default = \"\\x1b[39;20m\"\n+    yellow = \"\\x1b[33;20m\"\n+    red = \"\\x1b[31;20m\"\n+    bold_red = \"\\x1b[31;1m\"\n+    reset = \"\\x1b[0m\"\n+    format = \"%(asctime)s %(processName)15s %(levelname)-8s %(name)s: %(message)s\"\n+\n+    FORMATS = {\n+        logging.DEBUG: logging.Formatter(green + format + reset),\n+        logging.INFO: logging.Formatter(default + format + reset),\n+        logging.WARNING: logging.Formatter(yellow + format + reset),\n+        logging.ERROR: logging.Formatter(red + format + reset),\n+        logging.CRITICAL: logging.Formatter(bold_red + format + reset),\n+    }\n+\n+    def format(self, record):\n+        log_fmt = self.FORMATS.get(record.levelno)\n+        return log_fmt.format(record)\n+\n+\n+def configure_logging():\n+    mindsdb_level = os.environ.get(\"MINDSDB_LOG_LEVEL\", None)\n+    if mindsdb_level is not None:\n+        mindsdb_level = getattr(logging, mindsdb_level)\n+    else:\n+        mindsdb_level = logging.INFO\n+\n+    logging_config = dict(\n+        version=1,\n+        formatters={\"f\": {\"()\": ColorFormatter}},\n+        handlers={\n+            \"console\": {\n+                \"class\": \"logging.StreamHandler\",\n+                \"formatter\": \"f\",\n+            }\n+        },\n+        loggers={\n+            \"\": {  # root logger\n+                \"handlers\": [\"console\"],\n+                \"level\": logging.WARNING,\n+            },\n+            \"__main__\": {\n+                \"level\": mindsdb_level,\n+            },\n+            \"mindsdb\": {\n+                \"level\": mindsdb_level,\n+            },\n+            \"alembic\": {\n+                \"level\": logging.DEBUG,\n+            },\n+        },\n+    )\n+    dictConfig(logging_config)\n+\n+\n+# I would prefer to leave code to use logging.getLogger(), but there are a lot of complicated situations\n+# in MindsDB with processes being spawned that require logging to be configured again in a lot of cases.\n+# Using a custom logger-getter like this lets us do that logic here, once.\n+def getLogger(name=None):\n+    \"\"\"\n+    Get a new logger, configuring logging first if it hasn't been done yet.\n+    \"\"\"\n+    global logging_initialized\n+    if not logging_initialized:\n+        configure_logging()\n+        logging_initialized = True\n+\n+    return logging.getLogger(name)\nComment: This line will create a child logger of the root logger. It should be something like `logging.getLogger(\"mindsdb\").getChild(name)`",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "mindsdb/utilities/log.py",
    "pr_number": 6807,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1393332472,
    "comment_created_at": "2023-11-14T21:41:19Z"
  },
  {
    "code": "@@ -32,19 +32,40 @@ parameters = {\n <Tip>\n You need a Google account in order to use this integration. Here is how to get the credentials file:\n \n-1. Go to https://console.cloud.google.com/apis/dashboard.\n-2. In the `Enabled APIs & services` tab, click on `Enable APIs and Services` button, search for `Gmail API`, and enable it.\n-3. In the `OAuth consent screen` tab, create a new project and an app:\n-    - name your project,\n-    - choose user type as external,\n-    - add app name and user support email,\n-    - add the following scopes: `.../auth/userinfo.profile`, `.../auth/userinfo.email`, `openid`,\n-    - add a test user,\n-    - save and go back to the dashboard.\n-4. In the `Credentials` tab, click on `Create Credentials` button and choose `OAuth client ID`:\n-    - choose application type as `Desktop app` and give it a name,\n-    - click on `Create`,\n-    - download the JSON file.\n+1. Create a Google Cloud Platform (GCP) Project:\n+\n+    1.1 Go to the GCP Console (https://console.cloud.google.com/).\n+    \n+    1.2 If you haven't created a project before, you'll be prompted to do so now.\n+    \n+    1.3 Give your new project a name.\n+    \n+    1.4 Click `Create` to create the new project.\n+\n+2. Enable the Gmail API:\n+\n+    2.1 In the GCP Console, select your project.\n+\n+    2.2 Navigate to `APIs & Services` > `Library`.",
    "comment": "Actually, that's the tab name:\r\n2.2 Navigate to `APIs & Services` > `Enabled APIs & services`.",
    "line_number": 49,
    "enriched": "File: docs/app-integrations/gmail.mdx\nCode: @@ -32,19 +32,40 @@ parameters = {\n <Tip>\n You need a Google account in order to use this integration. Here is how to get the credentials file:\n \n-1. Go to https://console.cloud.google.com/apis/dashboard.\n-2. In the `Enabled APIs & services` tab, click on `Enable APIs and Services` button, search for `Gmail API`, and enable it.\n-3. In the `OAuth consent screen` tab, create a new project and an app:\n-    - name your project,\n-    - choose user type as external,\n-    - add app name and user support email,\n-    - add the following scopes: `.../auth/userinfo.profile`, `.../auth/userinfo.email`, `openid`,\n-    - add a test user,\n-    - save and go back to the dashboard.\n-4. In the `Credentials` tab, click on `Create Credentials` button and choose `OAuth client ID`:\n-    - choose application type as `Desktop app` and give it a name,\n-    - click on `Create`,\n-    - download the JSON file.\n+1. Create a Google Cloud Platform (GCP) Project:\n+\n+    1.1 Go to the GCP Console (https://console.cloud.google.com/).\n+    \n+    1.2 If you haven't created a project before, you'll be prompted to do so now.\n+    \n+    1.3 Give your new project a name.\n+    \n+    1.4 Click `Create` to create the new project.\n+\n+2. Enable the Gmail API:\n+\n+    2.1 In the GCP Console, select your project.\n+\n+    2.2 Navigate to `APIs & Services` > `Library`.\nComment: Actually, that's the tab name:\r\n2.2 Navigate to `APIs & Services` > `Enabled APIs & services`.",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "docs/app-integrations/gmail.mdx",
    "pr_number": 6590,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1229646786,
    "comment_created_at": "2023-06-14T13:44:33Z"
  },
  {
    "code": "@@ -62,11 +62,6 @@ def connect(self):\n         if re.match(r'/?.*tls=false', self.host.lower()):\n             kwargs['tls'] = False\n \n-        if re.match(r'.*.mongodb.net', self.host.lower()) and kwargs.get('tls', None) is None:",
    "comment": "@ZoranPandovski From what I have understood, this block of code was added to ensure secure connections to MongoDB Atlas. Is it OK to remove it?\r\n\r\nI think we should also be able to solve the CodeQL alert by changing the regular expression as follows?\r\n```\r\nif re.match(r'.*\\.mongodb.net', self.host.lower()) and kwargs.get('tls', None) is None:\r\n    kwargs['tlsCAFile'] = certifi.where()\r\n    if kwargs.get('tls', None) is None:\r\n        kwargs['tls'] = True\r\n```\r\n\r\nBy escaping the dot before `mongodb.net`, we can ensure that this dot matches a literal dot. This will not match other hosts as the CodeQL alert suggests. This is only necessary if keeping this block of code is required.",
    "line_number": 65,
    "enriched": "File: mindsdb/integrations/handlers/mongodb_handler/mongodb_handler.py\nCode: @@ -62,11 +62,6 @@ def connect(self):\n         if re.match(r'/?.*tls=false', self.host.lower()):\n             kwargs['tls'] = False\n \n-        if re.match(r'.*.mongodb.net', self.host.lower()) and kwargs.get('tls', None) is None:\nComment: @ZoranPandovski From what I have understood, this block of code was added to ensure secure connections to MongoDB Atlas. Is it OK to remove it?\r\n\r\nI think we should also be able to solve the CodeQL alert by changing the regular expression as follows?\r\n```\r\nif re.match(r'.*\\.mongodb.net', self.host.lower()) and kwargs.get('tls', None) is None:\r\n    kwargs['tlsCAFile'] = certifi.where()\r\n    if kwargs.get('tls', None) is None:\r\n        kwargs['tls'] = True\r\n```\r\n\r\nBy escaping the dot before `mongodb.net`, we can ensure that this dot matches a literal dot. This will not match other hosts as the CodeQL alert suggests. This is only necessary if keeping this block of code is required.",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/mongodb_handler/mongodb_handler.py",
    "pr_number": 9274,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1621836636,
    "comment_created_at": "2024-05-31T06:57:07Z"
  },
  {
    "code": "@@ -291,8 +304,13 @@ def run_agent(self, df: pd.DataFrame, agent: AgentExecutor, args: Dict, pred_arg\n         def _invoke_agent_executor_with_prompt(agent_executor, prompt):\n             if not prompt:\n                 return ''\n-\n-            answer = agent_executor.invoke(prompt)\n+            try:\n+                answer = agent_executor.invoke(prompt)\n+            except Exception as e:\n+                answer = str(e)",
    "comment": "Catches certain agent errors and only raises exception when absolutely necessary",
    "line_number": 310,
    "enriched": "File: mindsdb/integrations/handlers/langchain_handler/langchain_handler.py\nCode: @@ -291,8 +304,13 @@ def run_agent(self, df: pd.DataFrame, agent: AgentExecutor, args: Dict, pred_arg\n         def _invoke_agent_executor_with_prompt(agent_executor, prompt):\n             if not prompt:\n                 return ''\n-\n-            answer = agent_executor.invoke(prompt)\n+            try:\n+                answer = agent_executor.invoke(prompt)\n+            except Exception as e:\n+                answer = str(e)\nComment: Catches certain agent errors and only raises exception when absolutely necessary",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/langchain_handler/langchain_handler.py",
    "pr_number": 9093,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1570535861,
    "comment_created_at": "2024-04-18T11:29:23Z"
  },
  {
    "code": "@@ -1,12 +1,15 @@\n import paypalrestsdk\n+from mindsdb.integrations.utilities.sql_utils import extract_comparison_conditions",
    "comment": "@gyes51y767p This is not the ideal fix. If you take a closer look at the code, you will see that there are two variables that store the `WHERE` conditions; `where_conditions` (returned by `select_statement_parser.parse_query()`) and `conditions` (returned by extract_comparison_conditions). This is the same thing. So, we should keep only one; wherever it is used downstream should also be handled.",
    "line_number": 2,
    "enriched": "File: mindsdb/integrations/handlers/paypal_handler/paypal_tables.py\nCode: @@ -1,12 +1,15 @@\n import paypalrestsdk\n+from mindsdb.integrations.utilities.sql_utils import extract_comparison_conditions\nComment: @gyes51y767p This is not the ideal fix. If you take a closer look at the code, you will see that there are two variables that store the `WHERE` conditions; `where_conditions` (returned by `select_statement_parser.parse_query()`) and `conditions` (returned by extract_comparison_conditions). This is the same thing. So, we should keep only one; wherever it is used downstream should also be handled.",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/paypal_handler/paypal_tables.py",
    "pr_number": 8467,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1415562730,
    "comment_created_at": "2023-12-05T12:54:02Z"
  },
  {
    "code": "@@ -12,6 +12,10 @@\n logger = log.getLogger(__name__)\n \n \n+def get_tmp_dir() -> Path:",
    "comment": "It should be a constant, not a function.",
    "line_number": 15,
    "enriched": "File: mindsdb/utilities/fs.py\nCode: @@ -12,6 +12,10 @@\n logger = log.getLogger(__name__)\n \n \n+def get_tmp_dir() -> Path:\nComment: It should be a constant, not a function.",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "mindsdb/utilities/fs.py",
    "pr_number": 11411,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2262397945,
    "comment_created_at": "2025-08-08T09:14:11Z"
  },
  {
    "code": "@@ -1,37 +1,32 @@\n ---\n-title: MindsDB Mongo-QL\n+title: Overview\n sidebarTitle: Overview\n+icon: \"rectangle-code\"\n ---\n \n-MindsDB offers a tailored Mongo-QL syntax that enables seamless interaction with a wide range of objects, including collections, databases, and models. Moreover, you have the flexibility to connect MindsDB with MongoDB Compass and MongoDB Shell, facilitating streamlined integration.\n-\n-<Note>\n-After connecting to MindsDB from Mongo, switch to the default project by executing `> use mindsdb`.\n-</Note>\n-\n-### Connect to MindsDB\n-\n-This section contains guides on how to connect MindsDB to MongoDB clients.\n-\n-<CardGroup cols={4}>\n-\n-    <Card title=\"MongoDB Compass\" icon=\"link\" href=\"/connect/mongo-compass\"></Card>\n-    <Card title=\"MongoDB Shell\" icon=\"link\" href=\"/connect/mongo-shell\"></Card>\n-\n-</CardGroup>\n-\n-### Mongo-QL Syntax\n-\n-This section contains guides on how to work with collections, databases, and models.\n-\n-<CardGroup cols={4}>\n-\n-    <Card title=\"Collection Structure\" icon=\"link\" href=\"/sdks/mongo/collection-structure\"></Card>\n-    <Card title=\"ML Engines\" icon=\"link\" href=\"/sdks/mongo/ml_engine\"></Card>\n-    <Card title=\"Connect Databases\" icon=\"link\" href=\"/sdks/mongo/database\"></Card>\n-    <Card title=\"Create Models\" icon=\"link\" href=\"/sdks/mongo/insert\"></Card>\n-    <Card title=\"Describe Models\" icon=\"link\" href=\"/sdks/mongo/models/describe\"></Card>\n-    <Card title=\"Make Predictions\" icon=\"link\" href=\"/sdks/mongo/find\"></Card>\n-    <Card title=\"Delete Models\" icon=\"link\" href=\"/sdks/mongo/delete\"></Card>\n-\n-</CardGroup>\n+MindsDB provides MongoDB SDK, enabling its integration into MongoDB environments.",
    "comment": "This should be MQL API not SDK",
    "line_number": 7,
    "enriched": "File: docs/sdks/mongo/mindsdb-mongo-ql-overview.mdx\nCode: @@ -1,37 +1,32 @@\n ---\n-title: MindsDB Mongo-QL\n+title: Overview\n sidebarTitle: Overview\n+icon: \"rectangle-code\"\n ---\n \n-MindsDB offers a tailored Mongo-QL syntax that enables seamless interaction with a wide range of objects, including collections, databases, and models. Moreover, you have the flexibility to connect MindsDB with MongoDB Compass and MongoDB Shell, facilitating streamlined integration.\n-\n-<Note>\n-After connecting to MindsDB from Mongo, switch to the default project by executing `> use mindsdb`.\n-</Note>\n-\n-### Connect to MindsDB\n-\n-This section contains guides on how to connect MindsDB to MongoDB clients.\n-\n-<CardGroup cols={4}>\n-\n-    <Card title=\"MongoDB Compass\" icon=\"link\" href=\"/connect/mongo-compass\"></Card>\n-    <Card title=\"MongoDB Shell\" icon=\"link\" href=\"/connect/mongo-shell\"></Card>\n-\n-</CardGroup>\n-\n-### Mongo-QL Syntax\n-\n-This section contains guides on how to work with collections, databases, and models.\n-\n-<CardGroup cols={4}>\n-\n-    <Card title=\"Collection Structure\" icon=\"link\" href=\"/sdks/mongo/collection-structure\"></Card>\n-    <Card title=\"ML Engines\" icon=\"link\" href=\"/sdks/mongo/ml_engine\"></Card>\n-    <Card title=\"Connect Databases\" icon=\"link\" href=\"/sdks/mongo/database\"></Card>\n-    <Card title=\"Create Models\" icon=\"link\" href=\"/sdks/mongo/insert\"></Card>\n-    <Card title=\"Describe Models\" icon=\"link\" href=\"/sdks/mongo/models/describe\"></Card>\n-    <Card title=\"Make Predictions\" icon=\"link\" href=\"/sdks/mongo/find\"></Card>\n-    <Card title=\"Delete Models\" icon=\"link\" href=\"/sdks/mongo/delete\"></Card>\n-\n-</CardGroup>\n+MindsDB provides MongoDB SDK, enabling its integration into MongoDB environments.\nComment: This should be MQL API not SDK",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "docs/sdks/mongo/mindsdb-mongo-ql-overview.mdx",
    "pr_number": 10865,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2100373298,
    "comment_created_at": "2025-05-21T13:58:28Z"
  },
  {
    "code": "@@ -306,17 +317,36 @@ async def search_relevancy_score(self, query: str, document: str) -> Any:\n                 },\n             ],\n             temperature=self.temperature,\n-            n=1,\n-            logprobs=True,\n-            top_logprobs=4,\n-            max_tokens=3,\n+            n=self.n,\n+            logprobs=self.logprobs,\n+            top_logprobs=self.top_logprobs,\n+            max_tokens=self.max_tokens,\n         )\n \n         # Extract response and logprobs\n         token_logprobs = response.choices[0].logprobs.content\n-        # Reconstruct the prediction and extract the top logprobs from the final token (e.g., \"1\")\n-        final_token_logprob = token_logprobs[-1]\n-        top_logprobs = final_token_logprob.top_logprobs\n+        \n+        # Find the token that contains the class number (1, 2, 3, or 4)\n+        # Instead of just taking the last token, search for the actual class number token\n+        class_token_logprob = None\n+        for token_logprob in token_logprobs:\n+            if token_logprob.token in ['1', '2', '3', '4']:\n+                class_token_logprob = token_logprob\n+                break\n+        \n+        # If we couldn't find a class token, fall back to the last non-empty token\n+        if class_token_logprob is None:\n+            # Look for the last meaningful token (not empty string)\n+            for token_logprob in reversed(token_logprobs):\n+                if token_logprob.token.strip() and token_logprob.token in ['1', '2', '3', '4']:",
    "comment": "The condition `token_logprob.token.strip() and token_logprob.token in ['1', '2', '3', '4']` is redundant. If `token_logprob.token` is in ['1', '2', '3', '4'], it will never be empty after stripping, so the `strip()` check is unnecessary and makes the logic harder to read.\n```suggestion\n                if token_logprob.token in ['1', '2', '3', '4']:\n```",
    "line_number": 341,
    "enriched": "File: mindsdb/integrations/utilities/rag/rerankers/base_reranker.py\nCode: @@ -306,17 +317,36 @@ async def search_relevancy_score(self, query: str, document: str) -> Any:\n                 },\n             ],\n             temperature=self.temperature,\n-            n=1,\n-            logprobs=True,\n-            top_logprobs=4,\n-            max_tokens=3,\n+            n=self.n,\n+            logprobs=self.logprobs,\n+            top_logprobs=self.top_logprobs,\n+            max_tokens=self.max_tokens,\n         )\n \n         # Extract response and logprobs\n         token_logprobs = response.choices[0].logprobs.content\n-        # Reconstruct the prediction and extract the top logprobs from the final token (e.g., \"1\")\n-        final_token_logprob = token_logprobs[-1]\n-        top_logprobs = final_token_logprob.top_logprobs\n+        \n+        # Find the token that contains the class number (1, 2, 3, or 4)\n+        # Instead of just taking the last token, search for the actual class number token\n+        class_token_logprob = None\n+        for token_logprob in token_logprobs:\n+            if token_logprob.token in ['1', '2', '3', '4']:\n+                class_token_logprob = token_logprob\n+                break\n+        \n+        # If we couldn't find a class token, fall back to the last non-empty token\n+        if class_token_logprob is None:\n+            # Look for the last meaningful token (not empty string)\n+            for token_logprob in reversed(token_logprobs):\n+                if token_logprob.token.strip() and token_logprob.token in ['1', '2', '3', '4']:\nComment: The condition `token_logprob.token.strip() and token_logprob.token in ['1', '2', '3', '4']` is redundant. If `token_logprob.token` is in ['1', '2', '3', '4'], it will never be empty after stripping, so the `strip()` check is unnecessary and makes the logic harder to read.\n```suggestion\n                if token_logprob.token in ['1', '2', '3', '4']:\n```",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "mindsdb/integrations/utilities/rag/rerankers/base_reranker.py",
    "pr_number": 11490,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2308965773,
    "comment_created_at": "2025-08-29T02:25:03Z"
  },
  {
    "code": "@@ -0,0 +1,143 @@\n+import replicate\n+import pandas as pd\n+from mindsdb.integrations.libs.base import BaseMLEngine\n+from typing import Dict, Optional\n+import os\n+import types\n+from mindsdb.utilities.config import Config\n+\n+\n+class ReplicateHandler(BaseMLEngine):\n+    name = \"replicate\"\n+\n+    @staticmethod\n+    def create_validation(target, args=None, **kwargs):\n+        if 'using' not in args:\n+            raise Exception(\"Replicate engine requires a USING clause! Refer to its documentation for more details.\")\n+        else:\n+            args = args['using']\n+\n+        if 'model_name' and 'version' not in args:",
    "comment": "This conditional will not work as-is. You need to check that `model_name` is not in `args` too:\r\n\r\n```python\r\nif 'model_name' not in args or 'version' not in args \r\n```",
    "line_number": 20,
    "enriched": "File: mindsdb/integrations/handlers/replicate_handler/replicate_handler.py\nCode: @@ -0,0 +1,143 @@\n+import replicate\n+import pandas as pd\n+from mindsdb.integrations.libs.base import BaseMLEngine\n+from typing import Dict, Optional\n+import os\n+import types\n+from mindsdb.utilities.config import Config\n+\n+\n+class ReplicateHandler(BaseMLEngine):\n+    name = \"replicate\"\n+\n+    @staticmethod\n+    def create_validation(target, args=None, **kwargs):\n+        if 'using' not in args:\n+            raise Exception(\"Replicate engine requires a USING clause! Refer to its documentation for more details.\")\n+        else:\n+            args = args['using']\n+\n+        if 'model_name' and 'version' not in args:\nComment: This conditional will not work as-is. You need to check that `model_name` is not in `args` too:\r\n\r\n```python\r\nif 'model_name' not in args or 'version' not in args \r\n```",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/replicate_handler/replicate_handler.py",
    "pr_number": 6594,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1242655232,
    "comment_created_at": "2023-06-26T19:19:49Z"
  },
  {
    "code": "@@ -1,6 +1,9 @@\n import logging\n import os\n-import torch.multiprocessing as mp\n+try:\n+    import torch.multiprocessing as mp\n+except (ImportError, ModuleNotFoundError):\n+    import multiprocessing as mp",
    "comment": "Note: This will not work with anything in the code below that uses `mp.cpu_count()`.  Find an alternate way to do this without torch.",
    "line_number": 6,
    "enriched": "File: mindsdb/api/http/start.py\nCode: @@ -1,6 +1,9 @@\n import logging\n import os\n-import torch.multiprocessing as mp\n+try:\n+    import torch.multiprocessing as mp\n+except (ImportError, ModuleNotFoundError):\n+    import multiprocessing as mp\nComment: Note: This will not work with anything in the code below that uses `mp.cpu_count()`.  Find an alternate way to do this without torch.",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "mindsdb/api/http/start.py",
    "pr_number": 6463,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1233684247,
    "comment_created_at": "2023-06-19T08:09:38Z"
  },
  {
    "code": "@@ -1,7 +1,7 @@\n \"\"\"update_project_names\n \n Revision ID: 607709e1615b\n-Revises: b5bf593ba659\n+Revises: 4c26ad04eeaa",
    "comment": "Just to make sure this is expected (modifying the previous migration file)?",
    "line_number": 4,
    "enriched": "File: mindsdb/migrations/versions/2023-06-27_607709e1615b_update_project_names.py\nCode: @@ -1,7 +1,7 @@\n \"\"\"update_project_names\n \n Revision ID: 607709e1615b\n-Revises: b5bf593ba659\n+Revises: 4c26ad04eeaa\nComment: Just to make sure this is expected (modifying the previous migration file)?",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "mindsdb/migrations/versions/2023-06-27_607709e1615b_update_project_names.py",
    "pr_number": 7269,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1317732554,
    "comment_created_at": "2023-09-06T19:29:02Z"
  },
  {
    "code": "@@ -17,4 +17,6 @@\n                 tuple(f'code-search-{model}-text-001' for model in COMPLETION_LEGACY_BASE_MODELS) + \\\n                 tuple(f'code-search-{model}-code-001' for model in COMPLETION_LEGACY_BASE_MODELS)\n \n-ALL_MODELS = list(set(CHAT_MODELS + COMPLETION_MODELS + COMPLETION_LEGACY_MODELS + EMBEDDING_MODELS))  # noqa\n\\ No newline at end of file\n+IMAGE_MODELS = ('dall-e-3')",
    "comment": "This should be a tuple:\r\n\r\n```\r\nIMAGE_MODELS = ('dall-e-3',)\r\n```\r\n\r\nNotice that pesky little comma at the end which implies the type is `tuple` 👀 ",
    "line_number": 20,
    "enriched": "File: mindsdb/integrations/handlers/openai_handler/constants.py\nCode: @@ -17,4 +17,6 @@\n                 tuple(f'code-search-{model}-text-001' for model in COMPLETION_LEGACY_BASE_MODELS) + \\\n                 tuple(f'code-search-{model}-code-001' for model in COMPLETION_LEGACY_BASE_MODELS)\n \n-ALL_MODELS = list(set(CHAT_MODELS + COMPLETION_MODELS + COMPLETION_LEGACY_MODELS + EMBEDDING_MODELS))  # noqa\n\\ No newline at end of file\n+IMAGE_MODELS = ('dall-e-3')\nComment: This should be a tuple:\r\n\r\n```\r\nIMAGE_MODELS = ('dall-e-3',)\r\n```\r\n\r\nNotice that pesky little comma at the end which implies the type is `tuple` 👀 ",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/openai_handler/constants.py",
    "pr_number": 8285,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1384672061,
    "comment_created_at": "2023-11-07T10:03:22Z"
  },
  {
    "code": "@@ -475,6 +475,28 @@ def test_create_validation(self):\n                 '''\n             )\n \n+    def test_describe(self):\n+        self.run_sql(\n+            '''\n+                CREATE PREDICTOR mindsdb.pred",
    "comment": "`CREATE PREDICTOR` is obsolete, or not?",
    "line_number": 481,
    "enriched": "File: tests/unit/test_project_structure.py\nCode: @@ -475,6 +475,28 @@ def test_create_validation(self):\n                 '''\n             )\n \n+    def test_describe(self):\n+        self.run_sql(\n+            '''\n+                CREATE PREDICTOR mindsdb.pred\nComment: `CREATE PREDICTOR` is obsolete, or not?",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "tests/unit/test_project_structure.py",
    "pr_number": 6686,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1243290305,
    "comment_created_at": "2023-06-27T07:50:11Z"
  },
  {
    "code": "@@ -0,0 +1,94 @@\n+\"\"\"FastAPI application factory and startup hooks.\"\"\"\n+\n+from __future__ import annotations\n+\n+import mindsdb_sdk\n+from fastapi import FastAPI\n+from fastapi.middleware.cors import CORSMiddleware\n+from dotenv import load_dotenv\n+\n+from . import services\n+from .api import router as api_router\n+from .db import ensure_table_exists\n+from .jira_client import JiraClientError, build_default_client\n+\n+DB_CONFIG = {\n+    \"host\": \"localhost\",\n+    \"port\": 5432,\n+    \"database\": \"demo\",\n+    \"user\": \"postgresql\",\n+    \"password\": \"psqlpasswd\",\n+}\n+\n+MINDSDB_URL = \"http://127.0.0.1:47334\"\n+AGENT_NAME = \"classification_agent\"\n+\n+services.set_db_config(DB_CONFIG)\n+load_dotenv()\n+\n+app = FastAPI(title=\"Banking Customer Service API\", version=\"1.0.0\")\n+\n+app.add_middleware(\n+    CORSMiddleware,\n+    allow_origins=[\"*\"],\n+    allow_credentials=True,\n+    allow_methods=[\"*\"],\n+    allow_headers=[\"*\"],\n+)\n+\n+app.include_router(api_router)\n+\n+\n+@app.on_event(\"startup\")\n+async def startup_event() -> None:\n+    print(\"\\n\" + \"=\" * 70)\n+    print(\"Starting Banking Customer Service API Server...\")\n+    print(\"=\" * 70)\n+\n+    print(\"\\nChecking database table...\")\n+    try:\n+        if ensure_table_exists(db_config=DB_CONFIG, verbose=True):\n+            print(\"✓ Database ready\")\n+        else:\n+            print(\"✗ Warning: Could not verify or create database table\")\n+            print(\"  The server will start, but may encounter errors.\")\n+    except Exception as exc:  # pragma: no cover - startup diagnostics\n+        print(f\"✗ Error during database check: {exc}\")\n+        print(\"  The server will start, but may encounter errors.\")\n+\n+    print(\"\\nConnecting to MindsDB...\")\n+    try:\n+        mindsdb_server = mindsdb_sdk.connect(MINDSDB_URL)\n+        services.set_mindsdb_server(mindsdb_server)\n+        classification_agent = mindsdb_server.agents.get(AGENT_NAME)\n+        services.set_agent(classification_agent)",
    "comment": "**correctness**: `services.set_agent(classification_agent)` is called even if `classification_agent` is None, which can cause downstream runtime errors if the agent is not found in MindsDB.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb hacktoberfest/use-cases/AutoBankingCustomerService/app/__init__.py, lines 63-64, the code sets the agent in services even if the agent is not found (i.e., None), which can cause runtime errors later. Please update this block to only call services.set_agent if classification_agent is not None, and print a warning if the agent is missing.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n        classification_agent = mindsdb_server.agents.get(AGENT_NAME)\n        if classification_agent is not None:\n            services.set_agent(classification_agent)\n        else:\n            print(f\"✗ Agent '{AGENT_NAME}' not found in MindsDB. Agent queries will fail.\")\n```\n</details>\n<!-- suggestion_end -->\n",
    "line_number": 64,
    "enriched": "File: mindsdb hacktoberfest/use-cases/AutoBankingCustomerService/app/__init__.py\nCode: @@ -0,0 +1,94 @@\n+\"\"\"FastAPI application factory and startup hooks.\"\"\"\n+\n+from __future__ import annotations\n+\n+import mindsdb_sdk\n+from fastapi import FastAPI\n+from fastapi.middleware.cors import CORSMiddleware\n+from dotenv import load_dotenv\n+\n+from . import services\n+from .api import router as api_router\n+from .db import ensure_table_exists\n+from .jira_client import JiraClientError, build_default_client\n+\n+DB_CONFIG = {\n+    \"host\": \"localhost\",\n+    \"port\": 5432,\n+    \"database\": \"demo\",\n+    \"user\": \"postgresql\",\n+    \"password\": \"psqlpasswd\",\n+}\n+\n+MINDSDB_URL = \"http://127.0.0.1:47334\"\n+AGENT_NAME = \"classification_agent\"\n+\n+services.set_db_config(DB_CONFIG)\n+load_dotenv()\n+\n+app = FastAPI(title=\"Banking Customer Service API\", version=\"1.0.0\")\n+\n+app.add_middleware(\n+    CORSMiddleware,\n+    allow_origins=[\"*\"],\n+    allow_credentials=True,\n+    allow_methods=[\"*\"],\n+    allow_headers=[\"*\"],\n+)\n+\n+app.include_router(api_router)\n+\n+\n+@app.on_event(\"startup\")\n+async def startup_event() -> None:\n+    print(\"\\n\" + \"=\" * 70)\n+    print(\"Starting Banking Customer Service API Server...\")\n+    print(\"=\" * 70)\n+\n+    print(\"\\nChecking database table...\")\n+    try:\n+        if ensure_table_exists(db_config=DB_CONFIG, verbose=True):\n+            print(\"✓ Database ready\")\n+        else:\n+            print(\"✗ Warning: Could not verify or create database table\")\n+            print(\"  The server will start, but may encounter errors.\")\n+    except Exception as exc:  # pragma: no cover - startup diagnostics\n+        print(f\"✗ Error during database check: {exc}\")\n+        print(\"  The server will start, but may encounter errors.\")\n+\n+    print(\"\\nConnecting to MindsDB...\")\n+    try:\n+        mindsdb_server = mindsdb_sdk.connect(MINDSDB_URL)\n+        services.set_mindsdb_server(mindsdb_server)\n+        classification_agent = mindsdb_server.agents.get(AGENT_NAME)\n+        services.set_agent(classification_agent)\nComment: **correctness**: `services.set_agent(classification_agent)` is called even if `classification_agent` is None, which can cause downstream runtime errors if the agent is not found in MindsDB.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>🤖 AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> 📋 **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb hacktoberfest/use-cases/AutoBankingCustomerService/app/__init__.py, lines 63-64, the code sets the agent in services even if the agent is not found (i.e., None), which can cause runtime errors later. Please update this block to only call services.set_agent if classification_agent is not None, and print a warning if the agent is missing.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>📝 Committable Code Suggestion</strong></summary>\n\n> ‼️ Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n        classification_agent = mindsdb_server.agents.get(AGENT_NAME)\n        if classification_agent is not None:\n            services.set_agent(classification_agent)\n        else:\n            print(f\"✗ Agent '{AGENT_NAME}' not found in MindsDB. Agent queries will fail.\")\n```\n</details>\n<!-- suggestion_end -->\n",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "mindsdb hacktoberfest/use-cases/AutoBankingCustomerService/app/__init__.py",
    "pr_number": 11769,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2443582951,
    "comment_created_at": "2025-10-20T00:28:44Z"
  },
  {
    "code": "@@ -17,7 +17,6 @@\n from pandas import DataFrame\n \n from base_handler_test import BaseDatabaseHandlerTest\n-from mindsdb.integrations.handlers.snowflake_handler.snowflake_handler import SnowflakeHandler",
    "comment": "I think we need this Class in the tests bellow",
    "line_number": 20,
    "enriched": "File: tests/unit/handlers/test_snowflake.py\nCode: @@ -17,7 +17,6 @@\n from pandas import DataFrame\n \n from base_handler_test import BaseDatabaseHandlerTest\n-from mindsdb.integrations.handlers.snowflake_handler.snowflake_handler import SnowflakeHandler\nComment: I think we need this Class in the tests bellow",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "tests/unit/handlers/test_snowflake.py",
    "pr_number": 11047,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2139647421,
    "comment_created_at": "2025-06-11T09:23:41Z"
  },
  {
    "code": "@@ -31,17 +31,9 @@ def __init__(self, name, connection_data, **kwargs):\n         self.connection_data = connection_data\n         self.renderer = SqlalchemyRender(ClickHouseDialect)\n         self.is_connected = False\n-        self.protocol = connection_data.get('protocol', 'clickhouse')\n-\n-        # region added for back-compatibility with connections creatad before 11.05.2023\n-        protocols_map = {\n-            'native': 'clickhouse+native',\n-            'http': 'clickhouse+http',\n-            'https': 'clickhouse+https',\n-        }\n-        if self.protocol in protocols_map:\n-            self.protocol = protocols_map[self.protocol]\n-        # endregion\n+        self.protocol = connection_data.get('protocol', 'clickhouse+native')",
    "comment": "Default value should be just 'native', otherwise `clickhouse+http` will be set in line #49",
    "line_number": 34,
    "enriched": "File: mindsdb/integrations/handlers/clickhouse_handler/clickhouse_handler.py\nCode: @@ -31,17 +31,9 @@ def __init__(self, name, connection_data, **kwargs):\n         self.connection_data = connection_data\n         self.renderer = SqlalchemyRender(ClickHouseDialect)\n         self.is_connected = False\n-        self.protocol = connection_data.get('protocol', 'clickhouse')\n-\n-        # region added for back-compatibility with connections creatad before 11.05.2023\n-        protocols_map = {\n-            'native': 'clickhouse+native',\n-            'http': 'clickhouse+http',\n-            'https': 'clickhouse+https',\n-        }\n-        if self.protocol in protocols_map:\n-            self.protocol = protocols_map[self.protocol]\n-        # endregion\n+        self.protocol = connection_data.get('protocol', 'clickhouse+native')\nComment: Default value should be just 'native', otherwise `clickhouse+http` will be set in line #49",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/clickhouse_handler/clickhouse_handler.py",
    "pr_number": 8597,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1446257196,
    "comment_created_at": "2024-01-09T15:39:52Z"
  },
  {
    "code": "@@ -2,7 +2,8 @@\n import copy\n from collections import defaultdict\n \n-from mindsdb_sql.parser.ast import Identifier, Select, BinaryOperation, Last, Constant, Star, ASTNode\n+from mindsdb_sql.parser.ast import Identifier, Select, BinaryOperation, Last, Constant, Star, ASTNode,\\",
    "comment": "according to pep8:\r\n```\r\nThe preferred way of wrapping long lines is by using Python’s implied line continuation inside parentheses, brackets and braces. Long lines can be broken over multiple lines by wrapping expressions in parentheses. These should be used in preference to using a backslash for line continuation.\r\n```",
    "line_number": 5,
    "enriched": "File: mindsdb/interfaces/query_context/last_query.py\nCode: @@ -2,7 +2,8 @@\n import copy\n from collections import defaultdict\n \n-from mindsdb_sql.parser.ast import Identifier, Select, BinaryOperation, Last, Constant, Star, ASTNode\n+from mindsdb_sql.parser.ast import Identifier, Select, BinaryOperation, Last, Constant, Star, ASTNode,\\\nComment: according to pep8:\r\n```\r\nThe preferred way of wrapping long lines is by using Python’s implied line continuation inside parentheses, brackets and braces. Long lines can be broken over multiple lines by wrapping expressions in parentheses. These should be used in preference to using a backslash for line continuation.\r\n```",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "mindsdb/interfaces/query_context/last_query.py",
    "pr_number": 8218,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1377243215,
    "comment_created_at": "2023-10-31T08:53:23Z"
  },
  {
    "code": "@@ -101,85 +106,98 @@ def setFromBuff(self, buff):\n             self.value = self.value[:-1]\n         return buff[end:]\n \n-    def lenencInt(self, value):\n-        byte_count = int(math.ceil(math.log((value + 1), 2) / 8))\n+    @classmethod\n+    def serialize_int(cls, value):",
    "comment": "should it work if value < 0 ?",
    "line_number": 110,
    "enriched": "File: mindsdb/api/mysql/mysql_proxy/data_types/mysql_datum.py\nCode: @@ -101,85 +106,98 @@ def setFromBuff(self, buff):\n             self.value = self.value[:-1]\n         return buff[end:]\n \n-    def lenencInt(self, value):\n-        byte_count = int(math.ceil(math.log((value + 1), 2) / 8))\n+    @classmethod\n+    def serialize_int(cls, value):\nComment: should it work if value < 0 ?",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "mindsdb/api/mysql/mysql_proxy/data_types/mysql_datum.py",
    "pr_number": 9974,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1829392937,
    "comment_created_at": "2024-11-05T13:53:04Z"
  },
  {
    "code": "@@ -70,6 +70,7 @@\n     {\"source\": \"/app-integrations/shopify\",\"destination\": \"/integrations/app-integrations/shopify\"},\n     {\"source\": \"/app-integrations/twitter\",\"destination\": \"/integrations/app-integrations/twitter\"},\n     {\"source\": \"/app-integrations/youtube\",\"destination\": \"/integrations/app-integrations/youtube\"},\n+    {\"source\": \"/app-integrations/intercom\",\"destination\": \"/integrations/app-integrations/intercom\"},",
    "comment": "It is not required to add it to the redirects list.\r\n\r\nPlease remove this line.",
    "line_number": 73,
    "enriched": "File: docs/mint.json\nCode: @@ -70,6 +70,7 @@\n     {\"source\": \"/app-integrations/shopify\",\"destination\": \"/integrations/app-integrations/shopify\"},\n     {\"source\": \"/app-integrations/twitter\",\"destination\": \"/integrations/app-integrations/twitter\"},\n     {\"source\": \"/app-integrations/youtube\",\"destination\": \"/integrations/app-integrations/youtube\"},\n+    {\"source\": \"/app-integrations/intercom\",\"destination\": \"/integrations/app-integrations/intercom\"},\nComment: It is not required to add it to the redirects list.\r\n\r\nPlease remove this line.",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "docs/mint.json",
    "pr_number": 8277,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1388177779,
    "comment_created_at": "2023-11-09T15:31:30Z"
  },
  {
    "code": "@@ -116,14 +120,14 @@ def create(self, target, args=None, **kwargs):\n         # Check if pipeline has already been downloaded\n         try:\n             pipeline = transformers.pipeline(task=args['task_proper'], model=hf_model_storage_path,\n-                                             tokenizer=hf_model_storage_path)\n+                                             tokenizer=hf_model_storage_path,use_auth_token=use_auth_token)\n             logger.debug(f'Model already downloaded!')\n         ####\n         # Otherwise download it\n         except (ValueError, OSError):\n             try:\n                 logger.debug(f\"Downloading {model_name}...\")\n-                pipeline = transformers.pipeline(task=args['task_proper'], model=model_name)\n+                pipeline = transformers.pipeline(task=args['task_proper'], model=model_name,use_auth_token=use_auth_token)",
    "comment": "Let's not do this. Let's only accept it as a boolean argument. This applies only if the `token` argument is also a boolean operator.",
    "line_number": 130,
    "enriched": "File: mindsdb/integrations/handlers/huggingface_handler/huggingface_handler.py\nCode: @@ -116,14 +120,14 @@ def create(self, target, args=None, **kwargs):\n         # Check if pipeline has already been downloaded\n         try:\n             pipeline = transformers.pipeline(task=args['task_proper'], model=hf_model_storage_path,\n-                                             tokenizer=hf_model_storage_path)\n+                                             tokenizer=hf_model_storage_path,use_auth_token=use_auth_token)\n             logger.debug(f'Model already downloaded!')\n         ####\n         # Otherwise download it\n         except (ValueError, OSError):\n             try:\n                 logger.debug(f\"Downloading {model_name}...\")\n-                pipeline = transformers.pipeline(task=args['task_proper'], model=model_name)\n+                pipeline = transformers.pipeline(task=args['task_proper'], model=model_name,use_auth_token=use_auth_token)\nComment: Let's not do this. Let's only accept it as a boolean argument. This applies only if the `token` argument is also a boolean operator.",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/huggingface_handler/huggingface_handler.py",
    "pr_number": 9845,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1800428600,
    "comment_created_at": "2024-10-15T04:19:51Z"
  },
  {
    "code": "@@ -0,0 +1,100 @@\n+# -------------------------------------\n+# MindsDB Server Connection Details\n+# -------------------------------------\n+# Use these for a remote or authenticated local MindsDB instance\n+# MINDSDB_PROTOCOL=https\n+# MINDSDB_HOST=your.mindsdb.cloud.instance\n+# MINDSDB_PORT=443\n+# MINDSDB_USER=your_email@example.com\n+# MINDSDB_PASSWORD=your_mindsdb_password\n+\n+# Use these for a default local MindsDB instance (no auth)\n+MINDSDB_PROTOCOL=http\n+MINDSDB_HOST=127.0.0.1\n+MINDSDB_PORT=47334\n+MINDSDB_USER=\n+MINDSDB_PASSWORD=\n+\n+# -------------------------------------\n+# Handlers to be Tested\n+# Comma-separated list of handler names.\n+# -------------------------------------\n+HANDLERS_TO_TEST=postgres,s3,github,databricks,bigquery,mariadb,mysql\n+\n+# =====================================\n+# Data Source Credentials\n+# =====================================\n+\n+# --- PostgreSQL ---\n+PG_SOURCE_HOST=samples.mindsdb.com",
    "comment": "We should make as many of these DB params as possible just work out of the box. As it stands users have to manually copy this file",
    "line_number": 29,
    "enriched": "File: .env.example\nCode: @@ -0,0 +1,100 @@\n+# -------------------------------------\n+# MindsDB Server Connection Details\n+# -------------------------------------\n+# Use these for a remote or authenticated local MindsDB instance\n+# MINDSDB_PROTOCOL=https\n+# MINDSDB_HOST=your.mindsdb.cloud.instance\n+# MINDSDB_PORT=443\n+# MINDSDB_USER=your_email@example.com\n+# MINDSDB_PASSWORD=your_mindsdb_password\n+\n+# Use these for a default local MindsDB instance (no auth)\n+MINDSDB_PROTOCOL=http\n+MINDSDB_HOST=127.0.0.1\n+MINDSDB_PORT=47334\n+MINDSDB_USER=\n+MINDSDB_PASSWORD=\n+\n+# -------------------------------------\n+# Handlers to be Tested\n+# Comma-separated list of handler names.\n+# -------------------------------------\n+HANDLERS_TO_TEST=postgres,s3,github,databricks,bigquery,mariadb,mysql\n+\n+# =====================================\n+# Data Source Credentials\n+# =====================================\n+\n+# --- PostgreSQL ---\n+PG_SOURCE_HOST=samples.mindsdb.com\nComment: We should make as many of these DB params as possible just work out of the box. As it stands users have to manually copy this file",
    "subcategory": "logical",
    "category": "functional",
    "file_path": ".env.example",
    "pr_number": 11495,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2323931825,
    "comment_created_at": "2025-09-05T02:06:19Z"
  },
  {
    "code": "@@ -120,7 +123,7 @@ def on_file(file):\n                         \"Error getting file info\",\n                         \"Сan't determine remote file size\",\n                     )\n-                if file_size > 1024 * 1024 * 100:\n+                if file_size > MAX_FILE_SIZE:",
    "comment": "let also change error message to `f\"Upload limit for file is {MAX_FILE_SIZE >> 20}MB\"`",
    "line_number": 126,
    "enriched": "File: mindsdb/api/http/namespaces/file.py\nCode: @@ -120,7 +123,7 @@ def on_file(file):\n                         \"Error getting file info\",\n                         \"Сan't determine remote file size\",\n                     )\n-                if file_size > 1024 * 1024 * 100:\n+                if file_size > MAX_FILE_SIZE:\nComment: let also change error message to `f\"Upload limit for file is {MAX_FILE_SIZE >> 20}MB\"`",
    "subcategory": "alternate output",
    "category": "refactoring",
    "file_path": "mindsdb/api/http/namespaces/file.py",
    "pr_number": 9620,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1716580811,
    "comment_created_at": "2024-08-14T09:12:42Z"
  },
  {
    "code": "@@ -302,7 +302,7 @@ def before_request():\n             and check_auth() is False\n         ):\n             return http_error(\n-                403, 'Forbidden',\n+                HTTPStatus.UNAUTHORIZED, 'Forbidden',",
    "comment": "if status is UNAUTHORIZED maybe message should be too? (not 'Forbidden')",
    "line_number": 305,
    "enriched": "File: mindsdb/api/http/initialize.py\nCode: @@ -302,7 +302,7 @@ def before_request():\n             and check_auth() is False\n         ):\n             return http_error(\n-                403, 'Forbidden',\n+                HTTPStatus.UNAUTHORIZED, 'Forbidden',\nComment: if status is UNAUTHORIZED maybe message should be too? (not 'Forbidden')",
    "subcategory": "alternate output",
    "category": "refactoring",
    "file_path": "mindsdb/api/http/initialize.py",
    "pr_number": 10259,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1876109176,
    "comment_created_at": "2024-12-09T14:47:30Z"
  },
  {
    "code": "@@ -1556,7 +1556,7 @@ def get_date_format(samples):\n                     if isinstance(arg, Constant) and isinstance(arg.value, str):\n                         arg.value = fnc(arg.value)\n \n-        if self.model_types.get(order_col) in ('date', 'datetime'):\n+        if self.model_types.get(order_col) in ('date', 'datetime') or isinstance(predictor_data[0][order_col], pd.Timestamp):  # noqa",
    "comment": "imho we should simplify this time filter urgently, but realistically that's another issue (part of the TS refactor).\r\n\r\nAs of now, this is a good compromise to avoid breaking anything.",
    "line_number": 1559,
    "enriched": "File: mindsdb/api/mysql/mysql_proxy/classes/sql_query.py\nCode: @@ -1556,7 +1556,7 @@ def get_date_format(samples):\n                     if isinstance(arg, Constant) and isinstance(arg.value, str):\n                         arg.value = fnc(arg.value)\n \n-        if self.model_types.get(order_col) in ('date', 'datetime'):\n+        if self.model_types.get(order_col) in ('date', 'datetime') or isinstance(predictor_data[0][order_col], pd.Timestamp):  # noqa\nComment: imho we should simplify this time filter urgently, but realistically that's another issue (part of the TS refactor).\r\n\r\nAs of now, this is a good compromise to avoid breaking anything.",
    "subcategory": "praise",
    "category": "discussion",
    "file_path": "mindsdb/api/mysql/mysql_proxy/classes/sql_query.py",
    "pr_number": 7334,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1325228673,
    "comment_created_at": "2023-09-14T01:21:53Z"
  },
  {
    "code": "@@ -0,0 +1,208 @@\n+import time\n+import pandas as pd\n+from boto3 import client\n+from typing import Optional\n+\n+from mindsdb_sql import parse_sql\n+from mindsdb.integrations.libs.base import DatabaseHandler\n+from mindsdb_sql.parser.ast.base import ASTNode\n+from mindsdb.utilities import log\n+from mindsdb.integrations.libs.response import (\n+    HandlerStatusResponse as StatusResponse,\n+    HandlerResponse as Response,\n+    RESPONSE_TYPE\n+)\n+\n+logger = log.getLogger(__name__)\n+\n+class AthenaHandler(DatabaseHandler):\n+    \"\"\"\n+    This handler handles connection and execution of the Athena statements.\n+    \"\"\"\n+\n+    name = 'athena'\n+\n+    def __init__(self, name: str, connection_data: Optional[dict], **kwargs):\n+        \"\"\"\n+        Initialize the handler.\n+        Args:\n+            name (str): name of particular handler instance\n+            connection_data (dict): parameters for connecting to the database\n+            **kwargs: arbitrary keyword arguments.\n+        \"\"\"\n+        super().__init__(name)\n+        self.parser = parse_sql\n+        self.dialect = 'athena'\n+\n+        self.connection_data = connection_data\n+        self.kwargs = kwargs\n+\n+        self.connection = None\n+        self.is_connected = False\n+\n+    def connect(self) -> StatusResponse:\n+        \"\"\"\n+        Set up the connection required by the handler.\n+        Returns:\n+            HandlerStatusResponse\n+        \"\"\"\n+\n+        if self.is_connected:\n+            return StatusResponse(self.name, True, 'Already connected')\n+\n+        try:\n+            self.connection = client(\n+                'athena',\n+                aws_access_key_id=self.connection_data['aws_access_key_id'],\n+                aws_secret_access_key=self.connection_data['aws_secret_access_key'],\n+                region_name=self.connection_data['region_name'],\n+            )\n+            self.is_connected = True\n+            return StatusResponse(self.name, True, 'Connected successfully')\n+        except Exception as e:\n+            logger.error(f'Failed to connect to Athena: {str(e)}')\n+            return StatusResponse(self.name, False, str(e))\n+\n+    def disconnect(self):\n+        \"\"\"\n+        Close any existing connections.\n+        \"\"\"\n+        if self.is_connected:\n+            self.connection = None\n+            self.is_connected = False\n+\n+    def check_connection(self) -> StatusResponse:\n+        \"\"\"\n+        Check connection to the handler.\n+        Returns:\n+            HandlerStatusResponse\n+        \"\"\"\n+        if self.is_connected:\n+            return StatusResponse(self.name, True, 'Connection is alive')\n+        else:\n+            return self.connect()\n+\n+    def native_query(self, query: str) -> StatusResponse:\n+        \"\"\"\n+        Receive raw query and act upon it somehow.\n+        Args:\n+            query (str): query in native format\n+        Returns:\n+            HandlerResponse\n+        \"\"\"\n+        need_to_close = not self.is_connected\n+        self.connect()\n+        \n+        try:\n+            response = self.connection.start_query_execution(\n+                QueryString=query,\n+                QueryExecutionContext={\n+                    'Database': self.connection_data['database'],\n+                },\n+                ResultConfiguration={\n+                    'OutputLocation': self.connection_data['result_output_location'],",
    "comment": "I think this should be `results_output_location` ?",
    "line_number": 103,
    "enriched": "File: mindsdb/integrations/handlers/athena_handler/athena_handler.py\nCode: @@ -0,0 +1,208 @@\n+import time\n+import pandas as pd\n+from boto3 import client\n+from typing import Optional\n+\n+from mindsdb_sql import parse_sql\n+from mindsdb.integrations.libs.base import DatabaseHandler\n+from mindsdb_sql.parser.ast.base import ASTNode\n+from mindsdb.utilities import log\n+from mindsdb.integrations.libs.response import (\n+    HandlerStatusResponse as StatusResponse,\n+    HandlerResponse as Response,\n+    RESPONSE_TYPE\n+)\n+\n+logger = log.getLogger(__name__)\n+\n+class AthenaHandler(DatabaseHandler):\n+    \"\"\"\n+    This handler handles connection and execution of the Athena statements.\n+    \"\"\"\n+\n+    name = 'athena'\n+\n+    def __init__(self, name: str, connection_data: Optional[dict], **kwargs):\n+        \"\"\"\n+        Initialize the handler.\n+        Args:\n+            name (str): name of particular handler instance\n+            connection_data (dict): parameters for connecting to the database\n+            **kwargs: arbitrary keyword arguments.\n+        \"\"\"\n+        super().__init__(name)\n+        self.parser = parse_sql\n+        self.dialect = 'athena'\n+\n+        self.connection_data = connection_data\n+        self.kwargs = kwargs\n+\n+        self.connection = None\n+        self.is_connected = False\n+\n+    def connect(self) -> StatusResponse:\n+        \"\"\"\n+        Set up the connection required by the handler.\n+        Returns:\n+            HandlerStatusResponse\n+        \"\"\"\n+\n+        if self.is_connected:\n+            return StatusResponse(self.name, True, 'Already connected')\n+\n+        try:\n+            self.connection = client(\n+                'athena',\n+                aws_access_key_id=self.connection_data['aws_access_key_id'],\n+                aws_secret_access_key=self.connection_data['aws_secret_access_key'],\n+                region_name=self.connection_data['region_name'],\n+            )\n+            self.is_connected = True\n+            return StatusResponse(self.name, True, 'Connected successfully')\n+        except Exception as e:\n+            logger.error(f'Failed to connect to Athena: {str(e)}')\n+            return StatusResponse(self.name, False, str(e))\n+\n+    def disconnect(self):\n+        \"\"\"\n+        Close any existing connections.\n+        \"\"\"\n+        if self.is_connected:\n+            self.connection = None\n+            self.is_connected = False\n+\n+    def check_connection(self) -> StatusResponse:\n+        \"\"\"\n+        Check connection to the handler.\n+        Returns:\n+            HandlerStatusResponse\n+        \"\"\"\n+        if self.is_connected:\n+            return StatusResponse(self.name, True, 'Connection is alive')\n+        else:\n+            return self.connect()\n+\n+    def native_query(self, query: str) -> StatusResponse:\n+        \"\"\"\n+        Receive raw query and act upon it somehow.\n+        Args:\n+            query (str): query in native format\n+        Returns:\n+            HandlerResponse\n+        \"\"\"\n+        need_to_close = not self.is_connected\n+        self.connect()\n+        \n+        try:\n+            response = self.connection.start_query_execution(\n+                QueryString=query,\n+                QueryExecutionContext={\n+                    'Database': self.connection_data['database'],\n+                },\n+                ResultConfiguration={\n+                    'OutputLocation': self.connection_data['result_output_location'],\nComment: I think this should be `results_output_location` ?",
    "subcategory": "praise",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/athena_handler/athena_handler.py",
    "pr_number": 9282,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1624385021,
    "comment_created_at": "2024-06-03T12:35:46Z"
  },
  {
    "code": "@@ -99,6 +104,32 @@ def select(self, query: ast.Select) -> pd.DataFrame:\n \n         return customers_df\n \n+    def insert(self, query: ast.Insert) -> None:\n+        \"\"\"Inserts data into the Shopify \"POST /customers\" API endpoint.\n+\n+        Parameters\n+        ----------\n+        query : ast.Insert\n+           Given SQL INSERT query\n+\n+        Returns\n+        -------\n+        None\n+\n+        Raises\n+        ------\n+        ValueError\n+            If the query contains an unsupported condition\n+        \"\"\"\n+        insert_statement_parser = INSERTQueryParser(\n+            query,\n+            supported_columns=['first_name', 'last_name', 'email', 'phone', 'tags', 'currency'],",
    "comment": "Can we add this to the README. New section for INSERT and the required values?",
    "line_number": 126,
    "enriched": "File: mindsdb/integrations/handlers/shopify_handler/shopify_tables.py\nCode: @@ -99,6 +104,32 @@ def select(self, query: ast.Select) -> pd.DataFrame:\n \n         return customers_df\n \n+    def insert(self, query: ast.Insert) -> None:\n+        \"\"\"Inserts data into the Shopify \"POST /customers\" API endpoint.\n+\n+        Parameters\n+        ----------\n+        query : ast.Insert\n+           Given SQL INSERT query\n+\n+        Returns\n+        -------\n+        None\n+\n+        Raises\n+        ------\n+        ValueError\n+            If the query contains an unsupported condition\n+        \"\"\"\n+        insert_statement_parser = INSERTQueryParser(\n+            query,\n+            supported_columns=['first_name', 'last_name', 'email', 'phone', 'tags', 'currency'],\nComment: Can we add this to the README. New section for INSERT and the required values?",
    "subcategory": "design discussion",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/shopify_handler/shopify_tables.py",
    "pr_number": 6685,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1243602857,
    "comment_created_at": "2023-06-27T11:46:36Z"
  },
  {
    "code": "@@ -0,0 +1,1069 @@\n+---\n+title: Knowledge Base\n+sidebarTitle: Knowledge Bases\n+---\n+\n+A knowledge base is an advanced system that organizes information based on semantic meaning rather than simple keyword matching. It integrates embedding models, reranking models, and vector stores to enable context-aware data retrieval.\n+\n+By performing semantic reasoning across multiple data points, a knowledge base delivers deeper insights and more accurate responses, making it a powerful tool for intelligent data access.\n+\n+## How Knowledge Bases Work\n+\n+Before diving into the syntax, here is a quick walkthrough showing how knowledge bases work in MindsDB.\n+\n+We start by creating a knowledge base and inserting data. Next we can run semantic search queries with metadata filtering.\n+\n+<Steps>\n+  <Step title=\"Create a knowledge base\">\n+    Use the `create()` function to create a knowledge base, specifying all its components.\n+\n+    ```python\n+    server = mindsdb_sdk.connect()\n+    project = server.get_project()\n+\n+    my_kb = project.knowledge_bases.create(\n+        'my_kb',\n+        embedding_model={'provider': 'openai', 'model_name': 'text-embedding-3-small', 'api_key': 'sk-...'},",
    "comment": "Should we explain all parameters e.g `storage` looks confusing like what user needs to provide?",
    "line_number": 26,
    "enriched": "File: docs/sdks/python/knowledge-bases.mdx\nCode: @@ -0,0 +1,1069 @@\n+---\n+title: Knowledge Base\n+sidebarTitle: Knowledge Bases\n+---\n+\n+A knowledge base is an advanced system that organizes information based on semantic meaning rather than simple keyword matching. It integrates embedding models, reranking models, and vector stores to enable context-aware data retrieval.\n+\n+By performing semantic reasoning across multiple data points, a knowledge base delivers deeper insights and more accurate responses, making it a powerful tool for intelligent data access.\n+\n+## How Knowledge Bases Work\n+\n+Before diving into the syntax, here is a quick walkthrough showing how knowledge bases work in MindsDB.\n+\n+We start by creating a knowledge base and inserting data. Next we can run semantic search queries with metadata filtering.\n+\n+<Steps>\n+  <Step title=\"Create a knowledge base\">\n+    Use the `create()` function to create a knowledge base, specifying all its components.\n+\n+    ```python\n+    server = mindsdb_sdk.connect()\n+    project = server.get_project()\n+\n+    my_kb = project.knowledge_bases.create(\n+        'my_kb',\n+        embedding_model={'provider': 'openai', 'model_name': 'text-embedding-3-small', 'api_key': 'sk-...'},\nComment: Should we explain all parameters e.g `storage` looks confusing like what user needs to provide?",
    "subcategory": "design discussion",
    "category": "discussion",
    "file_path": "docs/sdks/python/knowledge-bases.mdx",
    "pr_number": 11162,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2166410819,
    "comment_created_at": "2025-06-25T10:45:31Z"
  },
  {
    "code": "@@ -306,6 +320,35 @@ def prepare_env_config(self) -> None:\n             self._env_config['logging']['handlers']['file']['enabled'] = True\n         # endregion\n \n+        # region server type\n+        server_type = os.environ.get('MINDSDB_HTTP_SERVER_TYPE', '').lower()",
    "comment": "Should we catch scenarios where server type is invalid, outside of supported values?",
    "line_number": 324,
    "enriched": "File: mindsdb/utilities/config.py\nCode: @@ -306,6 +320,35 @@ def prepare_env_config(self) -> None:\n             self._env_config['logging']['handlers']['file']['enabled'] = True\n         # endregion\n \n+        # region server type\n+        server_type = os.environ.get('MINDSDB_HTTP_SERVER_TYPE', '').lower()\nComment: Should we catch scenarios where server type is invalid, outside of supported values?",
    "subcategory": "design discussion",
    "category": "discussion",
    "file_path": "mindsdb/utilities/config.py",
    "pr_number": 10313,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1895908459,
    "comment_created_at": "2024-12-23T16:04:36Z"
  },
  {
    "code": "@@ -0,0 +1,42 @@\n+import os\n+import time\n+import pandas as pd\n+\n+from mindsdb_sql import parse_sql\n+from ..executor_test_base import BaseExecutorTest\n+\n+\n+class BaseMLTest(BaseExecutorTest):\n+    \"\"\"\n+    Base test class for ML engines\n+    \"\"\"\n+    def wait_predictor(self, project: str, name: str, timeout: int = 100) -> None:\n+        \"\"\"Wait for the predictor to be created, raising an exception if predictor creation fails or exceeds timeout\"\"\"\n+        for attempt in range(timeout):\n+            ret = self.run_sql(f\"select * from {project}.models where name='{name}'\")\n+            if not ret.empty:\n+                status = ret[\"STATUS\"][0]\n+                if status == \"complete\":\n+                    return\n+                elif status == \"error\":\n+                    raise RuntimeError(\"Predictor failed\", ret[\"ERROR\"][0])\n+            time.sleep(0.5)\n+        raise RuntimeError(\"Predictor wasn't created\")\n+\n+    def run_sql(self, sql: str) -> pd.DataFrame:\n+        \"\"\"Execute SQL and return a DataFrame, raising an AssertionError if an error occurs\"\"\"\n+        ret = self.command_executor.execute_command(parse_sql(sql, dialect=\"mindsdb\"))\n+        assert ret.error_code is None, f\"SQL execution failed with error: {ret.error_code}\"\n+        if ret.data is not None:\n+            columns = [col.alias if col.alias else col.name for col in ret.columns]\n+            return pd.DataFrame(ret.data, columns=columns)\n+\n+\n+class BaseMLAPITest(BaseMLTest):\n+    \"\"\"\n+    Base test class for API-based ML engines\n+    \"\"\"\n+    @staticmethod\n+    def get_api_key(env_var: str):",
    "comment": "There is a more robust method for this in \"integrations/utilities/handler_utils:get_api_key`, we probably want to use that instead.",
    "line_number": 40,
    "enriched": "File: tests/unit/ml_handlers/base_ml_test.py\nCode: @@ -0,0 +1,42 @@\n+import os\n+import time\n+import pandas as pd\n+\n+from mindsdb_sql import parse_sql\n+from ..executor_test_base import BaseExecutorTest\n+\n+\n+class BaseMLTest(BaseExecutorTest):\n+    \"\"\"\n+    Base test class for ML engines\n+    \"\"\"\n+    def wait_predictor(self, project: str, name: str, timeout: int = 100) -> None:\n+        \"\"\"Wait for the predictor to be created, raising an exception if predictor creation fails or exceeds timeout\"\"\"\n+        for attempt in range(timeout):\n+            ret = self.run_sql(f\"select * from {project}.models where name='{name}'\")\n+            if not ret.empty:\n+                status = ret[\"STATUS\"][0]\n+                if status == \"complete\":\n+                    return\n+                elif status == \"error\":\n+                    raise RuntimeError(\"Predictor failed\", ret[\"ERROR\"][0])\n+            time.sleep(0.5)\n+        raise RuntimeError(\"Predictor wasn't created\")\n+\n+    def run_sql(self, sql: str) -> pd.DataFrame:\n+        \"\"\"Execute SQL and return a DataFrame, raising an AssertionError if an error occurs\"\"\"\n+        ret = self.command_executor.execute_command(parse_sql(sql, dialect=\"mindsdb\"))\n+        assert ret.error_code is None, f\"SQL execution failed with error: {ret.error_code}\"\n+        if ret.data is not None:\n+            columns = [col.alias if col.alias else col.name for col in ret.columns]\n+            return pd.DataFrame(ret.data, columns=columns)\n+\n+\n+class BaseMLAPITest(BaseMLTest):\n+    \"\"\"\n+    Base test class for API-based ML engines\n+    \"\"\"\n+    @staticmethod\n+    def get_api_key(env_var: str):\nComment: There is a more robust method for this in \"integrations/utilities/handler_utils:get_api_key`, we probably want to use that instead.",
    "subcategory": "design discussion",
    "category": "discussion",
    "file_path": "tests/unit/ml_handlers/base_ml_test.py",
    "pr_number": 8262,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1381723312,
    "comment_created_at": "2023-11-03T13:58:38Z"
  },
  {
    "code": "@@ -0,0 +1,168 @@\n+import pandas as pd\n+from typing import Dict\n+import datetime\n+import hmac\n+import hashlib\n+import time\n+import requests\n+import pytz\n+from collections import OrderedDict\n+import base64\n+\n+from mindsdb.integrations.handlers.coinbase_handler.coinbase_tables import CoinBaseAggregatedTradesTable\n+from mindsdb.integrations.libs.api_handler import APIHandler\n+from mindsdb.integrations.libs.const import HANDLER_CONNECTION_ARG_TYPE as ARG_TYPE\n+from mindsdb.integrations.libs.response import (\n+    HandlerStatusResponse as StatusResponse,\n+    HandlerResponse as Response,\n+)\n+\n+from mindsdb_sql import parse_sql\n+\n+_BASE_COINBASE_US_URL = 'https://api.exchange.coinbase.com'",
    "comment": "Not necessary for this PR, but would be nice to have this configurable as well (mainly since Coinbase has an [Exchange Sandbox](https://docs.cloud.coinbase.com/exchange/docs/sandbox) users may want to use)",
    "line_number": 22,
    "enriched": "File: mindsdb/integrations/handlers/coinbase_handler/coinbase_handler.py\nCode: @@ -0,0 +1,168 @@\n+import pandas as pd\n+from typing import Dict\n+import datetime\n+import hmac\n+import hashlib\n+import time\n+import requests\n+import pytz\n+from collections import OrderedDict\n+import base64\n+\n+from mindsdb.integrations.handlers.coinbase_handler.coinbase_tables import CoinBaseAggregatedTradesTable\n+from mindsdb.integrations.libs.api_handler import APIHandler\n+from mindsdb.integrations.libs.const import HANDLER_CONNECTION_ARG_TYPE as ARG_TYPE\n+from mindsdb.integrations.libs.response import (\n+    HandlerStatusResponse as StatusResponse,\n+    HandlerResponse as Response,\n+)\n+\n+from mindsdb_sql import parse_sql\n+\n+_BASE_COINBASE_US_URL = 'https://api.exchange.coinbase.com'\nComment: Not necessary for this PR, but would be nice to have this configurable as well (mainly since Coinbase has an [Exchange Sandbox](https://docs.cloud.coinbase.com/exchange/docs/sandbox) users may want to use)",
    "subcategory": "design discussion",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/coinbase_handler/coinbase_handler.py",
    "pr_number": 8168,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1376525143,
    "comment_created_at": "2023-10-30T16:44:27Z"
  },
  {
    "code": "@@ -28,10 +30,25 @@ def image_extension_check(self, url):\n             return ext\n         raise Exception(\"Unknown image format. Currently jpg, jpeg & png are supported.\")\n \n-    def download_image(self, url):\n-        img_ext = self.image_extension_check(url)\n-        res = requests.get(url)\n-        return {\"img_ext\": img_ext, \"content\": res.content}\n+    def is_url(self, path):\n+        \"\"\"Check if a path is a URL.\"\"\"\n+        parsed = urlparse(path)\n+        return parsed.scheme in ('http', 'https')\n+\n+    def download_image(self, path):\n+        \"\"\"Download image from a URL or read from a local file path.\"\"\"\n+        if self.is_url(path):\n+            # Download image from URL\n+            img_ext = self.image_extension_check(path)\n+            res = requests.get(path)\n+            return {\"img_ext\": img_ext, \"content\": res.content}\n+        elif os.path.exists(path):",
    "comment": "Let's keep only URL for now, this will not work on cloud",
    "line_number": 45,
    "enriched": "File: mindsdb/integrations/handlers/clipdrop_handler/clipdrop.py\nCode: @@ -28,10 +30,25 @@ def image_extension_check(self, url):\n             return ext\n         raise Exception(\"Unknown image format. Currently jpg, jpeg & png are supported.\")\n \n-    def download_image(self, url):\n-        img_ext = self.image_extension_check(url)\n-        res = requests.get(url)\n-        return {\"img_ext\": img_ext, \"content\": res.content}\n+    def is_url(self, path):\n+        \"\"\"Check if a path is a URL.\"\"\"\n+        parsed = urlparse(path)\n+        return parsed.scheme in ('http', 'https')\n+\n+    def download_image(self, path):\n+        \"\"\"Download image from a URL or read from a local file path.\"\"\"\n+        if self.is_url(path):\n+            # Download image from URL\n+            img_ext = self.image_extension_check(path)\n+            res = requests.get(path)\n+            return {\"img_ext\": img_ext, \"content\": res.content}\n+        elif os.path.exists(path):\nComment: Let's keep only URL for now, this will not work on cloud",
    "subcategory": "design discussion",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/clipdrop_handler/clipdrop.py",
    "pr_number": 9944,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1824208861,
    "comment_created_at": "2024-10-31T10:14:31Z"
  },
  {
    "code": "@@ -0,0 +1,99 @@\n+import os\n+import pytest\n+\n+from .base_ml_test import BaseMLAPITest\n+\n+\n+@pytest.mark.skipif(os.environ.get('MDB_TEST_MDB_INFERENCE_API_KEY') is None, reason='Missing API key!')",
    "comment": "Should we keep the location for this type of test inside the unit?",
    "line_number": 7,
    "enriched": "File: tests/unit/ml_handlers/test_mindsdb_inference_integrations_tests.py\nCode: @@ -0,0 +1,99 @@\n+import os\n+import pytest\n+\n+from .base_ml_test import BaseMLAPITest\n+\n+\n+@pytest.mark.skipif(os.environ.get('MDB_TEST_MDB_INFERENCE_API_KEY') is None, reason='Missing API key!')\nComment: Should we keep the location for this type of test inside the unit?",
    "subcategory": "design discussion",
    "category": "discussion",
    "file_path": "tests/unit/ml_handlers/test_mindsdb_inference_integrations_tests.py",
    "pr_number": 9099,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1577806784,
    "comment_created_at": "2024-04-24T12:34:00Z"
  },
  {
    "code": "@@ -1,2 +1,2 @@\n flaml<=1.2.3\n-dill\n+type-infer",
    "comment": "`type-infer` is part of our ML packages. I think we probably want to make it part of our defaults given we're recommending people to actively use it when developing new ML engines.\r\n\r\nIt is fairly light in deps ([source](https://github.com/mindsdb/type_infer/blob/staging/pyproject.toml)), but if you think there are any here that would be problematic we can keep it as optional.",
    "line_number": 2,
    "enriched": "File: mindsdb/integrations/handlers/flaml_handler/requirements.txt\nCode: @@ -1,2 +1,2 @@\n flaml<=1.2.3\n-dill\n+type-infer\nComment: `type-infer` is part of our ML packages. I think we probably want to make it part of our defaults given we're recommending people to actively use it when developing new ML engines.\r\n\r\nIt is fairly light in deps ([source](https://github.com/mindsdb/type_infer/blob/staging/pyproject.toml)), but if you think there are any here that would be problematic we can keep it as optional.",
    "subcategory": "design discussion",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/flaml_handler/requirements.txt",
    "pr_number": 8134,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1376699002,
    "comment_created_at": "2023-10-30T19:15:47Z"
  },
  {
    "code": "@@ -0,0 +1,123 @@\n+import re\n+from typing import List, Dict,Optional\n+\n+import tempfile\n+import boto3\n+from botocore.exceptions import NoCredentialsError\n+from mindsdb.interfaces.knowledge_base.data_source_config import S3Config\n+\n+\n+def connect_s3(s3_config: S3Config):\n+    \"\"\"\n+    Establishes a connection to AWS S3 using the provided configuration.\n+\n+    :param s3_config: S3Config object containing AWS credentials and configuration.\n+    :return: boto3 S3 client.\n+    \"\"\"\n+    try:\n+        client_params = {\n+            'service_name': 's3',\n+            'aws_access_key_id': s3_config.aws_access_key_id,\n+            'aws_secret_access_key': s3_config.aws_secret_access_key,\n+            'region_name': s3_config.region_name,\n+        }\n+\n+        # Add session token if provided\n+        if s3_config.aws_session_token:\n+            client_params['aws_session_token'] = s3_config.aws_session_token\n+\n+        return boto3.client(**client_params)\n+    except NoCredentialsError:\n+        raise Exception(\"Credentials not available\")\n+\n+def get_filtered_files(s3_config: S3Config) -> List[Dict]:\n+    \"\"\"\n+    Retrieves and filters files from S3 buckets based on the provided configuration.\n+\n+    :param s3_config: S3Config object containing bucket and file regex patterns.\n+    :return: List of dictionaries with filtered file details.\n+    \"\"\"\n+    s3 = connect_s3(s3_config)\n+    results = []\n+\n+    # List and filter buckets based on regex patterns\n+    all_buckets = [bucket['Name'] for bucket in s3.list_buckets().get('Buckets', [])]\n+    filtered_buckets = [bucket for bucket in all_buckets if any(re.search(pattern, bucket) for pattern in s3_config.buckets)]\n+\n+    for bucket in filtered_buckets:\n+        continuation_token = None\n+\n+        while True:\n+            # Fetch objects, handle pagination with ContinuationToken\n+            list_kwargs = {'Bucket': bucket}\n+            if continuation_token:\n+                list_kwargs['ContinuationToken'] = continuation_token\n+",
    "comment": "Nit: The error handling when fetching objects from S3 lacks specificity. You might want to raise a more detailed custom exception, so it’s easier to debug issues specific to S3 connections.",
    "line_number": 55,
    "enriched": "File: mindsdb/integrations/utilities/rag/s3_knowledge_base.py\nCode: @@ -0,0 +1,123 @@\n+import re\n+from typing import List, Dict,Optional\n+\n+import tempfile\n+import boto3\n+from botocore.exceptions import NoCredentialsError\n+from mindsdb.interfaces.knowledge_base.data_source_config import S3Config\n+\n+\n+def connect_s3(s3_config: S3Config):\n+    \"\"\"\n+    Establishes a connection to AWS S3 using the provided configuration.\n+\n+    :param s3_config: S3Config object containing AWS credentials and configuration.\n+    :return: boto3 S3 client.\n+    \"\"\"\n+    try:\n+        client_params = {\n+            'service_name': 's3',\n+            'aws_access_key_id': s3_config.aws_access_key_id,\n+            'aws_secret_access_key': s3_config.aws_secret_access_key,\n+            'region_name': s3_config.region_name,\n+        }\n+\n+        # Add session token if provided\n+        if s3_config.aws_session_token:\n+            client_params['aws_session_token'] = s3_config.aws_session_token\n+\n+        return boto3.client(**client_params)\n+    except NoCredentialsError:\n+        raise Exception(\"Credentials not available\")\n+\n+def get_filtered_files(s3_config: S3Config) -> List[Dict]:\n+    \"\"\"\n+    Retrieves and filters files from S3 buckets based on the provided configuration.\n+\n+    :param s3_config: S3Config object containing bucket and file regex patterns.\n+    :return: List of dictionaries with filtered file details.\n+    \"\"\"\n+    s3 = connect_s3(s3_config)\n+    results = []\n+\n+    # List and filter buckets based on regex patterns\n+    all_buckets = [bucket['Name'] for bucket in s3.list_buckets().get('Buckets', [])]\n+    filtered_buckets = [bucket for bucket in all_buckets if any(re.search(pattern, bucket) for pattern in s3_config.buckets)]\n+\n+    for bucket in filtered_buckets:\n+        continuation_token = None\n+\n+        while True:\n+            # Fetch objects, handle pagination with ContinuationToken\n+            list_kwargs = {'Bucket': bucket}\n+            if continuation_token:\n+                list_kwargs['ContinuationToken'] = continuation_token\n+\nComment: Nit: The error handling when fetching objects from S3 lacks specificity. You might want to raise a more detailed custom exception, so it’s easier to debug issues specific to S3 connections.",
    "subcategory": "design discussion",
    "category": "discussion",
    "file_path": "mindsdb/integrations/utilities/rag/s3_knowledge_base.py",
    "pr_number": 9712,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1756741778,
    "comment_created_at": "2024-09-12T12:15:14Z"
  },
  {
    "code": "@@ -0,0 +1,3 @@\n+auto-sklearn\n+mindsdb-evaluator>=0.0.6",
    "comment": "This package seems like it's not being used yet?",
    "line_number": 2,
    "enriched": "File: mindsdb/integrations/handlers/autosklearn_handler/requirements.txt\nCode: @@ -0,0 +1,3 @@\n+auto-sklearn\n+mindsdb-evaluator>=0.0.6\nComment: This package seems like it's not being used yet?",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "mindsdb/integrations/handlers/autosklearn_handler/requirements.txt",
    "pr_number": 5312,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1154674367,
    "comment_created_at": "2023-03-31T16:24:03Z"
  },
  {
    "code": "@@ -0,0 +1,204 @@\n+import numpy as np\n+import pandas as pd\n+from typing import Text, Dict, List, Optional, Any\n+\n+from mindsdb.utilities import log\n+\n+from mindsdb.integrations.libs.base import BaseMLEngine\n+from mindsdb.integrations.libs.llm.utils import get_completed_prompts\n+from mindsdb.integrations.libs.api_handler_exceptions import MissingConnectionParams\n+from mindsdb.integrations.handlers.bedrock_handler.settings import AmazonBedrockHandlerEngineConfig, AmazonBedrockHandlerModelConfig\n+from mindsdb.integrations.handlers.bedrock_handler.utilities import create_amazon_bedrock_client, create_amazon_bedrock_runtime_client\n+\n+\n+logger = log.getLogger(__name__)\n+\n+\n+class AmazonBedrockHandler(BaseMLEngine):\n+    \"\"\"\n+    This handler handles connection and inference with the Amazon Bedrock API.\n+    \"\"\"\n+\n+    name = 'bedrock'\n+\n+    def __init__(self, *args, **kwargs):\n+        super().__init__(*args, **kwargs)\n+        self.generative = True\n+\n+    def create_engine(self, connection_args: Dict) -> None:\n+        \"\"\"\n+        Validates the AWS credentials provided on engine creation.\n+\n+        Args:\n+            connection_args (Dict): Parameters for the engine.\n+\n+        Raises:\n+            Exception: If the handler is not configured with valid API credentials.\n+        \"\"\"\n+        connection_args = {k.lower(): v for k, v in connection_args.items()}\n+        AmazonBedrockHandlerEngineConfig(**connection_args)\n+\n+    def create(self, target, args: Dict = None, **kwargs: Any) -> None:\n+        \"\"\"\n+        Creates a model by validating the model configuration and saving it to the storage.\n+\n+        Args:\n+            target (Text): Target column name.\n+            args (Dict): Parameters for the model.\n+            kwargs (Any): Other keyword arguments.\n+\n+        Raises:\n+            Exception: If the model is not configured with valid parameters.\n+\n+        Returns:\n+            None\n+        \"\"\"\n+        if 'using' not in args:\n+            raise MissingConnectionParams(\"Amazon Bedrock engine requires a USING clause! Refer to its documentation for more details.\")\n+        else:\n+            args = args['using']\n+            handler_model_config = AmazonBedrockHandlerModelConfig(**args, connection_args=self.engine_storage.get_connection_args())\n+\n+            # Save the model configuration to the storage.\n+            handler_model_params = handler_model_config.model_dump()\n+            logger.info(f\"Saving model configuration to storage: {handler_model_params}\")\n+\n+            args['target'] = target\n+            args['handler_model_params'] = handler_model_params\n+            self.model_storage.json_set('args', args)\n+\n+    def predict(self, df: Optional[pd.DataFrame] = None, args: Optional[Dict] = None) -> None:\n+        \"\"\"\n+        Make predictions using a model by invoking the Amazon Bedrock API.\n+\n+        Args:\n+            df (pd.DataFrame): Input data to make predictions on.\n+            args (Dict): Parameters passed when making predictions.\n+\n+        Raises:\n+            Exception: If the input does not match the configuration of the model.\n+\n+        Returns:\n+            pd.DataFrame: Input data with the predicted values in a new column.",
    "comment": "Type hint signature does not match return type (`None`  but should be `pd.DataFrame`)",
    "line_number": 82,
    "enriched": "File: mindsdb/integrations/handlers/bedrock_handler/bedrock_handler.py\nCode: @@ -0,0 +1,204 @@\n+import numpy as np\n+import pandas as pd\n+from typing import Text, Dict, List, Optional, Any\n+\n+from mindsdb.utilities import log\n+\n+from mindsdb.integrations.libs.base import BaseMLEngine\n+from mindsdb.integrations.libs.llm.utils import get_completed_prompts\n+from mindsdb.integrations.libs.api_handler_exceptions import MissingConnectionParams\n+from mindsdb.integrations.handlers.bedrock_handler.settings import AmazonBedrockHandlerEngineConfig, AmazonBedrockHandlerModelConfig\n+from mindsdb.integrations.handlers.bedrock_handler.utilities import create_amazon_bedrock_client, create_amazon_bedrock_runtime_client\n+\n+\n+logger = log.getLogger(__name__)\n+\n+\n+class AmazonBedrockHandler(BaseMLEngine):\n+    \"\"\"\n+    This handler handles connection and inference with the Amazon Bedrock API.\n+    \"\"\"\n+\n+    name = 'bedrock'\n+\n+    def __init__(self, *args, **kwargs):\n+        super().__init__(*args, **kwargs)\n+        self.generative = True\n+\n+    def create_engine(self, connection_args: Dict) -> None:\n+        \"\"\"\n+        Validates the AWS credentials provided on engine creation.\n+\n+        Args:\n+            connection_args (Dict): Parameters for the engine.\n+\n+        Raises:\n+            Exception: If the handler is not configured with valid API credentials.\n+        \"\"\"\n+        connection_args = {k.lower(): v for k, v in connection_args.items()}\n+        AmazonBedrockHandlerEngineConfig(**connection_args)\n+\n+    def create(self, target, args: Dict = None, **kwargs: Any) -> None:\n+        \"\"\"\n+        Creates a model by validating the model configuration and saving it to the storage.\n+\n+        Args:\n+            target (Text): Target column name.\n+            args (Dict): Parameters for the model.\n+            kwargs (Any): Other keyword arguments.\n+\n+        Raises:\n+            Exception: If the model is not configured with valid parameters.\n+\n+        Returns:\n+            None\n+        \"\"\"\n+        if 'using' not in args:\n+            raise MissingConnectionParams(\"Amazon Bedrock engine requires a USING clause! Refer to its documentation for more details.\")\n+        else:\n+            args = args['using']\n+            handler_model_config = AmazonBedrockHandlerModelConfig(**args, connection_args=self.engine_storage.get_connection_args())\n+\n+            # Save the model configuration to the storage.\n+            handler_model_params = handler_model_config.model_dump()\n+            logger.info(f\"Saving model configuration to storage: {handler_model_params}\")\n+\n+            args['target'] = target\n+            args['handler_model_params'] = handler_model_params\n+            self.model_storage.json_set('args', args)\n+\n+    def predict(self, df: Optional[pd.DataFrame] = None, args: Optional[Dict] = None) -> None:\n+        \"\"\"\n+        Make predictions using a model by invoking the Amazon Bedrock API.\n+\n+        Args:\n+            df (pd.DataFrame): Input data to make predictions on.\n+            args (Dict): Parameters passed when making predictions.\n+\n+        Raises:\n+            Exception: If the input does not match the configuration of the model.\n+\n+        Returns:\n+            pd.DataFrame: Input data with the predicted values in a new column.\nComment: Type hint signature does not match return type (`None`  but should be `pd.DataFrame`)",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "mindsdb/integrations/handlers/bedrock_handler/bedrock_handler.py",
    "pr_number": 9475,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1681918858,
    "comment_created_at": "2024-07-18T00:19:35Z"
  }
]