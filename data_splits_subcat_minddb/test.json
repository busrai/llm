[
  {
    "code": "@@ -34,19 +34,42 @@ The following functions are supported via the DuckDB engine. See the [complete l\n * [Utility Functions](https://duckdb.org/docs/stable/sql/functions/utility)\n * [Window Functions](https://duckdb.org/docs/stable/sql/functions/window_functions)\n \n-## Functions via MySQL\n-\n-The following functions are supported via the MySQL engine.\n-\n-* [`char()`](https://dev.mysql.com/doc/refman/8.4/en/string-functions.html#function_char)\n-* [`locate()`](https://dev.mysql.com/doc/refman/8.4/en/string-functions.html#function_locate)\n-* [`instr()`](https://dev.mysql.com/doc/refman/8.0/en/string-functions.html#function_instr)\n-* [`unhex()`](https://dev.mysql.com/doc/refman/8.4/en/string-functions.html#function_unhex)\n-* [`format()`](https://dev.mysql.com/doc/refman/8.4/en/string-functions.html#function_format)\n-* [`sha2()`](https://dev.mysql.com/doc/refman/8.4/en/encryption-functions.html#function_sha2)\n-* [`length()`](https://dev.mysql.com/doc/refman/8.4/en/string-functions.html#function_length)\n-* [`regexp_substr()`](https://dev.mysql.com/doc/refman/8.4/en/regexp.html#function_regexp-substr)\n-* [`substring_index()`](https://dev.mysql.com/doc/refman/8.4/en/string-functions.html#function_substring-index)\n-* [`curtime()`](https://dev.mysql.com/doc/refman/8.4/en/date-and-time-functions.html#function_curtime)\n-* [`timestampdiff()`](https://dev.mysql.com/doc/refman/8.4/en/date-and-time-functions.html#function_timestampdiff)\n-* [`extract()`](https://dev.mysql.com/doc/refman/8.4/en/date-and-time-functions.html#function_extract)\n+## MySQL Functions\n+\n+MindsDB executes MySQL-style functions on the underlying DuckDB engine. The following functions have been adapted to MySQL-style functions.\n+\n+String functions:\n+\n+* [`CHAR`](https://dev.mysql.com/doc/refman/8.4/en/string-functions.html#function_char)\n+* [`FORMAT`](https://dev.mysql.com/doc/refman/8.4/en/string-functions.html#function_format)\n+* [`INSTR`](https://dev.mysql.com/doc/refman/8.0/en/string-functions.html#function_instr)\n+* [`LENGTH`](https://dev.mysql.com/doc/refman/8.4/en/string-functions.html#function_length)\n+* [`LOCATE`](https://dev.mysql.com/doc/refman/8.4/en/string-functions.html#function_locate)\n+* [`SUBSTRING_INDEX`](https://dev.mysql.com/doc/refman/8.4/en/string-functions.html#function_substring-index)\n+* [`UNHEX`](https://dev.mysql.com/doc/refman/8.4/en/string-functions.html#function_unhex)\n+\n+Date and time functions:\n+\n+* [`ADDDATE`](https://dev.mysql.com/doc/refman/8.4/en/date-and-time-functions.html#function_adddate)\n+* <Tooltip tip=\"Both arguments can be either date or datetime but not time.\">[`ADDTIME`](https://dev.mysql.com/doc/refman/8.4/en/date-and-time-functions.html#function_addtime)</Tooltip>",
    "comment": "the second argument is not date or datetime, it has to be time-interval in mysql format:\r\n`ADDTIME('2007-12-31', '1 1:1:1.2')`",
    "line_number": 54,
    "enriched": "File: docs/mindsdb_sql/functions/standard-functions.mdx\nCode: @@ -34,19 +34,42 @@ The following functions are supported via the DuckDB engine. See the [complete l\n * [Utility Functions](https://duckdb.org/docs/stable/sql/functions/utility)\n * [Window Functions](https://duckdb.org/docs/stable/sql/functions/window_functions)\n \n-## Functions via MySQL\n-\n-The following functions are supported via the MySQL engine.\n-\n-* [`char()`](https://dev.mysql.com/doc/refman/8.4/en/string-functions.html#function_char)\n-* [`locate()`](https://dev.mysql.com/doc/refman/8.4/en/string-functions.html#function_locate)\n-* [`instr()`](https://dev.mysql.com/doc/refman/8.0/en/string-functions.html#function_instr)\n-* [`unhex()`](https://dev.mysql.com/doc/refman/8.4/en/string-functions.html#function_unhex)\n-* [`format()`](https://dev.mysql.com/doc/refman/8.4/en/string-functions.html#function_format)\n-* [`sha2()`](https://dev.mysql.com/doc/refman/8.4/en/encryption-functions.html#function_sha2)\n-* [`length()`](https://dev.mysql.com/doc/refman/8.4/en/string-functions.html#function_length)\n-* [`regexp_substr()`](https://dev.mysql.com/doc/refman/8.4/en/regexp.html#function_regexp-substr)\n-* [`substring_index()`](https://dev.mysql.com/doc/refman/8.4/en/string-functions.html#function_substring-index)\n-* [`curtime()`](https://dev.mysql.com/doc/refman/8.4/en/date-and-time-functions.html#function_curtime)\n-* [`timestampdiff()`](https://dev.mysql.com/doc/refman/8.4/en/date-and-time-functions.html#function_timestampdiff)\n-* [`extract()`](https://dev.mysql.com/doc/refman/8.4/en/date-and-time-functions.html#function_extract)\n+## MySQL Functions\n+\n+MindsDB executes MySQL-style functions on the underlying DuckDB engine. The following functions have been adapted to MySQL-style functions.\n+\n+String functions:\n+\n+* [`CHAR`](https://dev.mysql.com/doc/refman/8.4/en/string-functions.html#function_char)\n+* [`FORMAT`](https://dev.mysql.com/doc/refman/8.4/en/string-functions.html#function_format)\n+* [`INSTR`](https://dev.mysql.com/doc/refman/8.0/en/string-functions.html#function_instr)\n+* [`LENGTH`](https://dev.mysql.com/doc/refman/8.4/en/string-functions.html#function_length)\n+* [`LOCATE`](https://dev.mysql.com/doc/refman/8.4/en/string-functions.html#function_locate)\n+* [`SUBSTRING_INDEX`](https://dev.mysql.com/doc/refman/8.4/en/string-functions.html#function_substring-index)\n+* [`UNHEX`](https://dev.mysql.com/doc/refman/8.4/en/string-functions.html#function_unhex)\n+\n+Date and time functions:\n+\n+* [`ADDDATE`](https://dev.mysql.com/doc/refman/8.4/en/date-and-time-functions.html#function_adddate)\n+* <Tooltip tip=\"Both arguments can be either date or datetime but not time.\">[`ADDTIME`](https://dev.mysql.com/doc/refman/8.4/en/date-and-time-functions.html#function_addtime)</Tooltip>\nComment: the second argument is not date or datetime, it has to be time-interval in mysql format:\r\n`ADDTIME('2007-12-31', '1 1:1:1.2')`",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/mindsdb_sql/functions/standard-functions.mdx",
    "pr_number": 11488,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2313616761,
    "comment_created_at": "2025-09-01T10:49:26Z"
  },
  {
    "code": "@@ -53,7 +53,8 @@ def query(self, input_query: str):\n \n         for idx, document in enumerate(res[\"source_documents\"]):\n             sources[\"sources_document\"].append(document.metadata[\"source\"])\n-            sources[\"sources_row\"].append(document.metadata.get(\"row\", None))\n+            sources[\"column\"].append(document.metadata.get(\"column\"))",
    "comment": "adding column names to metadata",
    "line_number": 56,
    "enriched": "File: mindsdb/integrations/handlers/writer_handler/question_answer.py\nCode: @@ -53,7 +53,8 @@ def query(self, input_query: str):\n \n         for idx, document in enumerate(res[\"source_documents\"]):\n             sources[\"sources_document\"].append(document.metadata[\"source\"])\n-            sources[\"sources_row\"].append(document.metadata.get(\"row\", None))\n+            sources[\"column\"].append(document.metadata.get(\"column\"))\nComment: adding column names to metadata",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "mindsdb/integrations/handlers/writer_handler/question_answer.py",
    "pr_number": 6944,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1275106816,
    "comment_created_at": "2023-07-26T14:58:47Z"
  },
  {
    "code": "@@ -0,0 +1,27 @@\n+## LlamaIndex Handler\n+LlamaIndex ML handler for MindsDB, create indexes over data plugged in mindsdb and use it to create a Question & Answer (Q&A) systems\n+\n+## LlamaIndex\n+LlamaIndex is a data framework for your LLM application.In this handler,python client of LlamaIndex api is used and more information about this python client can be found (here)[https://gpt-index.readthedocs.io/en/latest/]\n+\n+## Implemented Features\n+- [x] LlamaIndex ML Handler\n+  - [x] [Support Web Page Reader](https://gpt-index.readthedocs.io/en/latest/examples/data_connectors/WebPageDemo.html)\n+\n+## Example Usage\n+~~~sql\n+CREATE MODEL my_qa_model\n+PREDICT answer\n+USING \n+  engine = 'llama_index', \n+  index_class = 'GPTVectorStoreIndex',\n+  query_engine = 'as_query_engine', ",
    "comment": "nit: is this configuration hooked up properly? In the code below I don't see this arg being used at all. ",
    "line_number": 18,
    "enriched": "File: mindsdb/integrations/handlers/llama_index_handler/README.md\nCode: @@ -0,0 +1,27 @@\n+## LlamaIndex Handler\n+LlamaIndex ML handler for MindsDB, create indexes over data plugged in mindsdb and use it to create a Question & Answer (Q&A) systems\n+\n+## LlamaIndex\n+LlamaIndex is a data framework for your LLM application.In this handler,python client of LlamaIndex api is used and more information about this python client can be found (here)[https://gpt-index.readthedocs.io/en/latest/]\n+\n+## Implemented Features\n+- [x] LlamaIndex ML Handler\n+  - [x] [Support Web Page Reader](https://gpt-index.readthedocs.io/en/latest/examples/data_connectors/WebPageDemo.html)\n+\n+## Example Usage\n+~~~sql\n+CREATE MODEL my_qa_model\n+PREDICT answer\n+USING \n+  engine = 'llama_index', \n+  index_class = 'GPTVectorStoreIndex',\n+  query_engine = 'as_query_engine', \nComment: nit: is this configuration hooked up properly? In the code below I don't see this arg being used at all. ",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "mindsdb/integrations/handlers/llama_index_handler/README.md",
    "pr_number": 6330,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1208759600,
    "comment_created_at": "2023-05-29T01:57:37Z"
  },
  {
    "code": "@@ -1,2 +1,2 @@\n-statsforecast>=1.4.0, <2.0",
    "comment": "Pinning for stability",
    "line_number": 1,
    "enriched": "File: mindsdb/integrations/handlers/statsforecast_handler/requirements.txt\nCode: @@ -1,2 +1,2 @@\n-statsforecast>=1.4.0, <2.0\nComment: Pinning for stability",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "mindsdb/integrations/handlers/statsforecast_handler/requirements.txt",
    "pr_number": 7617,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1355200629,
    "comment_created_at": "2023-10-11T15:18:36Z"
  },
  {
    "code": "@@ -3,24 +3,27 @@ title: TwelveLabs Large Video Model\n sidebarTitle: TweleveLabs\n ---\n \n+In this section, we present how to connect Twelve Labs API to MindsDB.\n \n+[Twelve Labs](https://twelvelabs.io/product\n+) provides a powerful and seamless video search infrastructure for your application. \n \n-The Twelve Labs handler for MindsDB provides an interface to interact with the Twelve Labs API.\n+## Prerequisites\n \n-## About Twelve Labs\n-Powerful and seamless video search infrastructure for your application. \n+Before proceeding, ensure the following prerequisites are met:\n \n+ 1. Install MindsDB on your system or obtain access to cloud options.\n+ 2. To use Twelve Labs with MindsDB, install the required dependencies following [this instruction](/setup/self-hosted/docker#install-dependencies).\n+ 3. Copy your Twelve Labs API Key by navigating to the [Dashboard page](https://api.twelvelabs.io/dashboard).\n \n+## Creating an ML Engine",
    "comment": "I'm creating a doc with templates for integrations doc pages - will share it soon.\r\n\r\nFor AI integrations, let's have the following sections:\r\n\r\n\\## Prerequisites\r\n...\r\n\\## Setup\r\n...\r\n\\### AI Engine\r\n...\r\n\\### AI Model\r\n...\r\n\\## Usage\r\n...\r\n\r\nMore less like here - https://docs.mindsdb.com/integrations/ai-engines/ollama",
    "line_number": 19,
    "enriched": "File: docs/integrations/ai-engines/twelvelabs.mdx\nCode: @@ -3,24 +3,27 @@ title: TwelveLabs Large Video Model\n sidebarTitle: TweleveLabs\n ---\n \n+In this section, we present how to connect Twelve Labs API to MindsDB.\n \n+[Twelve Labs](https://twelvelabs.io/product\n+) provides a powerful and seamless video search infrastructure for your application. \n \n-The Twelve Labs handler for MindsDB provides an interface to interact with the Twelve Labs API.\n+## Prerequisites\n \n-## About Twelve Labs\n-Powerful and seamless video search infrastructure for your application. \n+Before proceeding, ensure the following prerequisites are met:\n \n+ 1. Install MindsDB on your system or obtain access to cloud options.\n+ 2. To use Twelve Labs with MindsDB, install the required dependencies following [this instruction](/setup/self-hosted/docker#install-dependencies).\n+ 3. Copy your Twelve Labs API Key by navigating to the [Dashboard page](https://api.twelvelabs.io/dashboard).\n \n+## Creating an ML Engine\nComment: I'm creating a doc with templates for integrations doc pages - will share it soon.\r\n\r\nFor AI integrations, let's have the following sections:\r\n\r\n\\## Prerequisites\r\n...\r\n\\## Setup\r\n...\r\n\\### AI Engine\r\n...\r\n\\### AI Model\r\n...\r\n\\## Usage\r\n...\r\n\r\nMore less like here - https://docs.mindsdb.com/integrations/ai-engines/ollama",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/integrations/ai-engines/twelvelabs.mdx",
    "pr_number": 8748,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1481749544,
    "comment_created_at": "2024-02-07T16:22:21Z"
  },
  {
    "code": "@@ -44,16 +53,21 @@ Follow the instructions below to set up the Microsoft Teams app that will be use\n <Steps>\n   <Step title=\"Register an application in the Azure portal\">\n     - Navigate to Microsoft Entra ID in the Azure portal, click on *Add* and then on *App registration*.\n-    - Click on *New registration* and fill out the *Name* and select the `Accounts in any organizational directory (Any Azure AD directory - Multitenant)` option under *Supported account types*.\n-    - Select `Web` as the platform and enter URL where MindsDB has been deployed followed by /verify-auth under *Redirect URI*. For example, if you are running MindsDB locally (on https://localhost:47334), enter https://localhost:47334/verify-auth in the Redirect URIs field.\n+    - Click on *New registration* and fill out the *Name* and select the `Accounts in any organizational directory (Any Azure AD directory - Multitenant)` option under *Supported account types*. For the `application` permission mode, this step is unnecessary.",
    "comment": "Maybe we can add this sentence at the beginning of this point like this:\r\n```\r\n- *For the `application` permission mode, this step is unnecessary.* Click on *New registration* and fill out the *Name* and select the `Accounts in any organizational directory (Any Azure AD directory - Multitenant)` option under *Supported account types*.\r\n```",
    "line_number": 56,
    "enriched": "File: docs/integrations/app-integrations/microsoft-teams.mdx\nCode: @@ -44,16 +53,21 @@ Follow the instructions below to set up the Microsoft Teams app that will be use\n <Steps>\n   <Step title=\"Register an application in the Azure portal\">\n     - Navigate to Microsoft Entra ID in the Azure portal, click on *Add* and then on *App registration*.\n-    - Click on *New registration* and fill out the *Name* and select the `Accounts in any organizational directory (Any Azure AD directory - Multitenant)` option under *Supported account types*.\n-    - Select `Web` as the platform and enter URL where MindsDB has been deployed followed by /verify-auth under *Redirect URI*. For example, if you are running MindsDB locally (on https://localhost:47334), enter https://localhost:47334/verify-auth in the Redirect URIs field.\n+    - Click on *New registration* and fill out the *Name* and select the `Accounts in any organizational directory (Any Azure AD directory - Multitenant)` option under *Supported account types*. For the `application` permission mode, this step is unnecessary.\nComment: Maybe we can add this sentence at the beginning of this point like this:\r\n```\r\n- *For the `application` permission mode, this step is unnecessary.* Click on *New registration* and fill out the *Name* and select the `Accounts in any organizational directory (Any Azure AD directory - Multitenant)` option under *Supported account types*.\r\n```",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/integrations/app-integrations/microsoft-teams.mdx",
    "pr_number": 10618,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2016330535,
    "comment_created_at": "2025-03-27T11:38:22Z"
  },
  {
    "code": "@@ -195,6 +195,7 @@ def get_tables(self) -> Response:\n         for _scope in bucket.collections().get_all_scopes():\n                 for __collections in _scope.collections:\n                     collections.append(__collections.name)\n+        collections = list(set(collections))",
    "comment": "Maybe we can simplify this by using set in the beginning and then convert it to list, instead of list => set => list. e.g\r\n\r\n```\r\nunique_collections = set()\r\n\r\nfor scope in bucket.collections().get_all_scopes():\r\n    for collection in scope.collections:\r\n        unique_collections.add(collection.name)\r\n\r\ncollections = list(unique_collections)\r\n```",
    "line_number": 198,
    "enriched": "File: mindsdb/integrations/handlers/couchbase_handler/couchbase_handler.py\nCode: @@ -195,6 +195,7 @@ def get_tables(self) -> Response:\n         for _scope in bucket.collections().get_all_scopes():\n                 for __collections in _scope.collections:\n                     collections.append(__collections.name)\n+        collections = list(set(collections))\nComment: Maybe we can simplify this by using set in the beginning and then convert it to list, instead of list => set => list. e.g\r\n\r\n```\r\nunique_collections = set()\r\n\r\nfor scope in bucket.collections().get_all_scopes():\r\n    for collection in scope.collections:\r\n        unique_collections.add(collection.name)\r\n\r\ncollections = list(unique_collections)\r\n```",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/couchbase_handler/couchbase_handler.py",
    "pr_number": 7258,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1317210740,
    "comment_created_at": "2023-09-06T12:31:59Z"
  },
  {
    "code": "@@ -935,7 +935,16 @@ def add(\n             model_record = db.Predictor.query.get(model[\"id\"])\n             embedding_model_id = model_record.id\n \n-        reranking_model_params = get_model_params(params.get(\"reranking_model\", {}), \"default_reranking_model\")\n+        # if params.get(\"reranking_model\", {}) is bool and False we evaluate it to empty dictionary\n+        reranking_model_params = params.get(\"reranking_model\", {})\n+\n+        if isinstance(reranking_model_params, bool) and not reranking_model_params:\n+            params[\"reranking_model\"] = {}\n+        # if params.get(\"reranking_model\", {}) is string and false in any case we evaluate it to empty dictionary\n+        if isinstance(reranking_model_params, str) and reranking_model_params.lower() == \"false\":\n+            params[\"reranking_model\"] = {}\n+",
    "comment": "**security**: `params['reranking_model']` is set based on user input without type or content validation, allowing malicious or unexpected objects that could lead to code execution or privilege escalation if later deserialized or used unsafely.\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>üìù Committable Code Suggestion</strong></summary>\n\n> ‚ÄºÔ∏è Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n        reranking_model_params = params.get(\"reranking_model\", {})\n\n        if isinstance(reranking_model_params, bool) and not reranking_model_params:\n            params[\"reranking_model\"] = {}\n            reranking_model_params = {}\n        elif isinstance(reranking_model_params, str) and reranking_model_params.lower() == \"false\":\n            params[\"reranking_model\"] = {}\n            reranking_model_params = {}\n        elif not isinstance(reranking_model_params, (dict, bool, str)):\n            raise ValueError(\"reranking_model must be a dict, bool, or string.\")\n\n```\n</details>\n<!-- suggestion_end -->\n",
    "line_number": 946,
    "enriched": "File: mindsdb/interfaces/knowledge_base/controller.py\nCode: @@ -935,7 +935,16 @@ def add(\n             model_record = db.Predictor.query.get(model[\"id\"])\n             embedding_model_id = model_record.id\n \n-        reranking_model_params = get_model_params(params.get(\"reranking_model\", {}), \"default_reranking_model\")\n+        # if params.get(\"reranking_model\", {}) is bool and False we evaluate it to empty dictionary\n+        reranking_model_params = params.get(\"reranking_model\", {})\n+\n+        if isinstance(reranking_model_params, bool) and not reranking_model_params:\n+            params[\"reranking_model\"] = {}\n+        # if params.get(\"reranking_model\", {}) is string and false in any case we evaluate it to empty dictionary\n+        if isinstance(reranking_model_params, str) and reranking_model_params.lower() == \"false\":\n+            params[\"reranking_model\"] = {}\n+\nComment: **security**: `params['reranking_model']` is set based on user input without type or content validation, allowing malicious or unexpected objects that could lead to code execution or privilege escalation if later deserialized or used unsafely.\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>üìù Committable Code Suggestion</strong></summary>\n\n> ‚ÄºÔ∏è Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n        reranking_model_params = params.get(\"reranking_model\", {})\n\n        if isinstance(reranking_model_params, bool) and not reranking_model_params:\n            params[\"reranking_model\"] = {}\n            reranking_model_params = {}\n        elif isinstance(reranking_model_params, str) and reranking_model_params.lower() == \"false\":\n            params[\"reranking_model\"] = {}\n            reranking_model_params = {}\n        elif not isinstance(reranking_model_params, (dict, bool, str)):\n            raise ValueError(\"reranking_model must be a dict, bool, or string.\")\n\n```\n</details>\n<!-- suggestion_end -->\n",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/interfaces/knowledge_base/controller.py",
    "pr_number": 10941,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2114944127,
    "comment_created_at": "2025-05-30T00:34:31Z"
  },
  {
    "code": "@@ -0,0 +1,339 @@\n+import pandas as pd\n+from mindsdb.integrations.libs.api_handler import APIHandler\n+from mindsdb.integrations.libs.response import (\n+    HandlerStatusResponse as StatusResponse,\n+    HandlerResponse as Response,\n+    RESPONSE_TYPE\n+)\n+from mindsdb.utilities import log\n+import duckdb\n+from duckdb import DuckDBPyConnection\n+from azure.identity import DefaultAzureCredential\n+from azure.storage.blob import BlobServiceClient, generate_account_sas, ResourceTypes, AccountSasPermissions\n+from datetime import datetime, timedelta\n+from contextlib import contextmanager\n+from typing import List,Text, Dict\n+from mindsdb.integrations.libs.api_handler import APIResource, APIHandler\n+from mindsdb.integrations.utilities.sql_utils import FilterCondition\n+from mindsdb_sql.parser.ast.base import ASTNode\n+from mindsdb_sql.parser.ast import Select, Identifier, Insert, Star, Constant\n+\n+\n+logger = log.getLogger(__name__)\n+\n+class ListFilesTable(APIResource):\n+\n+    def list(self,\n+             targets: List[str] = None,\n+             conditions: List[FilterCondition] = None,\n+             *args, **kwargs) -> pd.DataFrame:\n+\n+        tables = self.handler.get_files()\n+        data = []\n+        for path in tables:\n+            path = path.replace('`', '')\n+            item = {\n+                'path': path,\n+                'name': path[path.rfind('/') + 1:],\n+                'extension': path[path.rfind('.') + 1:]\n+            }\n+\n+            data.append(item)\n+\n+        return pd.DataFrame(data=data, columns=self.get_columns())\n+\n+    def get_columns(self) -> List[str]:\n+        return [\"path\", \"name\", \"extension\", \"content\"]\n+\n+\n+class FileTable(APIResource):\n+\n+    def list(self, targets: List[str] = None, table_name=None, *args, **kwargs) -> pd.DataFrame:\n+        return self.handler.read_as_table(table_name)\n+\n+    def add(self, data, table_name=None):\n+        df = pd.DataFrame(data)\n+        return self.handler._add_data_to_table(table_name, df)\n+\n+\n+\n+class AzureBlobHandler(APIHandler):\n+    \"\"\"\n+    This handler handles connection and execution of the SQL statements on Azure Blob.\n+    \"\"\"\n+\n+    name = 'azureblob'\n+    supported_file_formats = ['csv', 'tsv', 'json', 'parquet']\n+\n+    def __init__(self, name: str, **kwargs):\n+        super().__init__(name)\n+        \"\"\" constructor\n+        Args:\n+            name (str): the handler name\n+        \"\"\"\n+\n+        self.connection = None\n+        self.is_connected = False\n+\n+        self._tables = {}\n+        self.storage_account_name = None\n+        self.account_access_key = None\n+        \n+        self._files_table = ListFilesTable(self)\n+        self.container_name = None\n+\n+        connection_data = kwargs.get('connection_data')\n+        if 'storage_account_name' in connection_data:\n+            self.storage_account_name = connection_data['storage_account_name']\n+\n+        if 'account_access_key' in connection_data:\n+            self.account_access_key = connection_data['account_access_key']\n+\n+        if 'container_name' in connection_data:\n+            self.container_name = connection_data['container_name']\n+\n+        if 'connection_string' in connection_data:\n+            self.connection_string = connection_data['connection_string']\n+        \n+\n+    def connect(self) -> BlobServiceClient:\n+        \"\"\" Set up any connections required by the handler\n+        Should return output of check_connection() method after attempting\n+        connection. Should switch self.is_connected.\n+        Returns:\n+            HandlerStatusResponse\n+        \"\"\"\n+        if self.is_connected is True:\n+            return self.connection\n+\n+        sas_token = generate_account_sas(\n+            account_name=self.storage_account_name,\n+            account_key=self.account_access_key,\n+            resource_types=ResourceTypes(service=True,container=True,object=True),\n+            permission=AccountSasPermissions(read=True,list=True),\n+            expiry=datetime.now() + timedelta(hours=1)\n+        )\n+\n+        blob_service_client = BlobServiceClient(\n+            account_url=f\"https://{self.storage_account_name}.blob.core.windows.net\",\n+            credential=sas_token\n+        )\n+        self.connection = blob_service_client\n+        self.is_connected = True\n+        return blob_service_client\n+\n+    def check_connection(self) -> StatusResponse:\n+        \"\"\" Check connection to the handler\n+        Returns:\n+            HandlerStatusResponse\n+        \"\"\"\n+        response = StatusResponse(False)\n+\n+        try:\n+            client = self.connect()\n+            client.get_account_information()\n+            response.success = True\n+\n+        except Exception as e:\n+            logger.error(f'Error connecting to Azure Blob: {e}!')\n+            response.error_message = e\n+\n+        self.is_connected = response.success\n+        return response\n+\n+    def native_query(self, query: str = None) -> Response:\n+        \"\"\"Receive raw query and act upon it somehow.\n+        Args:\n+            query (Any): query in native format (str for sql databases,\n+                dict for mongo, api's json etc)\n+        Returns:\n+            HandlerResponse\n+        \"\"\"\n+\n+    def call_application_api(self, method_name:str = None, params:dict = None) -> pd.DataFrame:",
    "comment": "This function is not implemented and does not seem to be in use anywhere. Let's remove it.",
    "line_number": 153,
    "enriched": "File: mindsdb/integrations/handlers/azure_blob_handler/azure_blob_handler.py\nCode: @@ -0,0 +1,339 @@\n+import pandas as pd\n+from mindsdb.integrations.libs.api_handler import APIHandler\n+from mindsdb.integrations.libs.response import (\n+    HandlerStatusResponse as StatusResponse,\n+    HandlerResponse as Response,\n+    RESPONSE_TYPE\n+)\n+from mindsdb.utilities import log\n+import duckdb\n+from duckdb import DuckDBPyConnection\n+from azure.identity import DefaultAzureCredential\n+from azure.storage.blob import BlobServiceClient, generate_account_sas, ResourceTypes, AccountSasPermissions\n+from datetime import datetime, timedelta\n+from contextlib import contextmanager\n+from typing import List,Text, Dict\n+from mindsdb.integrations.libs.api_handler import APIResource, APIHandler\n+from mindsdb.integrations.utilities.sql_utils import FilterCondition\n+from mindsdb_sql.parser.ast.base import ASTNode\n+from mindsdb_sql.parser.ast import Select, Identifier, Insert, Star, Constant\n+\n+\n+logger = log.getLogger(__name__)\n+\n+class ListFilesTable(APIResource):\n+\n+    def list(self,\n+             targets: List[str] = None,\n+             conditions: List[FilterCondition] = None,\n+             *args, **kwargs) -> pd.DataFrame:\n+\n+        tables = self.handler.get_files()\n+        data = []\n+        for path in tables:\n+            path = path.replace('`', '')\n+            item = {\n+                'path': path,\n+                'name': path[path.rfind('/') + 1:],\n+                'extension': path[path.rfind('.') + 1:]\n+            }\n+\n+            data.append(item)\n+\n+        return pd.DataFrame(data=data, columns=self.get_columns())\n+\n+    def get_columns(self) -> List[str]:\n+        return [\"path\", \"name\", \"extension\", \"content\"]\n+\n+\n+class FileTable(APIResource):\n+\n+    def list(self, targets: List[str] = None, table_name=None, *args, **kwargs) -> pd.DataFrame:\n+        return self.handler.read_as_table(table_name)\n+\n+    def add(self, data, table_name=None):\n+        df = pd.DataFrame(data)\n+        return self.handler._add_data_to_table(table_name, df)\n+\n+\n+\n+class AzureBlobHandler(APIHandler):\n+    \"\"\"\n+    This handler handles connection and execution of the SQL statements on Azure Blob.\n+    \"\"\"\n+\n+    name = 'azureblob'\n+    supported_file_formats = ['csv', 'tsv', 'json', 'parquet']\n+\n+    def __init__(self, name: str, **kwargs):\n+        super().__init__(name)\n+        \"\"\" constructor\n+        Args:\n+            name (str): the handler name\n+        \"\"\"\n+\n+        self.connection = None\n+        self.is_connected = False\n+\n+        self._tables = {}\n+        self.storage_account_name = None\n+        self.account_access_key = None\n+        \n+        self._files_table = ListFilesTable(self)\n+        self.container_name = None\n+\n+        connection_data = kwargs.get('connection_data')\n+        if 'storage_account_name' in connection_data:\n+            self.storage_account_name = connection_data['storage_account_name']\n+\n+        if 'account_access_key' in connection_data:\n+            self.account_access_key = connection_data['account_access_key']\n+\n+        if 'container_name' in connection_data:\n+            self.container_name = connection_data['container_name']\n+\n+        if 'connection_string' in connection_data:\n+            self.connection_string = connection_data['connection_string']\n+        \n+\n+    def connect(self) -> BlobServiceClient:\n+        \"\"\" Set up any connections required by the handler\n+        Should return output of check_connection() method after attempting\n+        connection. Should switch self.is_connected.\n+        Returns:\n+            HandlerStatusResponse\n+        \"\"\"\n+        if self.is_connected is True:\n+            return self.connection\n+\n+        sas_token = generate_account_sas(\n+            account_name=self.storage_account_name,\n+            account_key=self.account_access_key,\n+            resource_types=ResourceTypes(service=True,container=True,object=True),\n+            permission=AccountSasPermissions(read=True,list=True),\n+            expiry=datetime.now() + timedelta(hours=1)\n+        )\n+\n+        blob_service_client = BlobServiceClient(\n+            account_url=f\"https://{self.storage_account_name}.blob.core.windows.net\",\n+            credential=sas_token\n+        )\n+        self.connection = blob_service_client\n+        self.is_connected = True\n+        return blob_service_client\n+\n+    def check_connection(self) -> StatusResponse:\n+        \"\"\" Check connection to the handler\n+        Returns:\n+            HandlerStatusResponse\n+        \"\"\"\n+        response = StatusResponse(False)\n+\n+        try:\n+            client = self.connect()\n+            client.get_account_information()\n+            response.success = True\n+\n+        except Exception as e:\n+            logger.error(f'Error connecting to Azure Blob: {e}!')\n+            response.error_message = e\n+\n+        self.is_connected = response.success\n+        return response\n+\n+    def native_query(self, query: str = None) -> Response:\n+        \"\"\"Receive raw query and act upon it somehow.\n+        Args:\n+            query (Any): query in native format (str for sql databases,\n+                dict for mongo, api's json etc)\n+        Returns:\n+            HandlerResponse\n+        \"\"\"\n+\n+    def call_application_api(self, method_name:str = None, params:dict = None) -> pd.DataFrame:\nComment: This function is not implemented and does not seem to be in use anywhere. Let's remove it.",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/azure_blob_handler/azure_blob_handler.py",
    "pr_number": 9975,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1818409486,
    "comment_created_at": "2024-10-28T05:50:36Z"
  },
  {
    "code": "@@ -51,9 +51,9 @@ First, you should prepare a `config.json` file based on the following template;\n         \"autoupdate\": true\n     },\n     \"auth\":{\n-        \"http_auth_enabled\": False,\n-        \"username\": \"mindsdb\",\n-        \"password\": \"123\"\n+        \"http_auth_enabled\": True,",
    "comment": "This is False by default so we can keep it as False",
    "line_number": 54,
    "enriched": "File: docs/setup/custom-config.mdx\nCode: @@ -51,9 +51,9 @@ First, you should prepare a `config.json` file based on the following template;\n         \"autoupdate\": true\n     },\n     \"auth\":{\n-        \"http_auth_enabled\": False,\n-        \"username\": \"mindsdb\",\n-        \"password\": \"123\"\n+        \"http_auth_enabled\": True,\nComment: This is False by default so we can keep it as False",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "docs/setup/custom-config.mdx",
    "pr_number": 9809,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1793140573,
    "comment_created_at": "2024-10-09T08:57:32Z"
  },
  {
    "code": "@@ -240,9 +240,10 @@ def get_columns(self, table_name) -> Response:\n         'label': 'Keyspace'\n     },\n     secure_connect_bundle={\n-        'type': ARG_TYPE.STR,\n-        'description': 'Path or URL to the secure connect bundle',\n-        'required': True,\n-        'label': 'Host'\n+        'type': ARG_TYPE.PATH,",
    "comment": "We should keep this as string since can accept URLs or S3 files too",
    "line_number": 243,
    "enriched": "File: mindsdb/integrations/handlers/scylla_handler/scylla_handler.py\nCode: @@ -240,9 +240,10 @@ def get_columns(self, table_name) -> Response:\n         'label': 'Keyspace'\n     },\n     secure_connect_bundle={\n-        'type': ARG_TYPE.STR,\n-        'description': 'Path or URL to the secure connect bundle',\n-        'required': True,\n-        'label': 'Host'\n+        'type': ARG_TYPE.PATH,\nComment: We should keep this as string since can accept URLs or S3 files too",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/scylla_handler/scylla_handler.py",
    "pr_number": 7348,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1327583090,
    "comment_created_at": "2023-09-15T17:14:28Z"
  },
  {
    "code": "@@ -0,0 +1,244 @@\n+from typing import List\n+\n+import pandas as pd\n+from mindsdb_sql.parser import ast\n+\n+from mindsdb.integrations.handlers.utilities.query_utilities import (\n+    SELECTQueryExecutor,\n+    SELECTQueryParser,\n+)\n+from mindsdb.integrations.libs.api_handler import APIHandler, APITable\n+from mindsdb.integrations.utilities.sql_utils import conditions_to_filter\n+\n+\n+class CustomAPITable(APITable):\n+\n+    def __init__(self, handler: APIHandler):\n+        super().__init__(handler)\n+        self.handler.connect()\n+\n+    def get_columns(self, ignore: List[str] = []) -> List[str]:\n+        return [item for item in self.columns if item not in ignore]\n+\n+    def select(self, query: ast.Select) -> pd.DataFrame:\n+        raise NotImplementedError()\n+\n+    def parse_select(self, query: ast.Select, table_name: str):\n+        select_statement_parser = SELECTQueryParser(query, table_name, self.get_columns())\n+        self.selected_columns, self.where_conditions, self.order_by_conditions, self.result_limit = select_statement_parser.parse_query()\n+\n+    def get_package_name(self, query: ast.Select):\n+        params = conditions_to_filter(query.where)\n+        if \"package\" not in params:\n+            raise Exception(\"Where condition does not have 'package' selector\")\n+        return params[\"package\"]\n+\n+    def apply_query_params(self, df, query):\n+        select_statement_parser = SELECTQueryParser(query, self.name, self.get_columns())\n+        selected_columns, _, order_by_conditions, result_limit = select_statement_parser.parse_query()\n+        select_statement_executor = SELECTQueryExecutor(df, selected_columns, [], order_by_conditions)\n+        return select_statement_executor.execute_query()\n+\n+\n+class NPMMetadataTable(CustomAPITable):\n+    name: str = \"metadata\"\n+    columns: List[str] = [\n+        \"name\",\n+        \"scope\",\n+        \"version\",\n+        \"description\",\n+        \"author_name\",\n+        \"author_email\",\n+        \"publisher_username\",\n+        \"publisher_email\",\n+        \"repository_url\",\n+        \"license\",\n+        \"num_releases\",\n+        \"num_downloads\",\n+        \"num_stars\",\n+        \"score\",\n+    ]\n+\n+    def __init__(self, handler: APIHandler):\n+        super().__init__(handler)\n+        self.handler.connect()\n+\n+    def select(self, query: ast.Select) -> pd.DataFrame:\n+        package_name = self.get_package_name(query)\n+        connection = self.handler.connection(package_name)\n+        metadata = connection.get_cols_in(\n+            [\"collected\", \"metadata\"],\n+            [\"name\", \"scope\", \"version\", \"description\", \"author\", \"publisher\", \"repository\", \"license\", \"releases\"]\n+        )\n+        metadata[\"author_name\"] = metadata[\"author\"].get(\"name\")\n+        metadata[\"author_email\"] = metadata[\"author\"].get(\"email\")\n+        del metadata[\"author\"]\n+        metadata[\"publisher_username\"] = metadata[\"publisher\"].get(\"username\")\n+        metadata[\"publisher_email\"] = metadata[\"publisher\"].get(\"email\")\n+        del metadata[\"publisher\"]\n+        metadata[\"repository\"] = metadata[\"repository\"].get(\"url\")\n+        metadata[\"num_releases\"] = sum([x[\"count\"] for x in metadata[\"releases\"]])\n+        del metadata[\"releases\"]\n+        npm_data = connection.get_cols_in(\n+            [\"collected\", \"npm\"],\n+            [\"downloads\", \"starsCount\"]\n+        )\n+        npm_data[\"num_downloads\"] = sum([x[\"count\"] for x in npm_data[\"downloads\"]])\n+        del npm_data[\"downloads\"]\n+        npm_data[\"num_stars\"] = npm_data[\"starsCount\"]\n+        del npm_data[\"starsCount\"]\n+        score = connection.get_cols_in(\n+            [\"score\"],\n+            [\"final\"]\n+        )\n+        df = pd.DataFrame.from_records([{**metadata, **npm_data, \"score\": score[\"final\"]}])\n+        return self.apply_query_params(df, query)\n+\n+\n+class NPMMaintainersTable(CustomAPITable):\n+    name: str = \"maintainers\"\n+    columns: List[str] = [\n+        \"username\",\n+        \"email\"\n+    ]\n+\n+    def __init__(self, handler: APIHandler):\n+        super().__init__(handler)\n+        self.handler.connect()\n+\n+    def select(self, query: ast.Select) -> pd.DataFrame:\n+        package_name = self.get_package_name(query)\n+        connection = self.handler.connection(package_name)\n+        metadata = connection.get_cols_in(\n+            [\"collected\", \"metadata\"],\n+            [\"maintainers\"]\n+        )\n+        records = [{col: x[col] for col in self.columns} for x in metadata.get(\"maintainers\", [])]\n+        df = pd.DataFrame.from_records(records)\n+        return self.apply_query_params(df, query)\n+\n+\n+class NPMKeywordsTable(CustomAPITable):\n+    name: str = \"keywords\"\n+    columns: List[str] = [\n+        \"keyword\"\n+    ]\n+\n+    def __init__(self, handler: APIHandler):\n+        super().__init__(handler)\n+        self.handler.connect()\n+\n+    def select(self, query: ast.Select) -> pd.DataFrame:\n+        package_name = self.get_package_name(query)\n+        connection = self.handler.connection(package_name)\n+        metadata = connection.get_cols_in(\n+            [\"collected\", \"metadata\"],\n+            [\"keywords\"]\n+        )\n+        records = [{\"keyword\": keyword} for keyword in metadata[\"keywords\"]]\n+        df = pd.DataFrame.from_records(records)\n+        return self.apply_query_params(df, query)\n+\n+\n+class NPMDependenciesTable(CustomAPITable):\n+    name: str = \"dependencies\"\n+    columns: List[str] = [\n+        \"dependency\",\n+        \"version\"\n+    ]\n+\n+    def __init__(self, handler: APIHandler):\n+        super().__init__(handler)\n+        self.handler.connect()\n+\n+    def select(self, query: ast.Select) -> pd.DataFrame:\n+        package_name = self.get_package_name(query)\n+        connection = self.handler.connection(package_name)\n+        metadata = connection.get_cols_in(\n+            [\"collected\", \"metadata\"],\n+            [\"dependencies\"]\n+        )\n+        records = [{\"dependency\": d, \"version\": v} for d, v in metadata[\"dependencies\"].items()]\n+        df = pd.DataFrame.from_records(records)\n+        return self.apply_query_params(df, query)\n+\n+\n+class NPMDevDependenciesTable(CustomAPITable):\n+    name: str = \"dev_dependencies\"\n+    columns: List[str] = [\n+        \"dev_dependency\",\n+        \"version\"\n+    ]\n+\n+    def __init__(self, handler: APIHandler):\n+        super().__init__(handler)\n+        self.handler.connect()\n+\n+    def select(self, query: ast.Select) -> pd.DataFrame:\n+        package_name = self.get_package_name(query)\n+        connection = self.handler.connection(package_name)\n+        metadata = connection.get_cols_in(\n+            [\"collected\", \"metadata\"],\n+            [\"devDependencies\"]\n+        )\n+        records = [{\"dev_dependency\": d, \"version\": v} for d, v in metadata[\"devDependencies\"].items()]\n+        df = pd.DataFrame.from_records(records)\n+        return self.apply_query_params(df, query)\n+\n+\n+class NPMOptionalDependenciesTable(CustomAPITable):\n+    name: str = \"optional_dependencies\"\n+    columns: List[str] = [\n+        \"optional_dependency\",\n+        \"version\"\n+    ]\n+\n+    def __init__(self, handler: APIHandler):\n+        super().__init__(handler)\n+        self.handler.connect()\n+\n+    def select(self, query: ast.Select) -> pd.DataFrame:\n+        package_name = self.get_package_name(query)\n+        connection = self.handler.connection(package_name)\n+        metadata = connection.get_cols_in(\n+            [\"collected\", \"metadata\"],\n+            [\"optionalDependencies\"]\n+        )\n+        records = [{\"optional_dependency\": d, \"version\": v} for d, v in metadata[\"optionalDependencies\"].items()]\n+        df = pd.DataFrame.from_records(records)\n+        return self.apply_query_params(df, query)\n+\n+\n+class NPMGithubStatsTable(CustomAPITable):\n+    name: str = \"github_stats\"\n+    columns: List[str] = [\n+        \"homepage\",\n+        \"num_stars\",\n+        \"num_forks\",\n+        \"num_subscribers\",\n+        \"num_issues\",\n+        \"num_open_issues\",\n+    ]\n+\n+    def __init__(self, handler: APIHandler):\n+        super().__init__(handler)\n+        self.handler.connect()\n+\n+    def select(self, query: ast.Select) -> pd.DataFrame:\n+        package_name = self.get_package_name(query)\n+        connection = self.handler.connection(package_name)\n+        github_data = connection.get_cols_in(\n+            [\"collected\", \"github\"],\n+            [\"homepage\", \"starsCount\", \"forksCount\", \"subscribersCount\", \"issues\"]\n+        )\n+        github_data[\"num_stars\"] = github_data[\"starsCount\"]\n+        del github_data[\"starsCount\"]",
    "comment": "Maybe we can use the rename method instead of assigning new val and deleting ?",
    "line_number": 235,
    "enriched": "File: mindsdb/integrations/handlers/npm_handler/npm_tables.py\nCode: @@ -0,0 +1,244 @@\n+from typing import List\n+\n+import pandas as pd\n+from mindsdb_sql.parser import ast\n+\n+from mindsdb.integrations.handlers.utilities.query_utilities import (\n+    SELECTQueryExecutor,\n+    SELECTQueryParser,\n+)\n+from mindsdb.integrations.libs.api_handler import APIHandler, APITable\n+from mindsdb.integrations.utilities.sql_utils import conditions_to_filter\n+\n+\n+class CustomAPITable(APITable):\n+\n+    def __init__(self, handler: APIHandler):\n+        super().__init__(handler)\n+        self.handler.connect()\n+\n+    def get_columns(self, ignore: List[str] = []) -> List[str]:\n+        return [item for item in self.columns if item not in ignore]\n+\n+    def select(self, query: ast.Select) -> pd.DataFrame:\n+        raise NotImplementedError()\n+\n+    def parse_select(self, query: ast.Select, table_name: str):\n+        select_statement_parser = SELECTQueryParser(query, table_name, self.get_columns())\n+        self.selected_columns, self.where_conditions, self.order_by_conditions, self.result_limit = select_statement_parser.parse_query()\n+\n+    def get_package_name(self, query: ast.Select):\n+        params = conditions_to_filter(query.where)\n+        if \"package\" not in params:\n+            raise Exception(\"Where condition does not have 'package' selector\")\n+        return params[\"package\"]\n+\n+    def apply_query_params(self, df, query):\n+        select_statement_parser = SELECTQueryParser(query, self.name, self.get_columns())\n+        selected_columns, _, order_by_conditions, result_limit = select_statement_parser.parse_query()\n+        select_statement_executor = SELECTQueryExecutor(df, selected_columns, [], order_by_conditions)\n+        return select_statement_executor.execute_query()\n+\n+\n+class NPMMetadataTable(CustomAPITable):\n+    name: str = \"metadata\"\n+    columns: List[str] = [\n+        \"name\",\n+        \"scope\",\n+        \"version\",\n+        \"description\",\n+        \"author_name\",\n+        \"author_email\",\n+        \"publisher_username\",\n+        \"publisher_email\",\n+        \"repository_url\",\n+        \"license\",\n+        \"num_releases\",\n+        \"num_downloads\",\n+        \"num_stars\",\n+        \"score\",\n+    ]\n+\n+    def __init__(self, handler: APIHandler):\n+        super().__init__(handler)\n+        self.handler.connect()\n+\n+    def select(self, query: ast.Select) -> pd.DataFrame:\n+        package_name = self.get_package_name(query)\n+        connection = self.handler.connection(package_name)\n+        metadata = connection.get_cols_in(\n+            [\"collected\", \"metadata\"],\n+            [\"name\", \"scope\", \"version\", \"description\", \"author\", \"publisher\", \"repository\", \"license\", \"releases\"]\n+        )\n+        metadata[\"author_name\"] = metadata[\"author\"].get(\"name\")\n+        metadata[\"author_email\"] = metadata[\"author\"].get(\"email\")\n+        del metadata[\"author\"]\n+        metadata[\"publisher_username\"] = metadata[\"publisher\"].get(\"username\")\n+        metadata[\"publisher_email\"] = metadata[\"publisher\"].get(\"email\")\n+        del metadata[\"publisher\"]\n+        metadata[\"repository\"] = metadata[\"repository\"].get(\"url\")\n+        metadata[\"num_releases\"] = sum([x[\"count\"] for x in metadata[\"releases\"]])\n+        del metadata[\"releases\"]\n+        npm_data = connection.get_cols_in(\n+            [\"collected\", \"npm\"],\n+            [\"downloads\", \"starsCount\"]\n+        )\n+        npm_data[\"num_downloads\"] = sum([x[\"count\"] for x in npm_data[\"downloads\"]])\n+        del npm_data[\"downloads\"]\n+        npm_data[\"num_stars\"] = npm_data[\"starsCount\"]\n+        del npm_data[\"starsCount\"]\n+        score = connection.get_cols_in(\n+            [\"score\"],\n+            [\"final\"]\n+        )\n+        df = pd.DataFrame.from_records([{**metadata, **npm_data, \"score\": score[\"final\"]}])\n+        return self.apply_query_params(df, query)\n+\n+\n+class NPMMaintainersTable(CustomAPITable):\n+    name: str = \"maintainers\"\n+    columns: List[str] = [\n+        \"username\",\n+        \"email\"\n+    ]\n+\n+    def __init__(self, handler: APIHandler):\n+        super().__init__(handler)\n+        self.handler.connect()\n+\n+    def select(self, query: ast.Select) -> pd.DataFrame:\n+        package_name = self.get_package_name(query)\n+        connection = self.handler.connection(package_name)\n+        metadata = connection.get_cols_in(\n+            [\"collected\", \"metadata\"],\n+            [\"maintainers\"]\n+        )\n+        records = [{col: x[col] for col in self.columns} for x in metadata.get(\"maintainers\", [])]\n+        df = pd.DataFrame.from_records(records)\n+        return self.apply_query_params(df, query)\n+\n+\n+class NPMKeywordsTable(CustomAPITable):\n+    name: str = \"keywords\"\n+    columns: List[str] = [\n+        \"keyword\"\n+    ]\n+\n+    def __init__(self, handler: APIHandler):\n+        super().__init__(handler)\n+        self.handler.connect()\n+\n+    def select(self, query: ast.Select) -> pd.DataFrame:\n+        package_name = self.get_package_name(query)\n+        connection = self.handler.connection(package_name)\n+        metadata = connection.get_cols_in(\n+            [\"collected\", \"metadata\"],\n+            [\"keywords\"]\n+        )\n+        records = [{\"keyword\": keyword} for keyword in metadata[\"keywords\"]]\n+        df = pd.DataFrame.from_records(records)\n+        return self.apply_query_params(df, query)\n+\n+\n+class NPMDependenciesTable(CustomAPITable):\n+    name: str = \"dependencies\"\n+    columns: List[str] = [\n+        \"dependency\",\n+        \"version\"\n+    ]\n+\n+    def __init__(self, handler: APIHandler):\n+        super().__init__(handler)\n+        self.handler.connect()\n+\n+    def select(self, query: ast.Select) -> pd.DataFrame:\n+        package_name = self.get_package_name(query)\n+        connection = self.handler.connection(package_name)\n+        metadata = connection.get_cols_in(\n+            [\"collected\", \"metadata\"],\n+            [\"dependencies\"]\n+        )\n+        records = [{\"dependency\": d, \"version\": v} for d, v in metadata[\"dependencies\"].items()]\n+        df = pd.DataFrame.from_records(records)\n+        return self.apply_query_params(df, query)\n+\n+\n+class NPMDevDependenciesTable(CustomAPITable):\n+    name: str = \"dev_dependencies\"\n+    columns: List[str] = [\n+        \"dev_dependency\",\n+        \"version\"\n+    ]\n+\n+    def __init__(self, handler: APIHandler):\n+        super().__init__(handler)\n+        self.handler.connect()\n+\n+    def select(self, query: ast.Select) -> pd.DataFrame:\n+        package_name = self.get_package_name(query)\n+        connection = self.handler.connection(package_name)\n+        metadata = connection.get_cols_in(\n+            [\"collected\", \"metadata\"],\n+            [\"devDependencies\"]\n+        )\n+        records = [{\"dev_dependency\": d, \"version\": v} for d, v in metadata[\"devDependencies\"].items()]\n+        df = pd.DataFrame.from_records(records)\n+        return self.apply_query_params(df, query)\n+\n+\n+class NPMOptionalDependenciesTable(CustomAPITable):\n+    name: str = \"optional_dependencies\"\n+    columns: List[str] = [\n+        \"optional_dependency\",\n+        \"version\"\n+    ]\n+\n+    def __init__(self, handler: APIHandler):\n+        super().__init__(handler)\n+        self.handler.connect()\n+\n+    def select(self, query: ast.Select) -> pd.DataFrame:\n+        package_name = self.get_package_name(query)\n+        connection = self.handler.connection(package_name)\n+        metadata = connection.get_cols_in(\n+            [\"collected\", \"metadata\"],\n+            [\"optionalDependencies\"]\n+        )\n+        records = [{\"optional_dependency\": d, \"version\": v} for d, v in metadata[\"optionalDependencies\"].items()]\n+        df = pd.DataFrame.from_records(records)\n+        return self.apply_query_params(df, query)\n+\n+\n+class NPMGithubStatsTable(CustomAPITable):\n+    name: str = \"github_stats\"\n+    columns: List[str] = [\n+        \"homepage\",\n+        \"num_stars\",\n+        \"num_forks\",\n+        \"num_subscribers\",\n+        \"num_issues\",\n+        \"num_open_issues\",\n+    ]\n+\n+    def __init__(self, handler: APIHandler):\n+        super().__init__(handler)\n+        self.handler.connect()\n+\n+    def select(self, query: ast.Select) -> pd.DataFrame:\n+        package_name = self.get_package_name(query)\n+        connection = self.handler.connection(package_name)\n+        github_data = connection.get_cols_in(\n+            [\"collected\", \"github\"],\n+            [\"homepage\", \"starsCount\", \"forksCount\", \"subscribersCount\", \"issues\"]\n+        )\n+        github_data[\"num_stars\"] = github_data[\"starsCount\"]\n+        del github_data[\"starsCount\"]\nComment: Maybe we can use the rename method instead of assigning new val and deleting ?",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/npm_handler/npm_tables.py",
    "pr_number": 7858,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1363578616,
    "comment_created_at": "2023-10-18T09:45:55Z"
  },
  {
    "code": "@@ -61,11 +61,9 @@ def learn_process(data_integration_ref: dict, problem_definition: dict, fetch_da\n                         )\n                     )\n                     sqlquery = SQLQuery(query, session=sql_session)\n-                elif data_integration_ref['type'] == 'view':",
    "comment": "Maybe keep this for back compatibility?",
    "line_number": 64,
    "enriched": "File: mindsdb/integrations/libs/ml_handler_process/learn_process.py\nCode: @@ -61,11 +61,9 @@ def learn_process(data_integration_ref: dict, problem_definition: dict, fetch_da\n                         )\n                     )\n                     sqlquery = SQLQuery(query, session=sql_session)\n-                elif data_integration_ref['type'] == 'view':\nComment: Maybe keep this for back compatibility?",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/libs/ml_handler_process/learn_process.py",
    "pr_number": 9135,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1586333700,
    "comment_created_at": "2024-05-01T14:04:09Z"
  },
  {
    "code": "@@ -164,18 +164,17 @@ def __init__(self, session, context=None):\n         self.datahub = session.datahub\n \n     @profiler.profile()\n-    def execute_command(self, statement, database_name: str = None) -> ExecuteAnswer:\n-        sql = None\n-        if isinstance(statement, ASTNode):\n-            sql = statement.to_string()\n-        sql_lower = sql.lower()\n+    def execute_command(self, statement: ASTNode, database_name: str = None) -> ExecuteAnswer:\n+        sql: str = statement.to_string()\n+        sql_lower: str = sql.lower()\n \n         if database_name is None:\n             database_name = self.session.database\n \n-        if type(statement) is CreateDatabase:\n+        statement_type = type(statement)\n+        if statement_type is CreateDatabase:",
    "comment": "wouldn't be better to use isinstance(statement, CreateDatabase) ?",
    "line_number": 175,
    "enriched": "File: mindsdb/api/executor/command_executor.py\nCode: @@ -164,18 +164,17 @@ def __init__(self, session, context=None):\n         self.datahub = session.datahub\n \n     @profiler.profile()\n-    def execute_command(self, statement, database_name: str = None) -> ExecuteAnswer:\n-        sql = None\n-        if isinstance(statement, ASTNode):\n-            sql = statement.to_string()\n-        sql_lower = sql.lower()\n+    def execute_command(self, statement: ASTNode, database_name: str = None) -> ExecuteAnswer:\n+        sql: str = statement.to_string()\n+        sql_lower: str = sql.lower()\n \n         if database_name is None:\n             database_name = self.session.database\n \n-        if type(statement) is CreateDatabase:\n+        statement_type = type(statement)\n+        if statement_type is CreateDatabase:\nComment: wouldn't be better to use isinstance(statement, CreateDatabase) ?",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/api/executor/command_executor.py",
    "pr_number": 10632,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2026760569,
    "comment_created_at": "2025-04-03T11:03:06Z"
  },
  {
    "code": "@@ -0,0 +1,30 @@\n+import logging\n+\n+import requests\n+\n+from mindsdb.integrations.handlers.pypi_handler.pypi_tables import PyPIRecentTable\n+from mindsdb.integrations.libs.api_handler import APIHandler\n+from mindsdb.integrations.libs.response import HandlerStatusResponse as StatusResponse\n+from mindsdb.utilities.log import get_log\n+\n+logger = get_log(\"integrations.pypi_handler\")\n+\n+\n+class PyPIHandler(APIHandler):\n+    def __init__(self, name: str, **kwargs) -> None:\n+        super().__init__(name)\n+\n+        connection_data = kwargs.get(\"connection_data\", {})\n+        self.package = connection_data.get(\"package\", None)\n+\n+        pypi_recent_data = PyPIRecentTable(self)\n+        self._register_table(\"recent\", pypi_recent_data)\n+\n+    def check_connection(self):\n+        return StatusResponse(True)\n+\n+    def connect(self):",
    "comment": "Hey @lnxpy,\r\nI thought I would reply here since you asked. Since you don't seem to need to instantiate an object or anything here, I believe you can just return the module itself in your `connect()` method.\r\n```\r\ndef connect(self):\r\n        return pypistats\r\n```\r\n\r\nYou can now use this within your `Table` classes similar to the following example:\r\n```\r\nconnection = self.handler.connect()\r\ndf = connection.recent('<some-package>', total=True, format=\"pandas\")\r\n```\r\n",
    "line_number": 26,
    "enriched": "File: mindsdb/integrations/handlers/pypi_handler/pypi_handler.py\nCode: @@ -0,0 +1,30 @@\n+import logging\n+\n+import requests\n+\n+from mindsdb.integrations.handlers.pypi_handler.pypi_tables import PyPIRecentTable\n+from mindsdb.integrations.libs.api_handler import APIHandler\n+from mindsdb.integrations.libs.response import HandlerStatusResponse as StatusResponse\n+from mindsdb.utilities.log import get_log\n+\n+logger = get_log(\"integrations.pypi_handler\")\n+\n+\n+class PyPIHandler(APIHandler):\n+    def __init__(self, name: str, **kwargs) -> None:\n+        super().__init__(name)\n+\n+        connection_data = kwargs.get(\"connection_data\", {})\n+        self.package = connection_data.get(\"package\", None)\n+\n+        pypi_recent_data = PyPIRecentTable(self)\n+        self._register_table(\"recent\", pypi_recent_data)\n+\n+    def check_connection(self):\n+        return StatusResponse(True)\n+\n+    def connect(self):\nComment: Hey @lnxpy,\r\nI thought I would reply here since you asked. Since you don't seem to need to instantiate an object or anything here, I believe you can just return the module itself in your `connect()` method.\r\n```\r\ndef connect(self):\r\n        return pypistats\r\n```\r\n\r\nYou can now use this within your `Table` classes similar to the following example:\r\n```\r\nconnection = self.handler.connect()\r\ndf = connection.recent('<some-package>', total=True, format=\"pandas\")\r\n```\r\n",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/pypi_handler/pypi_handler.py",
    "pr_number": 7026,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1286312970,
    "comment_created_at": "2023-08-07T19:30:26Z"
  },
  {
    "code": "@@ -0,0 +1,323 @@\n+from typing import Dict, List, Text, Optional\n+from datetime import datetime, timedelta\n+import pandas as pd\n+\n+from mindsdb.integrations.libs.api_handler import APIResource\n+from mindsdb.integrations.utilities.sql_utils import (\n+    FilterCondition, FilterOperator, SortColumn\n+)\n+from mindsdb.utilities import log\n+\n+\n+logger = log.getLogger(__name__)\n+\n+\n+class GongCallsTable(APIResource):\n+    \"\"\"The Gong Calls Table implementation\"\"\"\n+\n+    def list(self,\n+             conditions: List[FilterCondition] = None,\n+             limit: int = None,\n+             sort: List[SortColumn] = None,\n+             targets: List[str] = None) -> pd.DataFrame:\n+        \"\"\"Pulls data from the Gong Calls API\n+\n+        Returns\n+        -------\n+        pd.DataFrame\n+            Gong calls matching the query\n+\n+        Raises\n+        ------\n+        ValueError\n+            If the query contains an unsupported condition\n+        \"\"\"\n+\n+        if limit is None:\n+            limit = 20\n+\n+        # Default parameters for the API call\n+        api_params = {\n+            'limit': limit,\n+            'offset': 0\n+        }\n+\n+        # Handle date filtering\n+        if conditions:\n+            for condition in conditions:\n+                if condition.column == 'date' and condition.op == FilterOperator.GREATER_THAN:\n+                    api_params['fromDateTime'] = condition.value\n+                    condition.applied = True\n+                elif condition.column == 'date' and condition.op == FilterOperator.LESS_THAN:\n+                    api_params['toDateTime'] = condition.value\n+                    condition.applied = True\n+                elif condition.column == 'user_id' and condition.op == FilterOperator.EQUAL:\n+                    api_params['userId'] = condition.value\n+                    condition.applied = True\n+                elif condition.column == 'call_type' and condition.op == FilterOperator.EQUAL:\n+                    api_params['callType'] = condition.value\n+                    condition.applied = True\n+\n+        # Handle sorting\n+        if sort:\n+            for col in sort:\n+                if col.column in ('date', 'duration'):\n+                    api_params['sortBy'] = col.column\n+                    api_params['sortOrder'] = 'asc' if col.ascending else 'desc'\n+                    sort.applied = True",
    "comment": "**correctness**: `sort.applied = True` in `GongCallsTable.list` will raise an AttributeError because `sort` is a list, not an object with an `applied` attribute.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>ü§ñ AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> üìã **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/integrations/handlers/gong_handler/gong_tables.py, lines 66-67, the code incorrectly sets `sort.applied = True`, but `sort` is a list, not an object with an `applied` attribute. Change this to `col.applied = True` so that the applied flag is set on the correct object.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>üìù Committable Code Suggestion</strong></summary>\n\n> ‚ÄºÔ∏è Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n                    api_params['sortBy'] = col.column\n                    api_params['sortOrder'] = 'asc' if col.ascending else 'desc'\n                    col.applied = True\n```\n</details>\n<!-- suggestion_end -->\n",
    "line_number": 67,
    "enriched": "File: mindsdb/integrations/handlers/gong_handler/gong_tables.py\nCode: @@ -0,0 +1,323 @@\n+from typing import Dict, List, Text, Optional\n+from datetime import datetime, timedelta\n+import pandas as pd\n+\n+from mindsdb.integrations.libs.api_handler import APIResource\n+from mindsdb.integrations.utilities.sql_utils import (\n+    FilterCondition, FilterOperator, SortColumn\n+)\n+from mindsdb.utilities import log\n+\n+\n+logger = log.getLogger(__name__)\n+\n+\n+class GongCallsTable(APIResource):\n+    \"\"\"The Gong Calls Table implementation\"\"\"\n+\n+    def list(self,\n+             conditions: List[FilterCondition] = None,\n+             limit: int = None,\n+             sort: List[SortColumn] = None,\n+             targets: List[str] = None) -> pd.DataFrame:\n+        \"\"\"Pulls data from the Gong Calls API\n+\n+        Returns\n+        -------\n+        pd.DataFrame\n+            Gong calls matching the query\n+\n+        Raises\n+        ------\n+        ValueError\n+            If the query contains an unsupported condition\n+        \"\"\"\n+\n+        if limit is None:\n+            limit = 20\n+\n+        # Default parameters for the API call\n+        api_params = {\n+            'limit': limit,\n+            'offset': 0\n+        }\n+\n+        # Handle date filtering\n+        if conditions:\n+            for condition in conditions:\n+                if condition.column == 'date' and condition.op == FilterOperator.GREATER_THAN:\n+                    api_params['fromDateTime'] = condition.value\n+                    condition.applied = True\n+                elif condition.column == 'date' and condition.op == FilterOperator.LESS_THAN:\n+                    api_params['toDateTime'] = condition.value\n+                    condition.applied = True\n+                elif condition.column == 'user_id' and condition.op == FilterOperator.EQUAL:\n+                    api_params['userId'] = condition.value\n+                    condition.applied = True\n+                elif condition.column == 'call_type' and condition.op == FilterOperator.EQUAL:\n+                    api_params['callType'] = condition.value\n+                    condition.applied = True\n+\n+        # Handle sorting\n+        if sort:\n+            for col in sort:\n+                if col.column in ('date', 'duration'):\n+                    api_params['sortBy'] = col.column\n+                    api_params['sortOrder'] = 'asc' if col.ascending else 'desc'\n+                    sort.applied = True\nComment: **correctness**: `sort.applied = True` in `GongCallsTable.list` will raise an AttributeError because `sort` is a list, not an object with an `applied` attribute.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>ü§ñ AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> üìã **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/integrations/handlers/gong_handler/gong_tables.py, lines 66-67, the code incorrectly sets `sort.applied = True`, but `sort` is a list, not an object with an `applied` attribute. Change this to `col.applied = True` so that the applied flag is set on the correct object.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>üìù Committable Code Suggestion</strong></summary>\n\n> ‚ÄºÔ∏è Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n                    api_params['sortBy'] = col.column\n                    api_params['sortOrder'] = 'asc' if col.ascending else 'desc'\n                    col.applied = True\n```\n</details>\n<!-- suggestion_end -->\n",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/gong_handler/gong_tables.py",
    "pr_number": 11291,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2202848440,
    "comment_created_at": "2025-07-12T17:58:54Z"
  },
  {
    "code": "@@ -0,0 +1,70 @@\n+---\n+title: MindsDB Inference Endpoints\n+sidebarTitle: MindsDB Inference Endpoints\n+---\n+\n+This documentation describes the integration of MindsDB with [MindsDB Inference Endpoints](https://mindsdb-docs.hashnode.space/), a cloud service that simplifies the way developers interact with cutting-edge LLMs through a universal API.\n+\n+## Prerequisites\n+\n+Before proceeding, ensure the following prerequisites are met:\n+\n+1. Install MindsDB [locally via Docker](https://docs.mindsdb.com/setup/self-hosted/docker) or use [MindsDB Cloud](https://cloud.mindsdb.com/).\n+2. To use MindsDB Inference Endpoints within MindsDB, install the required dependencies following [this instruction](/setup/self-hosted/docker#install-dependencies).\n+3. Obtain the MindsDB Inference API key required to deploy and use MindsDB Inference Endpoints models within MindsDB. Follow the [instructions for obtaining the API key](https://mindsdb-docs.hashnode.space/docs/authentication).\n+\n+## Setup\n+\n+Create an AI engine from the [MindsDB Inference Endpoints handler](https://github.com/mindsdb/mindsdb/tree/staging/mindsdb/integrations/handlers/mindsdb_inference_handler).\n+\n+```sql\n+CREATE ML_ENGINE mindsdb_serve\n+FROM mindsdb_inference\n+USING\n+      mindsdb_inference_api_key = 'api-key-value'\n+```\n+\n+Create a model using `mindsdb_serve` as an engine.\n+\n+```sql\n+CREATE MODEL mindsdb_inference_model\n+PREDICT answer\n+USING\n+      engine = 'mindsdb_serve',   -- engine name as created via CREATE ML_ENGINE\n+      model_name = 'model-name',              -- choose one of available models\n+      prompt_teplate = 'prompt-to-the-model'; -- prompt message to be completed by the model\n+      mode = 'mode_name' -- choose one of the avaiable modes",
    "comment": "What are the available modes? Let's list them here:\r\n```\r\nmode = 'mode_name' -- choose one of the available modes: mode1, mode2, ...\r\n```",
    "line_number": 36,
    "enriched": "File: mindsdb/integrations/handlers/mindsdb_inference/README.md\nCode: @@ -0,0 +1,70 @@\n+---\n+title: MindsDB Inference Endpoints\n+sidebarTitle: MindsDB Inference Endpoints\n+---\n+\n+This documentation describes the integration of MindsDB with [MindsDB Inference Endpoints](https://mindsdb-docs.hashnode.space/), a cloud service that simplifies the way developers interact with cutting-edge LLMs through a universal API.\n+\n+## Prerequisites\n+\n+Before proceeding, ensure the following prerequisites are met:\n+\n+1. Install MindsDB [locally via Docker](https://docs.mindsdb.com/setup/self-hosted/docker) or use [MindsDB Cloud](https://cloud.mindsdb.com/).\n+2. To use MindsDB Inference Endpoints within MindsDB, install the required dependencies following [this instruction](/setup/self-hosted/docker#install-dependencies).\n+3. Obtain the MindsDB Inference API key required to deploy and use MindsDB Inference Endpoints models within MindsDB. Follow the [instructions for obtaining the API key](https://mindsdb-docs.hashnode.space/docs/authentication).\n+\n+## Setup\n+\n+Create an AI engine from the [MindsDB Inference Endpoints handler](https://github.com/mindsdb/mindsdb/tree/staging/mindsdb/integrations/handlers/mindsdb_inference_handler).\n+\n+```sql\n+CREATE ML_ENGINE mindsdb_serve\n+FROM mindsdb_inference\n+USING\n+      mindsdb_inference_api_key = 'api-key-value'\n+```\n+\n+Create a model using `mindsdb_serve` as an engine.\n+\n+```sql\n+CREATE MODEL mindsdb_inference_model\n+PREDICT answer\n+USING\n+      engine = 'mindsdb_serve',   -- engine name as created via CREATE ML_ENGINE\n+      model_name = 'model-name',              -- choose one of available models\n+      prompt_teplate = 'prompt-to-the-model'; -- prompt message to be completed by the model\n+      mode = 'mode_name' -- choose one of the avaiable modes\nComment: What are the available modes? Let's list them here:\r\n```\r\nmode = 'mode_name' -- choose one of the available modes: mode1, mode2, ...\r\n```",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/mindsdb_inference/README.md",
    "pr_number": 9044,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1557675729,
    "comment_created_at": "2024-04-09T13:56:14Z"
  },
  {
    "code": "@@ -10,4 +10,6 @@ pytest-cov\n coveralls\n locust\n ollama >= 0.1.7 # Langchain tests\n-anthropic >= 0.21.3 # Langchain tests\n\\ No newline at end of file\n+anthropic >= 0.21.3 # Langchain tests\n+langchain-google-genai>=2.0.0 # Langchain tests\n+mindsdb-sdk",
    "comment": "I don't see it is used in test below",
    "line_number": 15,
    "enriched": "File: requirements/requirements-test.txt\nCode: @@ -10,4 +10,6 @@ pytest-cov\n coveralls\n locust\n ollama >= 0.1.7 # Langchain tests\n-anthropic >= 0.21.3 # Langchain tests\n\\ No newline at end of file\n+anthropic >= 0.21.3 # Langchain tests\n+langchain-google-genai>=2.0.0 # Langchain tests\n+mindsdb-sdk\nComment: I don't see it is used in test below",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "requirements/requirements-test.txt",
    "pr_number": 10690,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2036857898,
    "comment_created_at": "2025-04-10T08:54:44Z"
  },
  {
    "code": "@@ -1,9 +1,61 @@\n-__title__ = 'MindsDB Email handler'\n-__package_name__ = 'mindsdb_email_handler'\n-__version__ = '0.0.1'\n-__description__ = \"MindsDB handler for email\"\n-__author__ = 'MindsDB Inc'\n-__github__ = 'https://github.com/mindsdb/mindsdb'\n-__pypi__ = 'https://pypi.org/project/mindsdb/'\n-__license__ = 'MIT'\n-__copyright__ = 'Copyright 2022- mindsdb'\n+# mindsdb/integrations/handlers/email_handler/__about__.py\n+\n+from mindsdb.integrations.libs.const import HANDLER_TYPE\n+\n+__title__ = \"Email\"\n+__package_name__ = \"email_handler\"\n+__version__ = \"0.0.1\"\n+__description__ = \"MindsDB handler for retrieving emails through IMAP.\"\n+__author__ = \"MindsDB Inc\"\n+__github__ = \"https://github.com/mindsdb/mindsdb\"\n+__pypi__ = \"https://pypi.org/project/mindsdb/\"\n+__license__ = \"GPL-3.0\"\n+__copyright__ = \"Copyright 2023- mindsdb\"\n+__icon_path__ = \"icon.svg\"\n+\n+# Suggestion 1: Use hasattr for safer assignment\n+HANDLER_TYPE = HANDLER_TYPE.DATA if hasattr(HANDLER_TYPE, \"DATA\") else \"data\"",
    "comment": "**correctness**: `HANDLER_TYPE = HANDLER_TYPE.DATA if hasattr(HANDLER_TYPE, \"DATA\") else \"data\"` will raise `AttributeError` if `HANDLER_TYPE` is not an object (e.g., a string), causing a crash at import time.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>ü§ñ AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> üìã **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/integrations/handlers/email_handler/__about__.py, line 17, the assignment `HANDLER_TYPE = HANDLER_TYPE.DATA if hasattr(HANDLER_TYPE, \"DATA\") else \"data\"` can raise an AttributeError if HANDLER_TYPE is not an object (e.g., if it's a string). Replace it with `HANDLER_TYPE = getattr(HANDLER_TYPE, \"DATA\", \"data\")` to ensure safe assignment and prevent runtime crashes.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>üìù Committable Code Suggestion</strong></summary>\n\n> ‚ÄºÔ∏è Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\nHANDLER_TYPE = getattr(HANDLER_TYPE, \"DATA\", \"data\")\n```\n</details>\n<!-- suggestion_end -->\n",
    "line_number": 17,
    "enriched": "File: mindsdb/integrations/handlers/email_handler/__about__.py\nCode: @@ -1,9 +1,61 @@\n-__title__ = 'MindsDB Email handler'\n-__package_name__ = 'mindsdb_email_handler'\n-__version__ = '0.0.1'\n-__description__ = \"MindsDB handler for email\"\n-__author__ = 'MindsDB Inc'\n-__github__ = 'https://github.com/mindsdb/mindsdb'\n-__pypi__ = 'https://pypi.org/project/mindsdb/'\n-__license__ = 'MIT'\n-__copyright__ = 'Copyright 2022- mindsdb'\n+# mindsdb/integrations/handlers/email_handler/__about__.py\n+\n+from mindsdb.integrations.libs.const import HANDLER_TYPE\n+\n+__title__ = \"Email\"\n+__package_name__ = \"email_handler\"\n+__version__ = \"0.0.1\"\n+__description__ = \"MindsDB handler for retrieving emails through IMAP.\"\n+__author__ = \"MindsDB Inc\"\n+__github__ = \"https://github.com/mindsdb/mindsdb\"\n+__pypi__ = \"https://pypi.org/project/mindsdb/\"\n+__license__ = \"GPL-3.0\"\n+__copyright__ = \"Copyright 2023- mindsdb\"\n+__icon_path__ = \"icon.svg\"\n+\n+# Suggestion 1: Use hasattr for safer assignment\n+HANDLER_TYPE = HANDLER_TYPE.DATA if hasattr(HANDLER_TYPE, \"DATA\") else \"data\"\nComment: **correctness**: `HANDLER_TYPE = HANDLER_TYPE.DATA if hasattr(HANDLER_TYPE, \"DATA\") else \"data\"` will raise `AttributeError` if `HANDLER_TYPE` is not an object (e.g., a string), causing a crash at import time.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>ü§ñ AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> üìã **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/integrations/handlers/email_handler/__about__.py, line 17, the assignment `HANDLER_TYPE = HANDLER_TYPE.DATA if hasattr(HANDLER_TYPE, \"DATA\") else \"data\"` can raise an AttributeError if HANDLER_TYPE is not an object (e.g., if it's a string). Replace it with `HANDLER_TYPE = getattr(HANDLER_TYPE, \"DATA\", \"data\")` to ensure safe assignment and prevent runtime crashes.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>üìù Committable Code Suggestion</strong></summary>\n\n> ‚ÄºÔ∏è Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\nHANDLER_TYPE = getattr(HANDLER_TYPE, \"DATA\", \"data\")\n```\n</details>\n<!-- suggestion_end -->\n",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/email_handler/__about__.py",
    "pr_number": 11681,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2403989043,
    "comment_created_at": "2025-10-04T14:08:10Z"
  },
  {
    "code": "@@ -71,4 +71,4 @@ typing-extensions==4.13.2\n python-dotenv==1.1.0\n jwcrypto==1.5.6\n pyjwt==2.10.1\n-pydantic_core==2.18.4\n+pydantic_core==2.23.2",
    "comment": "Maybe better to try `pydantic_core>=2.23.2` to be a bit more flexible?",
    "line_number": 74,
    "enriched": "File: requirements/requirements.txt\nCode: @@ -71,4 +71,4 @@ typing-extensions==4.13.2\n python-dotenv==1.1.0\n jwcrypto==1.5.6\n pyjwt==2.10.1\n-pydantic_core==2.18.4\n+pydantic_core==2.23.2\nComment: Maybe better to try `pydantic_core>=2.23.2` to be a bit more flexible?",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "requirements/requirements.txt",
    "pr_number": 11026,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2132494037,
    "comment_created_at": "2025-06-06T16:24:36Z"
  },
  {
    "code": "@@ -33,6 +33,6 @@ pyOpenSSL >= 22.0.0\n pydateinfer==0.3.0\n pyarrow >= 10.0.1, < 10.1.0\n dataprep_ml\n-grpcio-tools\n python-magic >= 0.4.27\n+#python-magic-bin >= 0.4.14 #for M1 mac users, comment out the above line and uncomment this one",
    "comment": "Is this something that will affect _base MindsDB_ functionality? If not, let's move it to the `writer_handler` folder",
    "line_number": 37,
    "enriched": "File: requirements/requirements.txt\nCode: @@ -33,6 +33,6 @@ pyOpenSSL >= 22.0.0\n pydateinfer==0.3.0\n pyarrow >= 10.0.1, < 10.1.0\n dataprep_ml\n-grpcio-tools\n python-magic >= 0.4.27\n+#python-magic-bin >= 0.4.14 #for M1 mac users, comment out the above line and uncomment this one\nComment: Is this something that will affect _base MindsDB_ functionality? If not, let's move it to the `writer_handler` folder",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "requirements/requirements.txt",
    "pr_number": 6621,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1232502854,
    "comment_created_at": "2023-06-16T16:52:47Z"
  },
  {
    "code": "@@ -0,0 +1,445 @@\n+import ast\n+import uuid\n+from enum import Enum\n+from typing import Any, List, Optional\n+\n+import pandas as pd\n+from mindsdb_sql.parser.ast import (\n+    BinaryOperation,\n+    Constant,\n+    CreateTable,\n+    Delete,\n+    DropTables,\n+    Insert,\n+    Select,\n+    Star,\n+    Tuple,\n+    Update,\n+)\n+from mindsdb_sql.parser.ast.base import ASTNode\n+\n+from mindsdb.integrations.libs.response import RESPONSE_TYPE, HandlerResponse\n+from mindsdb.utilities.log import get_log\n+\n+from ..utilities.sql_utils import query_traversal\n+from .base import BaseHandler\n+\n+LOG = get_log(\"VectorStoreHandler\")\n+\n+\n+class FilterOperator(Enum):\n+    \"\"\"\n+    Enum for filter operators.\n+    \"\"\"\n+\n+    EQUAL = \"=\"\n+    NOT_EQUAL = \"!=\"\n+    LESS_THAN = \"<\"\n+    LESS_THAN_OR_EQUAL = \"<=\"\n+    GREATER_THAN = \">\"\n+    GREATER_THAN_OR_EQUAL = \">=\"\n+    IN = \"IN\"\n+    NOT_IN = \"NOT IN\"\n+    BETWEEN = \"BETWEEN\"\n+    NOT_BETWEEN = \"NOT BETWEEN\"\n+    LIKE = \"LIKE\"\n+    NOT_LIKE = \"NOT LIKE\"\n+    IS_NULL = \"IS NULL\"\n+    IS_NOT_NULL = \"IS NOT NULL\"\n+\n+\n+class FilterCondition:\n+    \"\"\"\n+    Base class for filter conditions.\n+    \"\"\"\n+\n+    def __init__(self, column: str, op: FilterOperator, value: Any):\n+        self.column = column\n+        self.op = op\n+        self.value = value\n+\n+    def __eq__(self, __value: object) -> bool:\n+        if isinstance(__value, FilterCondition):\n+            return (\n+                self.column == __value.column\n+                and self.op == __value.op\n+                and self.value == __value.value\n+            )\n+        else:\n+            return False\n+\n+    def __repr__(self) -> str:\n+        return f\"\"\"\n+            FilterCondition(\n+                column={self.column},\n+                op={self.op},\n+                value={self.value}\n+            )\n+        \"\"\"\n+\n+\n+class TableField(Enum):\n+    \"\"\"\n+    Enum for table fields.\n+    \"\"\"\n+\n+    ID = \"id\"\n+    CONTENT = \"content\"\n+    EMBEDDINGS = \"embeddings\"\n+    METADATA = \"metadata\"\n+    SEARCH_VECTOR = \"search_vector\"\n+    DISTANCE = \"distance\"\n+\n+\n+class VectorStoreHandler(BaseHandler):\n+    \"\"\"\n+    Base class for handlers associated to vector databases.\n+    \"\"\"\n+\n+    SCHEMA = [\n+        {\n+            \"name\": TableField.ID.value,\n+            \"data_type\": \"string\",",
    "comment": "Isn't this a number?",
    "line_number": 102,
    "enriched": "File: mindsdb/integrations/libs/vectordatabase_handler.py\nCode: @@ -0,0 +1,445 @@\n+import ast\n+import uuid\n+from enum import Enum\n+from typing import Any, List, Optional\n+\n+import pandas as pd\n+from mindsdb_sql.parser.ast import (\n+    BinaryOperation,\n+    Constant,\n+    CreateTable,\n+    Delete,\n+    DropTables,\n+    Insert,\n+    Select,\n+    Star,\n+    Tuple,\n+    Update,\n+)\n+from mindsdb_sql.parser.ast.base import ASTNode\n+\n+from mindsdb.integrations.libs.response import RESPONSE_TYPE, HandlerResponse\n+from mindsdb.utilities.log import get_log\n+\n+from ..utilities.sql_utils import query_traversal\n+from .base import BaseHandler\n+\n+LOG = get_log(\"VectorStoreHandler\")\n+\n+\n+class FilterOperator(Enum):\n+    \"\"\"\n+    Enum for filter operators.\n+    \"\"\"\n+\n+    EQUAL = \"=\"\n+    NOT_EQUAL = \"!=\"\n+    LESS_THAN = \"<\"\n+    LESS_THAN_OR_EQUAL = \"<=\"\n+    GREATER_THAN = \">\"\n+    GREATER_THAN_OR_EQUAL = \">=\"\n+    IN = \"IN\"\n+    NOT_IN = \"NOT IN\"\n+    BETWEEN = \"BETWEEN\"\n+    NOT_BETWEEN = \"NOT BETWEEN\"\n+    LIKE = \"LIKE\"\n+    NOT_LIKE = \"NOT LIKE\"\n+    IS_NULL = \"IS NULL\"\n+    IS_NOT_NULL = \"IS NOT NULL\"\n+\n+\n+class FilterCondition:\n+    \"\"\"\n+    Base class for filter conditions.\n+    \"\"\"\n+\n+    def __init__(self, column: str, op: FilterOperator, value: Any):\n+        self.column = column\n+        self.op = op\n+        self.value = value\n+\n+    def __eq__(self, __value: object) -> bool:\n+        if isinstance(__value, FilterCondition):\n+            return (\n+                self.column == __value.column\n+                and self.op == __value.op\n+                and self.value == __value.value\n+            )\n+        else:\n+            return False\n+\n+    def __repr__(self) -> str:\n+        return f\"\"\"\n+            FilterCondition(\n+                column={self.column},\n+                op={self.op},\n+                value={self.value}\n+            )\n+        \"\"\"\n+\n+\n+class TableField(Enum):\n+    \"\"\"\n+    Enum for table fields.\n+    \"\"\"\n+\n+    ID = \"id\"\n+    CONTENT = \"content\"\n+    EMBEDDINGS = \"embeddings\"\n+    METADATA = \"metadata\"\n+    SEARCH_VECTOR = \"search_vector\"\n+    DISTANCE = \"distance\"\n+\n+\n+class VectorStoreHandler(BaseHandler):\n+    \"\"\"\n+    Base class for handlers associated to vector databases.\n+    \"\"\"\n+\n+    SCHEMA = [\n+        {\n+            \"name\": TableField.ID.value,\n+            \"data_type\": \"string\",\nComment: Isn't this a number?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/libs/vectordatabase_handler.py",
    "pr_number": 7195,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1312359658,
    "comment_created_at": "2023-08-31T22:48:22Z"
  },
  {
    "code": "@@ -0,0 +1,429 @@\n+import os\n+import time\n+from collections import defaultdict\n+import fbmessenger\n+from fbmessenger import MessengerClient\n+from fbmessenger.elements import Messager\n+from fbmessenger.buttons import POSTBACK\n+import pandas as pd\n+from mindsdb.utilities import log\n+from mindsdb.utilities.config import Config\n+from mindsdb_sql.parser import ast\n+from mindsdb.integrations.libs.api_handler import APIHandler, APITable, FuncParser\n+from mindsdb.integrations.utilities.sql_utils import extract_comparison_conditions\n+from mindsdb.integrations.libs.response import (\n+    HandlerStatusResponse as StatusResponse,\n+    HandlerResponse as Response,\n+    RESPONSE_TYPE\n+)\n+\n+# Import additional required libraries\n+import requests\n+\n+from messengerapi import SendApi\n+from messengerapi.components import Elements, Element, Buttons, Button, QuickReply\n+\n+\n+class MessagesTable(APITable):\n+    def select(self, query: ast.Select) -> Response:\n+        conditions = extract_comparison_conditions(query.where)\n+        params = {}\n+        filters = []\n+\n+        for op, arg1, arg2 in conditions:\n+            if op == 'text':\n+                if arg1 == 'message':\n+                    client = Messager('<access_token>')\n+                    client.send_text(arg2, 'Recipient ID')\n+            elif op == 'image':\n+                if arg1 == 'message':\n+                    send_api = SendApi('<page_access_token>')\n+                    send_api.send_local_image(arg2, 'Recipient ID')\n+            elif op == 'audio':\n+                if arg1 == 'message':\n+                    send_api = SendApi('<page_access_token>')\n+                    send_api.send_local_audio(arg2, 'Recipient ID')\n+            elif op == 'video':\n+                if arg1 == 'message':\n+                    send_api = SendApi('<page_access_token>')\n+                    send_api.send_local_video(arg2, 'Recipient ID')\n+            elif op == 'file':\n+                if arg1 == 'message':\n+                    send_api = SendApi('<page_access_token>')\n+                    send_api.send_local_file(arg2, 'Recipient ID')\n+            elif op == 'quick_reply':\n+                if arg1 == 'message':\n+                    client = Messager('<access_token>')\n+                    client.send_quick_replies('Recipient ID', arg2, [\n+                        QuickReply(\"Option 1\", \"PAYLOAD_1\"),\n+                        QuickReply(\"Option 2\", \"PAYLOAD_2\"),\n+                        QuickReply(\"Option 3\", \"PAYLOAD_3\")\n+                    ])\n+            elif op == 'generic_template':\n+                if arg1 == 'message':\n+                    send_api = SendApi('<page_access_token>')\n+                    elements = Elements()\n+                    buttons = Buttons()\n+                    button = Button(button_type=POSTBACK, title=\"My button\")\n+                    buttons.add_button(button.get_content())\n+                    element = Element(title=\"My element\", subtitle=\"The element's subtitle\", image_url=arg2, buttons=buttons)\n+                    elements.add_element(element.get_content())\n+                    send_api.send_generic_message(elements.get_content(), 'Recipient ID', image_aspect_ratio=\"horizontal\")\n+\n+        if query.limit is not None:\n+            params['limit'] = query.limit.value\n+\n+        if 'thread_id' not in params:\n+            # Default to general thread if not specified\n+            params['thread_id'] = 'general'\n+\n+        result = self.handler.call_messenger_api(\n+            method_name='get_thread_messages',\n+            params=params,\n+            filters=filters\n+        )\n+\n+        # Filter targets\n+        columns = []\n+        for target in query.targets:\n+            if isinstance(target, ast.Star):\n+                columns = []\n+                break\n+            elif isinstance(target, ast.Identifier):\n+                columns.append(target.parts[-1])\n+            else:\n+                raise NotImplementedError\n+\n+        if len(columns) == 0:\n+            columns = self.get_columns()\n+\n+        # Convert columns to lower case\n+        columns = [name.lower() for name in columns]\n+\n+        if len(result) == 0:\n+            result = pd.DataFrame([], columns=columns)\n+        else:\n+            # Add absent columns\n+            for col in set(columns) & set(result.columns) ^ set(columns):\n+                result[col] = None\n+\n+            # Filter by columns\n+            result = result[columns]\n+\n+        return result\n+\n+    def get_columns(self):\n+        return [\n+            'id', 'created_time', 'message', 'from_id', 'from_name', 'to_id', 'to_name',\n+        ]\n+\n+\n+def insert(self, query):",
    "comment": "Why this function is outside of the class?",
    "line_number": 121,
    "enriched": "File: mindsdb/integrations/handlers/facebook_messenger_handler/messenger_handler.py\nCode: @@ -0,0 +1,429 @@\n+import os\n+import time\n+from collections import defaultdict\n+import fbmessenger\n+from fbmessenger import MessengerClient\n+from fbmessenger.elements import Messager\n+from fbmessenger.buttons import POSTBACK\n+import pandas as pd\n+from mindsdb.utilities import log\n+from mindsdb.utilities.config import Config\n+from mindsdb_sql.parser import ast\n+from mindsdb.integrations.libs.api_handler import APIHandler, APITable, FuncParser\n+from mindsdb.integrations.utilities.sql_utils import extract_comparison_conditions\n+from mindsdb.integrations.libs.response import (\n+    HandlerStatusResponse as StatusResponse,\n+    HandlerResponse as Response,\n+    RESPONSE_TYPE\n+)\n+\n+# Import additional required libraries\n+import requests\n+\n+from messengerapi import SendApi\n+from messengerapi.components import Elements, Element, Buttons, Button, QuickReply\n+\n+\n+class MessagesTable(APITable):\n+    def select(self, query: ast.Select) -> Response:\n+        conditions = extract_comparison_conditions(query.where)\n+        params = {}\n+        filters = []\n+\n+        for op, arg1, arg2 in conditions:\n+            if op == 'text':\n+                if arg1 == 'message':\n+                    client = Messager('<access_token>')\n+                    client.send_text(arg2, 'Recipient ID')\n+            elif op == 'image':\n+                if arg1 == 'message':\n+                    send_api = SendApi('<page_access_token>')\n+                    send_api.send_local_image(arg2, 'Recipient ID')\n+            elif op == 'audio':\n+                if arg1 == 'message':\n+                    send_api = SendApi('<page_access_token>')\n+                    send_api.send_local_audio(arg2, 'Recipient ID')\n+            elif op == 'video':\n+                if arg1 == 'message':\n+                    send_api = SendApi('<page_access_token>')\n+                    send_api.send_local_video(arg2, 'Recipient ID')\n+            elif op == 'file':\n+                if arg1 == 'message':\n+                    send_api = SendApi('<page_access_token>')\n+                    send_api.send_local_file(arg2, 'Recipient ID')\n+            elif op == 'quick_reply':\n+                if arg1 == 'message':\n+                    client = Messager('<access_token>')\n+                    client.send_quick_replies('Recipient ID', arg2, [\n+                        QuickReply(\"Option 1\", \"PAYLOAD_1\"),\n+                        QuickReply(\"Option 2\", \"PAYLOAD_2\"),\n+                        QuickReply(\"Option 3\", \"PAYLOAD_3\")\n+                    ])\n+            elif op == 'generic_template':\n+                if arg1 == 'message':\n+                    send_api = SendApi('<page_access_token>')\n+                    elements = Elements()\n+                    buttons = Buttons()\n+                    button = Button(button_type=POSTBACK, title=\"My button\")\n+                    buttons.add_button(button.get_content())\n+                    element = Element(title=\"My element\", subtitle=\"The element's subtitle\", image_url=arg2, buttons=buttons)\n+                    elements.add_element(element.get_content())\n+                    send_api.send_generic_message(elements.get_content(), 'Recipient ID', image_aspect_ratio=\"horizontal\")\n+\n+        if query.limit is not None:\n+            params['limit'] = query.limit.value\n+\n+        if 'thread_id' not in params:\n+            # Default to general thread if not specified\n+            params['thread_id'] = 'general'\n+\n+        result = self.handler.call_messenger_api(\n+            method_name='get_thread_messages',\n+            params=params,\n+            filters=filters\n+        )\n+\n+        # Filter targets\n+        columns = []\n+        for target in query.targets:\n+            if isinstance(target, ast.Star):\n+                columns = []\n+                break\n+            elif isinstance(target, ast.Identifier):\n+                columns.append(target.parts[-1])\n+            else:\n+                raise NotImplementedError\n+\n+        if len(columns) == 0:\n+            columns = self.get_columns()\n+\n+        # Convert columns to lower case\n+        columns = [name.lower() for name in columns]\n+\n+        if len(result) == 0:\n+            result = pd.DataFrame([], columns=columns)\n+        else:\n+            # Add absent columns\n+            for col in set(columns) & set(result.columns) ^ set(columns):\n+                result[col] = None\n+\n+            # Filter by columns\n+            result = result[columns]\n+\n+        return result\n+\n+    def get_columns(self):\n+        return [\n+            'id', 'created_time', 'message', 'from_id', 'from_name', 'to_id', 'to_name',\n+        ]\n+\n+\n+def insert(self, query):\nComment: Why this function is outside of the class?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/facebook_messenger_handler/messenger_handler.py",
    "pr_number": 8067,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1378775351,
    "comment_created_at": "2023-11-01T13:04:13Z"
  },
  {
    "code": "@@ -0,0 +1,231 @@\n+from collections import OrderedDict\n+from typing import Optional\n+from mindsdb_sql.parser.ast.base import ASTNode\n+from mindsdb.integrations.libs.base import DatabaseHandler\n+from mindsdb.utilities import log\n+from mindsdb_sql import parse_sql\n+from mindsdb.integrations.libs.response import (\n+    HandlerStatusResponse as StatusResponse,\n+    HandlerResponse as Response,\n+    RESPONSE_TYPE\n+)\n+from mindsdb.integrations.libs.const import HANDLER_CONNECTION_ARG_TYPE as ARG_TYPE\n+from .utils.surreal_get_info import *\n+\n+import pysurrealdb as surreal\n+import pandas as pd\n+\n+\n+class SurrealDBHandler(DatabaseHandler):\n+    \"\"\"\n+    This handler handles connection and execution of the SurrealDB statements.\n+    \"\"\"\n+    name = 'surrealdb'\n+\n+    def __init__(self, name: str, connection_data: Optional[dict], **kwargs):\n+        \"\"\" Initialize the handler\n+        Args:\n+            name (str): name of particular handler instance\n+            connection_data (dict): parameters for connecting to the database\n+            **kwargs: arbitrary keyword arguments.\n+        \"\"\"\n+        super().__init__(name)\n+        self.database = connection_data['database']\n+        self.parser = parse_sql\n+        self.dialect = \"surrealdb\"\n+        self.kwargs = kwargs\n+        self.namespace = connection_data['namespace']\n+        self.user = connection_data['user']\n+        self.password = connection_data['password']\n+        self.host = connection_data['host']\n+        self.port = connection_data['port']\n+\n+        self.connection = None\n+        self.is_connected = False\n+\n+    def connect(self):\n+        \"\"\"\n+        Establishes a connection to the MindsDB database.\n+        Returns:\n+            HandlerStatusResponse\n+        \"\"\"\n+        if self.is_connected is True:\n+            return self.connection\n+        try:\n+            self.connection = surreal.connect(\n+                database=self.database,\n+                host=self.host,\n+                port=self.port,\n+                user=self.user,\n+                password=self.password,\n+                namespace=self.namespace,\n+            )\n+            self.is_connected = True\n+        except Exception as e:\n+            log.logger.error(f\"Error while connecting to SurrealDB, {e}\")\n+\n+        return self.connection\n+\n+    def check_connection(self) -> StatusResponse:\n+        \"\"\"\n+        Check connection to the handler.\n+        Returns:\n+            HandlerStatusResponse\n+        \"\"\"\n+        response_code = StatusResponse(False)\n+        need_to_close = self.is_connected is False\n+        try:\n+            self.connect()\n+            response_code.success = True\n+        except Exception as e:\n+            log.logger.error(f'Error connecting to SurrealDB, {e}!')\n+            response_code.error_message = str(e)\n+        finally:\n+            if response_code.success is True and need_to_close:\n+                self.disconnect()\n+            if response_code.success is False and self.is_connected is True:\n+                self.is_connected = False\n+\n+        return response_code\n+\n+    def disconnect(self):\n+        \"\"\"\n+        Close the existing connection to the SurrealDB database\n+        \"\"\"\n+        if self.is_connected is False:\n+            return\n+        try:\n+            self.connection.close()\n+            self.is_connected = False\n+        except Exception as e:\n+            log.logger.error(f\"Error while disconnecting to SurrealDB, {e}\")\n+\n+        return\n+\n+    def native_query(self, query: str) -> Response:\n+        \"\"\"\n+        Receive raw query and act upon it somehow.\n+        Args:\n+            query (Any): query in SurrealQL to execute\n+        Returns:\n+            HandlerResponse\n+        \"\"\"\n+        need_to_close = self.is_connected is False\n+        conn = self.connect()\n+        cur = conn.cursor()\n+        try:\n+            cur.execute(query)\n+            result = cur.fetchall()\n+            if result:\n+                response = Response(\n+                    RESPONSE_TYPE.TABLE,\n+                    data_frame=pd.DataFrame(\n+                        result,\n+                        columns=[x[0] for x in cur.description],\n+                    )\n+                )\n+            else:\n+                response = Response(RESPONSE_TYPE.OK)\n+        except Exception as e:\n+            log.logger.error(f'Error running query: {query} on SurrealDB!')\n+            response = Response(\n+                RESPONSE_TYPE.ERROR,\n+                error_message=str(e)\n+            )\n+\n+        cur.close()\n+\n+        if need_to_close is True:\n+            self.disconnect()\n+\n+        return response\n+\n+    def query(self, query: ASTNode) -> Response:\n+        \"\"\"\n+        Receive query as AST (abstract syntax tree) and act upon it somehow.\n+        Args:\n+            query (ASTNode): sql query represented as AST. It may be any kind\n+                of query: SELECT, INSERT, DELETE, etc\n+        Returns:\n+            HandlerResponse\n+        \"\"\"\n+        query_string = query.to_string()\n+\n+        # ensure the correct query is passed\n+        last_word = query_string.split()[-1]",
    "comment": "Thanks @manosdell. Can you please clarify why we need these two lines? Is there some existing surrealdb dialect that we can re-use for SQLAlchemy?",
    "line_number": 155,
    "enriched": "File: mindsdb/integrations/handlers/surrealdb_handler/surrealdb_handler.py\nCode: @@ -0,0 +1,231 @@\n+from collections import OrderedDict\n+from typing import Optional\n+from mindsdb_sql.parser.ast.base import ASTNode\n+from mindsdb.integrations.libs.base import DatabaseHandler\n+from mindsdb.utilities import log\n+from mindsdb_sql import parse_sql\n+from mindsdb.integrations.libs.response import (\n+    HandlerStatusResponse as StatusResponse,\n+    HandlerResponse as Response,\n+    RESPONSE_TYPE\n+)\n+from mindsdb.integrations.libs.const import HANDLER_CONNECTION_ARG_TYPE as ARG_TYPE\n+from .utils.surreal_get_info import *\n+\n+import pysurrealdb as surreal\n+import pandas as pd\n+\n+\n+class SurrealDBHandler(DatabaseHandler):\n+    \"\"\"\n+    This handler handles connection and execution of the SurrealDB statements.\n+    \"\"\"\n+    name = 'surrealdb'\n+\n+    def __init__(self, name: str, connection_data: Optional[dict], **kwargs):\n+        \"\"\" Initialize the handler\n+        Args:\n+            name (str): name of particular handler instance\n+            connection_data (dict): parameters for connecting to the database\n+            **kwargs: arbitrary keyword arguments.\n+        \"\"\"\n+        super().__init__(name)\n+        self.database = connection_data['database']\n+        self.parser = parse_sql\n+        self.dialect = \"surrealdb\"\n+        self.kwargs = kwargs\n+        self.namespace = connection_data['namespace']\n+        self.user = connection_data['user']\n+        self.password = connection_data['password']\n+        self.host = connection_data['host']\n+        self.port = connection_data['port']\n+\n+        self.connection = None\n+        self.is_connected = False\n+\n+    def connect(self):\n+        \"\"\"\n+        Establishes a connection to the MindsDB database.\n+        Returns:\n+            HandlerStatusResponse\n+        \"\"\"\n+        if self.is_connected is True:\n+            return self.connection\n+        try:\n+            self.connection = surreal.connect(\n+                database=self.database,\n+                host=self.host,\n+                port=self.port,\n+                user=self.user,\n+                password=self.password,\n+                namespace=self.namespace,\n+            )\n+            self.is_connected = True\n+        except Exception as e:\n+            log.logger.error(f\"Error while connecting to SurrealDB, {e}\")\n+\n+        return self.connection\n+\n+    def check_connection(self) -> StatusResponse:\n+        \"\"\"\n+        Check connection to the handler.\n+        Returns:\n+            HandlerStatusResponse\n+        \"\"\"\n+        response_code = StatusResponse(False)\n+        need_to_close = self.is_connected is False\n+        try:\n+            self.connect()\n+            response_code.success = True\n+        except Exception as e:\n+            log.logger.error(f'Error connecting to SurrealDB, {e}!')\n+            response_code.error_message = str(e)\n+        finally:\n+            if response_code.success is True and need_to_close:\n+                self.disconnect()\n+            if response_code.success is False and self.is_connected is True:\n+                self.is_connected = False\n+\n+        return response_code\n+\n+    def disconnect(self):\n+        \"\"\"\n+        Close the existing connection to the SurrealDB database\n+        \"\"\"\n+        if self.is_connected is False:\n+            return\n+        try:\n+            self.connection.close()\n+            self.is_connected = False\n+        except Exception as e:\n+            log.logger.error(f\"Error while disconnecting to SurrealDB, {e}\")\n+\n+        return\n+\n+    def native_query(self, query: str) -> Response:\n+        \"\"\"\n+        Receive raw query and act upon it somehow.\n+        Args:\n+            query (Any): query in SurrealQL to execute\n+        Returns:\n+            HandlerResponse\n+        \"\"\"\n+        need_to_close = self.is_connected is False\n+        conn = self.connect()\n+        cur = conn.cursor()\n+        try:\n+            cur.execute(query)\n+            result = cur.fetchall()\n+            if result:\n+                response = Response(\n+                    RESPONSE_TYPE.TABLE,\n+                    data_frame=pd.DataFrame(\n+                        result,\n+                        columns=[x[0] for x in cur.description],\n+                    )\n+                )\n+            else:\n+                response = Response(RESPONSE_TYPE.OK)\n+        except Exception as e:\n+            log.logger.error(f'Error running query: {query} on SurrealDB!')\n+            response = Response(\n+                RESPONSE_TYPE.ERROR,\n+                error_message=str(e)\n+            )\n+\n+        cur.close()\n+\n+        if need_to_close is True:\n+            self.disconnect()\n+\n+        return response\n+\n+    def query(self, query: ASTNode) -> Response:\n+        \"\"\"\n+        Receive query as AST (abstract syntax tree) and act upon it somehow.\n+        Args:\n+            query (ASTNode): sql query represented as AST. It may be any kind\n+                of query: SELECT, INSERT, DELETE, etc\n+        Returns:\n+            HandlerResponse\n+        \"\"\"\n+        query_string = query.to_string()\n+\n+        # ensure the correct query is passed\n+        last_word = query_string.split()[-1]\nComment: Thanks @manosdell. Can you please clarify why we need these two lines? Is there some existing surrealdb dialect that we can re-use for SQLAlchemy?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/surrealdb_handler/surrealdb_handler.py",
    "pr_number": 5476,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1159662698,
    "comment_created_at": "2023-04-06T11:15:21Z"
  },
  {
    "code": "@@ -0,0 +1,36 @@\n+from langchain.agents.agent_toolkits import SQLDatabaseToolkit\n+from langchain.schema.language_model import BaseLanguageModel\n+from langchain.tools.base import BaseTool\n+\n+from mindsdb.interfaces.storage import db\n+from mindsdb.integrations.handlers.langchain_handler.mindsdb_database_agent import MindsDBSQL",
    "comment": "direct dependency from langchain. can we avoid this? ",
    "line_number": 6,
    "enriched": "File: mindsdb/interfaces/skills/skill_tool.py\nCode: @@ -0,0 +1,36 @@\n+from langchain.agents.agent_toolkits import SQLDatabaseToolkit\n+from langchain.schema.language_model import BaseLanguageModel\n+from langchain.tools.base import BaseTool\n+\n+from mindsdb.interfaces.storage import db\n+from mindsdb.integrations.handlers.langchain_handler.mindsdb_database_agent import MindsDBSQL\nComment: direct dependency from langchain. can we avoid this? ",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/interfaces/skills/skill_tool.py",
    "pr_number": 8380,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1406020867,
    "comment_created_at": "2023-11-27T11:24:02Z"
  },
  {
    "code": "@@ -398,6 +399,55 @@ def select(self, query: ast.Select) -> pd.DataFrame:\n \n         return orders_df\n \n+    def insert(self, query: ast.Insert) -> None:\n+        \"\"\"\n+        Inserts data into the Shopify \"POST /orders\" API endpoint.\n+\n+        Parameters\n+        ----------\n+        query : ast.Insert\n+            Given SQL INSERT query\n+        \n+        Returns\n+        -------\n+        None\n+\n+        Raises\n+        ------\n+        ValueError\n+            If the query contains an unsupported condition\n+        \"\"\"\n+        insert_statement_parser = INSERTQueryParser(\n+            query,\n+            supported_columns=['address1_ba', 'address2_ba', 'city_ba', 'company_ba', 'country_ba',\n+                               'country_code_ba', 'first_name_ba', 'last_name_ba', 'latitude_ba', \n+                               'longitude_ba', 'name_ba', 'phone_ba', 'province_ba', 'province_code_ba', \n+                               'zip_ba', \n+                               'address1_sa', 'address2_sa', 'city_sa', 'company_sa',\n+                               'country_sa', 'country_code_sa', 'first_name_sa', 'last_name_sa', \n+                               'latitude_sa', 'longitude_sa', 'name_sa', 'phone_sa', 'province_sa', \n+                               'province_code_sa', 'zip_sa', \n+                               'amount_dc', 'code_dc', 'type_dc',\n+                               'gift_card_li', 'grams_li',  'price_li', 'quantity_li', 'title_li', \n+                               'vendor_li', 'fulfillment_status_li', 'sku_li', 'variant_title_li', \n+                               'name_li', 'value_li',\n+                               'price_tl', 'rate_tl', 'title_tl', 'channel_liable_tl',\n+                               'name_na', 'value_na',\n+                               'code_sl', 'price_sl', 'discounted_price_sl', 'source_sl', \n+                               'title_sl', \n+                               'carrier_identifier_sl', 'requested_fulfillment_service_id_sl', \n+                               'is_removed_sl',\n+                               'buyer_accepts_marketing', 'currency', 'email', 'financial_status', \n+                               'fulfillment_status', 'note', 'phone', 'po_number', 'processed_at', \n+                               'referring_site', 'source_name', 'source_identifier', 'source_url', \n+                               'tags', 'taxes_included', 'test', 'total_tax', 'total_weight'],\n+            mandatory_columns=['price_li', 'title_li'],\n+            all_mandatory=False\n+        )\n+        order_data = insert_statement_parser.parse_query()\n+        self.create_orders(order_data)",
    "comment": "@marcoscandroglio\n\nCan we make this create_order function protected or private?",
    "line_number": 448,
    "enriched": "File: mindsdb/integrations/handlers/shopify_handler/shopify_tables.py\nCode: @@ -398,6 +399,55 @@ def select(self, query: ast.Select) -> pd.DataFrame:\n \n         return orders_df\n \n+    def insert(self, query: ast.Insert) -> None:\n+        \"\"\"\n+        Inserts data into the Shopify \"POST /orders\" API endpoint.\n+\n+        Parameters\n+        ----------\n+        query : ast.Insert\n+            Given SQL INSERT query\n+        \n+        Returns\n+        -------\n+        None\n+\n+        Raises\n+        ------\n+        ValueError\n+            If the query contains an unsupported condition\n+        \"\"\"\n+        insert_statement_parser = INSERTQueryParser(\n+            query,\n+            supported_columns=['address1_ba', 'address2_ba', 'city_ba', 'company_ba', 'country_ba',\n+                               'country_code_ba', 'first_name_ba', 'last_name_ba', 'latitude_ba', \n+                               'longitude_ba', 'name_ba', 'phone_ba', 'province_ba', 'province_code_ba', \n+                               'zip_ba', \n+                               'address1_sa', 'address2_sa', 'city_sa', 'company_sa',\n+                               'country_sa', 'country_code_sa', 'first_name_sa', 'last_name_sa', \n+                               'latitude_sa', 'longitude_sa', 'name_sa', 'phone_sa', 'province_sa', \n+                               'province_code_sa', 'zip_sa', \n+                               'amount_dc', 'code_dc', 'type_dc',\n+                               'gift_card_li', 'grams_li',  'price_li', 'quantity_li', 'title_li', \n+                               'vendor_li', 'fulfillment_status_li', 'sku_li', 'variant_title_li', \n+                               'name_li', 'value_li',\n+                               'price_tl', 'rate_tl', 'title_tl', 'channel_liable_tl',\n+                               'name_na', 'value_na',\n+                               'code_sl', 'price_sl', 'discounted_price_sl', 'source_sl', \n+                               'title_sl', \n+                               'carrier_identifier_sl', 'requested_fulfillment_service_id_sl', \n+                               'is_removed_sl',\n+                               'buyer_accepts_marketing', 'currency', 'email', 'financial_status', \n+                               'fulfillment_status', 'note', 'phone', 'po_number', 'processed_at', \n+                               'referring_site', 'source_name', 'source_identifier', 'source_url', \n+                               'tags', 'taxes_included', 'test', 'total_tax', 'total_weight'],\n+            mandatory_columns=['price_li', 'title_li'],\n+            all_mandatory=False\n+        )\n+        order_data = insert_statement_parser.parse_query()\n+        self.create_orders(order_data)\nComment: @marcoscandroglio\n\nCan we make this create_order function protected or private?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/shopify_handler/shopify_tables.py",
    "pr_number": 8550,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1702200817,
    "comment_created_at": "2024-08-02T18:46:15Z"
  },
  {
    "code": "@@ -0,0 +1,20 @@\n+def parse_transaction(res:list):\n+    parsed=[]\n+    for dic in res:\n+        dic=dic.to_dict()\n+        # location=dic['location']",
    "comment": "Can you please remove the commented code?",
    "line_number": 5,
    "enriched": "File: mindsdb/integrations/handlers/plaid_handler/utils.py\nCode: @@ -0,0 +1,20 @@\n+def parse_transaction(res:list):\n+    parsed=[]\n+    for dic in res:\n+        dic=dic.to_dict()\n+        # location=dic['location']\nComment: Can you please remove the commented code?",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/plaid_handler/utils.py",
    "pr_number": 5619,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1173681400,
    "comment_created_at": "2023-04-21T11:54:29Z"
  },
  {
    "code": "@@ -1,14 +1 @@\n-requests>=2.28.1\n-charset-normalizer>=2.1.1\n-protobuf<3.21.0 # for lightwood\n-SQLAlchemy\n-chromadb~=0.4.8\n-duckdb==0.7.1\n-pydantic>=1.10.8\n-sentence_transformers==2.2.2\n-markupsafe == 2.0.1\n-faiss-cpu>=1.7.4 #todo check support for 'faiss-gpu'\n-datasets>=2.14.4\n-nltk>=3.8.1\n-rouge-score>=0.1.2\n-dill==0.3.6 #for lightwood\n+-r ../rag_handler/requirements.txt # same requirements",
    "comment": "This will work with `pip install` right? do we need the -r or it can be used from `pip install -r ,,,`>",
    "line_number": 1,
    "enriched": "File: mindsdb/integrations/handlers/writer_handler/requirements.txt\nCode: @@ -1,14 +1 @@\n-requests>=2.28.1\n-charset-normalizer>=2.1.1\n-protobuf<3.21.0 # for lightwood\n-SQLAlchemy\n-chromadb~=0.4.8\n-duckdb==0.7.1\n-pydantic>=1.10.8\n-sentence_transformers==2.2.2\n-markupsafe == 2.0.1\n-faiss-cpu>=1.7.4 #todo check support for 'faiss-gpu'\n-datasets>=2.14.4\n-nltk>=3.8.1\n-rouge-score>=0.1.2\n-dill==0.3.6 #for lightwood\n+-r ../rag_handler/requirements.txt # same requirements\nComment: This will work with `pip install` right? do we need the -r or it can be used from `pip install -r ,,,`>",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/writer_handler/requirements.txt",
    "pr_number": 8264,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1383344958,
    "comment_created_at": "2023-11-06T13:38:26Z"
  },
  {
    "code": "@@ -106,9 +106,7 @@ def get_available_writer_model_ids(args: dict) -> list:\n def get_available_openai_model_ids(args: dict) -> list:\n     \"\"\"Get available openai LLM model ids\"\"\"\n \n-    openai.api_key = args[\"openai_api_key\"]\n-\n-    models = openai.OpenAI().models.list().data\n+    models = openai.OpenAI(api_key=args[\"openai_api_key\"]).models.list().data",
    "comment": "Maybe we want `api_base` as well here? Would count as futureproofing and may be more to it than just a one liner, feel free to ignore if so.",
    "line_number": 109,
    "enriched": "File: mindsdb/integrations/handlers/rag_handler/settings.py\nCode: @@ -106,9 +106,7 @@ def get_available_writer_model_ids(args: dict) -> list:\n def get_available_openai_model_ids(args: dict) -> list:\n     \"\"\"Get available openai LLM model ids\"\"\"\n \n-    openai.api_key = args[\"openai_api_key\"]\n-\n-    models = openai.OpenAI().models.list().data\n+    models = openai.OpenAI(api_key=args[\"openai_api_key\"]).models.list().data\nComment: Maybe we want `api_base` as well here? Would count as futureproofing and may be more to it than just a one liner, feel free to ignore if so.",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/rag_handler/settings.py",
    "pr_number": 8674,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1472853391,
    "comment_created_at": "2024-01-31T13:52:02Z"
  },
  {
    "code": "@@ -24,10 +24,104 @@\n \n from mindsdb.integrations.libs.api_handler import APITable\n from mindsdb.utilities import log\n+from mindsdb.integrations.utilities.query_traversal import query_traversal\n \n logger = log.getLogger(__name__)\n \n \n+def _cast_series_to_type(series: pd.Series, type_name: str) -> pd.Series:\n+    if type_name is None:\n+        return series\n+\n+    type_name = type_name.lower()\n+    if type_name in {\"int\", \"integer\", \"bigint\", \"smallint\", \"decimal\", \"numeric\", \"float\", \"double\", \"real\"}:\n+        return pd.to_numeric(series, errors=\"coerce\")\n+    if type_name in {\"date\", \"datetime\", \"timestamp\"}:\n+        return pd.to_datetime(series, errors=\"coerce\")\n+    if type_name in {\"text\", \"string\", \"varchar\", \"char\"}:\n+        return series.astype(str)\n+    return series\n+\n+\n+def _extract_where_casts(query: ast.Select) -> Dict[Text, Text]:\n+    casts: Dict[Text, Text] = {}\n+\n+    if query.where is None:\n+        return casts\n+\n+    def _capture_casts(node, **kwargs):\n+        if isinstance(node, ast.BinaryOperation):\n+            candidate = node.args[0]\n+        elif isinstance(node, ast.BetweenOperation):\n+            candidate = node.args[0]\n+        else:\n+            return\n+\n+        cast_type = None\n+        while isinstance(candidate, ast.TypeCast):",
    "comment": "why `while`?, casts might be nested? ",
    "line_number": 61,
    "enriched": "File: mindsdb/integrations/handlers/hubspot_handler/hubspot_tables.py\nCode: @@ -24,10 +24,104 @@\n \n from mindsdb.integrations.libs.api_handler import APITable\n from mindsdb.utilities import log\n+from mindsdb.integrations.utilities.query_traversal import query_traversal\n \n logger = log.getLogger(__name__)\n \n \n+def _cast_series_to_type(series: pd.Series, type_name: str) -> pd.Series:\n+    if type_name is None:\n+        return series\n+\n+    type_name = type_name.lower()\n+    if type_name in {\"int\", \"integer\", \"bigint\", \"smallint\", \"decimal\", \"numeric\", \"float\", \"double\", \"real\"}:\n+        return pd.to_numeric(series, errors=\"coerce\")\n+    if type_name in {\"date\", \"datetime\", \"timestamp\"}:\n+        return pd.to_datetime(series, errors=\"coerce\")\n+    if type_name in {\"text\", \"string\", \"varchar\", \"char\"}:\n+        return series.astype(str)\n+    return series\n+\n+\n+def _extract_where_casts(query: ast.Select) -> Dict[Text, Text]:\n+    casts: Dict[Text, Text] = {}\n+\n+    if query.where is None:\n+        return casts\n+\n+    def _capture_casts(node, **kwargs):\n+        if isinstance(node, ast.BinaryOperation):\n+            candidate = node.args[0]\n+        elif isinstance(node, ast.BetweenOperation):\n+            candidate = node.args[0]\n+        else:\n+            return\n+\n+        cast_type = None\n+        while isinstance(candidate, ast.TypeCast):\nComment: why `while`?, casts might be nested? ",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/hubspot_handler/hubspot_tables.py",
    "pr_number": 11885,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2527815110,
    "comment_created_at": "2025-11-14T14:59:31Z"
  },
  {
    "code": "@@ -5,6 +5,8 @@ services:\n     depends_on:\n       jaeger:\n         condition: service_started\n+      langfuse:",
    "comment": "I would either remove these two lines or comment it out so someone can \"force\" it to be online, but metrics generally are optional I'd say",
    "line_number": 8,
    "enriched": "File: docker-compose.yml\nCode: @@ -5,6 +5,8 @@ services:\n     depends_on:\n       jaeger:\n         condition: service_started\n+      langfuse:\nComment: I would either remove these two lines or comment it out so someone can \"force\" it to be online, but metrics generally are optional I'd say",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "docker-compose.yml",
    "pr_number": 10120,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1828861751,
    "comment_created_at": "2024-11-05T07:36:26Z"
  },
  {
    "code": "@@ -1,19 +1,27 @@\n-import pytest\n+# In tests/conftest.py",
    "comment": "remove unnecessary comments",
    "line_number": 1,
    "enriched": "File: tests/conftest.py\nCode: @@ -1,19 +1,27 @@\n-import pytest\n+# In tests/conftest.py\nComment: remove unnecessary comments",
    "subcategory": "question",
    "category": "discussion",
    "file_path": "tests/conftest.py",
    "pr_number": 11548,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2338453485,
    "comment_created_at": "2025-09-11T03:59:07Z"
  },
  {
    "code": "@@ -77,7 +77,7 @@ Here is how to connect MongoDB Compass to MindsDB using either MindsDB Cloud or\n     Here is what you need to connect:\n     \n     ```\n-    Host: <dedicated instace ip>:<port>\n+    Host: <dedicated instace ip>:3306",
    "comment": "Please correct `instace` to `instance` (as it was mentioned in the issue as well.",
    "line_number": 80,
    "enriched": "File: docs/connect/mongo-compass.mdx\nCode: @@ -77,7 +77,7 @@ Here is how to connect MongoDB Compass to MindsDB using either MindsDB Cloud or\n     Here is what you need to connect:\n     \n     ```\n-    Host: <dedicated instace ip>:<port>\n+    Host: <dedicated instace ip>:3306\nComment: Please correct `instace` to `instance` (as it was mentioned in the issue as well.",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "docs/connect/mongo-compass.mdx",
    "pr_number": 6253,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1203790778,
    "comment_created_at": "2023-05-24T09:32:00Z"
  },
  {
    "code": "@@ -34,13 +34,109 @@ CREATE MODEL anyscale_endpoints_model\n PREDICT target_column\n USING\n       engine = 'anyscale_endpoints_engine',   -- engine name as created via CREATE ML_ENGINE\n-      model_name = 'model-name',              -- choose one of available models\n-      prompt_teplate = 'prompt-to-the-model'; -- prompt message to be completed by the model\n+      mode = 'mode_name', -- optional, -- optional, mode to run the model in",
    "comment": "The `-- optional,` phrase is duplicated in this line.",
    "line_number": 37,
    "enriched": "File: docs/integrations/ai-engines/anyscale.mdx\nCode: @@ -34,13 +34,109 @@ CREATE MODEL anyscale_endpoints_model\n PREDICT target_column\n USING\n       engine = 'anyscale_endpoints_engine',   -- engine name as created via CREATE ML_ENGINE\n-      model_name = 'model-name',              -- choose one of available models\n-      prompt_teplate = 'prompt-to-the-model'; -- prompt message to be completed by the model\n+      mode = 'mode_name', -- optional, -- optional, mode to run the model in\nComment: The `-- optional,` phrase is duplicated in this line.",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "docs/integrations/ai-engines/anyscale.mdx",
    "pr_number": 9565,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1697086312,
    "comment_created_at": "2024-07-30T14:37:14Z"
  },
  {
    "code": "@@ -336,6 +336,7 @@\n             \"data-integrations/google-sheets\",\n             \"data-integrations/oceanbase\",\n             \"data-integrations/mariadb\"",
    "comment": "Please read the issue instructions carefully before solving a task.\r\n\r\nThere should be a comma at the end of this line.",
    "line_number": 338,
    "enriched": "File: docs/mint.json\nCode: @@ -336,6 +336,7 @@\n             \"data-integrations/google-sheets\",\n             \"data-integrations/oceanbase\",\n             \"data-integrations/mariadb\"\nComment: Please read the issue instructions carefully before solving a task.\r\n\r\nThere should be a comma at the end of this line.",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "docs/mint.json",
    "pr_number": 5322,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1154148661,
    "comment_created_at": "2023-03-31T08:00:39Z"
  },
  {
    "code": "@@ -339,13 +339,16 @@ def hybrid_search(\n         full_search_query = f'{semantic_search_cte}{full_text_search_cte}{hybrid_select}'\n         return self.raw_query(full_search_query)\n \n-    def create_table(self, table_name: str, if_not_exists=True):\n+    def create_table(self, table_name: str, sparse=False, if_not_exists=True):",
    "comment": "Long-term we should take kwargs for our vector DB handlers but this is fine for now",
    "line_number": 342,
    "enriched": "File: mindsdb/integrations/handlers/pgvector_handler/pgvector_handler.py\nCode: @@ -339,13 +339,16 @@ def hybrid_search(\n         full_search_query = f'{semantic_search_cte}{full_text_search_cte}{hybrid_select}'\n         return self.raw_query(full_search_query)\n \n-    def create_table(self, table_name: str, if_not_exists=True):\n+    def create_table(self, table_name: str, sparse=False, if_not_exists=True):\nComment: Long-term we should take kwargs for our vector DB handlers but this is fine for now",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "mindsdb/integrations/handlers/pgvector_handler/pgvector_handler.py",
    "pr_number": 10342,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1907965162,
    "comment_created_at": "2025-01-08T23:14:47Z"
  },
  {
    "code": "@@ -134,38 +160,49 @@ def predict(self, df: pd.DataFrame, args: Optional[Dict] = None) -> pd.DataFrame\n                 if endpoint == 'embeddings':\n                     completions.append(values)\n                 else:\n-                    completions.append(''.join(values))\n+                    completions.append(\"\".join(values))\n             else:\n-                completions.append('')\n+                completions.append(\"\")\n \n         # consolidate output\n         data = pd.DataFrame(completions)\n         data.columns = [target_col]\n         return data\n \n     def describe(self, attribute: Optional[str] = None) -> pd.DataFrame:\n-        args = self.model_storage.json_get('args')\n-        model_name, target_col = args['model_name'], args['target']\n-        prompt_template = args.get('prompt_template', 'Answer the following question: {{{{text}}}}')\n+        args = self.model_storage.json_get(\"args\")\n+        model_name, target_col = args[\"model_name\"], args[\"target\"]\n+        prompt_template = args.get(\n+            \"prompt_template\", \"Answer the following question: {{{{text}}}}\"\n+        )\n \n         if attribute == \"features\":\n-            return pd.DataFrame([[target_col, prompt_template]], columns=['target_column', 'mindsdb_prompt_template'])\n+            return pd.DataFrame(\n+                [[target_col, prompt_template]],\n+                columns=[\"target_column\", \"mindsdb_prompt_template\"],\n+            )\n \n         # get model info\n         else:\n-            connection = args.get('ollama_serve_url', OllamaHandler.DEFAULT_SERVE_URL)\n-            model_info = requests.post(connection + '/api/show', json={'name': model_name}).json()\n-            return pd.DataFrame([[\n-                model_name,\n-                model_info.get('license', 'N/A'),\n-                model_info.get('modelfile', 'N/A'),\n-                model_info.get('parameters', 'N/A'),\n-                model_info.get('template', 'N/A'),\n-            ]],\n+            connection = args.get(\"ollama_serve_url\", OllamaHandler.DEFAULT_SERVE_URL)\n+            model_info = requests.post(\n+                connection + \"/api/show\", json={\"name\": model_name}\n+            ).json()\n+            return pd.DataFrame(",
    "comment": "I would really prefer to keep unrelated formatting changes in separate PRs since it makes it hard to grok what's actually being changed that's relevant",
    "line_number": 191,
    "enriched": "File: mindsdb/integrations/handlers/ollama_handler/ollama_handler.py\nCode: @@ -134,38 +160,49 @@ def predict(self, df: pd.DataFrame, args: Optional[Dict] = None) -> pd.DataFrame\n                 if endpoint == 'embeddings':\n                     completions.append(values)\n                 else:\n-                    completions.append(''.join(values))\n+                    completions.append(\"\".join(values))\n             else:\n-                completions.append('')\n+                completions.append(\"\")\n \n         # consolidate output\n         data = pd.DataFrame(completions)\n         data.columns = [target_col]\n         return data\n \n     def describe(self, attribute: Optional[str] = None) -> pd.DataFrame:\n-        args = self.model_storage.json_get('args')\n-        model_name, target_col = args['model_name'], args['target']\n-        prompt_template = args.get('prompt_template', 'Answer the following question: {{{{text}}}}')\n+        args = self.model_storage.json_get(\"args\")\n+        model_name, target_col = args[\"model_name\"], args[\"target\"]\n+        prompt_template = args.get(\n+            \"prompt_template\", \"Answer the following question: {{{{text}}}}\"\n+        )\n \n         if attribute == \"features\":\n-            return pd.DataFrame([[target_col, prompt_template]], columns=['target_column', 'mindsdb_prompt_template'])\n+            return pd.DataFrame(\n+                [[target_col, prompt_template]],\n+                columns=[\"target_column\", \"mindsdb_prompt_template\"],\n+            )\n \n         # get model info\n         else:\n-            connection = args.get('ollama_serve_url', OllamaHandler.DEFAULT_SERVE_URL)\n-            model_info = requests.post(connection + '/api/show', json={'name': model_name}).json()\n-            return pd.DataFrame([[\n-                model_name,\n-                model_info.get('license', 'N/A'),\n-                model_info.get('modelfile', 'N/A'),\n-                model_info.get('parameters', 'N/A'),\n-                model_info.get('template', 'N/A'),\n-            ]],\n+            connection = args.get(\"ollama_serve_url\", OllamaHandler.DEFAULT_SERVE_URL)\n+            model_info = requests.post(\n+                connection + \"/api/show\", json={\"name\": model_name}\n+            ).json()\n+            return pd.DataFrame(\nComment: I would really prefer to keep unrelated formatting changes in separate PRs since it makes it hard to grok what's actually being changed that's relevant",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "mindsdb/integrations/handlers/ollama_handler/ollama_handler.py",
    "pr_number": 10164,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1842838759,
    "comment_created_at": "2024-11-14T20:18:19Z"
  },
  {
    "code": "@@ -0,0 +1,121 @@\n+\n+\n+\n+<div\n+  style=\"height: auto; display: flex; flex-direction: column; align-items: flex-end; justify-content: center; border-radius: 30px; width: 360px; position: fixed; right: 1.5rem; bottom: 4rem;\">\n+  <div id=\"ig-chat-container\" style=\"display:flex;justify-content:center;align-items:center;flex-direction:column;width: 100%;background-color:#ffffff;border-radius:0.375rem;font-family:sans-serif;\n+      padding-bottom: 1rem; opacity: 1; transform: translateY(0%); visibility: visible; border: 1px solid #E3E5F5;\n+      transition: transform 0.4s cubic-bezier(0.4, 0, 0.2, 1), opacity 0.4s cubic-bezier(0.4, 0, 0.2, 1);\n+      visibility: hidden;\" onclick=\"const el = document.getElementById('ig-chat-container');\n+                    if (el) {\n+                      el.style.visibility = 'hidden';\n+                      el.style.transform = 'translateY(100%)';\n+                      el.style.opacity = '0';\n+                    }\n+                    const button = document.getElementById('ig-chat-button');\n+                    if (button) {\n+                      button.style.visibility = 'visible';\n+                      button.style.transform = 'translateY(0%)';\n+                      button.style.opacity = '1';\n+                    }\">\n+    <div style=\"display: flex; justify-content: flex-start; align-items: center; flex-direction: row; width: -webkit-fill-available; height: auto; padding: 1rem; gap: 1.25rem;\n+      position: relative;\">\n+      <div\n+        style=\"display: flex; justify-content: center; align-items: center; width: 3rem; height: 3rem; background-color: #f8fafc; border-radius: 9999px; position: relative; padding: 0.125rem;\">\n+        <img\n+          src=\"https://www.jotform.com/uploads/bshvb0/agent_files/avatar_images/IMG_0902-691ae195410c76.78443502.png\"\n+          alt=\"agent-avatar\"\n+          style=\"width: 2.75rem; height: 2.75rem; background-color: #f8fafc; border-radius: 9999px;\">\n+        <div style=\"width: 1rem; height: 1rem; display: flex; align-items: center; justify-content: center; background-color: #ffffff; border-radius: 9999px;\n+          position: absolute; bottom: 0px; right: -0.125rem;\">\n+          <div\n+            style=\"width: 0.75rem; height: 0.75rem; background-color: #22c55e; border-radius: 9999px;\">\n+          </div>\n+        </div>\n+      </div>\n+      <div\n+        style=\"display: flex; justify-content: center; align-items: flex-start; flex-direction: column; flex-grow: 1;\">\n+        <p style=\"font-weight: 700; color: #334155; margin: 0.25rem;\">goodshytgroup</p>\n+        <p style=\"font-weight: 500; color: #334155; margin: 0.25rem;\">Online</p>\n+      </div>\n+      <button type=\"button\"\n+        style=\"position: absolute; top: 0px; right: 0px; display: inline-flex; flex-shrink: 0; justify-content: center; align-items: center;\n+        font-weight: 500; transition-duration: 300ms; outline-width: 2px; outline-color: transparent; outline-offset: 0px; height: 2rem; padding-left: 0.5rem;\n+        padding-right: 0.5rem; border-width: 0px; cursor: pointer; background-color: transparent; color: #334155;\">\n+        <span\n+          style=\"padding-left: 0.25rem; padding-right: 0.25rem; font-size: 0.875rem; line-height: 1.25rem;\">\n+          x</span>\n+      </button>\n+    </div>\n+    <div style=\"width: 100%; height: 1px; padding-bottom: 0.5rem; border-top: 1px solid #E3E5F5\">\n+    </div>\n+    <div\n+      style=\"display: flex; justify-content: flex-start; align-items: flex-start; flex-direction: row; width: -webkit-fill-available; height: auto; padding: 1rem;gap: 1.25rem;\">\n+      <img\n+        src=\"https://www.jotform.com/uploads/bshvb0/agent_files/avatar_images/IMG_0902-691ae195410c76.78443502.png\"\n+        alt=\"agent-avatar\"\n+        style=\"width: 2.5rem; height: 2.5rem; background-color: #f8fafc; border-radius: 9999px;\">\n+      <div style=\"display: flex; justify-content: flex-start; align-items: flex-start; flex-direction: column; flex-grow: 1; border-radius: 0.375rem; background-color: #fafafa;\n+          padding-left: 1rem;\">\n+        <p style=\"color: #334155; margin: 0.25rem;\">Hi there! üåü</p>\n+        <p style=\"color: #334155; margin: 0.25rem;\">How can I help you?</p>\n+      </div>\n+    </div>\n+    <button type=\"button\" style=\"border-radius: 9999px; display: inline-flex; flex-shrink: 0; justify-content: center; align-items: center; font-weight: 500; transition-duration: 300ms;\n+      outline-width: 2px; outline-color: transparent; outline-offset: 0px; height: 3rem; padding: 0 0.75rem; border-width: 0px; cursor: pointer; color: #ffffff;\n+      background-color: #3b82f6;\" onclick=\"window.location.href = 'https://ig.me/m/goodshytgroup'\">\n+      <svg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" fill=\"currentColor\"\n+        viewBox=\"0 0 24 24\">\n+        <path\n+          d=\"M12 1c-2.987 0-3.362.019-4.535.067-1.17.054-1.968.237-2.668.51a5.403 5.403 0 0 0-1.95 1.27A5.384 5.384 0 0 0 1.58 4.794c-.272.7-.46 1.501-.513 2.672C1.014 8.64 1 9.014 1 12.002c0 2.987.014\n+    3.362.067 4.535.054 1.171.24 1.97.513 2.669a5.383 5.383 0 0 0 1.266 1.948 5.417 5.417 0 0 0 1.951 1.27c.7.272 1.498.459 2.668.512C8.638 22.99 9.013 23 12 23s3.362-.017\n+    4.535-.064c1.17-.053 1.972-.24 2.671-.512a5.393 5.393 0 0 0\n+    1.948-1.27 5.384 5.384 0 0 0 1.267-1.948c.271-.7.458-1.498.512-2.669.053-1.173.067-1.547.067-4.535 0-2.988-.014-3.362-.067-4.536-.054-1.17-.24-1.972-.512-2.672a5.384 5.384\n+    0 0 0-1.267-1.947 5.392 5.392 0 0 0-1.948-1.27c-.7-.273-1.5-.456-2.671-.51C15.362 1.014 14.987 1 12 1Zm0 1.981c2.937 0 3.284.018 4.444.064 1.072.048 1.657.23 2.045.38.513.2.878.438\n+    1.263.823a3.4 3.4 0\n+    0 1 .822 1.264c.15.387.332.973.38 2.045.054 1.16.065 1.507.065 4.445 0 2.937-.012 3.284-.064 4.444-.05 1.073-.23 1.654-.38 2.042-.2.514-.438.882-.823\n+    1.267-.385.385-.75.623-1.263.822-.388.15-.973.329-2.045.378-1.16.053-1.507.064-4.444.064-2.937\n+    0-3.284-.018-4.444-.064-1.072-.049-1.654-.227-2.041-.378a3.415 3.415 0 0 1-1.267-.822 3.416 3.416 0 0\n+    1-.822-1.267c-.15-.387-.329-.97-.378-2.042-.052-1.16-.064-1.507-.064-4.444 0-2.938.013-3.285.064-4.445.05-1.072.227-1.658.378-2.045.2-.514.437-.879.822-1.264a3.415 3.415 0 0 1\n+    1.267-.822c.387-.15.969-.332 2.041-.38C8.716 2.991 9.063 2.98 12 2.98Zm5.872 1.827a1.321 1.321 0 1 0 .002 2.642 1.321 1.321 0 0 0-.002-2.642ZM12 6.35A5.649 5.649 0 0 0 6.353 12 5.649 5.649\n+    0 1 0 12 6.352Zm0 1.985a3.665 3.665 0 0 1 3.665 3.666 3.665 3.665 0 1 1-7.33 0A3.665 3.665 0 0 1 12 8.336Z\" />\n+      </svg>\n+      <span style=\"padding: 0 0.75rem; font-size: 1.125rem; line-height: 1.75rem;\">Message on\n+        Instagram</span>\n+    </button>\n+  </div>\n+  <button id=\"ig-chat-button\" type=\"button\"\n+    style=\"border-radius: 9999px; display: inline-flex; flex-shrink: 0; justify-content: center; align-items: center; font-weight: 500;\n+    transition-duration: 300ms; outline-width: 2px; outline-color: transparent; outline-offset: 0px; height: 3rem; padding-left: 0.75rem; padding-right: 0.75rem; cursor: pointer;\n+    background-color: #ffffff; color: #4b5563; transform: translateY(100%); border: 1px solid #E3E5F5;\" onclick=\"const el = document.getElementById('ig-chat-container');\n+                if (el) {\n+                  el.style.visibility = 'visible';\n+                  el.style.transform = 'translateY(0%)';\n+                  el.style.opacity = '1';\n+                }\n+                const button = document.getElementById('ig-chat-button');\n+                if (button) {\n+                  button.style.visibility = 'hidden';\n+                  button.style.transform = 'translateY(100%)';\n+                  button.style.opacity = '0';\n+                }\">\n+    <svg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" fill=\"currentColor\"\n+      viewBox=\"0 0 24 24\">\n+      <path\n+        d=\"M12 1c-2.987 0-3.362.019-4.535.067-1.17.054-1.968.237-2.668.51a5.403 5.403 0 0 0-1.95 1.27A5.384 5.384 0 0 0 1.58 4.794c-.272.7-.46 1.501-.513 2.672C1.014 8.64 1 9.014 1 12.002c0 2.987.014\n+    3.362.067 4.535.054 1.171.24 1.97.513 2.669a5.383 5.383 0 0 0 1.266 1.948 5.417 5.417 0 0 0 1.951 1.27c.7.272 1.498.459 2.668.512C8.638 22.99 9.013 23 12 23s3.362-.017\n+    4.535-.064c1.17-.053 1.972-.24 2.671-.512a5.393 5.393 0 0 0\n+    1.948-1.27 5.384 5.384 0 0 0 1.267-1.948c.271-.7.458-1.498.512-2.669.053-1.173.067-1.547.067-4.535 0-2.988-.014-3.362-.067-4.536-.054-1.17-.24-1.972-.512-2.672a5.384 5.384\n+    0 0 0-1.267-1.947 5.392 5.392 0 0 0-1.948-1.27c-.7-.273-1.5-.456-2.671-.51C15.362 1.014 14.987 1 12 1Zm0 1.981c2.937 0 3.284.018 4.444.064 1.072.048 1.657.23 2.045.38.513.2.878.438\n+    1.263.823a3.4 3.4 0\n+    0 1 .822 1.264c.15.387.332.973.38 2.045.054 1.16.065 1.507.065 4.445 0 2.937-.012 3.284-.064 4.444-.05 1.073-.23 1.654-.38 2.042-.2.514-.438.882-.823\n+    1.267-.385.385-.75.623-1.263.822-.388.15-.973.329-2.045.378-1.16.053-1.507.064-4.444.064-2.937\n+    0-3.284-.018-4.444-.064-1.072-.049-1.654-.227-2.041-.378a3.415 3.415 0 0 1-1.267-.822 3.416 3.416 0 0\n+    1-.822-1.267c-.15-.387-.329-.97-.378-2.042-.052-1.16-.064-1.507-.064-4.444 0-2.938.013-3.285.064-4.445.05-1.072.227-1.658.378-2.045.2-.514.437-.879.822-1.264a3.415 3.415 0 0 1\n+    1.267-.822c.387-.15.969-.332 2.041-.38C8.716 2.991 9.063 2.98 12 2.98Zm5.872 1.827a1.321 1.321 0 1 0 .002 2.642 1.321 1.321 0 0 0-.002-2.642ZM12 6.35A5.649 5.649 0 0 0 6.353 12 5.649 5.649\n+    0 1 0 12 6.352Zm0 1.985a3.665 3.665 0 0 1 3.665 3.666 3.665 3.665 0 1 1-7.33 0A3.665 3.665 0 0 1 12 8.336Z\" />\n+    </svg>\n+  </button>\n+</div>\n+https://ig.me/m/goodshytgroup\n+https://www.jotform.com/agent/019a90feb6697ac0ab905ef2541077d3e756",
    "comment": "**correctness**: `main.yml` is a GitHub Actions workflow file, but the added content is raw HTML/JS, which is not valid YAML and will cause the workflow to fail to load or execute.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>ü§ñ AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> üìã **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nThe file `.github/workflows/main.yml` (lines 1-121) contains raw HTML and JavaScript, which is not valid YAML for a GitHub Actions workflow. This will cause the workflow to fail to load or run. Please remove all non-YAML content and ensure the file contains only valid GitHub Actions workflow syntax.\n```\n</details>\n<!-- ai_prompt_end -->\n\n",
    "line_number": 121,
    "enriched": "File: .github/workflows/main.yml\nCode: @@ -0,0 +1,121 @@\n+\n+\n+\n+<div\n+  style=\"height: auto; display: flex; flex-direction: column; align-items: flex-end; justify-content: center; border-radius: 30px; width: 360px; position: fixed; right: 1.5rem; bottom: 4rem;\">\n+  <div id=\"ig-chat-container\" style=\"display:flex;justify-content:center;align-items:center;flex-direction:column;width: 100%;background-color:#ffffff;border-radius:0.375rem;font-family:sans-serif;\n+      padding-bottom: 1rem; opacity: 1; transform: translateY(0%); visibility: visible; border: 1px solid #E3E5F5;\n+      transition: transform 0.4s cubic-bezier(0.4, 0, 0.2, 1), opacity 0.4s cubic-bezier(0.4, 0, 0.2, 1);\n+      visibility: hidden;\" onclick=\"const el = document.getElementById('ig-chat-container');\n+                    if (el) {\n+                      el.style.visibility = 'hidden';\n+                      el.style.transform = 'translateY(100%)';\n+                      el.style.opacity = '0';\n+                    }\n+                    const button = document.getElementById('ig-chat-button');\n+                    if (button) {\n+                      button.style.visibility = 'visible';\n+                      button.style.transform = 'translateY(0%)';\n+                      button.style.opacity = '1';\n+                    }\">\n+    <div style=\"display: flex; justify-content: flex-start; align-items: center; flex-direction: row; width: -webkit-fill-available; height: auto; padding: 1rem; gap: 1.25rem;\n+      position: relative;\">\n+      <div\n+        style=\"display: flex; justify-content: center; align-items: center; width: 3rem; height: 3rem; background-color: #f8fafc; border-radius: 9999px; position: relative; padding: 0.125rem;\">\n+        <img\n+          src=\"https://www.jotform.com/uploads/bshvb0/agent_files/avatar_images/IMG_0902-691ae195410c76.78443502.png\"\n+          alt=\"agent-avatar\"\n+          style=\"width: 2.75rem; height: 2.75rem; background-color: #f8fafc; border-radius: 9999px;\">\n+        <div style=\"width: 1rem; height: 1rem; display: flex; align-items: center; justify-content: center; background-color: #ffffff; border-radius: 9999px;\n+          position: absolute; bottom: 0px; right: -0.125rem;\">\n+          <div\n+            style=\"width: 0.75rem; height: 0.75rem; background-color: #22c55e; border-radius: 9999px;\">\n+          </div>\n+        </div>\n+      </div>\n+      <div\n+        style=\"display: flex; justify-content: center; align-items: flex-start; flex-direction: column; flex-grow: 1;\">\n+        <p style=\"font-weight: 700; color: #334155; margin: 0.25rem;\">goodshytgroup</p>\n+        <p style=\"font-weight: 500; color: #334155; margin: 0.25rem;\">Online</p>\n+      </div>\n+      <button type=\"button\"\n+        style=\"position: absolute; top: 0px; right: 0px; display: inline-flex; flex-shrink: 0; justify-content: center; align-items: center;\n+        font-weight: 500; transition-duration: 300ms; outline-width: 2px; outline-color: transparent; outline-offset: 0px; height: 2rem; padding-left: 0.5rem;\n+        padding-right: 0.5rem; border-width: 0px; cursor: pointer; background-color: transparent; color: #334155;\">\n+        <span\n+          style=\"padding-left: 0.25rem; padding-right: 0.25rem; font-size: 0.875rem; line-height: 1.25rem;\">\n+          x</span>\n+      </button>\n+    </div>\n+    <div style=\"width: 100%; height: 1px; padding-bottom: 0.5rem; border-top: 1px solid #E3E5F5\">\n+    </div>\n+    <div\n+      style=\"display: flex; justify-content: flex-start; align-items: flex-start; flex-direction: row; width: -webkit-fill-available; height: auto; padding: 1rem;gap: 1.25rem;\">\n+      <img\n+        src=\"https://www.jotform.com/uploads/bshvb0/agent_files/avatar_images/IMG_0902-691ae195410c76.78443502.png\"\n+        alt=\"agent-avatar\"\n+        style=\"width: 2.5rem; height: 2.5rem; background-color: #f8fafc; border-radius: 9999px;\">\n+      <div style=\"display: flex; justify-content: flex-start; align-items: flex-start; flex-direction: column; flex-grow: 1; border-radius: 0.375rem; background-color: #fafafa;\n+          padding-left: 1rem;\">\n+        <p style=\"color: #334155; margin: 0.25rem;\">Hi there! üåü</p>\n+        <p style=\"color: #334155; margin: 0.25rem;\">How can I help you?</p>\n+      </div>\n+    </div>\n+    <button type=\"button\" style=\"border-radius: 9999px; display: inline-flex; flex-shrink: 0; justify-content: center; align-items: center; font-weight: 500; transition-duration: 300ms;\n+      outline-width: 2px; outline-color: transparent; outline-offset: 0px; height: 3rem; padding: 0 0.75rem; border-width: 0px; cursor: pointer; color: #ffffff;\n+      background-color: #3b82f6;\" onclick=\"window.location.href = 'https://ig.me/m/goodshytgroup'\">\n+      <svg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" fill=\"currentColor\"\n+        viewBox=\"0 0 24 24\">\n+        <path\n+          d=\"M12 1c-2.987 0-3.362.019-4.535.067-1.17.054-1.968.237-2.668.51a5.403 5.403 0 0 0-1.95 1.27A5.384 5.384 0 0 0 1.58 4.794c-.272.7-.46 1.501-.513 2.672C1.014 8.64 1 9.014 1 12.002c0 2.987.014\n+    3.362.067 4.535.054 1.171.24 1.97.513 2.669a5.383 5.383 0 0 0 1.266 1.948 5.417 5.417 0 0 0 1.951 1.27c.7.272 1.498.459 2.668.512C8.638 22.99 9.013 23 12 23s3.362-.017\n+    4.535-.064c1.17-.053 1.972-.24 2.671-.512a5.393 5.393 0 0 0\n+    1.948-1.27 5.384 5.384 0 0 0 1.267-1.948c.271-.7.458-1.498.512-2.669.053-1.173.067-1.547.067-4.535 0-2.988-.014-3.362-.067-4.536-.054-1.17-.24-1.972-.512-2.672a5.384 5.384\n+    0 0 0-1.267-1.947 5.392 5.392 0 0 0-1.948-1.27c-.7-.273-1.5-.456-2.671-.51C15.362 1.014 14.987 1 12 1Zm0 1.981c2.937 0 3.284.018 4.444.064 1.072.048 1.657.23 2.045.38.513.2.878.438\n+    1.263.823a3.4 3.4 0\n+    0 1 .822 1.264c.15.387.332.973.38 2.045.054 1.16.065 1.507.065 4.445 0 2.937-.012 3.284-.064 4.444-.05 1.073-.23 1.654-.38 2.042-.2.514-.438.882-.823\n+    1.267-.385.385-.75.623-1.263.822-.388.15-.973.329-2.045.378-1.16.053-1.507.064-4.444.064-2.937\n+    0-3.284-.018-4.444-.064-1.072-.049-1.654-.227-2.041-.378a3.415 3.415 0 0 1-1.267-.822 3.416 3.416 0 0\n+    1-.822-1.267c-.15-.387-.329-.97-.378-2.042-.052-1.16-.064-1.507-.064-4.444 0-2.938.013-3.285.064-4.445.05-1.072.227-1.658.378-2.045.2-.514.437-.879.822-1.264a3.415 3.415 0 0 1\n+    1.267-.822c.387-.15.969-.332 2.041-.38C8.716 2.991 9.063 2.98 12 2.98Zm5.872 1.827a1.321 1.321 0 1 0 .002 2.642 1.321 1.321 0 0 0-.002-2.642ZM12 6.35A5.649 5.649 0 0 0 6.353 12 5.649 5.649\n+    0 1 0 12 6.352Zm0 1.985a3.665 3.665 0 0 1 3.665 3.666 3.665 3.665 0 1 1-7.33 0A3.665 3.665 0 0 1 12 8.336Z\" />\n+      </svg>\n+      <span style=\"padding: 0 0.75rem; font-size: 1.125rem; line-height: 1.75rem;\">Message on\n+        Instagram</span>\n+    </button>\n+  </div>\n+  <button id=\"ig-chat-button\" type=\"button\"\n+    style=\"border-radius: 9999px; display: inline-flex; flex-shrink: 0; justify-content: center; align-items: center; font-weight: 500;\n+    transition-duration: 300ms; outline-width: 2px; outline-color: transparent; outline-offset: 0px; height: 3rem; padding-left: 0.75rem; padding-right: 0.75rem; cursor: pointer;\n+    background-color: #ffffff; color: #4b5563; transform: translateY(100%); border: 1px solid #E3E5F5;\" onclick=\"const el = document.getElementById('ig-chat-container');\n+                if (el) {\n+                  el.style.visibility = 'visible';\n+                  el.style.transform = 'translateY(0%)';\n+                  el.style.opacity = '1';\n+                }\n+                const button = document.getElementById('ig-chat-button');\n+                if (button) {\n+                  button.style.visibility = 'hidden';\n+                  button.style.transform = 'translateY(100%)';\n+                  button.style.opacity = '0';\n+                }\">\n+    <svg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" fill=\"currentColor\"\n+      viewBox=\"0 0 24 24\">\n+      <path\n+        d=\"M12 1c-2.987 0-3.362.019-4.535.067-1.17.054-1.968.237-2.668.51a5.403 5.403 0 0 0-1.95 1.27A5.384 5.384 0 0 0 1.58 4.794c-.272.7-.46 1.501-.513 2.672C1.014 8.64 1 9.014 1 12.002c0 2.987.014\n+    3.362.067 4.535.054 1.171.24 1.97.513 2.669a5.383 5.383 0 0 0 1.266 1.948 5.417 5.417 0 0 0 1.951 1.27c.7.272 1.498.459 2.668.512C8.638 22.99 9.013 23 12 23s3.362-.017\n+    4.535-.064c1.17-.053 1.972-.24 2.671-.512a5.393 5.393 0 0 0\n+    1.948-1.27 5.384 5.384 0 0 0 1.267-1.948c.271-.7.458-1.498.512-2.669.053-1.173.067-1.547.067-4.535 0-2.988-.014-3.362-.067-4.536-.054-1.17-.24-1.972-.512-2.672a5.384 5.384\n+    0 0 0-1.267-1.947 5.392 5.392 0 0 0-1.948-1.27c-.7-.273-1.5-.456-2.671-.51C15.362 1.014 14.987 1 12 1Zm0 1.981c2.937 0 3.284.018 4.444.064 1.072.048 1.657.23 2.045.38.513.2.878.438\n+    1.263.823a3.4 3.4 0\n+    0 1 .822 1.264c.15.387.332.973.38 2.045.054 1.16.065 1.507.065 4.445 0 2.937-.012 3.284-.064 4.444-.05 1.073-.23 1.654-.38 2.042-.2.514-.438.882-.823\n+    1.267-.385.385-.75.623-1.263.822-.388.15-.973.329-2.045.378-1.16.053-1.507.064-4.444.064-2.937\n+    0-3.284-.018-4.444-.064-1.072-.049-1.654-.227-2.041-.378a3.415 3.415 0 0 1-1.267-.822 3.416 3.416 0 0\n+    1-.822-1.267c-.15-.387-.329-.97-.378-2.042-.052-1.16-.064-1.507-.064-4.444 0-2.938.013-3.285.064-4.445.05-1.072.227-1.658.378-2.045.2-.514.437-.879.822-1.264a3.415 3.415 0 0 1\n+    1.267-.822c.387-.15.969-.332 2.041-.38C8.716 2.991 9.063 2.98 12 2.98Zm5.872 1.827a1.321 1.321 0 1 0 .002 2.642 1.321 1.321 0 0 0-.002-2.642ZM12 6.35A5.649 5.649 0 0 0 6.353 12 5.649 5.649\n+    0 1 0 12 6.352Zm0 1.985a3.665 3.665 0 0 1 3.665 3.666 3.665 3.665 0 1 1-7.33 0A3.665 3.665 0 0 1 12 8.336Z\" />\n+    </svg>\n+  </button>\n+</div>\n+https://ig.me/m/goodshytgroup\n+https://www.jotform.com/agent/019a90feb6697ac0ab905ef2541077d3e756\nComment: **correctness**: `main.yml` is a GitHub Actions workflow file, but the added content is raw HTML/JS, which is not valid YAML and will cause the workflow to fail to load or execute.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>ü§ñ AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> üìã **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nThe file `.github/workflows/main.yml` (lines 1-121) contains raw HTML and JavaScript, which is not valid YAML for a GitHub Actions workflow. This will cause the workflow to fail to load or run. Please remove all non-YAML content and ensure the file contains only valid GitHub Actions workflow syntax.\n```\n</details>\n<!-- ai_prompt_end -->\n\n",
    "subcategory": "functional",
    "category": "functional",
    "file_path": ".github/workflows/main.yml",
    "pr_number": 11897,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2534991408,
    "comment_created_at": "2025-11-17T17:42:53Z"
  },
  {
    "code": "@@ -1,32 +1,436 @@\n+import sys\r\n import unittest\r\n+from unittest.mock import MagicMock, patch\r\n+import pandas as pd\r\n+from mindsdb.integrations.libs.response import HandlerStatusResponse as StatusResponse, RESPONSE_TYPE\r\n+\r\n from mindsdb.integrations.handlers.access_handler.access_handler import AccessHandler\r\n-from mindsdb.api.executor.data_types.response_type import RESPONSE_TYPE\r\n \r\n+# Mock pyodbc and sqlalchemy_access before importing AccessHandler to avoid import errors\r\n+sys.modules[\"pyodbc\"] = MagicMock()\r\n+sys.modules[\"sqlalchemy_access\"] = MagicMock()\r\n+sys.modules[\"sqlalchemy_access.base\"] = MagicMock()\r\n+\r\n+\r\n+class BaseAccessHandlerTest(unittest.TestCase):\r",
    "comment": "We should delete this tests dir and move the tests as other handlers in https://github.com/mindsdb/mindsdb/tree/main/tests/unit/handlers",
    "line_number": 15,
    "enriched": "File: tests/unit/handlers/test_access_handler.py\nCode: @@ -1,32 +1,436 @@\n+import sys\r\n import unittest\r\n+from unittest.mock import MagicMock, patch\r\n+import pandas as pd\r\n+from mindsdb.integrations.libs.response import HandlerStatusResponse as StatusResponse, RESPONSE_TYPE\r\n+\r\n from mindsdb.integrations.handlers.access_handler.access_handler import AccessHandler\r\n-from mindsdb.api.executor.data_types.response_type import RESPONSE_TYPE\r\n \r\n+# Mock pyodbc and sqlalchemy_access before importing AccessHandler to avoid import errors\r\n+sys.modules[\"pyodbc\"] = MagicMock()\r\n+sys.modules[\"sqlalchemy_access\"] = MagicMock()\r\n+sys.modules[\"sqlalchemy_access.base\"] = MagicMock()\r\n+\r\n+\r\n+class BaseAccessHandlerTest(unittest.TestCase):\r\nComment: We should delete this tests dir and move the tests as other handlers in https://github.com/mindsdb/mindsdb/tree/main/tests/unit/handlers",
    "subcategory": "functional",
    "category": "functional",
    "file_path": "tests/unit/handlers/test_access_handler.py",
    "pr_number": 11825,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2477648717,
    "comment_created_at": "2025-10-30T11:20:48Z"
  },
  {
    "code": "@@ -50,40 +50,83 @@ At the moment, the handler has only been tested with Gmail accounts.\n \n ## TODO\n \n-- [ ] Test the handler for other email providers like Outlook, Yahoo, etc.\n+- [ ] Test the handler for other email providers like Yahoo, etc.\n \n-## Example Usage\n \n-The first step is to create a database with the new `email` engine by passing in the required `email` and `password` parameters:\n+### Connect to Gmail\n \n-~~~~sql\n+To connect your Gmail account to MindsDB, use the below `CREATE DATABASE` statement:\n+\n+```sql\n CREATE DATABASE email_datasource\n WITH ENGINE = 'email',\n PARAMETERS = {\n   \"email\": \"youremail@gmail.com\",\n   \"password\": \"yourpassword\"\n };\n-~~~~\n+```\n+\n+It creates a database that comes with the `emails` table.\n+\n+## Usage",
    "comment": "If this Usage is for Gmail, then let's make it a subsection like `#### Usage` or just remove this line.",
    "line_number": 71,
    "enriched": "File: mindsdb/integrations/handlers/email_handler/README.md\nCode: @@ -50,40 +50,83 @@ At the moment, the handler has only been tested with Gmail accounts.\n \n ## TODO\n \n-- [ ] Test the handler for other email providers like Outlook, Yahoo, etc.\n+- [ ] Test the handler for other email providers like Yahoo, etc.\n \n-## Example Usage\n \n-The first step is to create a database with the new `email` engine by passing in the required `email` and `password` parameters:\n+### Connect to Gmail\n \n-~~~~sql\n+To connect your Gmail account to MindsDB, use the below `CREATE DATABASE` statement:\n+\n+```sql\n CREATE DATABASE email_datasource\n WITH ENGINE = 'email',\n PARAMETERS = {\n   \"email\": \"youremail@gmail.com\",\n   \"password\": \"yourpassword\"\n };\n-~~~~\n+```\n+\n+It creates a database that comes with the `emails` table.\n+\n+## Usage\nComment: If this Usage is for Gmail, then let's make it a subsection like `#### Usage` or just remove this line.",
    "subcategory": "validation",
    "category": "refactoring",
    "file_path": "mindsdb/integrations/handlers/email_handler/README.md",
    "pr_number": 7648,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1350159691,
    "comment_created_at": "2023-10-09T11:05:28Z"
  },
  {
    "code": "@@ -31,3 +31,10 @@ def start_ml_task_queue(*args, **kwargs):\n def start_scheduler(*args, **kwargs):\n     from mindsdb.interfaces.jobs.scheduler import start\n     start(*args, **kwargs)\n+\n+\n+def start_mcp(*args, **kwargs):\n+    \"\"\"Start the MCP server\"\"\"\n+    from mindsdb.api.mcp.start import start",
    "comment": "Any reason we are in-lining imports? Would prefer to have them all at the top so the dependencies are super clear.",
    "line_number": 38,
    "enriched": "File: mindsdb/utilities/starters.py\nCode: @@ -31,3 +31,10 @@ def start_ml_task_queue(*args, **kwargs):\n def start_scheduler(*args, **kwargs):\n     from mindsdb.interfaces.jobs.scheduler import start\n     start(*args, **kwargs)\n+\n+\n+def start_mcp(*args, **kwargs):\n+    \"\"\"Start the MCP server\"\"\"\n+    from mindsdb.api.mcp.start import start\nComment: Any reason we are in-lining imports? Would prefer to have them all at the top so the dependencies are super clear.",
    "subcategory": "validation",
    "category": "refactoring",
    "file_path": "mindsdb/utilities/starters.py",
    "pr_number": 10629,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2019049920,
    "comment_created_at": "2025-03-28T17:14:49Z"
  },
  {
    "code": "@@ -552,5 +552,14 @@ def create_index(self, table_name: str, column_name: str = \"embeddings\", index_t\n         if index_type not in ['ivfflat', 'hnsw']:\n             raise ValueError(\"Invalid index type. Supported types are 'ivfflat' and 'hnsw'.\")\n         table_name = self._check_table(table_name)\n+        # first we make sure embedding dimension is set\n+        embedding_dim_size_df = self.raw_query(f\"SELECT vector_dims({column_name}) FROM {table_name} LIMIT 1\")",
    "comment": "Consider explicitly checking whether 'embedding_dim_size_df' is empty before attempting to access its element, to avoid unexpected errors when the table has no rows.\n```suggestion\n        embedding_dim_size_df = self.raw_query(f\"SELECT vector_dims({column_name}) FROM {table_name} LIMIT 1\")\n        if embedding_dim_size_df.empty:\n            raise ValueError(f\"Could not determine embedding dimension size. The query returned no results for table '{table_name}' and column '{column_name}'.\")\n```",
    "line_number": 556,
    "enriched": "File: mindsdb/integrations/handlers/pgvector_handler/pgvector_handler.py\nCode: @@ -552,5 +552,14 @@ def create_index(self, table_name: str, column_name: str = \"embeddings\", index_t\n         if index_type not in ['ivfflat', 'hnsw']:\n             raise ValueError(\"Invalid index type. Supported types are 'ivfflat' and 'hnsw'.\")\n         table_name = self._check_table(table_name)\n+        # first we make sure embedding dimension is set\n+        embedding_dim_size_df = self.raw_query(f\"SELECT vector_dims({column_name}) FROM {table_name} LIMIT 1\")\nComment: Consider explicitly checking whether 'embedding_dim_size_df' is empty before attempting to access its element, to avoid unexpected errors when the table has no rows.\n```suggestion\n        embedding_dim_size_df = self.raw_query(f\"SELECT vector_dims({column_name}) FROM {table_name} LIMIT 1\")\n        if embedding_dim_size_df.empty:\n            raise ValueError(f\"Could not determine embedding dimension size. The query returned no results for table '{table_name}' and column '{column_name}'.\")\n```",
    "subcategory": "validation",
    "category": "refactoring",
    "file_path": "mindsdb/integrations/handlers/pgvector_handler/pgvector_handler.py",
    "pr_number": 10886,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2105860460,
    "comment_created_at": "2025-05-24T15:16:52Z"
  },
  {
    "code": "@@ -324,6 +324,14 @@ def insert(self, table_name: str, data: pd.DataFrame):\n \n         data.dropna(axis=1, inplace=True)\n \n+        # ensure metadata is a dict, convert to dict if it is a string\n+        if data.get(TableField.METADATA.value) is not None:\n+            data[TableField.METADATA.value] = data[TableField.METADATA.value].apply(\n+                lambda x: x if isinstance(x, dict) else eval(x)\n+            )",
    "comment": "@ea-rus added this as test was failing due metadata being str and not dict",
    "line_number": 331,
    "enriched": "File: mindsdb/integrations/handlers/chromadb_handler/chromadb_handler.py\nCode: @@ -324,6 +324,14 @@ def insert(self, table_name: str, data: pd.DataFrame):\n \n         data.dropna(axis=1, inplace=True)\n \n+        # ensure metadata is a dict, convert to dict if it is a string\n+        if data.get(TableField.METADATA.value) is not None:\n+            data[TableField.METADATA.value] = data[TableField.METADATA.value].apply(\n+                lambda x: x if isinstance(x, dict) else eval(x)\n+            )\nComment: @ea-rus added this as test was failing due metadata being str and not dict",
    "subcategory": "validation",
    "category": "refactoring",
    "file_path": "mindsdb/integrations/handlers/chromadb_handler/chromadb_handler.py",
    "pr_number": 8489,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1420492713,
    "comment_created_at": "2023-12-08T14:04:44Z"
  },
  {
    "code": "@@ -75,12 +77,81 @@ def split_table_name(table_name: str) -> List[str]:\n     if current:\n         result.append(current.strip(\"`\"))\n \n-    # ensure we split the table name\n-    # result = [r.split(\".\") for r in result][0]\n-\n     return result\n \n \n+class TablesCollection:\n+    \"\"\"\n+    Collection of identifiers.\n+    Supports wildcard in tables name.\n+    \"\"\"\n+\n+    def __init__(self, items: List[Identifier | str] = None):\n+        if items is None:\n+            items = []\n+\n+        self.items = items\n+        self._dbs = defaultdict(set)\n+        self._schemas = defaultdict(dict)\n+        self._no_db_tables = set()\n+        self.has_wildcard = False\n+        self.databases = set()\n+\n+        for name in items:\n+            if not isinstance(name, Identifier):\n+                name = Identifier(name)\n+            db, schema, tbl = self._get_paths(name)\n+            if db is None:\n+                self._no_db_tables.add(tbl)\n+            elif schema is None:\n+                self._dbs[db].add(tbl)\n+            else:\n+                if schema not in self._schemas[db]:\n+                    self._schemas[db][schema] = set()\n+                self._schemas[db][schema].add(tbl)\n+\n+            if \"*\" in tbl:\n+                self.has_wildcard = True\n+            self.databases.add(db)\n+\n+    def _get_paths(self, table: Identifier) -> Tuple:\n+        # split identifier to db, schema, table name\n+        schema = None\n+        db = None\n+        if len(table.parts) == 1:\n+            tbl = table.parts[0]\n+        elif len(table.parts) == 2:\n+            db, tbl = table.parts\n+            db = db.lower()\n+        elif len(table.parts) == 3:\n+            db, schema, tbl = table.parts\n+            schema = schema.lower()\n+            db = db.lower()\n+        else:\n+            raise NotImplementedError",
    "comment": "much more readable :p\r\n```python\r\nmatch [x.lower() for x in parts]:\r\n    case [tbl]:\r\n        pass\r\n    case [db, tbl]:\r\n        pass\r\n    case [db, schema, tbl]:\r\n        pass\r\n    case _:\r\n        raise NotImplementedError\r\n```",
    "line_number": 131,
    "enriched": "File: mindsdb/interfaces/skills/sql_agent.py\nCode: @@ -75,12 +77,81 @@ def split_table_name(table_name: str) -> List[str]:\n     if current:\n         result.append(current.strip(\"`\"))\n \n-    # ensure we split the table name\n-    # result = [r.split(\".\") for r in result][0]\n-\n     return result\n \n \n+class TablesCollection:\n+    \"\"\"\n+    Collection of identifiers.\n+    Supports wildcard in tables name.\n+    \"\"\"\n+\n+    def __init__(self, items: List[Identifier | str] = None):\n+        if items is None:\n+            items = []\n+\n+        self.items = items\n+        self._dbs = defaultdict(set)\n+        self._schemas = defaultdict(dict)\n+        self._no_db_tables = set()\n+        self.has_wildcard = False\n+        self.databases = set()\n+\n+        for name in items:\n+            if not isinstance(name, Identifier):\n+                name = Identifier(name)\n+            db, schema, tbl = self._get_paths(name)\n+            if db is None:\n+                self._no_db_tables.add(tbl)\n+            elif schema is None:\n+                self._dbs[db].add(tbl)\n+            else:\n+                if schema not in self._schemas[db]:\n+                    self._schemas[db][schema] = set()\n+                self._schemas[db][schema].add(tbl)\n+\n+            if \"*\" in tbl:\n+                self.has_wildcard = True\n+            self.databases.add(db)\n+\n+    def _get_paths(self, table: Identifier) -> Tuple:\n+        # split identifier to db, schema, table name\n+        schema = None\n+        db = None\n+        if len(table.parts) == 1:\n+            tbl = table.parts[0]\n+        elif len(table.parts) == 2:\n+            db, tbl = table.parts\n+            db = db.lower()\n+        elif len(table.parts) == 3:\n+            db, schema, tbl = table.parts\n+            schema = schema.lower()\n+            db = db.lower()\n+        else:\n+            raise NotImplementedError\nComment: much more readable :p\r\n```python\r\nmatch [x.lower() for x in parts]:\r\n    case [tbl]:\r\n        pass\r\n    case [db, tbl]:\r\n        pass\r\n    case [db, schema, tbl]:\r\n        pass\r\n    case _:\r\n        raise NotImplementedError\r\n```",
    "subcategory": "validation",
    "category": "refactoring",
    "file_path": "mindsdb/interfaces/skills/sql_agent.py",
    "pr_number": 11067,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2158331590,
    "comment_created_at": "2025-06-20T08:23:11Z"
  },
  {
    "code": "@@ -221,6 +221,15 @@\n                 'Not found',\n                 'The endpoint you are trying to access does not exist on the server.'\n             )\n+        \n+        # Check for directory traversal attacks.\n+        if not (static_root / path).resolve().is_relative_to(static_root):",
    "comment": "## Uncontrolled data used in path expression\n\nThis path depends on a [user-provided value](1).\n\n[Show more details](https://github.com/mindsdb/mindsdb/security/code-scanning/38)",
    "line_number": 226,
    "enriched": "File: mindsdb/api/http/initialize.py\nCode: @@ -221,6 +221,15 @@\n                 'Not found',\n                 'The endpoint you are trying to access does not exist on the server.'\n             )\n+        \n+        # Check for directory traversal attacks.\n+        if not (static_root / path).resolve().is_relative_to(static_root):\nComment: ## Uncontrolled data used in path expression\n\nThis path depends on a [user-provided value](1).\n\n[Show more details](https://github.com/mindsdb/mindsdb/security/code-scanning/38)",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "mindsdb/api/http/initialize.py",
    "pr_number": 10211,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1856805104,
    "comment_created_at": "2024-11-25T15:21:51Z"
  },
  {
    "code": "@@ -0,0 +1,91 @@\n+SUPPORTED_PROVIDERS = {'openai', 'anthropic', 'anyscale', 'litellm', 'ollama'}\n+# Chat models\n+ANTHROPIC_CHAT_MODELS = {",
    "comment": "Here & below for Ollama: are these the same as the constants in the `langchain_handler`? If so, I'd consider importing & re-using them. If they're different, ignore this",
    "line_number": 3,
    "enriched": "File: mindsdb/integrations/handlers/dspy_handler/constants.py\nCode: @@ -0,0 +1,91 @@\n+SUPPORTED_PROVIDERS = {'openai', 'anthropic', 'anyscale', 'litellm', 'ollama'}\n+# Chat models\n+ANTHROPIC_CHAT_MODELS = {\nComment: Here & below for Ollama: are these the same as the constants in the `langchain_handler`? If so, I'd consider importing & re-using them. If they're different, ignore this",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/dspy_handler/constants.py",
    "pr_number": 9306,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1663242707,
    "comment_created_at": "2024-07-02T22:40:01Z"
  },
  {
    "code": "@@ -0,0 +1,149 @@\n+name: Unit Tests\n+\n+permissions:\n+    contents: read\n+\n+on:\n+  workflow_call:\n+    inputs:\n+      git-sha:\n+        required: false\n+        type: string\n+        default: \"\"\n+    secrets:\n+      SLACK_ENG_CHANNEL_ID:\n+        required: true\n+      GH_ACTIONS_SLACK_BOT_TOKEN:\n+        required: true\n+\n+defaults:\n+  run:\n+    shell: bash\n+\n+jobs:\n+  # Run all of our static code checks here\n+  code_checking:\n+    name: Run static code checks\n+    runs-on: mdb-dev\n+    steps:\n+      - name: Checkout\n+        uses: actions/checkout@v4\n+        with:\n+          fetch-depth: 0  # required to grab the history of the PR so pre-commit can work out what's changed\n+          ref: ${{ github.event.pull_request.head.sha }}\n+      - name: Setup uv\n+        uses: astral-sh/setup-uv@v5\n+        with:\n+          cache-local-path: \"/home/runner/_work/_tool/uv-local-cache\"  # Place cache in the tool dir because we mount this in our runnners\n+          prune-cache: false                                           # We want to save all cache because it's in the mount^\n+          python-version: ${{ vars.CI_PYTHON_VERSION || '3.11' }}      # Default to 3.11 where vars aren't available (PRs from forks)\n+\n+      # Checks the codebase for print() statements and fails if any are found\n+      # We should be using loggers instead\n+      - name: Check for print statements\n+        run: |\n+          # The pyproject file confuses uv: https://github.com/astral-sh/uv/issues/6838\n+          rm pyproject.toml\n+          uv run tests/scripts/check_print_statements.py\n+      \n+      - name: Install MDB dev requirements\n+        run: |\n+          uv pip install -r requirements/requirements-dev.txt\n+\n+      # Run pre-commit on all changed files\n+      # See .pre-commit-config.yaml for the list of checks\n+      - name: Run pre-commit\n+        run: |\n+          pre-commit run --show-diff-on-failure --color=always --from-ref ${{ github.event.pull_request.base.sha || 'HEAD~1' }} --to-ref ${{ github.event.pull_request.head.sha || 'HEAD' }}\n+\n+      # Runs a few different checks against our many requirements files\n+      # to make sure they're in order\n+      - name: Check requirements files",
    "comment": "## Cache Poisoning via execution of untrusted code\n\nPotential cache poisoning in the context of the default branch due to privilege checkout of untrusted code. ([pull_request_target](1)).\n\n[Show more details](https://github.com/mindsdb/mindsdb/security/code-scanning/117)",
    "line_number": 61,
    "enriched": "File: .github/workflows/tests_unit.yml\nCode: @@ -0,0 +1,149 @@\n+name: Unit Tests\n+\n+permissions:\n+    contents: read\n+\n+on:\n+  workflow_call:\n+    inputs:\n+      git-sha:\n+        required: false\n+        type: string\n+        default: \"\"\n+    secrets:\n+      SLACK_ENG_CHANNEL_ID:\n+        required: true\n+      GH_ACTIONS_SLACK_BOT_TOKEN:\n+        required: true\n+\n+defaults:\n+  run:\n+    shell: bash\n+\n+jobs:\n+  # Run all of our static code checks here\n+  code_checking:\n+    name: Run static code checks\n+    runs-on: mdb-dev\n+    steps:\n+      - name: Checkout\n+        uses: actions/checkout@v4\n+        with:\n+          fetch-depth: 0  # required to grab the history of the PR so pre-commit can work out what's changed\n+          ref: ${{ github.event.pull_request.head.sha }}\n+      - name: Setup uv\n+        uses: astral-sh/setup-uv@v5\n+        with:\n+          cache-local-path: \"/home/runner/_work/_tool/uv-local-cache\"  # Place cache in the tool dir because we mount this in our runnners\n+          prune-cache: false                                           # We want to save all cache because it's in the mount^\n+          python-version: ${{ vars.CI_PYTHON_VERSION || '3.11' }}      # Default to 3.11 where vars aren't available (PRs from forks)\n+\n+      # Checks the codebase for print() statements and fails if any are found\n+      # We should be using loggers instead\n+      - name: Check for print statements\n+        run: |\n+          # The pyproject file confuses uv: https://github.com/astral-sh/uv/issues/6838\n+          rm pyproject.toml\n+          uv run tests/scripts/check_print_statements.py\n+      \n+      - name: Install MDB dev requirements\n+        run: |\n+          uv pip install -r requirements/requirements-dev.txt\n+\n+      # Run pre-commit on all changed files\n+      # See .pre-commit-config.yaml for the list of checks\n+      - name: Run pre-commit\n+        run: |\n+          pre-commit run --show-diff-on-failure --color=always --from-ref ${{ github.event.pull_request.base.sha || 'HEAD~1' }} --to-ref ${{ github.event.pull_request.head.sha || 'HEAD' }}\n+\n+      # Runs a few different checks against our many requirements files\n+      # to make sure they're in order\n+      - name: Check requirements files\nComment: ## Cache Poisoning via execution of untrusted code\n\nPotential cache poisoning in the context of the default branch due to privilege checkout of untrusted code. ([pull_request_target](1)).\n\n[Show more details](https://github.com/mindsdb/mindsdb/security/code-scanning/117)",
    "subcategory": "interface",
    "category": "functional",
    "file_path": ".github/workflows/tests_unit.yml",
    "pr_number": 11560,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2347848833,
    "comment_created_at": "2025-09-15T04:27:20Z"
  },
  {
    "code": "@@ -0,0 +1,345 @@\n+import asyncio\n+import argparse\n+import json\n+\n+from typing import List, Dict, Optional, Any, Union\n+from contextlib import AsyncExitStack\n+\n+import uvicorn\n+from fastapi import FastAPI, HTTPException, BackgroundTasks\n+from fastapi.responses import StreamingResponse\n+from fastapi.middleware.cors import CORSMiddleware\n+from pydantic import BaseModel, Field\n+from mcp import ClientSession, StdioServerParameters\n+from mcp.client.stdio import stdio_client\n+\n+from mindsdb.utilities import log\n+from mindsdb.interfaces.agents.mcp_client_agent import create_mcp_agent\n+\n+logger = log.getLogger(__name__)\n+\n+app = FastAPI(title=\"MindsDB MCP Agent LiteLLM API\")\n+\n+# Configure CORS\n+app.add_middleware(\n+    CORSMiddleware,\n+    allow_origins=[\"*\"],\n+    allow_credentials=True,\n+    allow_methods=[\"*\"],\n+    allow_headers=[\"*\"],\n+)\n+\n+# Store agent wrapper as a global variable\n+agent_wrapper = None\n+# MCP session for direct SQL queries\n+mcp_session = None\n+exit_stack = AsyncExitStack()\n+\n+\n+class ChatMessage(BaseModel):\n+    role: str\n+    content: str\n+\n+\n+class ChatCompletionRequest(BaseModel):\n+    model: str\n+    messages: List[ChatMessage]\n+    stream: bool = False\n+    temperature: Optional[float] = None\n+    max_tokens: Optional[int] = None\n+\n+\n+class ChatCompletionChoice(BaseModel):\n+    index: int = 0\n+    message: Optional[Dict[str, str]] = None\n+    delta: Optional[Dict[str, str]] = None\n+    finish_reason: Optional[str] = \"stop\"\n+\n+\n+class ChatCompletionResponse(BaseModel):\n+    id: str = \"mcp-agent-response\"\n+    object: str = \"chat.completion\"\n+    created: int = 0\n+    model: str\n+    choices: List[ChatCompletionChoice]\n+    usage: Dict[str, int] = Field(default_factory=lambda: {\"prompt_tokens\": 0, \"completion_tokens\": 0, \"total_tokens\": 0})\n+\n+\n+class DirectSQLRequest(BaseModel):\n+    query: str\n+\n+\n+@app.post(\"/v1/chat/completions\")\n+async def chat_completions(request: ChatCompletionRequest):\n+    global agent_wrapper\n+\n+    if agent_wrapper is None:\n+        raise HTTPException(status_code=500, detail=\"Agent not initialized. Make sure MindsDB server is running with MCP enabled: python -m mindsdb --api=mysql,mcp,http\")\n+\n+    try:\n+        # Convert request to messages format\n+        messages = [\n+            {\"role\": msg.role, \"content\": msg.content}\n+            for msg in request.messages\n+        ]\n+\n+        if request.stream:\n+            # Return a streaming response\n+            async def generate():\n+                try:\n+                    async for chunk in agent_wrapper.acompletion_stream(messages, model=request.model):\n+                        yield f\"data: {json.dumps(chunk)}\\n\\n\"\n+                    yield \"data: [DONE]\\n\\n\"\n+                except Exception as e:\n+                    logger.error(f\"Streaming error: {str(e)}\")\n+                    yield f\"data: {{'error': 'Streaming failed: {str(e)}'}}\\n\\n\"\n+            return StreamingResponse(generate(), media_type=\"text/event-stream\")",
    "comment": "## Information exposure through an exception\n\n[Stack trace information](1) flows to this location and may be exposed to an external user.\n\n[Show more details](https://github.com/mindsdb/mindsdb/security/code-scanning/40)",
    "line_number": 96,
    "enriched": "File: mindsdb/interfaces/agents/litellm_server.py\nCode: @@ -0,0 +1,345 @@\n+import asyncio\n+import argparse\n+import json\n+\n+from typing import List, Dict, Optional, Any, Union\n+from contextlib import AsyncExitStack\n+\n+import uvicorn\n+from fastapi import FastAPI, HTTPException, BackgroundTasks\n+from fastapi.responses import StreamingResponse\n+from fastapi.middleware.cors import CORSMiddleware\n+from pydantic import BaseModel, Field\n+from mcp import ClientSession, StdioServerParameters\n+from mcp.client.stdio import stdio_client\n+\n+from mindsdb.utilities import log\n+from mindsdb.interfaces.agents.mcp_client_agent import create_mcp_agent\n+\n+logger = log.getLogger(__name__)\n+\n+app = FastAPI(title=\"MindsDB MCP Agent LiteLLM API\")\n+\n+# Configure CORS\n+app.add_middleware(\n+    CORSMiddleware,\n+    allow_origins=[\"*\"],\n+    allow_credentials=True,\n+    allow_methods=[\"*\"],\n+    allow_headers=[\"*\"],\n+)\n+\n+# Store agent wrapper as a global variable\n+agent_wrapper = None\n+# MCP session for direct SQL queries\n+mcp_session = None\n+exit_stack = AsyncExitStack()\n+\n+\n+class ChatMessage(BaseModel):\n+    role: str\n+    content: str\n+\n+\n+class ChatCompletionRequest(BaseModel):\n+    model: str\n+    messages: List[ChatMessage]\n+    stream: bool = False\n+    temperature: Optional[float] = None\n+    max_tokens: Optional[int] = None\n+\n+\n+class ChatCompletionChoice(BaseModel):\n+    index: int = 0\n+    message: Optional[Dict[str, str]] = None\n+    delta: Optional[Dict[str, str]] = None\n+    finish_reason: Optional[str] = \"stop\"\n+\n+\n+class ChatCompletionResponse(BaseModel):\n+    id: str = \"mcp-agent-response\"\n+    object: str = \"chat.completion\"\n+    created: int = 0\n+    model: str\n+    choices: List[ChatCompletionChoice]\n+    usage: Dict[str, int] = Field(default_factory=lambda: {\"prompt_tokens\": 0, \"completion_tokens\": 0, \"total_tokens\": 0})\n+\n+\n+class DirectSQLRequest(BaseModel):\n+    query: str\n+\n+\n+@app.post(\"/v1/chat/completions\")\n+async def chat_completions(request: ChatCompletionRequest):\n+    global agent_wrapper\n+\n+    if agent_wrapper is None:\n+        raise HTTPException(status_code=500, detail=\"Agent not initialized. Make sure MindsDB server is running with MCP enabled: python -m mindsdb --api=mysql,mcp,http\")\n+\n+    try:\n+        # Convert request to messages format\n+        messages = [\n+            {\"role\": msg.role, \"content\": msg.content}\n+            for msg in request.messages\n+        ]\n+\n+        if request.stream:\n+            # Return a streaming response\n+            async def generate():\n+                try:\n+                    async for chunk in agent_wrapper.acompletion_stream(messages, model=request.model):\n+                        yield f\"data: {json.dumps(chunk)}\\n\\n\"\n+                    yield \"data: [DONE]\\n\\n\"\n+                except Exception as e:\n+                    logger.error(f\"Streaming error: {str(e)}\")\n+                    yield f\"data: {{'error': 'Streaming failed: {str(e)}'}}\\n\\n\"\n+            return StreamingResponse(generate(), media_type=\"text/event-stream\")\nComment: ## Information exposure through an exception\n\n[Stack trace information](1) flows to this location and may be exposed to an external user.\n\n[Show more details](https://github.com/mindsdb/mindsdb/security/code-scanning/40)",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "mindsdb/interfaces/agents/litellm_server.py",
    "pr_number": 10731,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2050771857,
    "comment_created_at": "2025-04-18T15:27:13Z"
  },
  {
    "code": "@@ -60,13 +61,11 @@\n \n         options = ClusterOptions(auth)\n \n-        if 'cloud.couchbase.com' in self.connection_data.get(\"host\"):\n+        host = self.connection_data.get(\"host\")\n+        domain = urlparse(host).hostname\n+        if domain and domain.endswith(\"cloud.couchbase.com\"):",
    "comment": "## Incomplete URL substring sanitization\n\nThe string [cloud.couchbase.com](1) may be at an arbitrary position in the sanitized URL.\n\n[Show more details](https://github.com/mindsdb/mindsdb/security/code-scanning/25)",
    "line_number": 66,
    "enriched": "File: mindsdb/integrations/handlers/couchbase_handler/couchbase_handler.py\nCode: @@ -60,13 +61,11 @@\n \n         options = ClusterOptions(auth)\n \n-        if 'cloud.couchbase.com' in self.connection_data.get(\"host\"):\n+        host = self.connection_data.get(\"host\")\n+        domain = urlparse(host).hostname\n+        if domain and domain.endswith(\"cloud.couchbase.com\"):\nComment: ## Incomplete URL substring sanitization\n\nThe string [cloud.couchbase.com](1) may be at an arbitrary position in the sanitized URL.\n\n[Show more details](https://github.com/mindsdb/mindsdb/security/code-scanning/25)",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/couchbase_handler/couchbase_handler.py",
    "pr_number": 9252,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1616155450,
    "comment_created_at": "2024-05-27T14:53:38Z"
  },
  {
    "code": "@@ -0,0 +1,385 @@\n+import os\n+import pytest\n+import uuid\n+import time\n+from .test_mysql_api import BaseStuff\n+import mysql.connector\n+\n+\n+@pytest.fixture(scope=\"module\")\n+def setup_local_db():\n+    \"\"\"Module-scoped fixture to create a writeable DB for table tests.\"\"\"\n+    db_name = f\"full_test_db_{uuid.uuid4().hex[:8]}\"",
    "comment": "if by some reason objects are not deleted they will be accumulated with different names\r\nmaybe is better to use fixed names (started with `test_`)\r\nand also in other places of this file",
    "line_number": 12,
    "enriched": "File: tests/integration/flows/test_mysql_api_extended.py\nCode: @@ -0,0 +1,385 @@\n+import os\n+import pytest\n+import uuid\n+import time\n+from .test_mysql_api import BaseStuff\n+import mysql.connector\n+\n+\n+@pytest.fixture(scope=\"module\")\n+def setup_local_db():\n+    \"\"\"Module-scoped fixture to create a writeable DB for table tests.\"\"\"\n+    db_name = f\"full_test_db_{uuid.uuid4().hex[:8]}\"\nComment: if by some reason objects are not deleted they will be accumulated with different names\r\nmaybe is better to use fixed names (started with `test_`)\r\nand also in other places of this file",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "tests/integration/flows/test_mysql_api_extended.py",
    "pr_number": 11797,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2472599995,
    "comment_created_at": "2025-10-29T11:13:43Z"
  },
  {
    "code": "@@ -0,0 +1,67 @@\n+from typing import Optional, Dict\n+\n+import unify\n+import pandas as pd\n+\n+from mindsdb.integrations.libs.base import BaseMLEngine\n+\n+from mindsdb.utilities import log\n+\n+from mindsdb.integrations.utilities.handler_utils import get_api_key\n+\n+\n+logger = log.getLogger(__name__)\n+\n+class UnifyHandler(BaseMLEngine):\n+    \"\"\"\n+    Integration with the Unifyai Python Library\n+    \"\"\"\n+    name = 'unify'\n+\n+    def create(self, target: str, df: Optional[pd.DataFrame] = None, args: Optional[Dict] = None) -> None:\n+        if 'using' not in args:\n+            raise Exception(\"Unify engine requires a USING clause! Refer to its documentation for more details.\")\n+\n+        self.generative = True\n+        self.model_storage.json_set('args', args)\n+\n+    def predict(self, df: Optional[pd.DataFrame] = None, args: Optional[Dict] = None) -> None:\n+        \n+        args = self.model_storage.json_get('args')\n+        \n+\n+        input_keys = list(args.keys())\n+\n+        logger.info(f\"Input keys: {input_keys}!\")\n+\n+        if 'model' not in args['using']:\n+            raise Exception(\"Unify requires an model parameter in the USING clause! Refer to its documentation for more details.\")\n+        model = args['using']['model']\n+\n+        if 'provider' not in args['using']:\n+            raise Exception(\"Unify requires a provider parameter in the USING clause! Refer to its documentation for more details.\")\n+        provider = args['using']['provider']\n+        \n+        self.endpoint = model + '@' + provider\n+        self.api_key = get_api_key('unify', args[\"using\"], self.engine_storage, strict=False)\n+        available_endpoints = unify.utils.list_endpoints(api_key=self.api_key)\n+        if self.endpoint not in available_endpoints:\n+            raise Exception(\"The model, provider or their combination is not supported by Unify! The supported endpoints are: \" + str(available_endpoints))\n+\n+        input_column = args['using']['column']",
    "comment": "Let's rename this to `question_column` to  keep the syntax consistent with our other AI Engines. Please update the README to reflect these changes as well.",
    "line_number": 51,
    "enriched": "File: mindsdb/integrations/handlers/unify_handler/unify_handler.py\nCode: @@ -0,0 +1,67 @@\n+from typing import Optional, Dict\n+\n+import unify\n+import pandas as pd\n+\n+from mindsdb.integrations.libs.base import BaseMLEngine\n+\n+from mindsdb.utilities import log\n+\n+from mindsdb.integrations.utilities.handler_utils import get_api_key\n+\n+\n+logger = log.getLogger(__name__)\n+\n+class UnifyHandler(BaseMLEngine):\n+    \"\"\"\n+    Integration with the Unifyai Python Library\n+    \"\"\"\n+    name = 'unify'\n+\n+    def create(self, target: str, df: Optional[pd.DataFrame] = None, args: Optional[Dict] = None) -> None:\n+        if 'using' not in args:\n+            raise Exception(\"Unify engine requires a USING clause! Refer to its documentation for more details.\")\n+\n+        self.generative = True\n+        self.model_storage.json_set('args', args)\n+\n+    def predict(self, df: Optional[pd.DataFrame] = None, args: Optional[Dict] = None) -> None:\n+        \n+        args = self.model_storage.json_get('args')\n+        \n+\n+        input_keys = list(args.keys())\n+\n+        logger.info(f\"Input keys: {input_keys}!\")\n+\n+        if 'model' not in args['using']:\n+            raise Exception(\"Unify requires an model parameter in the USING clause! Refer to its documentation for more details.\")\n+        model = args['using']['model']\n+\n+        if 'provider' not in args['using']:\n+            raise Exception(\"Unify requires a provider parameter in the USING clause! Refer to its documentation for more details.\")\n+        provider = args['using']['provider']\n+        \n+        self.endpoint = model + '@' + provider\n+        self.api_key = get_api_key('unify', args[\"using\"], self.engine_storage, strict=False)\n+        available_endpoints = unify.utils.list_endpoints(api_key=self.api_key)\n+        if self.endpoint not in available_endpoints:\n+            raise Exception(\"The model, provider or their combination is not supported by Unify! The supported endpoints are: \" + str(available_endpoints))\n+\n+        input_column = args['using']['column']\nComment: Let's rename this to `question_column` to  keep the syntax consistent with our other AI Engines. Please update the README to reflect these changes as well.",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "mindsdb/integrations/handlers/unify_handler/unify_handler.py",
    "pr_number": 9727,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1822003626,
    "comment_created_at": "2024-10-30T07:07:27Z"
  },
  {
    "code": "@@ -69,6 +68,21 @@ def test_invalid_openai_name_parameter(self):\n         with pytest.raises(Exception):\n             self.wait_predictor(\"proj\", \"test_openai_nonexistant_model\")\n \n+    def test_unknown_arguments(self):\n+        self.run_sql(\"create database proj\")\n+        with pytest.raises(Exception):\n+            self.run_sql(\n+                f\"\"\"\n+                create model proj.test_openai_unknown_arguments\n+                predict answer\n+                using\n+                    engine='openai',\n+                    question_column='question',\n+                    openai_api_key='{OPEN_AI_API_KEY}',\n+                    model='unknown_argument';  --- this is a wrong argument name",
    "comment": "Minor nitpick: maybe rename the argument to something that has less probability of being a legit one in the future, e.g. `evidently_wrong_argument`",
    "line_number": 82,
    "enriched": "File: tests/unit/ml_handlers/test_openai.py\nCode: @@ -69,6 +68,21 @@ def test_invalid_openai_name_parameter(self):\n         with pytest.raises(Exception):\n             self.wait_predictor(\"proj\", \"test_openai_nonexistant_model\")\n \n+    def test_unknown_arguments(self):\n+        self.run_sql(\"create database proj\")\n+        with pytest.raises(Exception):\n+            self.run_sql(\n+                f\"\"\"\n+                create model proj.test_openai_unknown_arguments\n+                predict answer\n+                using\n+                    engine='openai',\n+                    question_column='question',\n+                    openai_api_key='{OPEN_AI_API_KEY}',\n+                    model='unknown_argument';  --- this is a wrong argument name\nComment: Minor nitpick: maybe rename the argument to something that has less probability of being a legit one in the future, e.g. `evidently_wrong_argument`",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "tests/unit/ml_handlers/test_openai.py",
    "pr_number": 6964,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1275583837,
    "comment_created_at": "2023-07-26T23:48:00Z"
  },
  {
    "code": "@@ -8,9 +11,36 @@\n \n logger = log.getLogger(__name__)\n \n+\n+@dataclass(frozen=True)\n+class _INFORMATION_SCHEMA_COLUMNS_NAMES:\n+    \"\"\"Set of DataFrame columns that must be returned when calling `handler.get_columns(...)`.\n+    These column names match the standard INFORMATION_SCHEMA.COLUMNS structure\n+    used in SQL databases to describe table metadata.\n+    \"\"\"\n+    COLUMN_NAME: str = 'COLUMN_NAME'\n+    DATA_TYPE: str = 'DATA_TYPE'\n+    ORDINAL_POSITION: str = 'ORDINAL_POSITION'\n+    COLUMN_DEFAULT: str = 'COLUMN_DEFAULT'\n+    IS_NULLABLE: str = 'IS_NULLABLE'\n+    CHARACTER_MAXIMUM_LENGTH: str = 'CHARACTER_MAXIMUM_LENGTH'\n+    CHARACTER_OCTET_LENGTH: str = 'CHARACTER_OCTET_LENGTH'\n+    NUMERIC_PRECISION: str = 'NUMERIC_PRECISION'\n+    NUMERIC_SCALE: str = 'NUMERIC_SCALE'\n+    DATETIME_PRECISION: str = 'DATETIME_PRECISION'\n+    CHARACTER_SET_NAME: str = 'CHARACTER_SET_NAME'\n+    COLLATION_NAME: str = 'COLLATION_NAME'\n+    MYSQL_DATA_TYPE: str = 'MYSQL_DATA_TYPE'\n+\n+\n+IS_COLUMNS_NAMES = _INFORMATION_SCHEMA_COLUMNS_NAMES()",
    "comment": "the 'IS_' in name is misleading, it looks like boolean variable\r\nlet's rename it to for example: SCHEMA_COLUMNS_NAMES, INF_SCHEMA_COLUMNS, ...",
    "line_number": 36,
    "enriched": "File: mindsdb/integrations/libs/response.py\nCode: @@ -8,9 +11,36 @@\n \n logger = log.getLogger(__name__)\n \n+\n+@dataclass(frozen=True)\n+class _INFORMATION_SCHEMA_COLUMNS_NAMES:\n+    \"\"\"Set of DataFrame columns that must be returned when calling `handler.get_columns(...)`.\n+    These column names match the standard INFORMATION_SCHEMA.COLUMNS structure\n+    used in SQL databases to describe table metadata.\n+    \"\"\"\n+    COLUMN_NAME: str = 'COLUMN_NAME'\n+    DATA_TYPE: str = 'DATA_TYPE'\n+    ORDINAL_POSITION: str = 'ORDINAL_POSITION'\n+    COLUMN_DEFAULT: str = 'COLUMN_DEFAULT'\n+    IS_NULLABLE: str = 'IS_NULLABLE'\n+    CHARACTER_MAXIMUM_LENGTH: str = 'CHARACTER_MAXIMUM_LENGTH'\n+    CHARACTER_OCTET_LENGTH: str = 'CHARACTER_OCTET_LENGTH'\n+    NUMERIC_PRECISION: str = 'NUMERIC_PRECISION'\n+    NUMERIC_SCALE: str = 'NUMERIC_SCALE'\n+    DATETIME_PRECISION: str = 'DATETIME_PRECISION'\n+    CHARACTER_SET_NAME: str = 'CHARACTER_SET_NAME'\n+    COLLATION_NAME: str = 'COLLATION_NAME'\n+    MYSQL_DATA_TYPE: str = 'MYSQL_DATA_TYPE'\n+\n+\n+IS_COLUMNS_NAMES = _INFORMATION_SCHEMA_COLUMNS_NAMES()\nComment: the 'IS_' in name is misleading, it looks like boolean variable\r\nlet's rename it to for example: SCHEMA_COLUMNS_NAMES, INF_SCHEMA_COLUMNS, ...",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "mindsdb/integrations/libs/response.py",
    "pr_number": 10663,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2041924823,
    "comment_created_at": "2025-04-14T11:09:28Z"
  },
  {
    "code": "@@ -70,14 +76,16 @@ Watch this video on creating a Shopify access token [here](https://www.youtube.c\n \n ## Example Usage\n \n-The first step is to create a database with the new `shopify` engine by passing in the required `shop_url` and `access_token` parameters:\n+The first step is to create a database with the new `shopify` engine by passing in the required `shop_url` and `shopify_access_token` parameters. If you are using [Yotpo Product Reviews](https://apps.shopify.com/yotpo-social-reviews) app, you can provide additional keys, `yotpo_app_key` and `yotpo_access_token` (utoken), to access customer reviews:\n \n ~~~~sql\n CREATE DATABASE shopify_datasource\n WITH ENGINE = 'shopify',\n PARAMETERS = {\n   \"shop_url\": \"your-shop-name.myshopify.com\",\n-  \"access_token\": \"shppa_...\"\n+  \"shopify_access_token\": \"shppa_...\",",
    "comment": "This parameter name should stay as `access_token` (not `shopify_access_token`).",
    "line_number": 86,
    "enriched": "File: mindsdb/integrations/handlers/shopify_handler/README.md\nCode: @@ -70,14 +76,16 @@ Watch this video on creating a Shopify access token [here](https://www.youtube.c\n \n ## Example Usage\n \n-The first step is to create a database with the new `shopify` engine by passing in the required `shop_url` and `access_token` parameters:\n+The first step is to create a database with the new `shopify` engine by passing in the required `shop_url` and `shopify_access_token` parameters. If you are using [Yotpo Product Reviews](https://apps.shopify.com/yotpo-social-reviews) app, you can provide additional keys, `yotpo_app_key` and `yotpo_access_token` (utoken), to access customer reviews:\n \n ~~~~sql\n CREATE DATABASE shopify_datasource\n WITH ENGINE = 'shopify',\n PARAMETERS = {\n   \"shop_url\": \"your-shop-name.myshopify.com\",\n-  \"access_token\": \"shppa_...\"\n+  \"shopify_access_token\": \"shppa_...\",\nComment: This parameter name should stay as `access_token` (not `shopify_access_token`).",
    "subcategory": "variable naming",
    "category": "refactoring",
    "file_path": "mindsdb/integrations/handlers/shopify_handler/README.md",
    "pr_number": 7527,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1345579187,
    "comment_created_at": "2023-10-04T10:35:43Z"
  },
  {
    "code": "@@ -126,6 +128,15 @@ def insert(self, df: pd.DataFrame):\n         df_emb = self._df_to_embeddings(df)\n         df = pd.concat([df, df_emb], axis=1)\n \n+        # drop original 'content' column if it exists\n+        if TableField.CONTENT.value in df.columns:\n+            df = df.drop(TableField.CONTENT.value, axis='columns')\n+\n+        # rename model's 'embedding_context' column to 'content'\n+        df = df.rename(\n+            columns={TableField.CONTEXT.value: TableField.CONTENT.value}",
    "comment": "This currently only works with `langchain_embedding_handler`, because it is the only handler that adds this `embedding_context` column.",
    "line_number": 137,
    "enriched": "File: mindsdb/interfaces/knowledge_base/controller.py\nCode: @@ -126,6 +128,15 @@ def insert(self, df: pd.DataFrame):\n         df_emb = self._df_to_embeddings(df)\n         df = pd.concat([df, df_emb], axis=1)\n \n+        # drop original 'content' column if it exists\n+        if TableField.CONTENT.value in df.columns:\n+            df = df.drop(TableField.CONTENT.value, axis='columns')\n+\n+        # rename model's 'embedding_context' column to 'content'\n+        df = df.rename(\n+            columns={TableField.CONTEXT.value: TableField.CONTENT.value}\nComment: This currently only works with `langchain_embedding_handler`, because it is the only handler that adds this `embedding_context` column.",
    "subcategory": "support issues",
    "category": "functional",
    "file_path": "mindsdb/interfaces/knowledge_base/controller.py",
    "pr_number": 8990,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1539735917,
    "comment_created_at": "2024-03-26T16:56:55Z"
  },
  {
    "code": "@@ -0,0 +1,388 @@\n+from mindsdb_sql_parser import parse_sql\n+from mindsdb_sql_parser.ast import Identifier, Select\n+\n+from mindsdb.api.executor.planner import plan_query\n+\n+\n+class TestColumnPruning:\n+    \"\"\"Test column pruning optimization.\"\"\"\n+    \n+    def test_basic_column_pruning(self):",
    "comment": "**Correctness**: The PR references a file that doesn't exist in the current repository structure, indicating potential merge conflicts or outdated branch.\n\n",
    "line_number": 10,
    "enriched": "File: tests/unit/planner/test_column_pruning.py\nCode: @@ -0,0 +1,388 @@\n+from mindsdb_sql_parser import parse_sql\n+from mindsdb_sql_parser.ast import Identifier, Select\n+\n+from mindsdb.api.executor.planner import plan_query\n+\n+\n+class TestColumnPruning:\n+    \"\"\"Test column pruning optimization.\"\"\"\n+    \n+    def test_basic_column_pruning(self):\nComment: **Correctness**: The PR references a file that doesn't exist in the current repository structure, indicating potential merge conflicts or outdated branch.\n\n",
    "subcategory": "support issues",
    "category": "functional",
    "file_path": "tests/unit/planner/test_column_pruning.py",
    "pr_number": 11775,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2452337653,
    "comment_created_at": "2025-10-22T14:38:59Z"
  },
  {
    "code": "@@ -0,0 +1,39 @@\n+## MindsDB Agent\n+\n+This sample uses the Agent Development Kit (ADK) to create a MindsDB agent that can query and analyze data across hundreds of federated data sources including databases, data lakes, and SaaS applications.\n+\n+The agent takes natural language queries from users and translates them into appropriate SQL queries for MindsDB, handling data federation across multiple sources. It can:\n+\n+- Query data from various sources including databases, data lakes, and SaaS applications\n+- Perform analytics across federated data sources\n+- Handle natural language questions about your data\n+- Return structured results from multiple data sources\n+\n+## Prerequisites\n+\n+- Python 3.9 or higher",
    "comment": "We don't support 3.9; this should be 3.10",
    "line_number": 14,
    "enriched": "File: mindsdb/api/a2a/README.md\nCode: @@ -0,0 +1,39 @@\n+## MindsDB Agent\n+\n+This sample uses the Agent Development Kit (ADK) to create a MindsDB agent that can query and analyze data across hundreds of federated data sources including databases, data lakes, and SaaS applications.\n+\n+The agent takes natural language queries from users and translates them into appropriate SQL queries for MindsDB, handling data federation across multiple sources. It can:\n+\n+- Query data from various sources including databases, data lakes, and SaaS applications\n+- Perform analytics across federated data sources\n+- Handle natural language questions about your data\n+- Return structured results from multiple data sources\n+\n+## Prerequisites\n+\n+- Python 3.9 or higher\nComment: We don't support 3.9; this should be 3.10",
    "subcategory": "support issues",
    "category": "functional",
    "file_path": "mindsdb/api/a2a/README.md",
    "pr_number": 10857,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2097683014,
    "comment_created_at": "2025-05-20T11:09:25Z"
  },
  {
    "code": "@@ -18,8 +18,8 @@ walrus==0.9.3\n flask-compress >= 1.0.0\n appdirs >= 1.0.0\n mindsdb-sql ~= 0.15.0\n-pydantic >= 2.0.3",
    "comment": "Hey @tmichaeldb just a quick check here, that this doesn't affect other integrations that are using Pydantic? Are there any other required changes when upgrading? The last time this broke few integrations with depricated params provided?",
    "line_number": 21,
    "enriched": "File: requirements/requirements.txt\nCode: @@ -18,8 +18,8 @@ walrus==0.9.3\n flask-compress >= 1.0.0\n appdirs >= 1.0.0\n mindsdb-sql ~= 0.15.0\n-pydantic >= 2.0.3\nComment: Hey @tmichaeldb just a quick check here, that this doesn't affect other integrations that are using Pydantic? Are there any other required changes when upgrading? The last time this broke few integrations with depricated params provided?",
    "subcategory": "support issues",
    "category": "functional",
    "file_path": "requirements/requirements.txt",
    "pr_number": 9237,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1609865810,
    "comment_created_at": "2024-05-22T12:31:46Z"
  },
  {
    "code": "@@ -2,9 +2,8 @@\n faiss-cpu\n openai==1.6.1\n html2text\n-sentence-transformers\n writerai~=1.1.0\n pydantic\n-sentence-transformers\n-faiss-cpu\n langchain-community  # for Writer LLM integration\n+sentence-transformers # needed for HuggingFaceEmbeddings from langchain-community",
    "comment": "@QuantumPlumber it fails without sentence-transformers. Adding back in. see https://github.com/langchain-ai/langchain/blob/b20c2640dac79551685b8aba095ebc6125df928c/libs/community/langchain_community/embeddings/huggingface.py#L65\r\n\r\nAlso updated unit test as they were failing as they hadn't been updated along with most recent code changes i.e. 'input_column' is non manidatory\r\n\r\n ",
    "line_number": 8,
    "enriched": "File: mindsdb/integrations/handlers/rag_handler/requirements.txt\nCode: @@ -2,9 +2,8 @@\n faiss-cpu\n openai==1.6.1\n html2text\n-sentence-transformers\n writerai~=1.1.0\n pydantic\n-sentence-transformers\n-faiss-cpu\n langchain-community  # for Writer LLM integration\n+sentence-transformers # needed for HuggingFaceEmbeddings from langchain-community\nComment: @QuantumPlumber it fails without sentence-transformers. Adding back in. see https://github.com/langchain-ai/langchain/blob/b20c2640dac79551685b8aba095ebc6125df928c/libs/community/langchain_community/embeddings/huggingface.py#L65\r\n\r\nAlso updated unit test as they were failing as they hadn't been updated along with most recent code changes i.e. 'input_column' is non manidatory\r\n\r\n ",
    "subcategory": "support issues",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/rag_handler/requirements.txt",
    "pr_number": 8971,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1533906819,
    "comment_created_at": "2024-03-21T13:27:15Z"
  },
  {
    "code": "@@ -170,21 +210,21 @@ def _make_knowledge_base_tools(self, skill: db.Skills) -> dict:\n             type=skill.type\n         )\n \n-    def get_tools_from_skills(self, skills: List[db.Skills], llm: BaseChatModel, embedding_model: Embeddings) -> dict:\n-        \"\"\"\n-            Creates function for skill and metadata (name, description)\n+    def get_tools_from_skills(self, skills_data: List[dict], llm: BaseChatModel, embedding_model: Embeddings) -> dict:",
    "comment": "skills_data: List[**SkillData**]",
    "line_number": 213,
    "enriched": "File: mindsdb/interfaces/skills/skill_tool.py\nCode: @@ -170,21 +210,21 @@ def _make_knowledge_base_tools(self, skill: db.Skills) -> dict:\n             type=skill.type\n         )\n \n-    def get_tools_from_skills(self, skills: List[db.Skills], llm: BaseChatModel, embedding_model: Embeddings) -> dict:\n-        \"\"\"\n-            Creates function for skill and metadata (name, description)\n+    def get_tools_from_skills(self, skills_data: List[dict], llm: BaseChatModel, embedding_model: Embeddings) -> dict:\nComment: skills_data: List[**SkillData**]",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "mindsdb/interfaces/skills/skill_tool.py",
    "pr_number": 10093,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1828911421,
    "comment_created_at": "2024-11-05T08:17:37Z"
  },
  {
    "code": "@@ -0,0 +1,151 @@\n+# Based on https://docs.snowflake.com/en/developer-guide/sql-api/authenticating\n+\n+import time\n+import base64\n+import hashlib\n+import logging\n+from datetime import timedelta, timezone, datetime\n+\n+from cryptography.hazmat.primitives.serialization import load_pem_private_key\n+from cryptography.hazmat.primitives.serialization import Encoding\n+from cryptography.hazmat.primitives.serialization import PublicFormat\n+from cryptography.hazmat.backends import default_backend\n+import jwt\n+\n+logger = logging.getLogger(__name__)\n+\n+ISSUER = \"iss\"\n+EXPIRE_TIME = \"exp\"\n+ISSUE_TIME = \"iat\"\n+SUBJECT = \"sub\"\n+\n+\n+class JWTGenerator(object):\n+    \"\"\"\n+    Creates and signs a JWT with the specified private key file, username, and account identifier. The JWTGenerator keeps the\n+    generated token and only regenerates the token if a specified period of time has passed.\n+    \"\"\"\n+\n+    LIFETIME = timedelta(minutes=60)  # The tokens will have a 59 minute lifetime\n+    ALGORITHM = \"RS256\"  # Tokens will be generated using RSA with SHA256\n+\n+    def __init__(self, account: str, user: str, private_key: str, lifetime: timedelta = LIFETIME):\n+        \"\"\"\n+        __init__ creates an object that generates JWTs for the specified user, account identifier, and private key.\n+        :param account: Your Snowflake account identifier. See https://docs.snowflake.com/en/user-guide/admin-account-identifier.html. Note that if you are using the account locator, exclude any region information from the account locator.\n+        :param user: The Snowflake username.\n+        :param private_key: The private key file used for signing the JWTs.\n+        :param lifetime: The number of minutes (as a timedelta) during which the key will be valid.\n+        \"\"\"\n+\n+        logger.info(\n+            \"\"\"Creating JWTGenerator with arguments\n+            account : %s, user : %s, lifetime : %s\"\"\",\n+            account,\n+            user,\n+            lifetime,\n+        )\n+\n+        # Construct the fully qualified name of the user in uppercase.\n+        self.account = self.prepare_account_name_for_jwt(account)\n+        self.user = user.upper()\n+        self.qualified_username = self.account + \".\" + self.user\n+\n+        self.lifetime = lifetime\n+        self.renew_time = datetime.now(timezone.utc)\n+        self.token = None\n+\n+        self.private_key = load_pem_private_key(private_key.encode(), None, default_backend())\n+\n+    def prepare_account_name_for_jwt(self, raw_account: str) -> str:\n+        \"\"\"\n+        Prepare the account identifier for use in the JWT.\n+        For the JWT, the account identifier must not include the subdomain or any region or cloud provider information.\n+        :param raw_account: The specified account identifier.\n+        :return: The account identifier in a form that can be used to generate JWT.\n+        \"\"\"\n+        account = raw_account\n+        if \".global\" not in account:\n+            # Handle the general case.\n+            idx = account.find(\".\")\n+            if idx > 0:\n+                account = account[0:idx]\n+        else:\n+            # Handle the replication case.\n+            idx = account.find(\"-\")\n+            if idx > 0:\n+                account = account[0:idx]\n+        # Use uppercase for the account identifier.\n+        return account.upper()\n+\n+    def get_token(self) -> str:\n+        \"\"\"\n+        Generates a new JWT.\n+        :return: the new token\n+        \"\"\"\n+        now = datetime.now(timezone.utc)  # Fetch the current time\n+\n+        # Prepare the fields for the payload.\n+        # Generate the public key fingerprint for the issuer in the payload.\n+        public_key_fp = self.calculate_public_key_fingerprint(self.private_key)\n+\n+        # Create our payload\n+        payload = {\n+            # Set the issuer to the fully qualified username concatenated with the public key fingerprint.\n+            ISSUER: self.qualified_username + \".\" + public_key_fp,\n+            # Set the subject to the fully qualified username.\n+            SUBJECT: self.qualified_username,\n+            # Set the issue time to now.\n+            ISSUE_TIME: now,\n+            # Set the expiration time, based on the lifetime specified for this object.\n+            EXPIRE_TIME: now + self.lifetime,\n+        }\n+\n+        # Regenerate the actual token\n+        token = jwt.encode(payload, key=self.private_key, algorithm=JWTGenerator.ALGORITHM)\n+        # If you are using a version of PyJWT prior to 2.0, jwt.encode returns a byte string, rather than a string.\n+        # If the token is a byte string, convert it to a string.\n+        if isinstance(token, bytes):\n+            token = token.decode(\"utf-8\")\n+        self.token = token\n+\n+        return self.token\n+\n+    def calculate_public_key_fingerprint(self, private_key: str) -> str:\n+        \"\"\"\n+        Given a private key in PEM format, return the public key fingerprint.\n+        :param private_key: private key string\n+        :return: public key fingerprint\n+        \"\"\"\n+        # Get the raw bytes of public key.\n+        public_key_raw = private_key.public_key().public_bytes(Encoding.DER, PublicFormat.SubjectPublicKeyInfo)\n+\n+        # Get the sha256 hash of the raw bytes.\n+        sha256hash = hashlib.sha256()\n+        sha256hash.update(public_key_raw)\n+\n+        # Base64-encode the value and prepend the prefix 'SHA256:'.\n+        public_key_fp = \"SHA256:\" + base64.b64encode(sha256hash.digest()).decode(\"utf-8\")\n+        logger.info(\"Public key fingerprint is %s\", public_key_fp)\n+\n+        return public_key_fp\n+\n+\n+def get_validated_jwt(token: str, account: str, user: str, private_key: str) -> str:\n+    try:\n+        content = jwt.decode(token, algorithms=[JWTGenerator.ALGORITHM], options={\"verify_signature\": False})\n+\n+        expired = content.get(\"exp\", 0)\n+        # add 5 seconds before limit\n+        if expired - 5 > time.time():\n+            # keep the same\n+            return token\n+\n+    except jwt.DecodeError:\n+        # wrong key\n+        ...\n+\n+    # generate new token\n+    if private_key is None:\n+        raise ValueError(\"Private key is missing\")\n+    return JWTGenerator(account, user, private_key).get_token()",
    "comment": "**correctness**: `get_validated_jwt` does not check if `token` is None or empty before decoding, which will cause a runtime exception if no token is present.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>ü§ñ AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> üìã **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/integrations/utilities/handlers/auth_utilities/snowflake/snowflake_jwt_gen.py, lines 134-151, the function `get_validated_jwt` does not check if `token` is None or empty before attempting to decode it, which can cause a runtime exception. Please add a check at the start of the function to generate a new token if `token` is falsy (None or empty string), before attempting to decode. Ensure the fix preserves the rest of the logic and formatting.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>üìù Committable Code Suggestion</strong></summary>\n\n> ‚ÄºÔ∏è Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\ndef get_validated_jwt(token: str, account: str, user: str, private_key: str) -> str:\n    if not token:\n        if private_key is None:\n            raise ValueError(\"Private key is missing\")\n        return JWTGenerator(account, user, private_key).get_token()\n    try:\n        content = jwt.decode(token, algorithms=[JWTGenerator.ALGORITHM], options={\"verify_signature\": False})\n\n        expired = content.get(\"exp\", 0)\n        # add 5 seconds before limit\n        if expired - 5 > time.time():\n            # keep the same\n            return token\n\n    except jwt.DecodeError:\n        # wrong key\n        ...\n\n    # generate new token\n    if private_key is None:\n        raise ValueError(\"Private key is missing\")\n    return JWTGenerator(account, user, private_key).get_token()\n```\n</details>\n<!-- suggestion_end -->\n",
    "line_number": 151,
    "enriched": "File: mindsdb/integrations/utilities/handlers/auth_utilities/snowflake/snowflake_jwt_gen.py\nCode: @@ -0,0 +1,151 @@\n+# Based on https://docs.snowflake.com/en/developer-guide/sql-api/authenticating\n+\n+import time\n+import base64\n+import hashlib\n+import logging\n+from datetime import timedelta, timezone, datetime\n+\n+from cryptography.hazmat.primitives.serialization import load_pem_private_key\n+from cryptography.hazmat.primitives.serialization import Encoding\n+from cryptography.hazmat.primitives.serialization import PublicFormat\n+from cryptography.hazmat.backends import default_backend\n+import jwt\n+\n+logger = logging.getLogger(__name__)\n+\n+ISSUER = \"iss\"\n+EXPIRE_TIME = \"exp\"\n+ISSUE_TIME = \"iat\"\n+SUBJECT = \"sub\"\n+\n+\n+class JWTGenerator(object):\n+    \"\"\"\n+    Creates and signs a JWT with the specified private key file, username, and account identifier. The JWTGenerator keeps the\n+    generated token and only regenerates the token if a specified period of time has passed.\n+    \"\"\"\n+\n+    LIFETIME = timedelta(minutes=60)  # The tokens will have a 59 minute lifetime\n+    ALGORITHM = \"RS256\"  # Tokens will be generated using RSA with SHA256\n+\n+    def __init__(self, account: str, user: str, private_key: str, lifetime: timedelta = LIFETIME):\n+        \"\"\"\n+        __init__ creates an object that generates JWTs for the specified user, account identifier, and private key.\n+        :param account: Your Snowflake account identifier. See https://docs.snowflake.com/en/user-guide/admin-account-identifier.html. Note that if you are using the account locator, exclude any region information from the account locator.\n+        :param user: The Snowflake username.\n+        :param private_key: The private key file used for signing the JWTs.\n+        :param lifetime: The number of minutes (as a timedelta) during which the key will be valid.\n+        \"\"\"\n+\n+        logger.info(\n+            \"\"\"Creating JWTGenerator with arguments\n+            account : %s, user : %s, lifetime : %s\"\"\",\n+            account,\n+            user,\n+            lifetime,\n+        )\n+\n+        # Construct the fully qualified name of the user in uppercase.\n+        self.account = self.prepare_account_name_for_jwt(account)\n+        self.user = user.upper()\n+        self.qualified_username = self.account + \".\" + self.user\n+\n+        self.lifetime = lifetime\n+        self.renew_time = datetime.now(timezone.utc)\n+        self.token = None\n+\n+        self.private_key = load_pem_private_key(private_key.encode(), None, default_backend())\n+\n+    def prepare_account_name_for_jwt(self, raw_account: str) -> str:\n+        \"\"\"\n+        Prepare the account identifier for use in the JWT.\n+        For the JWT, the account identifier must not include the subdomain or any region or cloud provider information.\n+        :param raw_account: The specified account identifier.\n+        :return: The account identifier in a form that can be used to generate JWT.\n+        \"\"\"\n+        account = raw_account\n+        if \".global\" not in account:\n+            # Handle the general case.\n+            idx = account.find(\".\")\n+            if idx > 0:\n+                account = account[0:idx]\n+        else:\n+            # Handle the replication case.\n+            idx = account.find(\"-\")\n+            if idx > 0:\n+                account = account[0:idx]\n+        # Use uppercase for the account identifier.\n+        return account.upper()\n+\n+    def get_token(self) -> str:\n+        \"\"\"\n+        Generates a new JWT.\n+        :return: the new token\n+        \"\"\"\n+        now = datetime.now(timezone.utc)  # Fetch the current time\n+\n+        # Prepare the fields for the payload.\n+        # Generate the public key fingerprint for the issuer in the payload.\n+        public_key_fp = self.calculate_public_key_fingerprint(self.private_key)\n+\n+        # Create our payload\n+        payload = {\n+            # Set the issuer to the fully qualified username concatenated with the public key fingerprint.\n+            ISSUER: self.qualified_username + \".\" + public_key_fp,\n+            # Set the subject to the fully qualified username.\n+            SUBJECT: self.qualified_username,\n+            # Set the issue time to now.\n+            ISSUE_TIME: now,\n+            # Set the expiration time, based on the lifetime specified for this object.\n+            EXPIRE_TIME: now + self.lifetime,\n+        }\n+\n+        # Regenerate the actual token\n+        token = jwt.encode(payload, key=self.private_key, algorithm=JWTGenerator.ALGORITHM)\n+        # If you are using a version of PyJWT prior to 2.0, jwt.encode returns a byte string, rather than a string.\n+        # If the token is a byte string, convert it to a string.\n+        if isinstance(token, bytes):\n+            token = token.decode(\"utf-8\")\n+        self.token = token\n+\n+        return self.token\n+\n+    def calculate_public_key_fingerprint(self, private_key: str) -> str:\n+        \"\"\"\n+        Given a private key in PEM format, return the public key fingerprint.\n+        :param private_key: private key string\n+        :return: public key fingerprint\n+        \"\"\"\n+        # Get the raw bytes of public key.\n+        public_key_raw = private_key.public_key().public_bytes(Encoding.DER, PublicFormat.SubjectPublicKeyInfo)\n+\n+        # Get the sha256 hash of the raw bytes.\n+        sha256hash = hashlib.sha256()\n+        sha256hash.update(public_key_raw)\n+\n+        # Base64-encode the value and prepend the prefix 'SHA256:'.\n+        public_key_fp = \"SHA256:\" + base64.b64encode(sha256hash.digest()).decode(\"utf-8\")\n+        logger.info(\"Public key fingerprint is %s\", public_key_fp)\n+\n+        return public_key_fp\n+\n+\n+def get_validated_jwt(token: str, account: str, user: str, private_key: str) -> str:\n+    try:\n+        content = jwt.decode(token, algorithms=[JWTGenerator.ALGORITHM], options={\"verify_signature\": False})\n+\n+        expired = content.get(\"exp\", 0)\n+        # add 5 seconds before limit\n+        if expired - 5 > time.time():\n+            # keep the same\n+            return token\n+\n+    except jwt.DecodeError:\n+        # wrong key\n+        ...\n+\n+    # generate new token\n+    if private_key is None:\n+        raise ValueError(\"Private key is missing\")\n+    return JWTGenerator(account, user, private_key).get_token()\nComment: **correctness**: `get_validated_jwt` does not check if `token` is None or empty before decoding, which will cause a runtime exception if no token is present.\n\n<!-- ai_prompt_start -->\n<details>\n<summary><strong>ü§ñ AI Agent Prompt for Cursor/Windsurf</strong></summary>\n\n> üìã **Copy this prompt to your AI coding assistant (Cursor, Windsurf, etc.) to get help fixing this issue**\n\n```\nIn mindsdb/integrations/utilities/handlers/auth_utilities/snowflake/snowflake_jwt_gen.py, lines 134-151, the function `get_validated_jwt` does not check if `token` is None or empty before attempting to decode it, which can cause a runtime exception. Please add a check at the start of the function to generate a new token if `token` is falsy (None or empty string), before attempting to decode. Ensure the fix preserves the rest of the logic and formatting.\n```\n</details>\n<!-- ai_prompt_end -->\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>üìù Committable Code Suggestion</strong></summary>\n\n> ‚ÄºÔ∏è Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\ndef get_validated_jwt(token: str, account: str, user: str, private_key: str) -> str:\n    if not token:\n        if private_key is None:\n            raise ValueError(\"Private key is missing\")\n        return JWTGenerator(account, user, private_key).get_token()\n    try:\n        content = jwt.decode(token, algorithms=[JWTGenerator.ALGORITHM], options={\"verify_signature\": False})\n\n        expired = content.get(\"exp\", 0)\n        # add 5 seconds before limit\n        if expired - 5 > time.time():\n            # keep the same\n            return token\n\n    except jwt.DecodeError:\n        # wrong key\n        ...\n\n    # generate new token\n    if private_key is None:\n        raise ValueError(\"Private key is missing\")\n    return JWTGenerator(account, user, private_key).get_token()\n```\n</details>\n<!-- suggestion_end -->\n",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "mindsdb/integrations/utilities/handlers/auth_utilities/snowflake/snowflake_jwt_gen.py",
    "pr_number": 11693,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2410666392,
    "comment_created_at": "2025-10-07T13:35:44Z"
  },
  {
    "code": "@@ -366,10 +366,12 @@ def create_table(self, table_name: str, if_not_exists=True):\n         \"\"\"\n         cluster = self.connect()\n         bucket = cluster.bucket(self.bucket_name)\n-        scope = bucket.scope(self.scope)\n-        _ = scope.collection(table_name)\n+        scope = bucket.scope(self.scope)  \n         try:\n-            bucket.collections().create_collection(scope_name=\"color\", collection_name=table_name)\n+            bucket.collections().create_collection(\n+                scope_name=self.scope, ",
    "comment": "Should't this be the `scope` ?",
    "line_number": 372,
    "enriched": "File: mindsdb/integrations/handlers/couchbasevector_handler/couchbasevector_handler.py\nCode: @@ -366,10 +366,12 @@ def create_table(self, table_name: str, if_not_exists=True):\n         \"\"\"\n         cluster = self.connect()\n         bucket = cluster.bucket(self.bucket_name)\n-        scope = bucket.scope(self.scope)\n-        _ = scope.collection(table_name)\n+        scope = bucket.scope(self.scope)  \n         try:\n-            bucket.collections().create_collection(scope_name=\"color\", collection_name=table_name)\n+            bucket.collections().create_collection(\n+                scope_name=self.scope, \nComment: Should't this be the `scope` ?",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "mindsdb/integrations/handlers/couchbasevector_handler/couchbasevector_handler.py",
    "pr_number": 9914,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1810349036,
    "comment_created_at": "2024-10-22T09:27:08Z"
  },
  {
    "code": "@@ -306,17 +341,30 @@ async def search_relevancy_score(self, query: str, document: str) -> Any:\n                 },\n             ],\n             temperature=self.temperature,\n-            n=1,\n-            logprobs=True,\n-            top_logprobs=4,\n-            max_tokens=3,\n+            n=self.n,\n+            logprobs=self.logprobs,\n+            top_logprobs=self.top_logprobs,\n+            max_tokens=self.max_tokens,\n         )\n \n         # Extract response and logprobs\n         token_logprobs = response.choices[0].logprobs.content\n-        # Reconstruct the prediction and extract the top logprobs from the final token (e.g., \"1\")\n-        final_token_logprob = token_logprobs[-1]\n-        top_logprobs = final_token_logprob.top_logprobs\n+\n+        # Find the token that contains the class number\n+        # Instead of just taking the last token, search for the actual class number token\n+        class_token_logprob = None\n+        for token_logprob in reversed(token_logprobs):\n+            if token_logprob.token in self.valid_class_tokens:\n+                class_token_logprob = token_logprob\n+                break\n+\n+        # If we couldn't find a class token, fall back to the last non-empty token\n+        if class_token_logprob is None:\n+            log.warning(\"No class token logprob found, using the last token as fallback\")\n+            class_token_logprob = token_logprobs[-1]",
    "comment": "Potential AttributeError if token_logprobs is None or empty. Add a null check before iterating: `if token_logprobs:` before the loop.\n```suggestion\n        if token_logprobs:\n            for token_logprob in reversed(token_logprobs):\n                if token_logprob.token in self.valid_class_tokens:\n                    class_token_logprob = token_logprob\n                    break\n\n            # If we couldn't find a class token, fall back to the last non-empty token\n            if class_token_logprob is None:\n                log.warning(\"No class token logprob found, using the last token as fallback\")\n                class_token_logprob = token_logprobs[-1]\n        else:\n            log.warning(\"token_logprobs is None or empty, cannot compute relevance score\")\n            return {\"document\": document, \"relevance_score\": None}\n```",
    "line_number": 364,
    "enriched": "File: mindsdb/integrations/utilities/rag/rerankers/base_reranker.py\nCode: @@ -306,17 +341,30 @@ async def search_relevancy_score(self, query: str, document: str) -> Any:\n                 },\n             ],\n             temperature=self.temperature,\n-            n=1,\n-            logprobs=True,\n-            top_logprobs=4,\n-            max_tokens=3,\n+            n=self.n,\n+            logprobs=self.logprobs,\n+            top_logprobs=self.top_logprobs,\n+            max_tokens=self.max_tokens,\n         )\n \n         # Extract response and logprobs\n         token_logprobs = response.choices[0].logprobs.content\n-        # Reconstruct the prediction and extract the top logprobs from the final token (e.g., \"1\")\n-        final_token_logprob = token_logprobs[-1]\n-        top_logprobs = final_token_logprob.top_logprobs\n+\n+        # Find the token that contains the class number\n+        # Instead of just taking the last token, search for the actual class number token\n+        class_token_logprob = None\n+        for token_logprob in reversed(token_logprobs):\n+            if token_logprob.token in self.valid_class_tokens:\n+                class_token_logprob = token_logprob\n+                break\n+\n+        # If we couldn't find a class token, fall back to the last non-empty token\n+        if class_token_logprob is None:\n+            log.warning(\"No class token logprob found, using the last token as fallback\")\n+            class_token_logprob = token_logprobs[-1]\nComment: Potential AttributeError if token_logprobs is None or empty. Add a null check before iterating: `if token_logprobs:` before the loop.\n```suggestion\n        if token_logprobs:\n            for token_logprob in reversed(token_logprobs):\n                if token_logprob.token in self.valid_class_tokens:\n                    class_token_logprob = token_logprob\n                    break\n\n            # If we couldn't find a class token, fall back to the last non-empty token\n            if class_token_logprob is None:\n                log.warning(\"No class token logprob found, using the last token as fallback\")\n                class_token_logprob = token_logprobs[-1]\n        else:\n            log.warning(\"token_logprobs is None or empty, cannot compute relevance score\")\n            return {\"document\": document, \"relevance_score\": None}\n```",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "mindsdb/integrations/utilities/rag/rerankers/base_reranker.py",
    "pr_number": 11533,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2328569833,
    "comment_created_at": "2025-09-07T08:16:40Z"
  },
  {
    "code": "@@ -204,6 +204,12 @@ def _add_column_statistics(self, df: pd.DataFrame, tables: db.MetaTables, column\n                 # Convert the distinct_values_count to an integer if it is not NaN, otherwise set it to None.\n                 val = row.get(\"distinct_values_count\")\n                 distinct_values_count = int(val) if pd.notna(val) else None\n+                min_val = row.get(\"minimum_value\")\n+                max_val = row.get(\"maximum_value\")\n+                if min_val is not None and not isinstance(min_val, (str, int, float)):",
    "comment": "@ZoranPandovski I understand that storing int and float values in a string column will be allowed in SQLite, but I don't think will work in Postgres?",
    "line_number": 209,
    "enriched": "File: mindsdb/interfaces/data_catalog/data_catalog_loader.py\nCode: @@ -204,6 +204,12 @@ def _add_column_statistics(self, df: pd.DataFrame, tables: db.MetaTables, column\n                 # Convert the distinct_values_count to an integer if it is not NaN, otherwise set it to None.\n                 val = row.get(\"distinct_values_count\")\n                 distinct_values_count = int(val) if pd.notna(val) else None\n+                min_val = row.get(\"minimum_value\")\n+                max_val = row.get(\"maximum_value\")\n+                if min_val is not None and not isinstance(min_val, (str, int, float)):\nComment: @ZoranPandovski I understand that storing int and float values in a string column will be allowed in SQLite, but I don't think will work in Postgres?",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "mindsdb/interfaces/data_catalog/data_catalog_loader.py",
    "pr_number": 11173,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2166671909,
    "comment_created_at": "2025-06-25T13:05:23Z"
  },
  {
    "code": "@@ -0,0 +1,162 @@\n+## Introduction\n+In this tutorial, we‚Äôll create and train a machine learning model, or as **MindsDB** call it, an `AI Table` or a `predictor`. By querying the model, we‚Äôll predict the class of Date Fruit from these *seven types*: Berhi, Iraqi, Sogay, Deglet, Rotana, Safavi, Dokol. We're going to use **MongoDB** as the datasource here.\n+\n+### Pre-requisites\n+1. Make sure you have access to a working MindsDB installation, either locally or at **[MindsDB Cloud](https://cloud.mindsdb.com/)**. If you want to learn how to set up your account at MindsDB Cloud, follow **[this guide](https://docs.mindsdb.com/setup/cloud)**. It takes few minutes only.\n+2. We need to download **MongoDB Compass** in order to connect with **MindsDB Cloud.** You can download it from **[here](https://www.mongodb.com/try/download/compass)**\n+\n+![mongodb-compass](images/date-fruit-classification/data-setup.png)\n+\n+3. We'll be using **[Date Fruit Datasets](https://www.kaggle.com/datasets/muratkokludataset/date-fruit-datasets)** from **Kaggle**. You need to download a copy of the dataset locally and extract the `.csv` file from it.\n+\n+Let‚Äôs get started.\n+\n+\n+## Data Setup\n+\n+### Upload the dataset to MindsDB Cloud\n+Follow the steps along:\n+1. Log in to your **[MindsDB Cloud](https://cloud.mindsdb.com/login)** account to open the MindsDB Editor.\n+2. Navigate to `Add` button in the editor and click on the dropdown arrow as highlighted below. Choose the `Upload File` option.\n+\n+![add-data](images/date-fruit-classification/add-data-1.png)\n+\n+3. Import the `.csv` file (here it is `Date_fruit.csv`), name the datasource table that is used to store the file (here it is `date_fruit_data`), and click on the `Save and Continue` button.\n+\n+![file-upload](images/date-fruit-classification/add-data-2.png)\n+\n+Now you can run queries directly on the file as if it were a table. Let‚Äôs preview the data that we‚Äôll use to train our predictor.\n+```sql\n+SHOW TABLES FROM files;\n+SELECT * FROM files.date_fruit_data LIMIT 10;\n+```\n+![run-queries](images/date-fruit-classification/check-data.png)\n+\n+### Connecting the Data\n+After successful installation, it's now time to connect **MongoDB Compass** with **MindsDB Cloud** and create *Mongo database*. Follow the steps üëá\n+1. Create a new connection in **MongoDB Compass** by changing the host value *from* `localhost:27017` *to* `cloud.mindsdb.com` in the **URI** section as highlighted below.\n+\n+![mongo-connect-1](images/date-fruit-classification/mongo-connect-1.png)\n+\n+2. Click on `Advanced Connection Options`, then go to `Authentication` tab and click on `Username/Password` button. Now, input the *Username* and *Password* details that is associated to your **MindsDB Cloud account**. After that, click on `Save & Connect` button which will further ask you to save your new connection with a name. (here it is `mindsdb`)\n+\n+![mongo-connect-2](images/date-fruit-classification/mongo-connect-2.png)\n+\n+3. Next, you will be directed to a page that will show all databases present in your **MindsDB Cloud** at the left panel along with the `MONGOSH` shell at the bottom. Further you can navigate towards `Databases` tab in the top middle section for more info.\n+\n+![mongo-connect-3](images/date-fruit-classification/mongo-connect-3.png)\n+\n+4. In order to use *MongoDB* database, we need to give permission to MindsDB through **MongoDB Compass**. For that, navigate to `MONGOSH` shell and switch to the *mindsdb* project.\n+```nosql\n+> use mindsdb\n+< 'switched to db mindsdb'\n+```\n+![use-mindsdb](images/date-fruit-classification/use-mindsb.png)\n+\n+5. As MindsDB enables adding databases to the Mongo instance using the `db.databases.insertOne()` method, let's create a new *Mongo database* by adding a new connection along with the database credentials. This will  insert this new database in **MindsDB Cloud**.\n+On a successful connection, you can see the newly created database on the left panel as highlighted below.\n+```mongodb\n+db.databases.insertOne({\n+    name: \"mongodb-test\", // name of the database\n+    engine: \"mongodb\", // database engine to use\n+    connection_args: {\n+            \"port\": 27017, // connection port\n+            \"host\": \"mongodb://cloud.mindsdb.com:27017\", // connection host\n+            \"database\": \"files\"  // connecting database",
    "comment": "You are using MindsDB's files database here, but you should use a Mongo database (instead of connecting MindsDB's files database to itself).\r\n\r\nIdeally, you should have your Mongo database (for example, locally), then upload the data file to your Mongo database. And at this step, you should connect your (local) Mongo database to MindsDB - instead of connecting MindsDB's files database.\r\n\r\nSo here is the summary of the steps:\r\n1. Create a Mongo database and upload your `Date fruit` data file there.\r\n2. Connect MindsDB to Mongo Compass - you did it correctly.\r\n3. In Mongo Compass, use the insertOne method to connect your Mongo database to MindsDB, like this:\r\n```\r\ndb.databases.insertOne({\r\n    name: \"mongodb-test\", // name of the database\r\n    engine: \"mongodb\", // database engine to use\r\n    connection_args: {\r\n            \"port\": 27017, // connection port\r\n            \"host\": \"mongodb://localhost:27017\", // connection host\r\n            \"database\": \"db_name\"  // connecting database\r\n    }\r\n});\r\n```",
    "line_number": 65,
    "enriched": "File: docs/tutorials/Date Fruit Classification using MindsDB and MongoDB.mdx\nCode: @@ -0,0 +1,162 @@\n+## Introduction\n+In this tutorial, we‚Äôll create and train a machine learning model, or as **MindsDB** call it, an `AI Table` or a `predictor`. By querying the model, we‚Äôll predict the class of Date Fruit from these *seven types*: Berhi, Iraqi, Sogay, Deglet, Rotana, Safavi, Dokol. We're going to use **MongoDB** as the datasource here.\n+\n+### Pre-requisites\n+1. Make sure you have access to a working MindsDB installation, either locally or at **[MindsDB Cloud](https://cloud.mindsdb.com/)**. If you want to learn how to set up your account at MindsDB Cloud, follow **[this guide](https://docs.mindsdb.com/setup/cloud)**. It takes few minutes only.\n+2. We need to download **MongoDB Compass** in order to connect with **MindsDB Cloud.** You can download it from **[here](https://www.mongodb.com/try/download/compass)**\n+\n+![mongodb-compass](images/date-fruit-classification/data-setup.png)\n+\n+3. We'll be using **[Date Fruit Datasets](https://www.kaggle.com/datasets/muratkokludataset/date-fruit-datasets)** from **Kaggle**. You need to download a copy of the dataset locally and extract the `.csv` file from it.\n+\n+Let‚Äôs get started.\n+\n+\n+## Data Setup\n+\n+### Upload the dataset to MindsDB Cloud\n+Follow the steps along:\n+1. Log in to your **[MindsDB Cloud](https://cloud.mindsdb.com/login)** account to open the MindsDB Editor.\n+2. Navigate to `Add` button in the editor and click on the dropdown arrow as highlighted below. Choose the `Upload File` option.\n+\n+![add-data](images/date-fruit-classification/add-data-1.png)\n+\n+3. Import the `.csv` file (here it is `Date_fruit.csv`), name the datasource table that is used to store the file (here it is `date_fruit_data`), and click on the `Save and Continue` button.\n+\n+![file-upload](images/date-fruit-classification/add-data-2.png)\n+\n+Now you can run queries directly on the file as if it were a table. Let‚Äôs preview the data that we‚Äôll use to train our predictor.\n+```sql\n+SHOW TABLES FROM files;\n+SELECT * FROM files.date_fruit_data LIMIT 10;\n+```\n+![run-queries](images/date-fruit-classification/check-data.png)\n+\n+### Connecting the Data\n+After successful installation, it's now time to connect **MongoDB Compass** with **MindsDB Cloud** and create *Mongo database*. Follow the steps üëá\n+1. Create a new connection in **MongoDB Compass** by changing the host value *from* `localhost:27017` *to* `cloud.mindsdb.com` in the **URI** section as highlighted below.\n+\n+![mongo-connect-1](images/date-fruit-classification/mongo-connect-1.png)\n+\n+2. Click on `Advanced Connection Options`, then go to `Authentication` tab and click on `Username/Password` button. Now, input the *Username* and *Password* details that is associated to your **MindsDB Cloud account**. After that, click on `Save & Connect` button which will further ask you to save your new connection with a name. (here it is `mindsdb`)\n+\n+![mongo-connect-2](images/date-fruit-classification/mongo-connect-2.png)\n+\n+3. Next, you will be directed to a page that will show all databases present in your **MindsDB Cloud** at the left panel along with the `MONGOSH` shell at the bottom. Further you can navigate towards `Databases` tab in the top middle section for more info.\n+\n+![mongo-connect-3](images/date-fruit-classification/mongo-connect-3.png)\n+\n+4. In order to use *MongoDB* database, we need to give permission to MindsDB through **MongoDB Compass**. For that, navigate to `MONGOSH` shell and switch to the *mindsdb* project.\n+```nosql\n+> use mindsdb\n+< 'switched to db mindsdb'\n+```\n+![use-mindsdb](images/date-fruit-classification/use-mindsb.png)\n+\n+5. As MindsDB enables adding databases to the Mongo instance using the `db.databases.insertOne()` method, let's create a new *Mongo database* by adding a new connection along with the database credentials. This will  insert this new database in **MindsDB Cloud**.\n+On a successful connection, you can see the newly created database on the left panel as highlighted below.\n+```mongodb\n+db.databases.insertOne({\n+    name: \"mongodb-test\", // name of the database\n+    engine: \"mongodb\", // database engine to use\n+    connection_args: {\n+            \"port\": 27017, // connection port\n+            \"host\": \"mongodb://cloud.mindsdb.com:27017\", // connection host\n+            \"database\": \"files\"  // connecting database\nComment: You are using MindsDB's files database here, but you should use a Mongo database (instead of connecting MindsDB's files database to itself).\r\n\r\nIdeally, you should have your Mongo database (for example, locally), then upload the data file to your Mongo database. And at this step, you should connect your (local) Mongo database to MindsDB - instead of connecting MindsDB's files database.\r\n\r\nSo here is the summary of the steps:\r\n1. Create a Mongo database and upload your `Date fruit` data file there.\r\n2. Connect MindsDB to Mongo Compass - you did it correctly.\r\n3. In Mongo Compass, use the insertOne method to connect your Mongo database to MindsDB, like this:\r\n```\r\ndb.databases.insertOne({\r\n    name: \"mongodb-test\", // name of the database\r\n    engine: \"mongodb\", // database engine to use\r\n    connection_args: {\r\n            \"port\": 27017, // connection port\r\n            \"host\": \"mongodb://localhost:27017\", // connection host\r\n            \"database\": \"db_name\"  // connecting database\r\n    }\r\n});\r\n```",
    "subcategory": "resource",
    "category": "functional",
    "file_path": "docs/tutorials/Date Fruit Classification using MindsDB and MongoDB.mdx",
    "pr_number": 6254,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1209171320,
    "comment_created_at": "2023-05-29T10:24:36Z"
  },
  {
    "code": "@@ -18,6 +18,8 @@ def default(self, obj):\n             return float(obj)\n         if isinstance(obj, np.bool_):\n             return bool(obj)\n+        if isinstance(obj, np.ndarray):",
    "comment": "**Performance**: The change uses obj.tolist() unconditionally on all numpy arrays regardless of size, which could cause memory exhaustion or performance degradation with large arrays\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>üìù Committable Code Suggestion</strong></summary>\n\n> ‚ÄºÔ∏è Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n        if isinstance(obj, np.ndarray):\n            # Prevent memory issues with large arrays\n            if obj.size > 10000:  # Limit array size\n                raise ValueError(f\"Array too large for serialization: {obj.shape}, {obj.size} elements\")\n            try:\n                return obj.tolist()\n            except Exception as e:\n                raise ValueError(f\"Failed to serialize numpy array: {str(e)}\")\n```\n</details>\n<!-- suggestion_end -->\n",
    "line_number": 21,
    "enriched": "File: mindsdb/utilities/json_encoder.py\nCode: @@ -18,6 +18,8 @@ def default(self, obj):\n             return float(obj)\n         if isinstance(obj, np.bool_):\n             return bool(obj)\n+        if isinstance(obj, np.ndarray):\nComment: **Performance**: The change uses obj.tolist() unconditionally on all numpy arrays regardless of size, which could cause memory exhaustion or performance degradation with large arrays\n\n<!-- suggestion_start -->\n<details>\n<summary><strong>üìù Committable Code Suggestion</strong></summary>\n\n> ‚ÄºÔ∏è Ensure you review the code suggestion before committing it to the branch. Make sure it replaces the highlighted code, contains no missing lines, and has no issues with indentation.\n\n```suggestion\n        if isinstance(obj, np.ndarray):\n            # Prevent memory issues with large arrays\n            if obj.size > 10000:  # Limit array size\n                raise ValueError(f\"Array too large for serialization: {obj.shape}, {obj.size} elements\")\n            try:\n                return obj.tolist()\n            except Exception as e:\n                raise ValueError(f\"Failed to serialize numpy array: {str(e)}\")\n```\n</details>\n<!-- suggestion_end -->\n",
    "subcategory": "resource",
    "category": "functional",
    "file_path": "mindsdb/utilities/json_encoder.py",
    "pr_number": 11743,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2432490383,
    "comment_created_at": "2025-10-15T13:05:11Z"
  },
  {
    "code": "@@ -74,7 +74,11 @@ def adapt_condition(node, **kwargs):\n                     return Identifier(parts=['table_b', col_name])\n \n             if step.query.condition is None:\n-                raise NotSupportedYet('Unable to join table without condition')\n+                # prevent memory overflow\n+                if len(left_data) * len(right_data) < 10 ** 7:",
    "comment": "Are `left_data` and `right_data` dataframes? If so, then may be better to get real size (`df.memory_usage(index=True, deep=True).sum()`) and compare with free memory?",
    "line_number": 78,
    "enriched": "File: mindsdb/api/executor/sql_query/steps/join_step.py\nCode: @@ -74,7 +74,11 @@ def adapt_condition(node, **kwargs):\n                     return Identifier(parts=['table_b', col_name])\n \n             if step.query.condition is None:\n-                raise NotSupportedYet('Unable to join table without condition')\n+                # prevent memory overflow\n+                if len(left_data) * len(right_data) < 10 ** 7:\nComment: Are `left_data` and `right_data` dataframes? If so, then may be better to get real size (`df.memory_usage(index=True, deep=True).sum()`) and compare with free memory?",
    "subcategory": "resource",
    "category": "functional",
    "file_path": "mindsdb/api/executor/sql_query/steps/join_step.py",
    "pr_number": 10146,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1837727428,
    "comment_created_at": "2024-11-12T09:07:33Z"
  },
  {
    "code": "@@ -107,16 +126,64 @@ def check_connection(self) -> StatusResponse:\n             shopify.Shop.current()\n             response.success = True\n         except Exception as e:\n-            logger.error('Error connecting to Shopify!')\n-            raise ConnectionFailed(\"Conenction to Shopify failed.\")\n+            logger.error(f\"Error connecting to Shopify: {str(e)}\")\n             response.error_message = str(e)\n \n+            # Parse the error to extract clean, user-friendly messages\n+            error_msg = str(e)\n+\n+            # Handle ClientError (Shopify API errors)\n+            if \"ClientError\" in error_msg and \"Response(\" in error_msg:",
    "comment": "As I understand here a rest api error is checked. \r\nIt is better not convert exception to string (and check substrings) but check attributes and types.\r\nHere you can check \r\n```\r\nfrom pyactiveresource.connection import Error\r\nif isinstance(e, Error):\r\n   ...\r\n```",
    "line_number": 136,
    "enriched": "File: mindsdb/integrations/handlers/shopify_handler/shopify_handler.py\nCode: @@ -107,16 +126,64 @@ def check_connection(self) -> StatusResponse:\n             shopify.Shop.current()\n             response.success = True\n         except Exception as e:\n-            logger.error('Error connecting to Shopify!')\n-            raise ConnectionFailed(\"Conenction to Shopify failed.\")\n+            logger.error(f\"Error connecting to Shopify: {str(e)}\")\n             response.error_message = str(e)\n \n+            # Parse the error to extract clean, user-friendly messages\n+            error_msg = str(e)\n+\n+            # Handle ClientError (Shopify API errors)\n+            if \"ClientError\" in error_msg and \"Response(\" in error_msg:\nComment: As I understand here a rest api error is checked. \r\nIt is better not convert exception to string (and check substrings) but check attributes and types.\r\nHere you can check \r\n```\r\nfrom pyactiveresource.connection import Error\r\nif isinstance(e, Error):\r\n   ...\r\n```",
    "subcategory": "alternate output",
    "category": "refactoring",
    "file_path": "mindsdb/integrations/handlers/shopify_handler/shopify_handler.py",
    "pr_number": 11701,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2419289750,
    "comment_created_at": "2025-10-10T10:27:15Z"
  },
  {
    "code": "@@ -111,8 +123,14 @@ def check_connection(self) -> StatusResponse:\n             response.success = True\n             response.copy_storage = True\n         except Exception as e:\n-            logger.error(f\"Error connecting to Youtube API: {e}!\")\n-            response.error_message = e\n+            error_message = str(e)\n+            if error_message.startswith(\"AUTHORIZATION_REQUIRED:\"):\n+                auth_url = error_message.split(\"AUTHORIZATION_REQUIRED:\")[1].strip()",
    "comment": "why don't use different exception types instead of parsing message?",
    "line_number": 128,
    "enriched": "File: mindsdb/integrations/handlers/youtube_handler/youtube_handler.py\nCode: @@ -111,8 +123,14 @@ def check_connection(self) -> StatusResponse:\n             response.success = True\n             response.copy_storage = True\n         except Exception as e:\n-            logger.error(f\"Error connecting to Youtube API: {e}!\")\n-            response.error_message = e\n+            error_message = str(e)\n+            if error_message.startswith(\"AUTHORIZATION_REQUIRED:\"):\n+                auth_url = error_message.split(\"AUTHORIZATION_REQUIRED:\")[1].strip()\nComment: why don't use different exception types instead of parsing message?",
    "subcategory": "alternate output",
    "category": "refactoring",
    "file_path": "mindsdb/integrations/handlers/youtube_handler/youtube_handler.py",
    "pr_number": 9934,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1811977079,
    "comment_created_at": "2024-10-23T06:45:31Z"
  },
  {
    "code": "@@ -85,6 +87,27 @@ def send_message(self, message: ChatBotMessage) -> ChatBotResponse:\n             response.validate()\n             return ChatBotResponse(message.text)\n         except SlackApiError as e:\n+            slack_message = {",
    "comment": "Seems like this would be better as its own class, so we don't have to copy+paste this whenever we want to send an alert in the future. For example, we could create `interfaces/chatbot/chatbot_alerter.py`:\r\n\r\n```\r\nclass ChatbotAlerter():\r\n   ...\r\n   ...\r\n   def send_alert(self, text, attachments):\r\n      ...\r\n```\r\n\r\nAlso, it would be helpful to have the integration, the params of the chat handler, and the actual error as attachment fields",
    "line_number": 90,
    "enriched": "File: mindsdb/integrations/handlers/realtime_slack_chat_handler/realtime_slack_chat_handler.py\nCode: @@ -85,6 +87,27 @@ def send_message(self, message: ChatBotMessage) -> ChatBotResponse:\n             response.validate()\n             return ChatBotResponse(message.text)\n         except SlackApiError as e:\n+            slack_message = {\nComment: Seems like this would be better as its own class, so we don't have to copy+paste this whenever we want to send an alert in the future. For example, we could create `interfaces/chatbot/chatbot_alerter.py`:\r\n\r\n```\r\nclass ChatbotAlerter():\r\n   ...\r\n   ...\r\n   def send_alert(self, text, attachments):\r\n      ...\r\n```\r\n\r\nAlso, it would be helpful to have the integration, the params of the chat handler, and the actual error as attachment fields",
    "subcategory": "alternate output",
    "category": "refactoring",
    "file_path": "mindsdb/integrations/handlers/realtime_slack_chat_handler/realtime_slack_chat_handler.py",
    "pr_number": 6837,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1263085052,
    "comment_created_at": "2023-07-13T21:53:14Z"
  },
  {
    "code": "@@ -118,203 +87,59 @@ def invoke(self, query, session_id) -> Dict[str, Any]:\n                 \"parts\": [{\"type\": \"text\", \"text\": error_msg}],\n             }\n \n-    def streaming_invoke(self, messages: List[dict], timeout: int = DEFAULT_STREAM_TIMEOUT) -> Iterator[Dict[str, Any]]:\n-        \"\"\"Stream responses from the MindsDB agent using the direct API endpoint.\n-\n-        Args:\n-            messages: List of message dictionaries, each containing 'question' and optionally 'answer'.\n-                Example: [{'question': 'what is the average rental price for a three bedroom?', 'answer': None}]\n-            timeout: Request timeout in seconds (default: 300)\n-\n-        Returns:\n-            Iterator yielding chunks of the streaming response.\n-        \"\"\"\n-        try:\n-            # Construct the URL for the streaming completions endpoint\n-            url = f\"{self.base_url}/api/projects/{self.project_name}/agents/{self.agent_name}/completions/stream\"\n-\n-            # Log request for debugging\n-            logger.info(f\"Sending streaming request to MindsDB agent: {self.agent_name}\")\n-            logger.debug(f\"Request messages: {json.dumps(messages)[:200]}...\")\n-\n-            # Send the request to MindsDB streaming API with timeout\n-            stream = requests.post(url, json={\"messages\": messages}, stream=True, timeout=timeout)\n-            stream.raise_for_status()\n-\n-            # Process the streaming response directly\n-            for line in stream.iter_lines():\n-                if line:\n-                    # Parse each non-empty line\n-                    try:\n-                        line = line.decode(\"utf-8\")\n-                        if line.startswith(\"data: \"):\n-                            # Extract the JSON data from the line that starts with 'data: '\n-                            data = line[6:]  # Remove 'data: ' prefix\n-                            try:\n-                                chunk = json.loads(data)\n-                                # Pass through the chunk with minimal modifications\n-                                yield chunk\n-                            except json.JSONDecodeError as e:\n-                                logger.warning(f\"Failed to parse JSON from line: {data}. Error: {str(e)}\")\n-                                # Yield error information but continue processing\n-                                yield {\n-                                    \"error\": f\"JSON parse error: {str(e)}\",\n-                                    \"data\": data,\n-                                    \"is_task_complete\": False,\n-                                    \"parts\": [\n-                                        {\n-                                            \"type\": \"text\",\n-                                            \"text\": f\"Error parsing response: {str(e)}\",\n-                                        }\n-                                    ],\n-                                    \"metadata\": {},\n-                                }\n-                        else:\n-                            # Log other lines for debugging\n-                            logger.debug(f\"Received non-data line: {line}\")\n-\n-                            # If it looks like a raw text response (not SSE format), wrap it\n-                            if not line.startswith(\"event:\") and not line.startswith(\":\"):\n-                                yield {\"content\": line, \"is_task_complete\": False}\n-                    except UnicodeDecodeError as e:\n-                        logger.warning(f\"Failed to decode line: {str(e)}\")\n-                        # Continue processing despite decode errors\n-\n-        except requests.exceptions.Timeout as e:\n-            error_msg = f\"Request timed out after {timeout} seconds: {str(e)}\"\n-            logger.error(error_msg)\n-            yield {\n-                \"content\": error_msg,\n-                \"parts\": [{\"type\": \"text\", \"text\": error_msg}],\n-                \"is_task_complete\": True,\n-                \"error\": \"timeout\",\n-                \"metadata\": {\"error\": True},\n-            }\n-\n-        except requests.exceptions.ChunkedEncodingError as e:\n-            error_msg = f\"Stream was interrupted: {str(e)}\"\n-            logger.error(error_msg)\n-            yield {\n-                \"content\": error_msg,\n-                \"parts\": [{\"type\": \"text\", \"text\": error_msg}],\n-                \"is_task_complete\": True,\n-                \"error\": \"stream_interrupted\",\n-                \"metadata\": {\"error\": True},\n-            }\n-\n-        except requests.exceptions.ConnectionError as e:\n-            error_msg = f\"Connection error: {str(e)}\"\n-            logger.error(error_msg)\n-            yield {\n-                \"content\": error_msg,\n-                \"parts\": [{\"type\": \"text\", \"text\": error_msg}],\n-                \"is_task_complete\": True,\n-                \"error\": \"connection_error\",\n-                \"metadata\": {\"error\": True},\n-            }\n-\n-        except requests.exceptions.RequestException as e:\n-            error_msg = f\"Error connecting to MindsDB streaming API: {str(e)}\"\n-            logger.error(error_msg)\n-            yield {\n-                \"content\": error_msg,\n-                \"parts\": [{\"type\": \"text\", \"text\": error_msg}],\n-                \"is_task_complete\": True,\n-                \"error\": \"request_error\",\n-                \"metadata\": {\"error\": True},\n-            }\n-\n-        except Exception as e:\n-            error_msg = f\"Error in streaming: {str(e)}\"\n-            logger.error(error_msg)\n-            yield {\n-                \"content\": error_msg,\n-                \"parts\": [{\"type\": \"text\", \"text\": error_msg}],\n-                \"is_task_complete\": True,\n-                \"error\": \"unknown_error\",\n-                \"metadata\": {\"error\": True},\n-            }\n-\n-        # Send a final completion message\n-        yield {\"is_task_complete\": True, \"metadata\": {\"complete\": True}}\n+    async def streaming_invoke(self, messages, timeout=60):\n+        url = f\"{self.base_url}/api/projects/{self.project_name}/agents/{self.agent_name}/completions/stream\"\n+        logger.info(f\"Sending streaming request to MindsDB agent: {self.agent_name}\")\n+        async with httpx.AsyncClient(timeout=timeout) as client:\n+            async with client.stream(\"POST\", url, json={\"messages\": to_serializable(messages)}) as response:\n+                response.raise_for_status()\n+                async for line in response.aiter_lines():",
    "comment": "The new `streaming_invoke` yields raw lines (strings) without parsing JSON, so downstream code expecting dict chunks will break. You should detect and parse JSON lines (e.g., lines starting with `data:`) into dicts before yielding.",
    "line_number": 96,
    "enriched": "File: mindsdb/api/a2a/agent.py\nCode: @@ -118,203 +87,59 @@ def invoke(self, query, session_id) -> Dict[str, Any]:\n                 \"parts\": [{\"type\": \"text\", \"text\": error_msg}],\n             }\n \n-    def streaming_invoke(self, messages: List[dict], timeout: int = DEFAULT_STREAM_TIMEOUT) -> Iterator[Dict[str, Any]]:\n-        \"\"\"Stream responses from the MindsDB agent using the direct API endpoint.\n-\n-        Args:\n-            messages: List of message dictionaries, each containing 'question' and optionally 'answer'.\n-                Example: [{'question': 'what is the average rental price for a three bedroom?', 'answer': None}]\n-            timeout: Request timeout in seconds (default: 300)\n-\n-        Returns:\n-            Iterator yielding chunks of the streaming response.\n-        \"\"\"\n-        try:\n-            # Construct the URL for the streaming completions endpoint\n-            url = f\"{self.base_url}/api/projects/{self.project_name}/agents/{self.agent_name}/completions/stream\"\n-\n-            # Log request for debugging\n-            logger.info(f\"Sending streaming request to MindsDB agent: {self.agent_name}\")\n-            logger.debug(f\"Request messages: {json.dumps(messages)[:200]}...\")\n-\n-            # Send the request to MindsDB streaming API with timeout\n-            stream = requests.post(url, json={\"messages\": messages}, stream=True, timeout=timeout)\n-            stream.raise_for_status()\n-\n-            # Process the streaming response directly\n-            for line in stream.iter_lines():\n-                if line:\n-                    # Parse each non-empty line\n-                    try:\n-                        line = line.decode(\"utf-8\")\n-                        if line.startswith(\"data: \"):\n-                            # Extract the JSON data from the line that starts with 'data: '\n-                            data = line[6:]  # Remove 'data: ' prefix\n-                            try:\n-                                chunk = json.loads(data)\n-                                # Pass through the chunk with minimal modifications\n-                                yield chunk\n-                            except json.JSONDecodeError as e:\n-                                logger.warning(f\"Failed to parse JSON from line: {data}. Error: {str(e)}\")\n-                                # Yield error information but continue processing\n-                                yield {\n-                                    \"error\": f\"JSON parse error: {str(e)}\",\n-                                    \"data\": data,\n-                                    \"is_task_complete\": False,\n-                                    \"parts\": [\n-                                        {\n-                                            \"type\": \"text\",\n-                                            \"text\": f\"Error parsing response: {str(e)}\",\n-                                        }\n-                                    ],\n-                                    \"metadata\": {},\n-                                }\n-                        else:\n-                            # Log other lines for debugging\n-                            logger.debug(f\"Received non-data line: {line}\")\n-\n-                            # If it looks like a raw text response (not SSE format), wrap it\n-                            if not line.startswith(\"event:\") and not line.startswith(\":\"):\n-                                yield {\"content\": line, \"is_task_complete\": False}\n-                    except UnicodeDecodeError as e:\n-                        logger.warning(f\"Failed to decode line: {str(e)}\")\n-                        # Continue processing despite decode errors\n-\n-        except requests.exceptions.Timeout as e:\n-            error_msg = f\"Request timed out after {timeout} seconds: {str(e)}\"\n-            logger.error(error_msg)\n-            yield {\n-                \"content\": error_msg,\n-                \"parts\": [{\"type\": \"text\", \"text\": error_msg}],\n-                \"is_task_complete\": True,\n-                \"error\": \"timeout\",\n-                \"metadata\": {\"error\": True},\n-            }\n-\n-        except requests.exceptions.ChunkedEncodingError as e:\n-            error_msg = f\"Stream was interrupted: {str(e)}\"\n-            logger.error(error_msg)\n-            yield {\n-                \"content\": error_msg,\n-                \"parts\": [{\"type\": \"text\", \"text\": error_msg}],\n-                \"is_task_complete\": True,\n-                \"error\": \"stream_interrupted\",\n-                \"metadata\": {\"error\": True},\n-            }\n-\n-        except requests.exceptions.ConnectionError as e:\n-            error_msg = f\"Connection error: {str(e)}\"\n-            logger.error(error_msg)\n-            yield {\n-                \"content\": error_msg,\n-                \"parts\": [{\"type\": \"text\", \"text\": error_msg}],\n-                \"is_task_complete\": True,\n-                \"error\": \"connection_error\",\n-                \"metadata\": {\"error\": True},\n-            }\n-\n-        except requests.exceptions.RequestException as e:\n-            error_msg = f\"Error connecting to MindsDB streaming API: {str(e)}\"\n-            logger.error(error_msg)\n-            yield {\n-                \"content\": error_msg,\n-                \"parts\": [{\"type\": \"text\", \"text\": error_msg}],\n-                \"is_task_complete\": True,\n-                \"error\": \"request_error\",\n-                \"metadata\": {\"error\": True},\n-            }\n-\n-        except Exception as e:\n-            error_msg = f\"Error in streaming: {str(e)}\"\n-            logger.error(error_msg)\n-            yield {\n-                \"content\": error_msg,\n-                \"parts\": [{\"type\": \"text\", \"text\": error_msg}],\n-                \"is_task_complete\": True,\n-                \"error\": \"unknown_error\",\n-                \"metadata\": {\"error\": True},\n-            }\n-\n-        # Send a final completion message\n-        yield {\"is_task_complete\": True, \"metadata\": {\"complete\": True}}\n+    async def streaming_invoke(self, messages, timeout=60):\n+        url = f\"{self.base_url}/api/projects/{self.project_name}/agents/{self.agent_name}/completions/stream\"\n+        logger.info(f\"Sending streaming request to MindsDB agent: {self.agent_name}\")\n+        async with httpx.AsyncClient(timeout=timeout) as client:\n+            async with client.stream(\"POST\", url, json={\"messages\": to_serializable(messages)}) as response:\n+                response.raise_for_status()\n+                async for line in response.aiter_lines():\nComment: The new `streaming_invoke` yields raw lines (strings) without parsing JSON, so downstream code expecting dict chunks will break. You should detect and parse JSON lines (e.g., lines starting with `data:`) into dicts before yielding.",
    "subcategory": "alternate output",
    "category": "refactoring",
    "file_path": "mindsdb/api/a2a/agent.py",
    "pr_number": 11248,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2185047161,
    "comment_created_at": "2025-07-04T10:54:41Z"
  },
  {
    "code": "@@ -73,17 +73,15 @@ def invoke(self, query, session_id) -> Dict[str, Any]:\n                     \"parts\": [{\"type\": \"text\", \"text\": error_msg}],\n                 }\n         except requests.exceptions.RequestException as e:\n-            error_msg = f\"Error connecting to MindsDB: {str(e)}\"\n-            logger.error(error_msg)\n+            logger.exception(\"Error connecting to MindsDB:\")\n             return {\n-                \"content\": error_msg,\n+                \"content\": f\"Error connecting to MindsDB: {e}\",\n                 \"parts\": [{\"type\": \"text\", \"text\": error_msg}],\n             }\n         except Exception as e:\n-            error_msg = f\"Error: {str(e)}\"\n-            logger.error(error_msg)\n+            logger.exception(\"Error: \")",
    "comment": "should it content a message?",
    "line_number": 82,
    "enriched": "File: mindsdb/api/a2a/agent.py\nCode: @@ -73,17 +73,15 @@ def invoke(self, query, session_id) -> Dict[str, Any]:\n                     \"parts\": [{\"type\": \"text\", \"text\": error_msg}],\n                 }\n         except requests.exceptions.RequestException as e:\n-            error_msg = f\"Error connecting to MindsDB: {str(e)}\"\n-            logger.error(error_msg)\n+            logger.exception(\"Error connecting to MindsDB:\")\n             return {\n-                \"content\": error_msg,\n+                \"content\": f\"Error connecting to MindsDB: {e}\",\n                 \"parts\": [{\"type\": \"text\", \"text\": error_msg}],\n             }\n         except Exception as e:\n-            error_msg = f\"Error: {str(e)}\"\n-            logger.error(error_msg)\n+            logger.exception(\"Error: \")\nComment: should it content a message?",
    "subcategory": "alternate output",
    "category": "refactoring",
    "file_path": "mindsdb/api/a2a/agent.py",
    "pr_number": 11572,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2366967430,
    "comment_created_at": "2025-09-22T07:27:50Z"
  },
  {
    "code": "@@ -15,6 +15,7 @@ tests/integration/flows/my_model.zip\n distributions/docker-compose/*/storage\n venv/*\n env/*\n+mindsdb-venv/*",
    "comment": "it is duplicate of line 21",
    "line_number": 18,
    "enriched": "File: .gitignore\nCode: @@ -15,6 +15,7 @@ tests/integration/flows/my_model.zip\n distributions/docker-compose/*/storage\n venv/*\n env/*\n+mindsdb-venv/*\nComment: it is duplicate of line 21",
    "subcategory": "praise",
    "category": "discussion",
    "file_path": ".gitignore",
    "pr_number": 11637,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2389852602,
    "comment_created_at": "2025-09-30T04:59:10Z"
  },
  {
    "code": "@@ -71,7 +72,7 @@ def get_model_accuracy_dict(nixtla_results_df, metric=r2_score):\n     for column in nixtla_results_df.columns:\n         if column in [\"unique_id\", \"ds\", \"y\", \"cutoff\"]:\n             continue\n-        model_error = metric(nixtla_results_df[column], nixtla_results_df[\"y\"])\n+        model_error = metric(nixtla_results_df[\"y\"], nixtla_results_df[column])",
    "comment": "Ooh, nice catch üêû ",
    "line_number": 75,
    "enriched": "File: mindsdb/integrations/utilities/time_series_utils.py\nCode: @@ -71,7 +72,7 @@ def get_model_accuracy_dict(nixtla_results_df, metric=r2_score):\n     for column in nixtla_results_df.columns:\n         if column in [\"unique_id\", \"ds\", \"y\", \"cutoff\"]:\n             continue\n-        model_error = metric(nixtla_results_df[column], nixtla_results_df[\"y\"])\n+        model_error = metric(nixtla_results_df[\"y\"], nixtla_results_df[column])\nComment: Ooh, nice catch üêû ",
    "subcategory": "praise",
    "category": "discussion",
    "file_path": "mindsdb/integrations/utilities/time_series_utils.py",
    "pr_number": 5445,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1160356417,
    "comment_created_at": "2023-04-07T00:43:21Z"
  },
  {
    "code": "@@ -18,6 +18,8 @@\n     DistanceFunction,\n     TableField\n )\n+from mindsdb.integrations.libs.keyword_search_base import KeywordSearchBase",
    "comment": "Looks like `keyword_search_base `module hasn't been commited",
    "line_number": 21,
    "enriched": "File: mindsdb/integrations/handlers/pgvector_handler/pgvector_handler.py\nCode: @@ -18,6 +18,8 @@\n     DistanceFunction,\n     TableField\n )\n+from mindsdb.integrations.libs.keyword_search_base import KeywordSearchBase\nComment: Looks like `keyword_search_base `module hasn't been commited",
    "subcategory": "praise",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/pgvector_handler/pgvector_handler.py",
    "pr_number": 11221,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2175480465,
    "comment_created_at": "2025-06-30T16:32:19Z"
  },
  {
    "code": "@@ -669,6 +669,22 @@ FROM sample_data.orders;\n ```\n </Note>\n \n+## `CREATE INDEX ON KNOWLEDGE_BASE` Syntax\n+\n+Users can create index on the knowledge base to speed up the search operations.\n+\n+```sql\n+CREATE INDEX ON KNOWLEDGE_BASE my_kb;\n+```\n+\n+<Warning>\n+Note that this feature works only when PGVector is used as the [storage of the knowledge base](/mindsdb_sql/knowledge-bases#storage).\n+</Warning>\n+\n+Upon executing this statement, an index is created on the knowledge base's underlying vector store. This is essentially a database index created on the vector database.\n+\n+Note that having an index on the knowledge base may reduce the speed of the insert operations. Therefore, it is recommended to insert bulk data into the knowledge base before creating an index. The index improves performance of querying the knowledge base, while it may slow down subsequent data inserts.\n+",
    "comment": "looks good, but let's clarify that is it only for postgres (pgvector database) ",
    "line_number": 687,
    "enriched": "File: docs/mindsdb_sql/knowledge-bases.mdx\nCode: @@ -669,6 +669,22 @@ FROM sample_data.orders;\n ```\n </Note>\n \n+## `CREATE INDEX ON KNOWLEDGE_BASE` Syntax\n+\n+Users can create index on the knowledge base to speed up the search operations.\n+\n+```sql\n+CREATE INDEX ON KNOWLEDGE_BASE my_kb;\n+```\n+\n+<Warning>\n+Note that this feature works only when PGVector is used as the [storage of the knowledge base](/mindsdb_sql/knowledge-bases#storage).\n+</Warning>\n+\n+Upon executing this statement, an index is created on the knowledge base's underlying vector store. This is essentially a database index created on the vector database.\n+\n+Note that having an index on the knowledge base may reduce the speed of the insert operations. Therefore, it is recommended to insert bulk data into the knowledge base before creating an index. The index improves performance of querying the knowledge base, while it may slow down subsequent data inserts.\n+\nComment: looks good, but let's clarify that is it only for postgres (pgvector database) ",
    "subcategory": "praise",
    "category": "discussion",
    "file_path": "docs/mindsdb_sql/knowledge-bases.mdx",
    "pr_number": 10986,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2121277259,
    "comment_created_at": "2025-06-02T14:11:58Z"
  },
  {
    "code": "@@ -34,6 +34,7 @@ def get_season_length(frequency):\n     season_dict = {  # https://pandas.pydata.org/docs/user_guide/timeseries.html#timeseries-offset-aliases\n         \"H\": 24,\n         \"M\": 12,\n+        \"MS\": 12,",
    "comment": "Good catch, I've ran into this previously. I recall there's also the option for doing \"[X]F\", with `X` an integer and `F` the frequency. That would definitely be a new feature though.",
    "line_number": 37,
    "enriched": "File: mindsdb/integrations/handlers/statsforecast_handler/statsforecast_handler.py\nCode: @@ -34,6 +34,7 @@ def get_season_length(frequency):\n     season_dict = {  # https://pandas.pydata.org/docs/user_guide/timeseries.html#timeseries-offset-aliases\n         \"H\": 24,\n         \"M\": 12,\n+        \"MS\": 12,\nComment: Good catch, I've ran into this previously. I recall there's also the option for doing \"[X]F\", with `X` an integer and `F` the frequency. That would definitely be a new feature though.",
    "subcategory": "praise",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/statsforecast_handler/statsforecast_handler.py",
    "pr_number": 5704,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1170269104,
    "comment_created_at": "2023-04-18T16:07:13Z"
  },
  {
    "code": "@@ -367,8 +369,8 @@ def send_message(self, params, ret_as_dict=False) -> Response:\n \n         except Exception as e:\n             # Log the exception for debugging purposes\n-            log.logger.error(f\"Error sending message: {str(e)}\")\n-            raise Exception(f\"Error posting message to the user '{params['to_number']}': {e.response['error']}\")\n+            logger.exception(f\"Error sending message: {str(e)}\")",
    "comment": "Should we log error here too?",
    "line_number": 372,
    "enriched": "File: mindsdb/integrations/handlers/whatsapp_handler/whatsapp_handler.py\nCode: @@ -367,8 +369,8 @@ def send_message(self, params, ret_as_dict=False) -> Response:\n \n         except Exception as e:\n             # Log the exception for debugging purposes\n-            log.logger.error(f\"Error sending message: {str(e)}\")\n-            raise Exception(f\"Error posting message to the user '{params['to_number']}': {e.response['error']}\")\n+            logger.exception(f\"Error sending message: {str(e)}\")\nComment: Should we log error here too?",
    "subcategory": "design discussion",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/whatsapp_handler/whatsapp_handler.py",
    "pr_number": 8717,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1472832984,
    "comment_created_at": "2024-01-31T13:36:29Z"
  },
  {
    "code": "@@ -0,0 +1,21 @@\n+from mindsdb.integrations.libs.const import HANDLER_TYPE",
    "comment": "Contents in this file seem duplicated from `__init__.py`. Probably not the culprit, but would be good to fix this and follow the pattern that other ML handlers show.",
    "line_number": 1,
    "enriched": "File: mindsdb/integrations/handlers/auto_ts_handler/setup.py\nCode: @@ -0,0 +1,21 @@\n+from mindsdb.integrations.libs.const import HANDLER_TYPE\nComment: Contents in this file seem duplicated from `__init__.py`. Probably not the culprit, but would be good to fix this and follow the pattern that other ML handlers show.",
    "subcategory": "design discussion",
    "category": "discussion",
    "file_path": "mindsdb/integrations/handlers/auto_ts_handler/setup.py",
    "pr_number": 7663,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1350766352,
    "comment_created_at": "2023-10-09T20:34:02Z"
  },
  {
    "code": "@@ -1,23 +1,85 @@\n ---\n title: Anomaly Detection Handler\n-sidebarTitle: Anomaly Detection Handler\n+sidebarTitle: Anomaly Detection\n ---\n \n+The Anomaly Detection handler implements supervised, semi-supervised, and unsupervised anomaly detection algorithms using the pyod, catboost, xgboost, and sklearn libraries. The models were chosen based on the results in the [ADBench benchmark paper](https://proceedings.neurips.cc/paper_files/paper/2022/hash/cf93972b116ca5268827d575f2cc226b-Abstract-Datasets_and_Benchmarks.html).\n \n-The Anomaly Detection handler implements supervised, semi-supervised, and unsupervised anomaly detection algorithms using the pyod, catboost, xgboost, and sklearn libraries. The models were chosen based on the results in the [following benchmark paper](https://www.andrew.cmu.edu/user/yuezhao2/papers/22-neurips-adbench.pdf \n-).\n-\n-# Additional information\n+<Info>\n+**Additional information**\n \n - If no labelled data, we use an unsupervised learner with the syntax `CREATE ANOMALY DETECTION MODEL <model_name>` without specifying the target to predict. MindsDB then adds a column called `outlier` when generating results.\n \n - If we have labelled data, we use the regular model creation syntax. There is backend logic that chooses between a semi-supervised algorithm (currently XGBOD) vs. a supervised algorithm (currently CatBoost).\n \n - If multiple models are provided, then we create an ensemble and use majority voting.\n \n-- See the anomaly detection proposal  [document](https://docs.google.com/document/d/1Yd7ARZVg_67xlcY-JR2kuO7mak9Ia2YER1Jk0EdpEa0/edit#heading=h.mo4wxsae6t1d) for more information.\n+- See the anomaly detection proposal [document](https://docs.google.com/document/d/1Yd7ARZVg_67xlcY-JR2kuO7mak9Ia2YER1Jk0EdpEa0/edit#heading=h.mo4wxsae6t1d) for more information.\n+\n+</Info>\n+\n+<Info>\n+**Context about types of anomaly detection**\n+\n+- Supervised: we have inlier/outlier labels, so we can train a classifier the normal way. This is very similar to a standard classification problem.\n+\n+- Semi-supervised: we have inlier/outlier labels and perform an unsupervised preprocessing step, and then a supervised classification algorithm.\n+\n+- Unsupervised: we don‚Äôt have inlier/outlier labels and cannot assume all training data are inliers. These methods construct inlier criteria that will classify some training data as outliers too based on distributional traits. New observations are classified against these criteria. However, it‚Äôs not possible to evaluate how well the model detects outliers without labels.\n+\n+</Info>\n+\n+<Info>\n+**Default dispatch logic**\n+\n+We propose the following logic to determine type of learning:\n+- Use supervised learning if labels are available and the dataset contains at least 3000 samples.\n+- Use semi-supervised learning if labels are available and number of samples in the dataset is less than 3000.\n+- If the dataset is unlabelled, use unsupervised learning.\n+\n+We‚Äôve chosen 3000 based on the results of the NeurIPS AD Benchmark paper (linked above). The authors report that semi-supervised learning outperforms supervised learning when the number of samples used is less than 5% of the size of the training dataset. The average size of the training datasets in their study is 60,000, therefore this 5% corresponds to 3000 samples on average.\n+\n+</Info>\n+\n+<Info>\n+**Reasoning for default models on each type**\n+\n+We refer to the NeurIPS AD Benchmark paper (linked above) to make these choices:\n+- For supervised learning, use CatBoost. It often outperforms classic algorithms.\n+- For semi-supervised, XGBod is a good default from PyOD.\n+- There‚Äôs no clear winner for unsupervised methods, it depends on the use case. ECOD is a sensible default with a fast runtime. If we‚Äôre not concerned about runtime, we can use an ensemble.\n+\n+</Info>\n+\n+## Prerequisites\n+\n+Before proceeding, ensure the following prerequisites are met:\n+\n+1. Install MindsDB locally via [Docker](https://docs.mindsdb.com/setup/self-hosted/docker) or [Docker Desktop](https://docs.mindsdb.com/setup/self-hosted/docker-desktop).\n+2. To use Anomaly Detection handler within MindsDB, install the required dependencies following [this instruction](https://docs.mindsdb.com/setup/self-hosted/docker#install-dependencies).\n \n-# Example usage\n+## Setup\n+\n+Create an AI engine from the [Anomaly Detection handler](https://github.com/mindsdb/mindsdb/tree/staging/mindsdb/integrations/handlers/anthropic_handler).\n+\n+```sql\n+CREATE ML_ENGINE anomaly_detection_engine\n+FROM anomaly_detection;\n+```\n+\n+Create a model using `anthropic_engine` as an engine.",
    "comment": "Should we change the `anthropic_engine`?",
    "line_number": 70,
    "enriched": "File: docs/integrations/ai-engines/anomaly.mdx\nCode: @@ -1,23 +1,85 @@\n ---\n title: Anomaly Detection Handler\n-sidebarTitle: Anomaly Detection Handler\n+sidebarTitle: Anomaly Detection\n ---\n \n+The Anomaly Detection handler implements supervised, semi-supervised, and unsupervised anomaly detection algorithms using the pyod, catboost, xgboost, and sklearn libraries. The models were chosen based on the results in the [ADBench benchmark paper](https://proceedings.neurips.cc/paper_files/paper/2022/hash/cf93972b116ca5268827d575f2cc226b-Abstract-Datasets_and_Benchmarks.html).\n \n-The Anomaly Detection handler implements supervised, semi-supervised, and unsupervised anomaly detection algorithms using the pyod, catboost, xgboost, and sklearn libraries. The models were chosen based on the results in the [following benchmark paper](https://www.andrew.cmu.edu/user/yuezhao2/papers/22-neurips-adbench.pdf \n-).\n-\n-# Additional information\n+<Info>\n+**Additional information**\n \n - If no labelled data, we use an unsupervised learner with the syntax `CREATE ANOMALY DETECTION MODEL <model_name>` without specifying the target to predict. MindsDB then adds a column called `outlier` when generating results.\n \n - If we have labelled data, we use the regular model creation syntax. There is backend logic that chooses between a semi-supervised algorithm (currently XGBOD) vs. a supervised algorithm (currently CatBoost).\n \n - If multiple models are provided, then we create an ensemble and use majority voting.\n \n-- See the anomaly detection proposal  [document](https://docs.google.com/document/d/1Yd7ARZVg_67xlcY-JR2kuO7mak9Ia2YER1Jk0EdpEa0/edit#heading=h.mo4wxsae6t1d) for more information.\n+- See the anomaly detection proposal [document](https://docs.google.com/document/d/1Yd7ARZVg_67xlcY-JR2kuO7mak9Ia2YER1Jk0EdpEa0/edit#heading=h.mo4wxsae6t1d) for more information.\n+\n+</Info>\n+\n+<Info>\n+**Context about types of anomaly detection**\n+\n+- Supervised: we have inlier/outlier labels, so we can train a classifier the normal way. This is very similar to a standard classification problem.\n+\n+- Semi-supervised: we have inlier/outlier labels and perform an unsupervised preprocessing step, and then a supervised classification algorithm.\n+\n+- Unsupervised: we don‚Äôt have inlier/outlier labels and cannot assume all training data are inliers. These methods construct inlier criteria that will classify some training data as outliers too based on distributional traits. New observations are classified against these criteria. However, it‚Äôs not possible to evaluate how well the model detects outliers without labels.\n+\n+</Info>\n+\n+<Info>\n+**Default dispatch logic**\n+\n+We propose the following logic to determine type of learning:\n+- Use supervised learning if labels are available and the dataset contains at least 3000 samples.\n+- Use semi-supervised learning if labels are available and number of samples in the dataset is less than 3000.\n+- If the dataset is unlabelled, use unsupervised learning.\n+\n+We‚Äôve chosen 3000 based on the results of the NeurIPS AD Benchmark paper (linked above). The authors report that semi-supervised learning outperforms supervised learning when the number of samples used is less than 5% of the size of the training dataset. The average size of the training datasets in their study is 60,000, therefore this 5% corresponds to 3000 samples on average.\n+\n+</Info>\n+\n+<Info>\n+**Reasoning for default models on each type**\n+\n+We refer to the NeurIPS AD Benchmark paper (linked above) to make these choices:\n+- For supervised learning, use CatBoost. It often outperforms classic algorithms.\n+- For semi-supervised, XGBod is a good default from PyOD.\n+- There‚Äôs no clear winner for unsupervised methods, it depends on the use case. ECOD is a sensible default with a fast runtime. If we‚Äôre not concerned about runtime, we can use an ensemble.\n+\n+</Info>\n+\n+## Prerequisites\n+\n+Before proceeding, ensure the following prerequisites are met:\n+\n+1. Install MindsDB locally via [Docker](https://docs.mindsdb.com/setup/self-hosted/docker) or [Docker Desktop](https://docs.mindsdb.com/setup/self-hosted/docker-desktop).\n+2. To use Anomaly Detection handler within MindsDB, install the required dependencies following [this instruction](https://docs.mindsdb.com/setup/self-hosted/docker#install-dependencies).\n \n-# Example usage\n+## Setup\n+\n+Create an AI engine from the [Anomaly Detection handler](https://github.com/mindsdb/mindsdb/tree/staging/mindsdb/integrations/handlers/anthropic_handler).\n+\n+```sql\n+CREATE ML_ENGINE anomaly_detection_engine\n+FROM anomaly_detection;\n+```\n+\n+Create a model using `anthropic_engine` as an engine.\nComment: Should we change the `anthropic_engine`?",
    "subcategory": "design discussion",
    "category": "discussion",
    "file_path": "docs/integrations/ai-engines/anomaly.mdx",
    "pr_number": 9152,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1587535104,
    "comment_created_at": "2024-05-02T12:14:07Z"
  },
  {
    "code": "@@ -115,6 +116,16 @@ def select_query(self, query: Select) -> pd.DataFrame:\n         else:\n             logger.warning(\"Query returned no data\")\n \n+        rerank_model = self._kb.params.get(\"rerank_model\")\n+        if rerank_model and df is not None:\n+            reranker = LLMReranker()",
    "comment": "rerank_model is not used by reranker, should we pass it there? ",
    "line_number": 121,
    "enriched": "File: mindsdb/interfaces/knowledge_base/controller.py\nCode: @@ -115,6 +116,16 @@ def select_query(self, query: Select) -> pd.DataFrame:\n         else:\n             logger.warning(\"Query returned no data\")\n \n+        rerank_model = self._kb.params.get(\"rerank_model\")\n+        if rerank_model and df is not None:\n+            reranker = LLMReranker()\nComment: rerank_model is not used by reranker, should we pass it there? ",
    "subcategory": "design discussion",
    "category": "discussion",
    "file_path": "mindsdb/interfaces/knowledge_base/controller.py",
    "pr_number": 10601,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2012307162,
    "comment_created_at": "2025-03-25T14:59:56Z"
  },
  {
    "code": "@@ -110,5 +107,38 @@ def main(\n         sys.exit(1)\n \n \n+# This function is called by mindsdb/api/a2a/run_a2a.py\n+def main_with_config(config=None, *args, **kwargs):\n+    \"\"\"\n+    Alternative entry point that accepts configuration as a dictionary.\n+    This is used when A2A is started as a subprocess of the main MindsDB process.\n+\n+    Args:\n+        config (dict): Configuration dictionary\n+        args: Additional positional arguments\n+        kwargs: Additional keyword arguments\n+    \"\"\"\n+    if config is None:\n+        config = {}\n+\n+    # Extract configuration values with fallbacks to default values\n+    host = config.get(\"host\", \"localhost\")\n+    port = int(config.get(\"port\", 47338))\n+    mindsdb_host = config.get(\"mindsdb_host\", \"localhost\")\n+    mindsdb_port = int(config.get(\"mindsdb_port\", 47334))\n+    project_name = config.get(\"project_name\", \"mindsdb\")",
    "comment": "@dusvyat We have allowed the option of changing the name of the default project with a config option or environment variable, so it could be different to 'mindsdb'. I think we should default to that.",
    "line_number": 129,
    "enriched": "File: mindsdb/api/a2a/__main__.py\nCode: @@ -110,5 +107,38 @@ def main(\n         sys.exit(1)\n \n \n+# This function is called by mindsdb/api/a2a/run_a2a.py\n+def main_with_config(config=None, *args, **kwargs):\n+    \"\"\"\n+    Alternative entry point that accepts configuration as a dictionary.\n+    This is used when A2A is started as a subprocess of the main MindsDB process.\n+\n+    Args:\n+        config (dict): Configuration dictionary\n+        args: Additional positional arguments\n+        kwargs: Additional keyword arguments\n+    \"\"\"\n+    if config is None:\n+        config = {}\n+\n+    # Extract configuration values with fallbacks to default values\n+    host = config.get(\"host\", \"localhost\")\n+    port = int(config.get(\"port\", 47338))\n+    mindsdb_host = config.get(\"mindsdb_host\", \"localhost\")\n+    mindsdb_port = int(config.get(\"mindsdb_port\", 47334))\n+    project_name = config.get(\"project_name\", \"mindsdb\")\nComment: @dusvyat We have allowed the option of changing the name of the default project with a config option or environment variable, so it could be different to 'mindsdb'. I think we should default to that.",
    "subcategory": "design discussion",
    "category": "discussion",
    "file_path": "mindsdb/api/a2a/__main__.py",
    "pr_number": 10919,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2112462323,
    "comment_created_at": "2025-05-28T17:58:20Z"
  },
  {
    "code": "@@ -674,6 +674,14 @@ def _df_to_embeddings(self, df: pd.DataFrame) -> pd.DataFrame:\n \n         model_id = self._kb.embedding_model_id\n \n+        if model_id is None:\n+            # call litellm handler\n+            messages = list(df[TableField.CONTENT.value])\n+            args = self._kb.params['embedding_model']",
    "comment": "The `default_embedding_model` should be used here if the `embedding_model` is not provided.",
    "line_number": 680,
    "enriched": "File: mindsdb/interfaces/knowledge_base/controller.py\nCode: @@ -674,6 +674,14 @@ def _df_to_embeddings(self, df: pd.DataFrame) -> pd.DataFrame:\n \n         model_id = self._kb.embedding_model_id\n \n+        if model_id is None:\n+            # call litellm handler\n+            messages = list(df[TableField.CONTENT.value])\n+            args = self._kb.params['embedding_model']\nComment: The `default_embedding_model` should be used here if the `embedding_model` is not provided.",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "mindsdb/interfaces/knowledge_base/controller.py",
    "pr_number": 10855,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 2103815982,
    "comment_created_at": "2025-05-23T05:11:19Z"
  },
  {
    "code": "@@ -0,0 +1,64 @@\n+import os",
    "comment": "file added by mistake?",
    "line_number": 1,
    "enriched": "File: mindsdb/utilities/hooks/profiling.py\nCode: @@ -0,0 +1,64 @@\n+import os\nComment: file added by mistake?",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "mindsdb/utilities/hooks/profiling.py",
    "pr_number": 5755,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1174007905,
    "comment_created_at": "2023-04-21T17:20:20Z"
  },
  {
    "code": "@@ -0,0 +1,104 @@\n+from mindsdb.integrations.handlers.youtube_handler.youtube_tables import YoutubeGetCommentsTable\n+from mindsdb.integrations.libs.api_handler import APIHandler\n+from mindsdb.integrations.libs.response import (\n+    HandlerStatusResponse as StatusResponse,\n+)\n+from mindsdb.utilities.log import get_log\n+from mindsdb_sql import parse_sql\n+\n+import requests\n+import pandas as pd\n+import json\n+from collections import OrderedDict\n+from mindsdb.integrations.libs.const import HANDLER_CONNECTION_ARG_TYPE as ARG_TYPE\n+\n+from googleapiclient.discovery import build\n+from googleapiclient.errors import HttpError\n+\n+logger = get_log(\"integrations.youtube_handler\")\n+\n+class YoutubeHandler(APIHandler):\n+    \"\"\"Strava handler implementation\"\"\"\n+\n+    def __init__(self, name=None, **kwargs):\n+        \"\"\"Initialize the Strava handler.",
    "comment": "Please change Strava",
    "line_number": 24,
    "enriched": "File: mindsdb/integrations/handlers/youtube_handler/youtube_handler.py\nCode: @@ -0,0 +1,104 @@\n+from mindsdb.integrations.handlers.youtube_handler.youtube_tables import YoutubeGetCommentsTable\n+from mindsdb.integrations.libs.api_handler import APIHandler\n+from mindsdb.integrations.libs.response import (\n+    HandlerStatusResponse as StatusResponse,\n+)\n+from mindsdb.utilities.log import get_log\n+from mindsdb_sql import parse_sql\n+\n+import requests\n+import pandas as pd\n+import json\n+from collections import OrderedDict\n+from mindsdb.integrations.libs.const import HANDLER_CONNECTION_ARG_TYPE as ARG_TYPE\n+\n+from googleapiclient.discovery import build\n+from googleapiclient.errors import HttpError\n+\n+logger = get_log(\"integrations.youtube_handler\")\n+\n+class YoutubeHandler(APIHandler):\n+    \"\"\"Strava handler implementation\"\"\"\n+\n+    def __init__(self, name=None, **kwargs):\n+        \"\"\"Initialize the Strava handler.\nComment: Please change Strava",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "mindsdb/integrations/handlers/youtube_handler/youtube_handler.py",
    "pr_number": 5891,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1185902066,
    "comment_created_at": "2023-05-05T09:49:08Z"
  },
  {
    "code": "@@ -0,0 +1,225 @@\n+.Dd May 18, 2004",
    "comment": "Please exclude this file",
    "line_number": 1,
    "enriched": "File: mindsdb/share/man/man1/ttx.1\nCode: @@ -0,0 +1,225 @@\n+.Dd May 18, 2004\nComment: Please exclude this file",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "mindsdb/share/man/man1/ttx.1",
    "pr_number": 9427,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1680801874,
    "comment_created_at": "2024-07-17T10:16:32Z"
  },
  {
    "code": "@@ -0,0 +1,18 @@\n+class TableNotFound(Exception):",
    "comment": "I think this file should be located in \r\nmindsdb/integrations/libs, near by api_handler.py",
    "line_number": 1,
    "enriched": "File: mindsdb/exceptions.py\nCode: @@ -0,0 +1,18 @@\n+class TableNotFound(Exception):\nComment: I think this file should be located in \r\nmindsdb/integrations/libs, near by api_handler.py",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "mindsdb/exceptions.py",
    "pr_number": 7956,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1374543836,
    "comment_created_at": "2023-10-27T13:00:14Z"
  },
  {
    "code": "@@ -0,0 +1,253 @@\n+import os\n+import time\n+import pickle\n+import signal\n+import tempfile\n+import threading\n+from pathlib import Path\n+from functools import wraps\n+from collections.abc import Callable\n+\n+import psutil\n+from walrus import Database\n+from pandas import DataFrame\n+\n+from mindsdb.utilities.config import Config\n+from mindsdb.utilities.context import context as ctx\n+from mindsdb.integrations.libs.process_cache import process_cache\n+from mindsdb.utilities.ml_task_queue.utils import RedisKey, StatusNotifier, to_bytes, from_bytes\n+from mindsdb.utilities.fs import clean_unlinked_process_marks\n+from mindsdb.utilities.functions import mark_process\n+from mindsdb.utilities.ml_task_queue.const import (\n+    ML_TASK_TYPE,\n+    ML_TASK_STATUS,\n+    TASKS_STREAM_NAME,\n+    TASKS_STREAM_CONSUMER_NAME,\n+    TASKS_STREAM_CONSUMER_GROUP_NAME\n+)\n+\n+\n+def _save_thread_link(func: Callable) -> Callable:\n+    \"\"\" Decorator for MLTaskConsumer.\n+        Save thread in which func is executed to a list.\n+    \"\"\"\n+    @wraps(func)\n+    def wrapper(self, *args, **kwargs) -> None:\n+        current_thread = threading.current_thread()\n+        self._listen_message_threads.append(current_thread)\n+        try:\n+            result = func(self, *args, **kwargs)\n+        finally:\n+            self._listen_message_threads.remove(current_thread)\n+        return result\n+    return wrapper\n+\n+\n+class MLTaskConsumer:\n+    \"\"\" Listener of ML tasks queue and tasks executioner.\n+        Each new message waited and executed in separate thread.\n+\n+        Attributes:\n+            _ready_event (Event): set if ready to start new queue listen thread\n+            _stop_event (Event): set if need to stop all threads/processes\n+            cpu_stat (list[float]): CPU usage statistic. Each value is 0-100 float representing CPU usage in %\n+            _collect_cpu_stat_thread (Thread): pointer to thread that collecting CPU usage statistic\n+            _listen_message_threads (list[Thread]): list of pointers to threads where queue messages are listening/processing\n+            db (Redis): database object\n+            cache: redis cache abstrtaction\n+            consumer_group: redis consumer group object\n+    \"\"\"\n+\n+    def __init__(self) -> None:\n+        self._ready_event = threading.Event()\n+        self._ready_event.set()\n+\n+        self._stop_event = threading.Event()\n+        self._stop_event.clear()\n+\n+        # region preload ml handlers\n+        from mindsdb.interfaces.database.integrations import integration_controller\n+\n+        config = Config()\n+        is_cloud = config.get('cloud', False)\n+\n+        preload_hendlers = {}\n+        lightwood_handler = integration_controller.handler_modules['lightwood']\n+        if lightwood_handler.Handler is not None:\n+            preload_hendlers[lightwood_handler.Handler] = 4 if is_cloud else 1\n+\n+        huggingface_handler = integration_controller.handler_modules['huggingface']\n+        if huggingface_handler.Handler is not None:\n+            preload_hendlers[huggingface_handler.Handler] = 1 if is_cloud else 0\n+\n+        openai_handler = integration_controller.handler_modules['openai']\n+        if openai_handler.Handler is not None:\n+            preload_hendlers[openai_handler.Handler] = 1 if is_cloud else 0\n+\n+        process_cache.init(preload_hendlers)\n+        # endregion\n+\n+        # region collect cpu usage statistic\n+        self.cpu_stat = [0] * 10\n+        self._collect_cpu_stat_thread = threading.Thread(target=self._collect_cpu_stat)\n+        self._collect_cpu_stat_thread.start()\n+        # endregion\n+\n+        self._listen_message_threads = []\n+\n+        # region connect to redis\n+        config = config.get('ml_task_queue', {})\n+        self.db = Database(\n+            host=config.get('host', 'localhost'),\n+            port=config.get('port', 6379),\n+            db=config.get('db', 0),\n+            username=config.get('username'),\n+            password=config.get('password'),\n+            protocol=3\n+        )\n+        try:\n+            self.db.ping()\n+        except ConnectionError:\n+            print('Cant connect to redis')\n+            raise\n+        self.db.Stream(TASKS_STREAM_NAME)\n+        self.cache = self.db.cache()\n+        self.consumer_group = self.db.consumer_group(TASKS_STREAM_CONSUMER_GROUP_NAME, [TASKS_STREAM_NAME])\n+        self.consumer_group.create()\n+        self.consumer_group.consumer(TASKS_STREAM_CONSUMER_NAME)\n+        # endregion\n+\n+    def _collect_cpu_stat(self) -> None:\n+        \"\"\" Collect CPU usage statistic. Executerd in thread.\n+        \"\"\"\n+        while self._stop_event.is_set() is False:\n+            self.cpu_stat = self.cpu_stat[1:]\n+            self.cpu_stat.append(psutil.cpu_percent())\n+            time.sleep(1)\n+\n+    def get_avg_cpu_usage(self) -> float:\n+        \"\"\" get average CPU usage for last period (10s by default)\n+\n+            Returns:\n+                float: 0-100 value, average CPU usage\n+        \"\"\"\n+        return sum(self.cpu_stat) / len(self.cpu_stat)\n+\n+    def wait_free_resources(self) -> None:\n+        \"\"\" Sleep in thread untill there are free resources. Checks:\n+            - avg CPU usage is less than 60%\n+            - current CPU usage is less than 60%\n+            - current tasks count is less than (N CPU cores) / 2\n+        \"\"\"\n+        config = Config()\n+        is_cloud = config.get('cloud', False)\n+        processes_dir = Path(tempfile.gettempdir()).joinpath('mindsdb/processes/learn/')\n+        while True:\n+            while self.get_avg_cpu_usage() > 60 or max(self.cpu_stat[-3:]) > 60:\n+                time.sleep(1)\n+            if is_cloud and processes_dir.is_dir():\n+                clean_unlinked_process_marks()\n+                while (len(list(processes_dir.iterdir())) * 2) >= os.cpu_count():\n+                    time.sleep(1)\n+                    clean_unlinked_process_marks()\n+            if (self.get_avg_cpu_usage() > 60 or max(self.cpu_stat[-3:]) > 60) is False:\n+                return\n+\n+    @_save_thread_link\n+    def _listen(self) -> None:\n+        \"\"\" Listen message queue untill get new message. Execute task.\n+        \"\"\"\n+        message = None\n+        while message is None:\n+            self.wait_free_resources()\n+            if self._stop_event.is_set():\n+                return\n+            message = self.consumer_group.read(count=1, block=1000, consumer=TASKS_STREAM_CONSUMER_NAME)\n+            if message.get(TASKS_STREAM_NAME) is None or len(message.get(TASKS_STREAM_NAME)) == 0:\n+                message = None\n+\n+        try:\n+            message = message[TASKS_STREAM_NAME][0][0]\n+            message_id = message[0].decode()\n+            message_content = message[1]\n+            self.consumer_group.streams[TASKS_STREAM_NAME].ack(message_id)\n+\n+            payload = pickle.loads(message_content[b'payload'])\n+            task_type = ML_TASK_TYPE(message_content[b'task_type'])\n+            model_id = int(message_content[b'model_id'])\n+            company_id = message_content[b'company_id']\n+            if len(company_id) == 0:\n+                company_id = None\n+            redis_key = RedisKey(message_content.get(b'redis_key'))\n+\n+            # region read dataframe\n+            dataframe_bytes = self.cache.get(redis_key.dataframe)\n+            dataframe = None\n+            if dataframe_bytes is not None:\n+                dataframe = from_bytes(dataframe_bytes)\n+                self.cache.delete(redis_key.dataframe)\n+            # endregion\n+\n+            ctx.load(payload['context'])\n+        finally:\n+            self._ready_event.set()\n+\n+        try:\n+            task = process_cache.apply_async(\n+                task_type=task_type,\n+                model_id=model_id,\n+                payload=payload,\n+                dataframe=dataframe\n+            )\n+            status_notifier = StatusNotifier(redis_key, ML_TASK_STATUS.PROCESSING, self.db, self.cache)\n+            status_notifier.start()\n+            result = task.result()\n+        except Exception as e:\n+            status_notifier.stop()\n+            exception_bytes = to_bytes(e)\n+            self.cache.set(redis_key.exception, exception_bytes, 10)\n+            self.db.publish(redis_key.status, ML_TASK_STATUS.ERROR.value)\n+            self.cache.set(redis_key.status, ML_TASK_STATUS.ERROR.value, 180)\n+        else:\n+            status_notifier.stop()\n+            if isinstance(result, DataFrame):\n+                dataframe_bytes = to_bytes(result)\n+                self.cache.set(redis_key.dataframe, dataframe_bytes, 10)\n+            self.db.publish(redis_key.status, ML_TASK_STATUS.COMPLETE.value)\n+            self.cache.set(redis_key.status, ML_TASK_STATUS.COMPLETE.value, 180)\n+\n+    def run(self) -> None:\n+        \"\"\" Start new listen thread each time when _ready_event is set\n+        \"\"\"\n+        self._ready_event.set()\n+        while self._stop_event.is_set() is False:\n+            self._ready_event.wait(timeout=1)\n+            if self._ready_event.is_set() is False:\n+                continue\n+            self._ready_event.clear()\n+            threading.Thread(target=self._listen).start()",
    "comment": "it starts new thread for every new task. does the thread close itself ?",
    "line_number": 228,
    "enriched": "File: mindsdb/utilities/ml_task_queue/consumer.py\nCode: @@ -0,0 +1,253 @@\n+import os\n+import time\n+import pickle\n+import signal\n+import tempfile\n+import threading\n+from pathlib import Path\n+from functools import wraps\n+from collections.abc import Callable\n+\n+import psutil\n+from walrus import Database\n+from pandas import DataFrame\n+\n+from mindsdb.utilities.config import Config\n+from mindsdb.utilities.context import context as ctx\n+from mindsdb.integrations.libs.process_cache import process_cache\n+from mindsdb.utilities.ml_task_queue.utils import RedisKey, StatusNotifier, to_bytes, from_bytes\n+from mindsdb.utilities.fs import clean_unlinked_process_marks\n+from mindsdb.utilities.functions import mark_process\n+from mindsdb.utilities.ml_task_queue.const import (\n+    ML_TASK_TYPE,\n+    ML_TASK_STATUS,\n+    TASKS_STREAM_NAME,\n+    TASKS_STREAM_CONSUMER_NAME,\n+    TASKS_STREAM_CONSUMER_GROUP_NAME\n+)\n+\n+\n+def _save_thread_link(func: Callable) -> Callable:\n+    \"\"\" Decorator for MLTaskConsumer.\n+        Save thread in which func is executed to a list.\n+    \"\"\"\n+    @wraps(func)\n+    def wrapper(self, *args, **kwargs) -> None:\n+        current_thread = threading.current_thread()\n+        self._listen_message_threads.append(current_thread)\n+        try:\n+            result = func(self, *args, **kwargs)\n+        finally:\n+            self._listen_message_threads.remove(current_thread)\n+        return result\n+    return wrapper\n+\n+\n+class MLTaskConsumer:\n+    \"\"\" Listener of ML tasks queue and tasks executioner.\n+        Each new message waited and executed in separate thread.\n+\n+        Attributes:\n+            _ready_event (Event): set if ready to start new queue listen thread\n+            _stop_event (Event): set if need to stop all threads/processes\n+            cpu_stat (list[float]): CPU usage statistic. Each value is 0-100 float representing CPU usage in %\n+            _collect_cpu_stat_thread (Thread): pointer to thread that collecting CPU usage statistic\n+            _listen_message_threads (list[Thread]): list of pointers to threads where queue messages are listening/processing\n+            db (Redis): database object\n+            cache: redis cache abstrtaction\n+            consumer_group: redis consumer group object\n+    \"\"\"\n+\n+    def __init__(self) -> None:\n+        self._ready_event = threading.Event()\n+        self._ready_event.set()\n+\n+        self._stop_event = threading.Event()\n+        self._stop_event.clear()\n+\n+        # region preload ml handlers\n+        from mindsdb.interfaces.database.integrations import integration_controller\n+\n+        config = Config()\n+        is_cloud = config.get('cloud', False)\n+\n+        preload_hendlers = {}\n+        lightwood_handler = integration_controller.handler_modules['lightwood']\n+        if lightwood_handler.Handler is not None:\n+            preload_hendlers[lightwood_handler.Handler] = 4 if is_cloud else 1\n+\n+        huggingface_handler = integration_controller.handler_modules['huggingface']\n+        if huggingface_handler.Handler is not None:\n+            preload_hendlers[huggingface_handler.Handler] = 1 if is_cloud else 0\n+\n+        openai_handler = integration_controller.handler_modules['openai']\n+        if openai_handler.Handler is not None:\n+            preload_hendlers[openai_handler.Handler] = 1 if is_cloud else 0\n+\n+        process_cache.init(preload_hendlers)\n+        # endregion\n+\n+        # region collect cpu usage statistic\n+        self.cpu_stat = [0] * 10\n+        self._collect_cpu_stat_thread = threading.Thread(target=self._collect_cpu_stat)\n+        self._collect_cpu_stat_thread.start()\n+        # endregion\n+\n+        self._listen_message_threads = []\n+\n+        # region connect to redis\n+        config = config.get('ml_task_queue', {})\n+        self.db = Database(\n+            host=config.get('host', 'localhost'),\n+            port=config.get('port', 6379),\n+            db=config.get('db', 0),\n+            username=config.get('username'),\n+            password=config.get('password'),\n+            protocol=3\n+        )\n+        try:\n+            self.db.ping()\n+        except ConnectionError:\n+            print('Cant connect to redis')\n+            raise\n+        self.db.Stream(TASKS_STREAM_NAME)\n+        self.cache = self.db.cache()\n+        self.consumer_group = self.db.consumer_group(TASKS_STREAM_CONSUMER_GROUP_NAME, [TASKS_STREAM_NAME])\n+        self.consumer_group.create()\n+        self.consumer_group.consumer(TASKS_STREAM_CONSUMER_NAME)\n+        # endregion\n+\n+    def _collect_cpu_stat(self) -> None:\n+        \"\"\" Collect CPU usage statistic. Executerd in thread.\n+        \"\"\"\n+        while self._stop_event.is_set() is False:\n+            self.cpu_stat = self.cpu_stat[1:]\n+            self.cpu_stat.append(psutil.cpu_percent())\n+            time.sleep(1)\n+\n+    def get_avg_cpu_usage(self) -> float:\n+        \"\"\" get average CPU usage for last period (10s by default)\n+\n+            Returns:\n+                float: 0-100 value, average CPU usage\n+        \"\"\"\n+        return sum(self.cpu_stat) / len(self.cpu_stat)\n+\n+    def wait_free_resources(self) -> None:\n+        \"\"\" Sleep in thread untill there are free resources. Checks:\n+            - avg CPU usage is less than 60%\n+            - current CPU usage is less than 60%\n+            - current tasks count is less than (N CPU cores) / 2\n+        \"\"\"\n+        config = Config()\n+        is_cloud = config.get('cloud', False)\n+        processes_dir = Path(tempfile.gettempdir()).joinpath('mindsdb/processes/learn/')\n+        while True:\n+            while self.get_avg_cpu_usage() > 60 or max(self.cpu_stat[-3:]) > 60:\n+                time.sleep(1)\n+            if is_cloud and processes_dir.is_dir():\n+                clean_unlinked_process_marks()\n+                while (len(list(processes_dir.iterdir())) * 2) >= os.cpu_count():\n+                    time.sleep(1)\n+                    clean_unlinked_process_marks()\n+            if (self.get_avg_cpu_usage() > 60 or max(self.cpu_stat[-3:]) > 60) is False:\n+                return\n+\n+    @_save_thread_link\n+    def _listen(self) -> None:\n+        \"\"\" Listen message queue untill get new message. Execute task.\n+        \"\"\"\n+        message = None\n+        while message is None:\n+            self.wait_free_resources()\n+            if self._stop_event.is_set():\n+                return\n+            message = self.consumer_group.read(count=1, block=1000, consumer=TASKS_STREAM_CONSUMER_NAME)\n+            if message.get(TASKS_STREAM_NAME) is None or len(message.get(TASKS_STREAM_NAME)) == 0:\n+                message = None\n+\n+        try:\n+            message = message[TASKS_STREAM_NAME][0][0]\n+            message_id = message[0].decode()\n+            message_content = message[1]\n+            self.consumer_group.streams[TASKS_STREAM_NAME].ack(message_id)\n+\n+            payload = pickle.loads(message_content[b'payload'])\n+            task_type = ML_TASK_TYPE(message_content[b'task_type'])\n+            model_id = int(message_content[b'model_id'])\n+            company_id = message_content[b'company_id']\n+            if len(company_id) == 0:\n+                company_id = None\n+            redis_key = RedisKey(message_content.get(b'redis_key'))\n+\n+            # region read dataframe\n+            dataframe_bytes = self.cache.get(redis_key.dataframe)\n+            dataframe = None\n+            if dataframe_bytes is not None:\n+                dataframe = from_bytes(dataframe_bytes)\n+                self.cache.delete(redis_key.dataframe)\n+            # endregion\n+\n+            ctx.load(payload['context'])\n+        finally:\n+            self._ready_event.set()\n+\n+        try:\n+            task = process_cache.apply_async(\n+                task_type=task_type,\n+                model_id=model_id,\n+                payload=payload,\n+                dataframe=dataframe\n+            )\n+            status_notifier = StatusNotifier(redis_key, ML_TASK_STATUS.PROCESSING, self.db, self.cache)\n+            status_notifier.start()\n+            result = task.result()\n+        except Exception as e:\n+            status_notifier.stop()\n+            exception_bytes = to_bytes(e)\n+            self.cache.set(redis_key.exception, exception_bytes, 10)\n+            self.db.publish(redis_key.status, ML_TASK_STATUS.ERROR.value)\n+            self.cache.set(redis_key.status, ML_TASK_STATUS.ERROR.value, 180)\n+        else:\n+            status_notifier.stop()\n+            if isinstance(result, DataFrame):\n+                dataframe_bytes = to_bytes(result)\n+                self.cache.set(redis_key.dataframe, dataframe_bytes, 10)\n+            self.db.publish(redis_key.status, ML_TASK_STATUS.COMPLETE.value)\n+            self.cache.set(redis_key.status, ML_TASK_STATUS.COMPLETE.value, 180)\n+\n+    def run(self) -> None:\n+        \"\"\" Start new listen thread each time when _ready_event is set\n+        \"\"\"\n+        self._ready_event.set()\n+        while self._stop_event.is_set() is False:\n+            self._ready_event.wait(timeout=1)\n+            if self._ready_event.is_set() is False:\n+                continue\n+            self._ready_event.clear()\n+            threading.Thread(target=self._listen).start()\nComment: it starts new thread for every new task. does the thread close itself ?",
    "subcategory": "timing",
    "category": "functional",
    "file_path": "mindsdb/utilities/ml_task_queue/consumer.py",
    "pr_number": 7437,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1356818190,
    "comment_created_at": "2023-10-12T13:23:30Z"
  },
  {
    "code": "@@ -695,33 +691,97 @@ def _load_handler_modules(self):\n         for handler_dir in handlers_path.iterdir():\n             if handler_dir.is_dir() is False or handler_dir.name.startswith('__'):\n                 continue\n-            self.import_handler('mindsdb.integrations.handlers.', handler_dir)\n-\n-    def import_handler(self, base_import: str, handler_dir: Path):\n-        handler_folder_name = str(handler_dir.name)\n \n-        try:\n-            handler_module = importlib.import_module(f'{base_import}{handler_folder_name}')\n-            handler_meta = self._get_handler_meta(handler_module)\n-        except Exception as e:\n-            handler_name = handler_folder_name\n-            if handler_name.endswith('_handler'):\n-                handler_name = handler_name[:-8]\n+            handler_info = self._get_handler_info(handler_dir)\n+            if 'name' not in handler_info:\n+                continue\n+            handler_name = handler_info['name']\n             dependencies = self._read_dependencies(handler_dir)\n             handler_meta = {\n+                'path': handler_dir,\n                 'import': {\n-                    'success': False,\n-                    'error_message': str(e),\n-                    'folder': handler_folder_name,\n-                    'dependencies': dependencies\n+                    'success': None,\n+                    'error_message': None,\n+                    'folder': handler_dir.name,\n+                    'dependencies': dependencies,\n                 },\n-                'name': handler_name\n+                'name': handler_name,\n+                'permanent': handler_info.get('permanent', False),\n             }\n+            self.handlers_import_status[handler_name] = handler_meta\n+\n+        # import all handlers in thread\n+        def import_handlers():\n+            # give time to start server\n+            time.sleep(3)\n+            self.get_handlers_import_status()\n \n-        self.handlers_import_status[handler_meta['name']] = handler_meta\n+        thread = threading.Thread(target=import_handlers)",
    "comment": "This is the importing of all handlers in thread. \r\n",
    "line_number": 719,
    "enriched": "File: mindsdb/interfaces/database/integrations.py\nCode: @@ -695,33 +691,97 @@ def _load_handler_modules(self):\n         for handler_dir in handlers_path.iterdir():\n             if handler_dir.is_dir() is False or handler_dir.name.startswith('__'):\n                 continue\n-            self.import_handler('mindsdb.integrations.handlers.', handler_dir)\n-\n-    def import_handler(self, base_import: str, handler_dir: Path):\n-        handler_folder_name = str(handler_dir.name)\n \n-        try:\n-            handler_module = importlib.import_module(f'{base_import}{handler_folder_name}')\n-            handler_meta = self._get_handler_meta(handler_module)\n-        except Exception as e:\n-            handler_name = handler_folder_name\n-            if handler_name.endswith('_handler'):\n-                handler_name = handler_name[:-8]\n+            handler_info = self._get_handler_info(handler_dir)\n+            if 'name' not in handler_info:\n+                continue\n+            handler_name = handler_info['name']\n             dependencies = self._read_dependencies(handler_dir)\n             handler_meta = {\n+                'path': handler_dir,\n                 'import': {\n-                    'success': False,\n-                    'error_message': str(e),\n-                    'folder': handler_folder_name,\n-                    'dependencies': dependencies\n+                    'success': None,\n+                    'error_message': None,\n+                    'folder': handler_dir.name,\n+                    'dependencies': dependencies,\n                 },\n-                'name': handler_name\n+                'name': handler_name,\n+                'permanent': handler_info.get('permanent', False),\n             }\n+            self.handlers_import_status[handler_name] = handler_meta\n+\n+        # import all handlers in thread\n+        def import_handlers():\n+            # give time to start server\n+            time.sleep(3)\n+            self.get_handlers_import_status()\n \n-        self.handlers_import_status[handler_meta['name']] = handler_meta\n+        thread = threading.Thread(target=import_handlers)\nComment: This is the importing of all handlers in thread. \r\n",
    "subcategory": "timing",
    "category": "functional",
    "file_path": "mindsdb/interfaces/database/integrations.py",
    "pr_number": 9493,
    "repo": "mindsdb",
    "owner": "mindsdb",
    "comment_id": 1684452473,
    "comment_created_at": "2024-07-19T14:18:01Z"
  }
]