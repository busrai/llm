[
  {
    "code": "@@ -106,6 +106,10 @@ python -c \"import samgeo; print('samgeo import successful')\"\n pip install -U sam3 geoai-py segment-geospatial\n ```\n \n+You can follow this [video tutorial](https://youtu.be/a-Ns9peiuu8) to install the GeoAI QGIS Plugin on Windows:\n+\n+[![windows](https://github.com/user-attachments/assets/8d89d535-1d66-45d2-a6c0-171416c259c9)](https://youtu.be/a-Ns9peiuu8)",
    "comment": "The alt text \"windows\" for the video thumbnail image is not descriptive enough for accessibility. Consider using more descriptive alt text such as \"GeoAI QGIS Plugin Windows Installation Video Tutorial\" to provide better context for screen reader users.\n```suggestion\n[![GeoAI QGIS Plugin Windows Installation Video Tutorial](https://github.com/user-attachments/assets/8d89d535-1d66-45d2-a6c0-171416c259c9)](https://youtu.be/a-Ns9peiuu8)\n```",
    "line_number": 111,
    "enriched": "File: qgis_plugin/README.md\nCode: @@ -106,6 +106,10 @@ python -c \"import samgeo; print('samgeo import successful')\"\n pip install -U sam3 geoai-py segment-geospatial\n ```\n \n+You can follow this [video tutorial](https://youtu.be/a-Ns9peiuu8) to install the GeoAI QGIS Plugin on Windows:\n+\n+[![windows](https://github.com/user-attachments/assets/8d89d535-1d66-45d2-a6c0-171416c259c9)](https://youtu.be/a-Ns9peiuu8)\nComment: The alt text \"windows\" for the video thumbnail image is not descriptive enough for accessibility. Consider using more descriptive alt text such as \"GeoAI QGIS Plugin Windows Installation Video Tutorial\" to provide better context for screen reader users.\n```suggestion\n[![GeoAI QGIS Plugin Windows Installation Video Tutorial](https://github.com/user-attachments/assets/8d89d535-1d66-45d2-a6c0-171416c259c9)](https://youtu.be/a-Ns9peiuu8)\n```",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "qgis_plugin/README.md",
    "pr_number": 404,
    "repo": "geoai",
    "owner": "opengeos",
    "comment_id": 2616436655,
    "comment_created_at": "2025-12-13T17:43:48Z"
  },
  {
    "code": "@@ -99,10 +99,10 @@ Windows + SAM3 often needs the PyTorch wheels from NVIDIA and PyPI:\n conda create -n geo python=3.12\n conda activate geo\n conda install pytorch torchvision pytorch-cuda=12.1 -c pytorch -c nvidia\n-conda install -c conda-forge geoai\n+conda install -c conda-forge geoai qgis\n python -c \"import geoai; print('geoai import successful')\"\n-conda install -c conda-forge qgis\n pip install -U \"segment-geospatial[samgeo3]\" triton-windows\n+python -c \"import samgeo; print('samgeo import successful')\"\n pip install -U sam3 geoai-py segment-geospatial",
    "comment": "Installing `segment-geospatial` twice could cause confusion and is redundant. Line 104 installs `segment-geospatial[samgeo3]` and line 106 installs `segment-geospatial` again. Consider removing `segment-geospatial` from line 106 since it's already installed on line 104 with the samgeo3 extra dependencies.\n```suggestion\npip install -U sam3 geoai-py\n```",
    "line_number": 106,
    "enriched": "File: qgis_plugin/README.md\nCode: @@ -99,10 +99,10 @@ Windows + SAM3 often needs the PyTorch wheels from NVIDIA and PyPI:\n conda create -n geo python=3.12\n conda activate geo\n conda install pytorch torchvision pytorch-cuda=12.1 -c pytorch -c nvidia\n-conda install -c conda-forge geoai\n+conda install -c conda-forge geoai qgis\n python -c \"import geoai; print('geoai import successful')\"\n-conda install -c conda-forge qgis\n pip install -U \"segment-geospatial[samgeo3]\" triton-windows\n+python -c \"import samgeo; print('samgeo import successful')\"\n pip install -U sam3 geoai-py segment-geospatial\nComment: Installing `segment-geospatial` twice could cause confusion and is redundant. Line 104 installs `segment-geospatial[samgeo3]` and line 106 installs `segment-geospatial` again. Consider removing `segment-geospatial` from line 106 since it's already installed on line 104 with the samgeo3 extra dependencies.\n```suggestion\npip install -U sam3 geoai-py\n```",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "qgis_plugin/README.md",
    "pr_number": 403,
    "repo": "geoai",
    "owner": "opengeos",
    "comment_id": 2616408079,
    "comment_created_at": "2025-12-13T16:48:15Z"
  },
  {
    "code": "@@ -103,7 +103,7 @@ conda install -c conda-forge geoai\n python -c \"import geoai; print('geoai import successful')\"\n conda install -c conda-forge qgis\n pip install -U \"segment-geospatial[samgeo3]\" triton-windows\n-pip install -U sam3 geoai\n+pip install -U sam3 geoai-py segment-geospatial",
    "comment": "The PR title states \"Fix typo\" but this change appears to be a functional modification rather than a typo correction. The change modifies the package name from 'geoai' to 'geoai-py' and adds 'segment-geospatial' as an additional package to install. This is a substantive change to the installation instructions that could affect functionality. Consider updating the PR title and description to accurately reflect the nature of these changes.",
    "line_number": 106,
    "enriched": "File: qgis_plugin/README.md\nCode: @@ -103,7 +103,7 @@ conda install -c conda-forge geoai\n python -c \"import geoai; print('geoai import successful')\"\n conda install -c conda-forge qgis\n pip install -U \"segment-geospatial[samgeo3]\" triton-windows\n-pip install -U sam3 geoai\n+pip install -U sam3 geoai-py segment-geospatial\nComment: The PR title states \"Fix typo\" but this change appears to be a functional modification rather than a typo correction. The change modifies the package name from 'geoai' to 'geoai-py' and adds 'segment-geospatial' as an additional package to install. This is a substantive change to the installation instructions that could affect functionality. Consider updating the PR title and description to accurately reflect the nature of these changes.",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "qgis_plugin/README.md",
    "pr_number": 402,
    "repo": "geoai",
    "owner": "opengeos",
    "comment_id": 2616397850,
    "comment_created_at": "2025-12-13T16:29:05Z"
  },
  {
    "code": "@@ -102,7 +102,8 @@ conda install pytorch torchvision pytorch-cuda=12.1 -c pytorch -c nvidia\n conda install -c conda-forge geoai\n python -c \"import geoai; print('geoai import successful')\"\n conda install -c conda-forge qgis\n-pip install -U \"segment-geospatial[samgeo3]\" sam3 triton-windows\n+pip install -U \"segment-geospatial[samgeo3]\" triton-windows\n+pip install -U sam3 geoai",
    "comment": "The package `geoai` is being installed twice: first via conda on line 102 (`conda install -c conda-forge geoai`) and then again via pip on line 106 (`pip install -U sam3 geoai`). Installing the same package through both conda and pip in the same environment can lead to dependency conflicts and package management issues. Consider removing `geoai` from the pip install command on line 106 since it's already installed via conda.\n```suggestion\npip install -U sam3\n```",
    "line_number": 106,
    "enriched": "File: qgis_plugin/README.md\nCode: @@ -102,7 +102,8 @@ conda install pytorch torchvision pytorch-cuda=12.1 -c pytorch -c nvidia\n conda install -c conda-forge geoai\n python -c \"import geoai; print('geoai import successful')\"\n conda install -c conda-forge qgis\n-pip install -U \"segment-geospatial[samgeo3]\" sam3 triton-windows\n+pip install -U \"segment-geospatial[samgeo3]\" triton-windows\n+pip install -U sam3 geoai\nComment: The package `geoai` is being installed twice: first via conda on line 102 (`conda install -c conda-forge geoai`) and then again via pip on line 106 (`pip install -U sam3 geoai`). Installing the same package through both conda and pip in the same environment can lead to dependency conflicts and package management issues. Consider removing `geoai` from the pip install command on line 106 since it's already installed via conda.\n```suggestion\npip install -U sam3\n```",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "qgis_plugin/README.md",
    "pr_number": 401,
    "repo": "geoai",
    "owner": "opengeos",
    "comment_id": 2616394083,
    "comment_created_at": "2025-12-13T16:20:26Z"
  },
  {
    "code": "@@ -0,0 +1,475 @@\n+\"\"\"\n+Update Checker Dialog for GeoAI Plugin\n+\n+This dialog provides functionality to check for updates from GitHub\n+and download/install the latest version of the plugin.\n+\"\"\"\n+\n+import os\n+import re\n+import shutil\n+import tempfile\n+import zipfile\n+from urllib.request import urlopen, urlretrieve\n+from urllib.error import URLError, HTTPError\n+\n+from qgis.PyQt.QtCore import Qt, QThread, pyqtSignal\n+from qgis.PyQt.QtWidgets import (\n+    QDialog,\n+    QVBoxLayout,\n+    QHBoxLayout,\n+    QLabel,\n+    QPushButton,\n+    QProgressBar,\n+    QMessageBox,\n+    QGroupBox,\n+    QFormLayout,\n+    QTextEdit,\n+)\n+from qgis.PyQt.QtGui import QFont\n+from qgis.core import QgsApplication\n+\n+\n+# GitHub URLs for the plugin\n+GITHUB_REPO = \"opengeos/geoai\"\n+GITHUB_BRANCH = \"main\"\n+PLUGIN_PATH = \"qgis_plugin/geoai_plugin\"\n+METADATA_URL = f\"https://raw.githubusercontent.com/{GITHUB_REPO}/{GITHUB_BRANCH}/{PLUGIN_PATH}/metadata.txt\"\n+ZIP_URL = f\"https://github.com/{GITHUB_REPO}/archive/refs/heads/{GITHUB_BRANCH}.zip\"\n+\n+\n+class VersionCheckWorker(QThread):\n+    \"\"\"Worker thread for checking the latest version from GitHub.\"\"\"\n+\n+    finished = pyqtSignal(dict)\n+    error = pyqtSignal(str)\n+\n+    def run(self):\n+        \"\"\"Fetch the latest metadata from GitHub.\"\"\"\n+        try:\n+            with urlopen(METADATA_URL, timeout=15) as response:\n+                content = response.read().decode(\"utf-8\")\n+\n+            # Parse version from metadata\n+            version_match = re.search(r\"^version=(.+)$\", content, re.MULTILINE)\n+            if version_match:\n+                latest_version = version_match.group(1).strip()\n+            else:\n+                self.error.emit(\"Could not parse version from remote metadata\")\n+                return\n+\n+            # Parse changelog\n+            changelog_match = re.search(\n+                r\"^changelog=(.+?)(?=^\\[|\\Z)\", content, re.MULTILINE | re.DOTALL\n+            )\n+            changelog = \"\"\n+            if changelog_match:\n+                changelog = changelog_match.group(1).strip()\n+\n+            self.finished.emit(\n+                {\"version\": latest_version, \"changelog\": changelog, \"metadata\": content}\n+            )\n+\n+        except HTTPError as e:\n+            self.error.emit(f\"HTTP Error: {e.code} - {e.reason}\")\n+        except URLError as e:\n+            self.error.emit(f\"URL Error: {e.reason}\")\n+        except Exception as e:\n+            self.error.emit(f\"Error checking for updates: {str(e)}\")\n+\n+\n+class DownloadWorker(QThread):\n+    \"\"\"Worker thread for downloading and installing the plugin update.\"\"\"\n+\n+    finished = pyqtSignal(str)\n+    error = pyqtSignal(str)\n+    progress = pyqtSignal(int, str)\n+\n+    def __init__(self, plugin_dir):\n+        super().__init__()\n+        self.plugin_dir = plugin_dir\n+\n+    def run(self):\n+        \"\"\"Download and install the latest plugin version.\"\"\"\n+        temp_dir = None\n+        try:\n+            # Create temporary directory\n+            temp_dir = tempfile.mkdtemp(prefix=\"geoai_update_\")\n+            zip_path = os.path.join(temp_dir, \"geoai.zip\")\n+\n+            # Download the zip file\n+            self.progress.emit(10, \"Downloading plugin from GitHub...\")\n+\n+            def reporthook(block_num, block_size, total_size):\n+                if total_size > 0:\n+                    downloaded = block_num * block_size\n+                    percent = min(int((downloaded / total_size) * 50), 50)\n+                    self.progress.emit(10 + percent, \"Downloading...\")\n+\n+            urlretrieve(ZIP_URL, zip_path, reporthook)",
    "comment": "The file download and installation process lacks integrity verification. Downloading code from the internet without checksum validation poses a security risk. Consider adding SHA256 hash verification of the downloaded zip file against a known hash published in the release metadata to ensure the downloaded file hasn't been tampered with.",
    "line_number": 109,
    "enriched": "File: qgis_plugin/geoai_plugin/dialogs/update_checker.py\nCode: @@ -0,0 +1,475 @@\n+\"\"\"\n+Update Checker Dialog for GeoAI Plugin\n+\n+This dialog provides functionality to check for updates from GitHub\n+and download/install the latest version of the plugin.\n+\"\"\"\n+\n+import os\n+import re\n+import shutil\n+import tempfile\n+import zipfile\n+from urllib.request import urlopen, urlretrieve\n+from urllib.error import URLError, HTTPError\n+\n+from qgis.PyQt.QtCore import Qt, QThread, pyqtSignal\n+from qgis.PyQt.QtWidgets import (\n+    QDialog,\n+    QVBoxLayout,\n+    QHBoxLayout,\n+    QLabel,\n+    QPushButton,\n+    QProgressBar,\n+    QMessageBox,\n+    QGroupBox,\n+    QFormLayout,\n+    QTextEdit,\n+)\n+from qgis.PyQt.QtGui import QFont\n+from qgis.core import QgsApplication\n+\n+\n+# GitHub URLs for the plugin\n+GITHUB_REPO = \"opengeos/geoai\"\n+GITHUB_BRANCH = \"main\"\n+PLUGIN_PATH = \"qgis_plugin/geoai_plugin\"\n+METADATA_URL = f\"https://raw.githubusercontent.com/{GITHUB_REPO}/{GITHUB_BRANCH}/{PLUGIN_PATH}/metadata.txt\"\n+ZIP_URL = f\"https://github.com/{GITHUB_REPO}/archive/refs/heads/{GITHUB_BRANCH}.zip\"\n+\n+\n+class VersionCheckWorker(QThread):\n+    \"\"\"Worker thread for checking the latest version from GitHub.\"\"\"\n+\n+    finished = pyqtSignal(dict)\n+    error = pyqtSignal(str)\n+\n+    def run(self):\n+        \"\"\"Fetch the latest metadata from GitHub.\"\"\"\n+        try:\n+            with urlopen(METADATA_URL, timeout=15) as response:\n+                content = response.read().decode(\"utf-8\")\n+\n+            # Parse version from metadata\n+            version_match = re.search(r\"^version=(.+)$\", content, re.MULTILINE)\n+            if version_match:\n+                latest_version = version_match.group(1).strip()\n+            else:\n+                self.error.emit(\"Could not parse version from remote metadata\")\n+                return\n+\n+            # Parse changelog\n+            changelog_match = re.search(\n+                r\"^changelog=(.+?)(?=^\\[|\\Z)\", content, re.MULTILINE | re.DOTALL\n+            )\n+            changelog = \"\"\n+            if changelog_match:\n+                changelog = changelog_match.group(1).strip()\n+\n+            self.finished.emit(\n+                {\"version\": latest_version, \"changelog\": changelog, \"metadata\": content}\n+            )\n+\n+        except HTTPError as e:\n+            self.error.emit(f\"HTTP Error: {e.code} - {e.reason}\")\n+        except URLError as e:\n+            self.error.emit(f\"URL Error: {e.reason}\")\n+        except Exception as e:\n+            self.error.emit(f\"Error checking for updates: {str(e)}\")\n+\n+\n+class DownloadWorker(QThread):\n+    \"\"\"Worker thread for downloading and installing the plugin update.\"\"\"\n+\n+    finished = pyqtSignal(str)\n+    error = pyqtSignal(str)\n+    progress = pyqtSignal(int, str)\n+\n+    def __init__(self, plugin_dir):\n+        super().__init__()\n+        self.plugin_dir = plugin_dir\n+\n+    def run(self):\n+        \"\"\"Download and install the latest plugin version.\"\"\"\n+        temp_dir = None\n+        try:\n+            # Create temporary directory\n+            temp_dir = tempfile.mkdtemp(prefix=\"geoai_update_\")\n+            zip_path = os.path.join(temp_dir, \"geoai.zip\")\n+\n+            # Download the zip file\n+            self.progress.emit(10, \"Downloading plugin from GitHub...\")\n+\n+            def reporthook(block_num, block_size, total_size):\n+                if total_size > 0:\n+                    downloaded = block_num * block_size\n+                    percent = min(int((downloaded / total_size) * 50), 50)\n+                    self.progress.emit(10 + percent, \"Downloading...\")\n+\n+            urlretrieve(ZIP_URL, zip_path, reporthook)\nComment: The file download and installation process lacks integrity verification. Downloading code from the internet without checksum validation poses a security risk. Consider adding SHA256 hash verification of the downloaded zip file against a known hash published in the release metadata to ensure the downloaded file hasn't been tampered with.",
    "subcategory": "validation",
    "category": "refactoring",
    "file_path": "qgis_plugin/geoai_plugin/dialogs/update_checker.py",
    "pr_number": 400,
    "repo": "geoai",
    "owner": "opengeos",
    "comment_id": 2616374902,
    "comment_created_at": "2025-12-13T15:40:56Z"
  },
  {
    "code": "@@ -223,6 +223,44 @@ def _create_model_tab(self):\n         self.set_file_btn.clicked.connect(self.set_image_from_file)\n         layer_layout.addWidget(self.set_file_btn)\n \n+        # Band selection for multi-band GeoTIFFs\n+        self.custom_bands_check = QCheckBox(\"Use custom RGB bands\")\n+        self.custom_bands_check.setChecked(False)\n+        self.custom_bands_check.setToolTip(\n+            \"Enable to specify custom bands for RGB display.\\n\"\n+            \"Useful for GeoTIFFs with more than 3 bands.\\n\"",
    "comment": "The tooltip text contains inconsistent capitalization and terminology. The phrase \"Useful for GeoTIFFs with more than 3 bands\" uses \"GeoTIFFs\" (plural) while elsewhere in the codebase the term \"GeoTIFF\" is used consistently. For consistency, consider using \"multi-band GeoTIFF files\" or similar phrasing.\n```suggestion\n            \"Useful for multi-band GeoTIFF files (with more than 3 bands).\\n\"\n```",
    "line_number": 231,
    "enriched": "File: qgis_plugin/geoai_plugin/dialogs/samgeo.py\nCode: @@ -223,6 +223,44 @@ def _create_model_tab(self):\n         self.set_file_btn.clicked.connect(self.set_image_from_file)\n         layer_layout.addWidget(self.set_file_btn)\n \n+        # Band selection for multi-band GeoTIFFs\n+        self.custom_bands_check = QCheckBox(\"Use custom RGB bands\")\n+        self.custom_bands_check.setChecked(False)\n+        self.custom_bands_check.setToolTip(\n+            \"Enable to specify custom bands for RGB display.\\n\"\n+            \"Useful for GeoTIFFs with more than 3 bands.\\n\"\nComment: The tooltip text contains inconsistent capitalization and terminology. The phrase \"Useful for GeoTIFFs with more than 3 bands\" uses \"GeoTIFFs\" (plural) while elsewhere in the codebase the term \"GeoTIFF\" is used consistently. For consistency, consider using \"multi-band GeoTIFF files\" or similar phrasing.\n```suggestion\n            \"Useful for multi-band GeoTIFF files (with more than 3 bands).\\n\"\n```",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "qgis_plugin/geoai_plugin/dialogs/samgeo.py",
    "pr_number": 398,
    "repo": "geoai",
    "owner": "opengeos",
    "comment_id": 2616085632,
    "comment_created_at": "2025-12-13T05:23:32Z"
  },
  {
    "code": "@@ -1277,13 +1310,17 @@ def segment_by_points_batch(self):\n             # Deactivate batch point tool if active\n             self.batch_add_point_btn.setChecked(False)\n \n-            # Add output to map if saved and option is checked\n+            # Add output to map if saved and option is checked (for batch-specific output)\n             if output_path and self.add_to_map_check.isChecked():\n                 layer = QgsRasterLayer(output_path, os.path.basename(output_path))\n                 if layer.isValid():\n                     QgsProject.instance().addMapLayer(layer)\n                     self.results_text.append(\"Added result layer to map.\")\n \n+            # Auto-show results if enabled and no batch-specific output was provided\n+            if num_masks > 0 and not output_path:",
    "comment": "The logic for determining when to call _auto_show_results() in segment_by_points_batch() differs from other segmentation methods. In segment_by_text(), segment_by_points(), and segment_by_box(), _auto_show_results() is called unconditionally when num_masks > 0. However, in segment_by_points_batch(), it's only called when no batch-specific output_path was provided (line 1321). This inconsistency could confuse users about when auto-show behavior applies. Consider documenting this difference or making the behavior consistent across all segmentation methods.\n```suggestion\n            # Auto-show results if enabled\n            if num_masks > 0:\n```",
    "line_number": 1321,
    "enriched": "File: qgis_plugin/geoai_plugin/dialogs/samgeo.py\nCode: @@ -1277,13 +1310,17 @@ def segment_by_points_batch(self):\n             # Deactivate batch point tool if active\n             self.batch_add_point_btn.setChecked(False)\n \n-            # Add output to map if saved and option is checked\n+            # Add output to map if saved and option is checked (for batch-specific output)\n             if output_path and self.add_to_map_check.isChecked():\n                 layer = QgsRasterLayer(output_path, os.path.basename(output_path))\n                 if layer.isValid():\n                     QgsProject.instance().addMapLayer(layer)\n                     self.results_text.append(\"Added result layer to map.\")\n \n+            # Auto-show results if enabled and no batch-specific output was provided\n+            if num_masks > 0 and not output_path:\nComment: The logic for determining when to call _auto_show_results() in segment_by_points_batch() differs from other segmentation methods. In segment_by_text(), segment_by_points(), and segment_by_box(), _auto_show_results() is called unconditionally when num_masks > 0. However, in segment_by_points_batch(), it's only called when no batch-specific output_path was provided (line 1321). This inconsistency could confuse users about when auto-show behavior applies. Consider documenting this difference or making the behavior consistent across all segmentation methods.\n```suggestion\n            # Auto-show results if enabled\n            if num_masks > 0:\n```",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "qgis_plugin/geoai_plugin/dialogs/samgeo.py",
    "pr_number": 397,
    "repo": "geoai",
    "owner": "opengeos",
    "comment_id": 2616047725,
    "comment_created_at": "2025-12-13T04:01:30Z"
  },
  {
    "code": "@@ -171,7 +171,12 @@ Steps:\n \n ### SamGeo Panel (Segment Anything Model)\n \n-Sample dataset: [uc_berkeley.tif](https://huggingface.co/datasets/giswqs/geospatial/resolve/main/uc_berkeley.tif)\n+Sample dataset:",
    "comment": "The label should be \"Sample datasets:\" (plural) instead of \"Sample dataset:\" (singular) since multiple datasets are listed below. This would also maintain consistency with the Segmentation Panel section above (line 150).\n```suggestion\nSample datasets:\n```",
    "line_number": 174,
    "enriched": "File: qgis_plugin/README.md\nCode: @@ -171,7 +171,12 @@ Steps:\n \n ### SamGeo Panel (Segment Anything Model)\n \n-Sample dataset: [uc_berkeley.tif](https://huggingface.co/datasets/giswqs/geospatial/resolve/main/uc_berkeley.tif)\n+Sample dataset:\nComment: The label should be \"Sample datasets:\" (plural) instead of \"Sample dataset:\" (singular) since multiple datasets are listed below. This would also maintain consistency with the Segmentation Panel section above (line 150).\n```suggestion\nSample datasets:\n```",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "qgis_plugin/README.md",
    "pr_number": 394,
    "repo": "geoai",
    "owner": "opengeos",
    "comment_id": 2614947921,
    "comment_created_at": "2025-12-12T16:57:27Z"
  },
  {
    "code": "@@ -0,0 +1,163 @@\n+\"\"\"\n+Helper to import the *external* geoai Python library from within the QGIS plugin.\n+\n+Why this exists:\n+- When the plugin folder is named \"geoai\" (as required by the official QGIS plugin repo),\n+  importing `geoai` from plugin code resolves to the plugin package itself, shadowing the\n+  external `geoai` library (distributed as \"geoai-py\").\n+- Some plugin dialogs need the external library (e.g., `MoondreamGeo`, segmentation helpers).\n+\n+This module provides `get_geoai()` which returns the external library module even when\n+shadowed by the plugin package name.\n+\"\"\"\n+\n+from __future__ import annotations\n+\n+from pathlib import Path\n+from types import ModuleType\n+from typing import Optional\n+\n+import importlib\n+import importlib.metadata\n+import importlib.util\n+import sys\n+\n+\n+_CACHED: Optional[ModuleType] = None\n+\n+\n+def _is_module_from_dir(mod: ModuleType, directory: Path) -> bool:\n+    try:\n+        mod_file = getattr(mod, \"__file__\", None)\n+        if not mod_file:\n+            return False\n+        return Path(mod_file).resolve().is_relative_to(directory.resolve())\n+    except Exception:\n+        return False\n+\n+\n+def _import_geoai_without_plugin_shadow(plugin_pkg_dir: Path) -> Optional[ModuleType]:\n+    \"\"\"\n+    Try to import `geoai` while temporarily removing the plugin path from sys.path.\n+\n+    This avoids relying on `importlib.metadata` (which can be incomplete in some QGIS\n+    Python setups) and is the most direct way to bypass the shadowing.\n+    \"\"\"\n+    plugin_parent = plugin_pkg_dir.parent\n+    orig_sys_path = list(sys.path)\n+    orig_geoai_mod = sys.modules.get(\"geoai\")\n+\n+    try:\n+        # Remove the directory that contains the plugin package named `geoai`\n+        sys.path = [p for p in sys.path if Path(p).resolve() != plugin_parent.resolve()]\n+\n+        # Remove shadowed module entry so import attempts re-resolution\n+        if \"geoai\" in sys.modules:\n+            del sys.modules[\"geoai\"]\n+\n+        imported = importlib.import_module(\"geoai\")\n+\n+        # If we somehow still got the plugin, treat as failure.\n+        if _is_module_from_dir(imported, plugin_pkg_dir):",
    "comment": "After temporarily removing the plugin module from sys.modules and attempting import, if the import succeeds but returns the plugin package (line 61), the function returns None but does not restore the original geoai module to sys.modules before the finally block executes. This could leave sys.modules in an inconsistent state where \"geoai\" is missing between lines 61-70. Consider restoring sys.modules[\"geoai\"] immediately when detecting the plugin was re-imported, before returning None.\n```suggestion\n        if _is_module_from_dir(imported, plugin_pkg_dir):\n            if orig_geoai_mod is not None:\n                sys.modules[\"geoai\"] = orig_geoai_mod\n```",
    "line_number": 61,
    "enriched": "File: qgis_plugin/geoai_plugin/_geoai_lib.py\nCode: @@ -0,0 +1,163 @@\n+\"\"\"\n+Helper to import the *external* geoai Python library from within the QGIS plugin.\n+\n+Why this exists:\n+- When the plugin folder is named \"geoai\" (as required by the official QGIS plugin repo),\n+  importing `geoai` from plugin code resolves to the plugin package itself, shadowing the\n+  external `geoai` library (distributed as \"geoai-py\").\n+- Some plugin dialogs need the external library (e.g., `MoondreamGeo`, segmentation helpers).\n+\n+This module provides `get_geoai()` which returns the external library module even when\n+shadowed by the plugin package name.\n+\"\"\"\n+\n+from __future__ import annotations\n+\n+from pathlib import Path\n+from types import ModuleType\n+from typing import Optional\n+\n+import importlib\n+import importlib.metadata\n+import importlib.util\n+import sys\n+\n+\n+_CACHED: Optional[ModuleType] = None\n+\n+\n+def _is_module_from_dir(mod: ModuleType, directory: Path) -> bool:\n+    try:\n+        mod_file = getattr(mod, \"__file__\", None)\n+        if not mod_file:\n+            return False\n+        return Path(mod_file).resolve().is_relative_to(directory.resolve())\n+    except Exception:\n+        return False\n+\n+\n+def _import_geoai_without_plugin_shadow(plugin_pkg_dir: Path) -> Optional[ModuleType]:\n+    \"\"\"\n+    Try to import `geoai` while temporarily removing the plugin path from sys.path.\n+\n+    This avoids relying on `importlib.metadata` (which can be incomplete in some QGIS\n+    Python setups) and is the most direct way to bypass the shadowing.\n+    \"\"\"\n+    plugin_parent = plugin_pkg_dir.parent\n+    orig_sys_path = list(sys.path)\n+    orig_geoai_mod = sys.modules.get(\"geoai\")\n+\n+    try:\n+        # Remove the directory that contains the plugin package named `geoai`\n+        sys.path = [p for p in sys.path if Path(p).resolve() != plugin_parent.resolve()]\n+\n+        # Remove shadowed module entry so import attempts re-resolution\n+        if \"geoai\" in sys.modules:\n+            del sys.modules[\"geoai\"]\n+\n+        imported = importlib.import_module(\"geoai\")\n+\n+        # If we somehow still got the plugin, treat as failure.\n+        if _is_module_from_dir(imported, plugin_pkg_dir):\nComment: After temporarily removing the plugin module from sys.modules and attempting import, if the import succeeds but returns the plugin package (line 61), the function returns None but does not restore the original geoai module to sys.modules before the finally block executes. This could leave sys.modules in an inconsistent state where \"geoai\" is missing between lines 61-70. Consider restoring sys.modules[\"geoai\"] immediately when detecting the plugin was re-imported, before returning None.\n```suggestion\n        if _is_module_from_dir(imported, plugin_pkg_dir):\n            if orig_geoai_mod is not None:\n                sys.modules[\"geoai\"] = orig_geoai_mod\n```",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "qgis_plugin/geoai_plugin/_geoai_lib.py",
    "pr_number": 392,
    "repo": "geoai",
    "owner": "opengeos",
    "comment_id": 2614290121,
    "comment_created_at": "2025-12-12T13:53:26Z"
  },
  {
    "code": "@@ -40,7 +40,7 @@ The package's design philosophy emphasizes simplicity without sacrificing functi\n \n If you find GeoAI useful in your research, please consider citing the following paper to support my work. Thank you for your support.\n \n--   Wu, Q. (2025). GeoAI: A Python package for integrating artificial intelligence with geospatial data analysis and visualization. _Journal of Open Source Software_, 9025. [https://doi.org/10.21105/joss.09025](https://github.com/openjournals/joss-papers/blob/joss.09025/joss.09025/10.21105.joss.09025.pdf) (Under Review)\n+-   Wu, Q. (2025). GeoAI: A Python package for integrating artificial intelligence with geospatial data analysis and visualization. _Journal of Open Source Software_, 9025. [https://doi.org/10.21105/joss.09025](https://github.com/openjournals/joss-papers/blob/joss.09605/joss.09605/10.21105.joss.09605.pdf) (Under Review)",
    "comment": "The citation has inconsistencies: (1) The paper number in the text is '9025' but the URL path uses '09605', and (2) The URL itself is inconsistent with 'joss.09025' in the DOI portion but 'joss.09605' in the file path. Please verify the correct JOSS paper number and ensure all references (text and URL) are consistent.\n```suggestion\n-   Wu, Q. (2025). GeoAI: A Python package for integrating artificial intelligence with geospatial data analysis and visualization. _Journal of Open Source Software_, 9025. [https://doi.org/10.21105/joss.09025](https://github.com/openjournals/joss-papers/blob/joss.09025/joss.09025/10.21105.joss.09025.pdf) (Under Review)\n```",
    "line_number": 43,
    "enriched": "File: docs/index.md\nCode: @@ -40,7 +40,7 @@ The package's design philosophy emphasizes simplicity without sacrificing functi\n \n If you find GeoAI useful in your research, please consider citing the following paper to support my work. Thank you for your support.\n \n--   Wu, Q. (2025). GeoAI: A Python package for integrating artificial intelligence with geospatial data analysis and visualization. _Journal of Open Source Software_, 9025. [https://doi.org/10.21105/joss.09025](https://github.com/openjournals/joss-papers/blob/joss.09025/joss.09025/10.21105.joss.09025.pdf) (Under Review)\n+-   Wu, Q. (2025). GeoAI: A Python package for integrating artificial intelligence with geospatial data analysis and visualization. _Journal of Open Source Software_, 9025. [https://doi.org/10.21105/joss.09025](https://github.com/openjournals/joss-papers/blob/joss.09605/joss.09605/10.21105.joss.09605.pdf) (Under Review)\nComment: The citation has inconsistencies: (1) The paper number in the text is '9025' but the URL path uses '09605', and (2) The URL itself is inconsistent with 'joss.09025' in the DOI portion but 'joss.09605' in the file path. Please verify the correct JOSS paper number and ensure all references (text and URL) are consistent.\n```suggestion\n-   Wu, Q. (2025). GeoAI: A Python package for integrating artificial intelligence with geospatial data analysis and visualization. _Journal of Open Source Software_, 9025. [https://doi.org/10.21105/joss.09025](https://github.com/openjournals/joss-papers/blob/joss.09025/joss.09025/10.21105.joss.09025.pdf) (Under Review)\n```",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/index.md",
    "pr_number": 391,
    "repo": "geoai",
    "owner": "opengeos",
    "comment_id": 2612810476,
    "comment_created_at": "2025-12-12T03:38:41Z"
  },
  {
    "code": "@@ -0,0 +1,1975 @@\n+\"\"\"Auto classes for geospatial model inference with GeoTIFF support.\n+\n+This module provides AutoGeoModel and AutoGeoImageProcessor that extend\n+Hugging Face transformers' AutoModel and AutoImageProcessor to support\n+processing geospatial data (GeoTIFF) and saving outputs as GeoTIFF or vector data.\n+\n+Supported tasks:\n+    - Semantic segmentation (e.g., SegFormer, Mask2Former)\n+    - Image classification (e.g., ViT, ResNet)\n+    - Object detection (e.g., DETR, YOLOS)\n+    - Zero-shot object detection (e.g., Grounding DINO, OWL-ViT)\n+    - Depth estimation (e.g., Depth Anything, DPT)\n+    - Mask generation (e.g., SAM)\n+\n+Example:\n+    >>> from geoai import AutoGeoModel\n+    >>> model = AutoGeoModel.from_pretrained(\n+    ...     \"nvidia/segformer-b0-finetuned-ade-512-512\",\n+    ...     task=\"semantic-segmentation\"\n+    ... )\n+    >>> result = model.predict(\"input.tif\", output_path=\"output.tif\")\n+\"\"\"\n+\n+import os\n+from typing import Any, Dict, List, Optional, Tuple, Union\n+\n+import cv2\n+import geopandas as gpd\n+import numpy as np\n+import rasterio\n+import requests\n+import torch\n+from PIL import Image\n+from rasterio.features import shapes\n+from rasterio.windows import Window\n+from shapely.geometry import box, shape\n+from tqdm import tqdm\n+from transformers import (\n+    AutoConfig,\n+    AutoImageProcessor,\n+    AutoModel,\n+    AutoModelForImageClassification,\n+    AutoModelForImageSegmentation,\n+    AutoModelForSemanticSegmentation,\n+    AutoModelForUniversalSegmentation,\n+    AutoModelForDepthEstimation,\n+    AutoModelForMaskGeneration,\n+    AutoModelForObjectDetection,\n+    AutoModelForZeroShotObjectDetection,\n+    AutoProcessor,\n+)\n+\n+from transformers.utils import logging as hf_logging\n+\n+from .utils import get_device\n+\n+\n+hf_logging.set_verbosity_error()  # silence HF load reports\n+\n+\n+class AutoGeoImageProcessor:\n+    \"\"\"\n+    Image processor for geospatial data that wraps AutoImageProcessor.\n+\n+    This class provides functionality to load and preprocess GeoTIFF images\n+    while preserving geospatial metadata (CRS, transform, bounds). It wraps\n+    Hugging Face's AutoImageProcessor and adds geospatial capabilities.\n+\n+    Use `from_pretrained` to instantiate this class, following the transformers pattern.\n+\n+    Attributes:\n+        processor: The underlying AutoImageProcessor instance.\n+        device (str): The device being used ('cuda' or 'cpu').\n+\n+    Example:\n+        >>> processor = AutoGeoImageProcessor.from_pretrained(\"facebook/sam-vit-base\")\n+        >>> data, metadata = processor.load_geotiff(\"input.tif\")\n+        >>> inputs = processor.preprocess(data)\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        processor: \"AutoImageProcessor\",\n+        processor_name: Optional[str] = None,\n+        device: Optional[str] = None,\n+    ) -> None:\n+        \"\"\"Initialize the AutoGeoImageProcessor with an existing processor.\n+\n+        Note: Use `from_pretrained` class method to load from Hugging Face Hub.\n+\n+        Args:\n+            processor: An AutoImageProcessor instance.\n+            processor_name: Name or path of the processor (for reference).\n+            device: Device to use ('cuda', 'cpu'). If None, auto-detect.\n+        \"\"\"\n+        self.processor = processor\n+        self.processor_name = processor_name\n+\n+        if device is None:\n+            self.device = get_device()\n+        else:\n+            self.device = device\n+\n+    @classmethod\n+    def from_pretrained(\n+        cls,\n+        pretrained_model_name_or_path: str,\n+        device: Optional[str] = None,\n+        use_full_processor: bool = False,\n+        **kwargs: Any,\n+    ) -> \"AutoGeoImageProcessor\":\n+        \"\"\"Load an AutoGeoImageProcessor from a pretrained processor.\n+\n+        This method wraps AutoImageProcessor.from_pretrained and adds\n+        geospatial capabilities for processing GeoTIFF files.\n+\n+        Args:\n+            pretrained_model_name_or_path: Hugging Face model/processor name or local path.\n+                Can be a model ID from huggingface.co or a local directory path.\n+            device: Device to use ('cuda', 'cpu'). If None, auto-detect.\n+            use_full_processor: If True, use AutoProcessor instead of AutoImageProcessor.\n+                Required for models that need text inputs (e.g., Grounding DINO).\n+            **kwargs: Additional arguments passed to AutoImageProcessor.from_pretrained.\n+                Common options include:\n+                - trust_remote_code (bool): Whether to trust remote code.\n+                - revision (str): Specific model version to use.\n+                - use_fast (bool): Whether to use fast tokenizer.\n+\n+        Returns:\n+            AutoGeoImageProcessor instance with geospatial support.\n+\n+        Example:\n+            >>> processor = AutoGeoImageProcessor.from_pretrained(\"facebook/sam-vit-base\")\n+            >>> processor = AutoGeoImageProcessor.from_pretrained(\n+            ...     \"nvidia/segformer-b0-finetuned-ade-512-512\",\n+            ...     device=\"cuda\"\n+            ... )\n+        \"\"\"\n+        # Check if this is a model that needs the full processor (text + image)\n+        model_name_lower = pretrained_model_name_or_path.lower()\n+        needs_full_processor = use_full_processor or any(\n+            name in model_name_lower\n+            for name in [\"grounding-dino\", \"owl\", \"clip\", \"blip\"]\n+        )\n+\n+        if needs_full_processor:\n+            processor = AutoProcessor.from_pretrained(\n+                pretrained_model_name_or_path, **kwargs\n+            )\n+        else:\n+            try:\n+                processor = AutoImageProcessor.from_pretrained(\n+                    pretrained_model_name_or_path, **kwargs\n+                )\n+            except Exception:\n+                processor = AutoProcessor.from_pretrained(\n+                    pretrained_model_name_or_path, **kwargs\n+                )\n+        return cls(\n+            processor=processor,\n+            processor_name=pretrained_model_name_or_path,\n+            device=device,\n+        )\n+\n+    def load_geotiff(\n+        self,\n+        source: Union[str, \"rasterio.DatasetReader\"],\n+        window: Optional[Window] = None,\n+        bands: Optional[List[int]] = None,\n+    ) -> Tuple[np.ndarray, Dict]:\n+        \"\"\"Load a GeoTIFF file and return data with metadata.\n+\n+        Args:\n+            source: Path to GeoTIFF file or open rasterio DatasetReader.\n+            window: Optional rasterio Window for reading a subset.\n+            bands: List of band indices to read (1-indexed). If None, read all bands.\n+\n+        Returns:\n+            Tuple of (image array in CHW format, metadata dict).\n+\n+        Example:\n+            >>> processor = AutoGeoImageProcessor(\"facebook/sam-vit-base\")",
    "comment": "The example in the docstring calls `AutoGeoImageProcessor(\"facebook/sam-vit-base\")` directly with a string argument, but the `__init__` method expects an `AutoImageProcessor` instance, not a string. The example should use `AutoGeoImageProcessor.from_pretrained(\"facebook/sam-vit-base\")` instead.\n```suggestion\n            >>> processor = AutoGeoImageProcessor.from_pretrained(\"facebook/sam-vit-base\")\n```",
    "line_number": 182,
    "enriched": "File: geoai/auto.py\nCode: @@ -0,0 +1,1975 @@\n+\"\"\"Auto classes for geospatial model inference with GeoTIFF support.\n+\n+This module provides AutoGeoModel and AutoGeoImageProcessor that extend\n+Hugging Face transformers' AutoModel and AutoImageProcessor to support\n+processing geospatial data (GeoTIFF) and saving outputs as GeoTIFF or vector data.\n+\n+Supported tasks:\n+    - Semantic segmentation (e.g., SegFormer, Mask2Former)\n+    - Image classification (e.g., ViT, ResNet)\n+    - Object detection (e.g., DETR, YOLOS)\n+    - Zero-shot object detection (e.g., Grounding DINO, OWL-ViT)\n+    - Depth estimation (e.g., Depth Anything, DPT)\n+    - Mask generation (e.g., SAM)\n+\n+Example:\n+    >>> from geoai import AutoGeoModel\n+    >>> model = AutoGeoModel.from_pretrained(\n+    ...     \"nvidia/segformer-b0-finetuned-ade-512-512\",\n+    ...     task=\"semantic-segmentation\"\n+    ... )\n+    >>> result = model.predict(\"input.tif\", output_path=\"output.tif\")\n+\"\"\"\n+\n+import os\n+from typing import Any, Dict, List, Optional, Tuple, Union\n+\n+import cv2\n+import geopandas as gpd\n+import numpy as np\n+import rasterio\n+import requests\n+import torch\n+from PIL import Image\n+from rasterio.features import shapes\n+from rasterio.windows import Window\n+from shapely.geometry import box, shape\n+from tqdm import tqdm\n+from transformers import (\n+    AutoConfig,\n+    AutoImageProcessor,\n+    AutoModel,\n+    AutoModelForImageClassification,\n+    AutoModelForImageSegmentation,\n+    AutoModelForSemanticSegmentation,\n+    AutoModelForUniversalSegmentation,\n+    AutoModelForDepthEstimation,\n+    AutoModelForMaskGeneration,\n+    AutoModelForObjectDetection,\n+    AutoModelForZeroShotObjectDetection,\n+    AutoProcessor,\n+)\n+\n+from transformers.utils import logging as hf_logging\n+\n+from .utils import get_device\n+\n+\n+hf_logging.set_verbosity_error()  # silence HF load reports\n+\n+\n+class AutoGeoImageProcessor:\n+    \"\"\"\n+    Image processor for geospatial data that wraps AutoImageProcessor.\n+\n+    This class provides functionality to load and preprocess GeoTIFF images\n+    while preserving geospatial metadata (CRS, transform, bounds). It wraps\n+    Hugging Face's AutoImageProcessor and adds geospatial capabilities.\n+\n+    Use `from_pretrained` to instantiate this class, following the transformers pattern.\n+\n+    Attributes:\n+        processor: The underlying AutoImageProcessor instance.\n+        device (str): The device being used ('cuda' or 'cpu').\n+\n+    Example:\n+        >>> processor = AutoGeoImageProcessor.from_pretrained(\"facebook/sam-vit-base\")\n+        >>> data, metadata = processor.load_geotiff(\"input.tif\")\n+        >>> inputs = processor.preprocess(data)\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        processor: \"AutoImageProcessor\",\n+        processor_name: Optional[str] = None,\n+        device: Optional[str] = None,\n+    ) -> None:\n+        \"\"\"Initialize the AutoGeoImageProcessor with an existing processor.\n+\n+        Note: Use `from_pretrained` class method to load from Hugging Face Hub.\n+\n+        Args:\n+            processor: An AutoImageProcessor instance.\n+            processor_name: Name or path of the processor (for reference).\n+            device: Device to use ('cuda', 'cpu'). If None, auto-detect.\n+        \"\"\"\n+        self.processor = processor\n+        self.processor_name = processor_name\n+\n+        if device is None:\n+            self.device = get_device()\n+        else:\n+            self.device = device\n+\n+    @classmethod\n+    def from_pretrained(\n+        cls,\n+        pretrained_model_name_or_path: str,\n+        device: Optional[str] = None,\n+        use_full_processor: bool = False,\n+        **kwargs: Any,\n+    ) -> \"AutoGeoImageProcessor\":\n+        \"\"\"Load an AutoGeoImageProcessor from a pretrained processor.\n+\n+        This method wraps AutoImageProcessor.from_pretrained and adds\n+        geospatial capabilities for processing GeoTIFF files.\n+\n+        Args:\n+            pretrained_model_name_or_path: Hugging Face model/processor name or local path.\n+                Can be a model ID from huggingface.co or a local directory path.\n+            device: Device to use ('cuda', 'cpu'). If None, auto-detect.\n+            use_full_processor: If True, use AutoProcessor instead of AutoImageProcessor.\n+                Required for models that need text inputs (e.g., Grounding DINO).\n+            **kwargs: Additional arguments passed to AutoImageProcessor.from_pretrained.\n+                Common options include:\n+                - trust_remote_code (bool): Whether to trust remote code.\n+                - revision (str): Specific model version to use.\n+                - use_fast (bool): Whether to use fast tokenizer.\n+\n+        Returns:\n+            AutoGeoImageProcessor instance with geospatial support.\n+\n+        Example:\n+            >>> processor = AutoGeoImageProcessor.from_pretrained(\"facebook/sam-vit-base\")\n+            >>> processor = AutoGeoImageProcessor.from_pretrained(\n+            ...     \"nvidia/segformer-b0-finetuned-ade-512-512\",\n+            ...     device=\"cuda\"\n+            ... )\n+        \"\"\"\n+        # Check if this is a model that needs the full processor (text + image)\n+        model_name_lower = pretrained_model_name_or_path.lower()\n+        needs_full_processor = use_full_processor or any(\n+            name in model_name_lower\n+            for name in [\"grounding-dino\", \"owl\", \"clip\", \"blip\"]\n+        )\n+\n+        if needs_full_processor:\n+            processor = AutoProcessor.from_pretrained(\n+                pretrained_model_name_or_path, **kwargs\n+            )\n+        else:\n+            try:\n+                processor = AutoImageProcessor.from_pretrained(\n+                    pretrained_model_name_or_path, **kwargs\n+                )\n+            except Exception:\n+                processor = AutoProcessor.from_pretrained(\n+                    pretrained_model_name_or_path, **kwargs\n+                )\n+        return cls(\n+            processor=processor,\n+            processor_name=pretrained_model_name_or_path,\n+            device=device,\n+        )\n+\n+    def load_geotiff(\n+        self,\n+        source: Union[str, \"rasterio.DatasetReader\"],\n+        window: Optional[Window] = None,\n+        bands: Optional[List[int]] = None,\n+    ) -> Tuple[np.ndarray, Dict]:\n+        \"\"\"Load a GeoTIFF file and return data with metadata.\n+\n+        Args:\n+            source: Path to GeoTIFF file or open rasterio DatasetReader.\n+            window: Optional rasterio Window for reading a subset.\n+            bands: List of band indices to read (1-indexed). If None, read all bands.\n+\n+        Returns:\n+            Tuple of (image array in CHW format, metadata dict).\n+\n+        Example:\n+            >>> processor = AutoGeoImageProcessor(\"facebook/sam-vit-base\")\nComment: The example in the docstring calls `AutoGeoImageProcessor(\"facebook/sam-vit-base\")` directly with a string argument, but the `__init__` method expects an `AutoImageProcessor` instance, not a string. The example should use `AutoGeoImageProcessor.from_pretrained(\"facebook/sam-vit-base\")` instead.\n```suggestion\n            >>> processor = AutoGeoImageProcessor.from_pretrained(\"facebook/sam-vit-base\")\n```",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "geoai/auto.py",
    "pr_number": 390,
    "repo": "geoai",
    "owner": "opengeos",
    "comment_id": 2612779015,
    "comment_created_at": "2025-12-12T03:15:20Z"
  },
  {
    "code": "@@ -0,0 +1,1430 @@\n+\"\"\"\n+SamGeo Dock Widget for GeoAI Plugin\n+\n+This dock widget provides an interface for remote sensing image segmentation\n+using the SamGeo library (SAM, SAM2, and SAM3 models).\n+\"\"\"\n+\n+import os\n+\n+from qgis.PyQt.QtCore import Qt, QCoreApplication\n+from qgis.PyQt.QtGui import QColor\n+from qgis.PyQt.QtWidgets import (\n+    QDockWidget,\n+    QWidget,\n+    QVBoxLayout,\n+    QHBoxLayout,\n+    QLabel,\n+    QPushButton,\n+    QLineEdit,\n+    QComboBox,\n+    QSpinBox,\n+    QDoubleSpinBox,\n+    QCheckBox,\n+    QGroupBox,\n+    QFileDialog,\n+    QMessageBox,\n+    QProgressBar,\n+    QTabWidget,\n+    QTextEdit,\n+    QListWidget,\n+    QListWidgetItem,\n+    QScrollArea,\n+)\n+from qgis.core import (\n+    QgsProject,\n+    QgsRasterLayer,\n+    QgsVectorLayer,\n+    QgsWkbTypes,\n+    Qgis,\n+    QgsMessageLog,\n+)\n+\n+from .map_tools import PointPromptTool, BoxPromptTool\n+\n+\n+class SamGeoDockWidget(QDockWidget):\n+    \"\"\"Dock widget for SamGeo segmentation operations.\"\"\"\n+\n+    def __init__(self, iface, parent=None):\n+        \"\"\"Initialize the SamGeo dock widget.\n+\n+        Args:\n+            iface: The QGIS interface instance.\n+            parent: Parent widget.\n+        \"\"\"\n+        super().__init__(\"SamGeo Segmentation\", parent)\n+        self.iface = iface\n+        self.canvas = iface.mapCanvas()\n+        self.setAllowedAreas(Qt.LeftDockWidgetArea | Qt.RightDockWidgetArea)\n+\n+        # SamGeo model instance\n+        self.sam = None\n+        self.current_layer = None\n+        self.current_image_path = None\n+\n+        # Point and box prompts\n+        self.point_coords = []\n+        self.point_labels = []\n+        self.box_coords = None\n+\n+        # Batch point prompts\n+        self.batch_point_coords = []\n+        self.batch_point_coords_map = []  # Map coordinates for display\n+\n+        # Map tools\n+        self.point_tool = None\n+        self.batch_point_tool = None\n+        self.box_tool = None\n+        self.previous_tool = None\n+\n+        self._setup_ui()\n+\n+    def _setup_ui(self):\n+        \"\"\"Set up the user interface.\"\"\"\n+        # Main widget with scroll area\n+        scroll_area = QScrollArea()\n+        scroll_area.setWidgetResizable(True)\n+        scroll_area.setHorizontalScrollBarPolicy(Qt.ScrollBarAlwaysOff)\n+\n+        main_widget = QWidget()\n+        main_layout = QVBoxLayout()\n+        main_widget.setLayout(main_layout)\n+\n+        # Tab widget for different modes\n+        tab_widget = QTabWidget()\n+\n+        # === Model Settings Tab ===\n+        model_tab = self._create_model_tab()\n+        tab_widget.addTab(model_tab, \"Model\")\n+\n+        # === Text Prompts Tab ===\n+        text_tab = self._create_text_tab()\n+        tab_widget.addTab(text_tab, \"Text\")\n+\n+        # === Interactive Tab ===\n+        interactive_tab = self._create_interactive_tab()\n+        tab_widget.addTab(interactive_tab, \"Interactive\")\n+\n+        # === Batch Tab ===\n+        batch_tab = self._create_batch_tab()\n+        tab_widget.addTab(batch_tab, \"Batch\")\n+\n+        # === Output Tab ===\n+        output_tab = self._create_output_tab()\n+        tab_widget.addTab(output_tab, \"Output\")\n+\n+        # Add tab widget to main layout\n+        main_layout.addWidget(tab_widget)\n+\n+        # Progress bar\n+        self.progress_bar = QProgressBar()\n+        self.progress_bar.setVisible(False)\n+        main_layout.addWidget(self.progress_bar)\n+\n+        scroll_area.setWidget(main_widget)\n+        self.setWidget(scroll_area)\n+\n+    def _create_model_tab(self):\n+        \"\"\"Create the model settings tab.\"\"\"\n+        model_tab = QWidget()\n+        model_layout = QVBoxLayout()\n+        model_tab.setLayout(model_layout)\n+\n+        # Backend selection\n+        backend_group = QGroupBox(\"Model Settings\")\n+        backend_layout = QVBoxLayout()\n+\n+        # Model version selection\n+        version_row = QHBoxLayout()\n+        version_row.addWidget(QLabel(\"Model:\"))\n+        self.model_combo = QComboBox()\n+        self.model_combo.addItems([\"SamGeo3 (SAM3)\", \"SamGeo2 (SAM2)\", \"SamGeo (SAM1)\"])\n+        version_row.addWidget(self.model_combo)\n+        backend_layout.addLayout(version_row)\n+\n+        backend_row = QHBoxLayout()\n+        backend_row.addWidget(QLabel(\"Backend:\"))\n+        self.backend_combo = QComboBox()\n+        self.backend_combo.addItems([\"meta\", \"transformers\"])\n+        backend_row.addWidget(self.backend_combo)\n+        backend_layout.addLayout(backend_row)\n+\n+        # Device selection\n+        device_row = QHBoxLayout()\n+        device_row.addWidget(QLabel(\"Device:\"))\n+        self.device_combo = QComboBox()\n+        self.device_combo.addItems([\"auto\", \"cuda\", \"cpu\"])\n+        device_row.addWidget(self.device_combo)\n+        backend_layout.addLayout(device_row)\n+\n+        # Confidence threshold\n+        conf_row = QHBoxLayout()\n+        conf_row.addWidget(QLabel(\"Confidence:\"))\n+        self.conf_spin = QDoubleSpinBox()\n+        self.conf_spin.setRange(0.0, 1.0)\n+        self.conf_spin.setValue(0.5)\n+        self.conf_spin.setSingleStep(0.05)\n+        conf_row.addWidget(self.conf_spin)\n+        backend_layout.addLayout(conf_row)\n+\n+        # Interactive mode checkbox\n+        self.interactive_check = QCheckBox(\n+            \"Enable Interactive Mode (Point/Box Prompts)\"\n+        )\n+        self.interactive_check.setChecked(True)\n+        backend_layout.addWidget(self.interactive_check)\n+\n+        # Load model button\n+        self.load_model_btn = QPushButton(\"Load Model\")\n+        self.load_model_btn.clicked.connect(self.load_model)\n+        backend_layout.addWidget(self.load_model_btn)\n+\n+        # Model status\n+        self.model_status = QLabel(\"Model: Not loaded\")\n+        self.model_status.setStyleSheet(\"color: gray;\")\n+        backend_layout.addWidget(self.model_status)\n+\n+        backend_group.setLayout(backend_layout)\n+        model_layout.addWidget(backend_group)\n+\n+        # Layer selection\n+        layer_group = QGroupBox(\"Input Layer\")\n+        layer_layout = QVBoxLayout()\n+\n+        layer_row = QHBoxLayout()\n+        self.layer_combo = QComboBox()\n+        self.refresh_layers()\n+        layer_row.addWidget(self.layer_combo)\n+\n+        refresh_btn = QPushButton(\"\")\n+        refresh_btn.setMaximumWidth(30)\n+        refresh_btn.clicked.connect(self.refresh_layers)\n+        layer_row.addWidget(refresh_btn)\n+        layer_layout.addLayout(layer_row)\n+\n+        self.set_layer_btn = QPushButton(\"Set Image from Layer\")\n+        self.set_layer_btn.clicked.connect(self.set_image_from_layer)\n+        layer_layout.addWidget(self.set_layer_btn)\n+\n+        # Or load from file\n+        file_row = QHBoxLayout()\n+        self.image_path_edit = QLineEdit()\n+        self.image_path_edit.setPlaceholderText(\"Or select image file...\")\n+        file_row.addWidget(self.image_path_edit)\n+\n+        browse_btn = QPushButton(\"...\")\n+        browse_btn.setMaximumWidth(30)\n+        browse_btn.clicked.connect(self.browse_image)\n+        file_row.addWidget(browse_btn)\n+        layer_layout.addLayout(file_row)\n+\n+        self.set_file_btn = QPushButton(\"Set Image from File\")\n+        self.set_file_btn.clicked.connect(self.set_image_from_file)\n+        layer_layout.addWidget(self.set_file_btn)\n+\n+        self.image_status = QLabel(\"Image: Not set\")\n+        self.image_status.setStyleSheet(\"color: gray;\")\n+        layer_layout.addWidget(self.image_status)\n+\n+        layer_group.setLayout(layer_layout)\n+        model_layout.addWidget(layer_group)\n+\n+        model_layout.addStretch()\n+        return model_tab\n+\n+    def _create_text_tab(self):\n+        \"\"\"Create the text prompts tab.\"\"\"\n+        text_tab = QWidget()\n+        text_layout = QVBoxLayout()\n+        text_tab.setLayout(text_layout)\n+\n+        text_group = QGroupBox(\"Text-Based Segmentation\")\n+        text_group_layout = QVBoxLayout()\n+\n+        text_group_layout.addWidget(QLabel(\"Describe objects to segment:\"))\n+        self.text_prompt_edit = QLineEdit()\n+        self.text_prompt_edit.setPlaceholderText(\"e.g., tree, building, road...\")\n+        text_group_layout.addWidget(self.text_prompt_edit)\n+\n+        # Size filters\n+        size_row = QHBoxLayout()\n+        size_row.addWidget(QLabel(\"Min size:\"))\n+        self.min_size_spin = QSpinBox()\n+        self.min_size_spin.setRange(0, 1000000)\n+        self.min_size_spin.setValue(0)\n+        size_row.addWidget(self.min_size_spin)\n+\n+        size_row.addWidget(QLabel(\"Max size:\"))\n+        self.max_size_spin = QSpinBox()\n+        self.max_size_spin.setRange(0, 10000000)\n+        self.max_size_spin.setValue(0)\n+        self.max_size_spin.setSpecialValueText(\"No limit\")\n+        size_row.addWidget(self.max_size_spin)\n+        text_group_layout.addLayout(size_row)\n+\n+        self.text_segment_btn = QPushButton(\"Segment by Text\")\n+        self.text_segment_btn.clicked.connect(self.segment_by_text)\n+        text_group_layout.addWidget(self.text_segment_btn)\n+\n+        # Text segmentation status\n+        self.text_status_label = QLabel(\"\")\n+        text_group_layout.addWidget(self.text_status_label)\n+\n+        text_group.setLayout(text_group_layout)\n+        text_layout.addWidget(text_group)\n+\n+        text_layout.addStretch()\n+        return text_tab\n+\n+    def _create_interactive_tab(self):\n+        \"\"\"Create the interactive prompts tab.\"\"\"\n+        interactive_tab = QWidget()\n+        interactive_layout = QVBoxLayout()\n+        interactive_tab.setLayout(interactive_layout)\n+\n+        # Point prompts\n+        point_group = QGroupBox(\"Point Prompts\")\n+        point_layout = QVBoxLayout()\n+\n+        point_btn_row = QHBoxLayout()\n+        self.add_fg_point_btn = QPushButton(\"Add Foreground Points\")\n+        self.add_fg_point_btn.setCheckable(True)\n+        self.add_fg_point_btn.clicked.connect(\n+            lambda: self.start_point_tool(foreground=True)\n+        )\n+        point_btn_row.addWidget(self.add_fg_point_btn)\n+\n+        self.add_bg_point_btn = QPushButton(\"Add Background Points\")\n+        self.add_bg_point_btn.setCheckable(True)\n+        self.add_bg_point_btn.clicked.connect(\n+            lambda: self.start_point_tool(foreground=False)\n+        )\n+        point_btn_row.addWidget(self.add_bg_point_btn)\n+        point_layout.addLayout(point_btn_row)\n+\n+        self.points_list = QListWidget()\n+        self.points_list.setMaximumHeight(100)\n+        point_layout.addWidget(self.points_list)\n+\n+        point_action_row = QHBoxLayout()\n+        clear_points_btn = QPushButton(\"Clear Points\")\n+        clear_points_btn.clicked.connect(self.clear_points)\n+        point_action_row.addWidget(clear_points_btn)\n+\n+        self.point_segment_btn = QPushButton(\"Segment by Points\")\n+        self.point_segment_btn.clicked.connect(self.segment_by_points)\n+        point_action_row.addWidget(self.point_segment_btn)\n+        point_layout.addLayout(point_action_row)\n+\n+        # Point segmentation status\n+        self.point_status_label = QLabel(\"\")\n+        point_layout.addWidget(self.point_status_label)\n+\n+        point_group.setLayout(point_layout)\n+        interactive_layout.addWidget(point_group)\n+\n+        # Box prompts\n+        box_group = QGroupBox(\"Box Prompts\")\n+        box_layout = QVBoxLayout()\n+\n+        self.draw_box_btn = QPushButton(\"Draw Box\")\n+        self.draw_box_btn.setCheckable(True)\n+        self.draw_box_btn.clicked.connect(self.start_box_tool)\n+        box_layout.addWidget(self.draw_box_btn)\n+\n+        self.box_label = QLabel(\"Box: Not set\")\n+        box_layout.addWidget(self.box_label)\n+\n+        box_action_row = QHBoxLayout()\n+        clear_box_btn = QPushButton(\"Clear Box\")\n+        clear_box_btn.clicked.connect(self.clear_box)\n+        box_action_row.addWidget(clear_box_btn)\n+\n+        self.box_segment_btn = QPushButton(\"Segment by Box\")\n+        self.box_segment_btn.clicked.connect(self.segment_by_box)\n+        box_action_row.addWidget(self.box_segment_btn)\n+        box_layout.addLayout(box_action_row)\n+\n+        # Box segmentation status\n+        self.box_status_label = QLabel(\"\")\n+        box_layout.addWidget(self.box_status_label)\n+\n+        box_group.setLayout(box_layout)\n+        interactive_layout.addWidget(box_group)\n+\n+        interactive_layout.addStretch()\n+        return interactive_tab\n+\n+    def _create_batch_tab(self):\n+        \"\"\"Create the batch processing tab.\"\"\"\n+        batch_tab = QWidget()\n+        batch_layout = QVBoxLayout()\n+        batch_tab.setLayout(batch_layout)\n+\n+        # Interactive Points for Batch\n+        batch_interactive_group = QGroupBox(\"Create Points Interactively\")\n+        batch_interactive_layout = QVBoxLayout()\n+\n+        batch_point_btn_row = QHBoxLayout()\n+        self.batch_add_point_btn = QPushButton(\"Add Points on Map\")\n+        self.batch_add_point_btn.setCheckable(True)\n+        self.batch_add_point_btn.clicked.connect(self.start_batch_point_tool)\n+        batch_point_btn_row.addWidget(self.batch_add_point_btn)\n+\n+        self.batch_clear_points_btn = QPushButton(\"Clear Points\")\n+        self.batch_clear_points_btn.clicked.connect(self.clear_batch_points)\n+        batch_point_btn_row.addWidget(self.batch_clear_points_btn)\n+        batch_interactive_layout.addLayout(batch_point_btn_row)\n+\n+        # Batch points list\n+        self.batch_points_list = QListWidget()\n+        self.batch_points_list.setMaximumHeight(80)\n+        batch_interactive_layout.addWidget(self.batch_points_list)\n+\n+        self.batch_points_count_label = QLabel(\"Points: 0\")\n+        batch_interactive_layout.addWidget(self.batch_points_count_label)\n+\n+        batch_interactive_group.setLayout(batch_interactive_layout)\n+        batch_layout.addWidget(batch_interactive_group)\n+\n+        # Or Load from File/Layer\n+        batch_file_group = QGroupBox(\"Or Load Points from File/Layer\")\n+        batch_file_layout = QVBoxLayout()\n+\n+        # Vector layer selection\n+        vector_layer_row = QHBoxLayout()\n+        vector_layer_row.addWidget(QLabel(\"Layer:\"))\n+        self.vector_layer_combo = QComboBox()\n+        self.refresh_vector_layers()\n+        vector_layer_row.addWidget(self.vector_layer_combo)\n+\n+        refresh_vector_btn = QPushButton(\"\")\n+        refresh_vector_btn.setMaximumWidth(30)\n+        refresh_vector_btn.clicked.connect(self.refresh_vector_layers)\n+        vector_layer_row.addWidget(refresh_vector_btn)\n+        batch_file_layout.addLayout(vector_layer_row)\n+\n+        # Or load from file\n+        vector_file_row = QHBoxLayout()\n+        self.vector_file_edit = QLineEdit()\n+        self.vector_file_edit.setPlaceholderText(\"Or select vector file...\")\n+        vector_file_row.addWidget(self.vector_file_edit)\n+\n+        browse_vector_btn = QPushButton(\"...\")\n+        browse_vector_btn.setMaximumWidth(30)\n+        browse_vector_btn.clicked.connect(self.browse_vector_file)\n+        vector_file_row.addWidget(browse_vector_btn)\n+        batch_file_layout.addLayout(vector_file_row)\n+\n+        # CRS selection\n+        crs_row = QHBoxLayout()\n+        crs_row.addWidget(QLabel(\"CRS:\"))\n+        self.point_crs_edit = QLineEdit()\n+        self.point_crs_edit.setPlaceholderText(\"e.g., EPSG:4326 (auto-detect if empty)\")\n+        crs_row.addWidget(self.point_crs_edit)\n+        batch_file_layout.addLayout(crs_row)\n+\n+        batch_file_group.setLayout(batch_file_layout)\n+        batch_layout.addWidget(batch_file_group)\n+\n+        # Batch Settings\n+        batch_settings_group = QGroupBox(\"Batch Settings\")\n+        batch_settings_layout = QVBoxLayout()\n+\n+        # Batch size filters\n+        batch_size_row = QHBoxLayout()\n+        batch_size_row.addWidget(QLabel(\"Min size:\"))\n+        self.batch_min_size_spin = QSpinBox()\n+        self.batch_min_size_spin.setRange(0, 1000000)\n+        self.batch_min_size_spin.setValue(0)\n+        batch_size_row.addWidget(self.batch_min_size_spin)\n+\n+        batch_size_row.addWidget(QLabel(\"Max size:\"))\n+        self.batch_max_size_spin = QSpinBox()\n+        self.batch_max_size_spin.setRange(0, 10000000)\n+        self.batch_max_size_spin.setValue(0)\n+        self.batch_max_size_spin.setSpecialValueText(\"No limit\")\n+        batch_size_row.addWidget(self.batch_max_size_spin)\n+        batch_settings_layout.addLayout(batch_size_row)\n+\n+        # Output options for batch\n+        batch_output_row = QHBoxLayout()\n+        self.batch_output_edit = QLineEdit()\n+        self.batch_output_edit.setPlaceholderText(\"Output raster file (optional)...\")\n+        batch_output_row.addWidget(self.batch_output_edit)\n+\n+        browse_batch_output_btn = QPushButton(\"...\")\n+        browse_batch_output_btn.setMaximumWidth(30)\n+        browse_batch_output_btn.clicked.connect(self.browse_batch_output)\n+        batch_output_row.addWidget(browse_batch_output_btn)\n+        batch_settings_layout.addLayout(batch_output_row)\n+\n+        # Batch unique values\n+        self.batch_unique_check = QCheckBox(\"Unique values for each object\")\n+        self.batch_unique_check.setChecked(True)\n+        batch_settings_layout.addWidget(self.batch_unique_check)\n+\n+        batch_settings_group.setLayout(batch_settings_layout)\n+        batch_layout.addWidget(batch_settings_group)\n+\n+        # Segment button\n+        self.batch_segment_btn = QPushButton(\"Run Batch Segmentation\")\n+        self.batch_segment_btn.clicked.connect(self.segment_by_points_batch)\n+        batch_layout.addWidget(self.batch_segment_btn)\n+\n+        # Batch status\n+        self.batch_status_label = QLabel(\"\")\n+        batch_layout.addWidget(self.batch_status_label)\n+\n+        # Info text\n+        batch_info = QLabel(\n+            \"<i>Batch mode processes each point as a separate prompt, \"\n+            \"generating individual masks for each point.</i>\"\n+        )\n+        batch_info.setWordWrap(True)\n+        batch_layout.addWidget(batch_info)\n+\n+        batch_layout.addStretch()\n+        return batch_tab\n+\n+    def _create_output_tab(self):\n+        \"\"\"Create the output settings tab.\"\"\"\n+        output_tab = QWidget()\n+        output_layout = QVBoxLayout()\n+        output_tab.setLayout(output_layout)\n+\n+        output_group = QGroupBox(\"Output Settings\")\n+        output_group_layout = QVBoxLayout()\n+\n+        # Output format\n+        format_row = QHBoxLayout()\n+        format_row.addWidget(QLabel(\"Format:\"))\n+        self.output_format_combo = QComboBox()\n+        self.output_format_combo.addItems(\n+            [\"Raster (GeoTIFF)\", \"Vector (GeoPackage)\", \"Vector (Shapefile)\"]\n+        )\n+        self.output_format_combo.currentIndexChanged.connect(\n+            self._on_output_format_changed\n+        )\n+        format_row.addWidget(self.output_format_combo)\n+        output_group_layout.addLayout(format_row)\n+\n+        # Unique values\n+        self.unique_check = QCheckBox(\"Unique values for each object\")\n+        self.unique_check.setChecked(True)\n+        output_group_layout.addWidget(self.unique_check)\n+\n+        # Add to map\n+        self.add_to_map_check = QCheckBox(\"Add result to map\")\n+        self.add_to_map_check.setChecked(True)\n+        output_group_layout.addWidget(self.add_to_map_check)\n+\n+        # Output path\n+        output_path_row = QHBoxLayout()\n+        self.output_path_edit = QLineEdit()\n+        self.output_path_edit.setPlaceholderText(\n+            \"Output file path (optional, uses temp file if empty)...\"\n+        )\n+        output_path_row.addWidget(self.output_path_edit)\n+\n+        output_browse_btn = QPushButton(\"...\")\n+        output_browse_btn.setMaximumWidth(30)\n+        output_browse_btn.clicked.connect(self.browse_output)\n+        output_path_row.addWidget(output_browse_btn)\n+        output_group_layout.addLayout(output_path_row)\n+\n+        output_group.setLayout(output_group_layout)\n+        output_layout.addWidget(output_group)\n+\n+        # Regularize options (for vector output)\n+        self.regularize_group = QGroupBox(\"Regularize Options (Vector Only)\")\n+        regularize_layout = QVBoxLayout()\n+\n+        # Regularize checkbox\n+        self.regularize_check = QCheckBox(\"Regularize polygons (orthogonalize)\")\n+        self.regularize_check.setChecked(False)\n+        self.regularize_check.stateChanged.connect(self._on_regularize_changed)\n+        regularize_layout.addWidget(self.regularize_check)\n+\n+        # Epsilon (Douglas-Peucker tolerance)\n+        epsilon_row = QHBoxLayout()\n+        epsilon_row.addWidget(QLabel(\"Epsilon:\"))\n+        self.epsilon_spin = QDoubleSpinBox()\n+        self.epsilon_spin.setRange(0.0, 100.0)\n+        self.epsilon_spin.setValue(2.0)\n+        self.epsilon_spin.setSingleStep(0.5)\n+        self.epsilon_spin.setToolTip(\n+            \"Douglas-Peucker simplification tolerance for orthogonalization\"\n+        )\n+        self.epsilon_spin.setEnabled(False)\n+        epsilon_row.addWidget(self.epsilon_spin)\n+        regularize_layout.addLayout(epsilon_row)\n+\n+        # Min Area filter\n+        min_area_row = QHBoxLayout()\n+        min_area_row.addWidget(QLabel(\"Min Area:\"))\n+        self.min_area_spin = QDoubleSpinBox()\n+        self.min_area_spin.setRange(0.0, 100000.0)\n+        self.min_area_spin.setValue(0.0)\n+        self.min_area_spin.setSuffix(\" m\")\n+        self.min_area_spin.setToolTip(\n+            \"Minimum area filter - polygons smaller than this will be removed\"\n+        )\n+        self.min_area_spin.setEnabled(False)\n+        min_area_row.addWidget(self.min_area_spin)\n+        regularize_layout.addLayout(min_area_row)\n+\n+        self.regularize_group.setLayout(regularize_layout)\n+        self.regularize_group.setVisible(False)  # Hidden by default (raster selected)\n+        output_layout.addWidget(self.regularize_group)\n+\n+        # Save button\n+        self.save_btn = QPushButton(\"Save Masks\")\n+        self.save_btn.clicked.connect(self.save_masks)\n+        output_layout.addWidget(self.save_btn)\n+\n+        # Results info\n+        results_group = QGroupBox(\"Results\")\n+        results_layout = QVBoxLayout()\n+\n+        self.results_text = QTextEdit()\n+        self.results_text.setReadOnly(True)\n+        self.results_text.setMaximumHeight(150)\n+        results_layout.addWidget(self.results_text)\n+\n+        results_group.setLayout(results_layout)\n+        output_layout.addWidget(results_group)\n+\n+        output_layout.addStretch()\n+        return output_tab\n+\n+    def _on_output_format_changed(self, index):\n+        \"\"\"Handle output format combo box change.\"\"\"\n+        format_text = self.output_format_combo.currentText()\n+        is_vector = \"Vector\" in format_text\n+        self.regularize_group.setVisible(is_vector)\n+\n+    def _on_regularize_changed(self, state):\n+        \"\"\"Handle regularize checkbox state change.\"\"\"\n+        enabled = state == Qt.Checked\n+        self.epsilon_spin.setEnabled(enabled)\n+        self.min_area_spin.setEnabled(enabled)\n+\n+    def refresh_layers(self):\n+        \"\"\"Refresh the list of raster layers.\"\"\"\n+        self.layer_combo.clear()\n+        layers = QgsProject.instance().mapLayers().values()\n+        for layer in layers:\n+            if isinstance(layer, QgsRasterLayer):\n+                self.layer_combo.addItem(layer.name(), layer.id())\n+\n+    def refresh_vector_layers(self):\n+        \"\"\"Refresh the list of vector layers (for batch point mode).\"\"\"\n+        self.vector_layer_combo.clear()\n+        self.vector_layer_combo.addItem(\"-- Select from file instead --\", None)\n+        layers = QgsProject.instance().mapLayers().values()\n+        for layer in layers:\n+            if isinstance(layer, QgsVectorLayer):\n+                # Only include point layers\n+                if layer.geometryType() == QgsWkbTypes.PointGeometry:\n+                    self.vector_layer_combo.addItem(layer.name(), layer.id())\n+\n+    def browse_vector_file(self):\n+        \"\"\"Browse for a vector file containing points.\"\"\"\n+        file_path, _ = QFileDialog.getOpenFileName(\n+            self,\n+            \"Select Vector File\",\n+            \"\",\n+            \"Vector files (*.geojson *.json *.shp *.gpkg *.kml);;All files (*.*)\",\n+        )\n+        if file_path:\n+            self.vector_file_edit.setText(file_path)\n+\n+    def browse_batch_output(self):\n+        \"\"\"Browse for batch output file location.\"\"\"\n+        file_path, _ = QFileDialog.getSaveFileName(\n+            self, \"Save Batch Output\", \"\", \"GeoTIFF (*.tif)\"\n+        )\n+        if file_path:\n+            self.batch_output_edit.setText(file_path)\n+\n+    def browse_image(self):\n+        \"\"\"Browse for an image file.\"\"\"\n+        file_path, _ = QFileDialog.getOpenFileName(\n+            self,\n+            \"Select Image\",\n+            \"\",\n+            \"Images (*.tif *.tiff *.jpg *.jpeg *.png);;All files (*.*)\",\n+        )\n+        if file_path:\n+            self.image_path_edit.setText(file_path)\n+\n+    def browse_output(self):\n+        \"\"\"Browse for output file location.\"\"\"\n+        format_text = self.output_format_combo.currentText()\n+        if \"GeoPackage\" in format_text:\n+            filter_str = \"GeoPackage (*.gpkg)\"\n+        elif \"Shapefile\" in format_text:\n+            filter_str = \"Shapefile (*.shp)\"\n+        else:\n+            filter_str = \"GeoTIFF (*.tif)\"\n+\n+        file_path, _ = QFileDialog.getSaveFileName(self, \"Save Output\", \"\", filter_str)\n+        if file_path:\n+            self.output_path_edit.setText(file_path)\n+\n+    def load_model(self):\n+        \"\"\"Load the SamGeo model.\"\"\"\n+        try:\n+            self.progress_bar.setVisible(True)\n+            self.progress_bar.setRange(0, 0)  # Indeterminate\n+            self.model_status.setText(\"Loading model...\")\n+            self.model_status.setStyleSheet(\"color: orange;\")\n+            QCoreApplication.processEvents()\n+\n+            model_version = self.model_combo.currentText()\n+            backend = self.backend_combo.currentText()\n+            device = self.device_combo.currentText()\n+            if device == \"auto\":\n+                device = None\n+\n+            confidence = self.conf_spin.value()\n+            enable_interactive = self.interactive_check.isChecked()\n+\n+            # Import and initialize the appropriate model\n+            if \"SamGeo3\" in model_version:\n+                from samgeo import SamGeo3\n+\n+                self.sam = SamGeo3(\n+                    backend=backend,\n+                    device=device,\n+                    confidence_threshold=confidence,\n+                    enable_inst_interactivity=enable_interactive,\n+                )\n+                model_name = \"SamGeo3\"\n+            elif \"SamGeo2\" in model_version:\n+                from samgeo import SamGeo2\n+\n+                self.sam = SamGeo2(\n+                    device=device,\n+                )\n+                model_name = \"SamGeo2\"\n+            else:\n+                from samgeo import SamGeo\n+\n+                self.sam = SamGeo(\n+                    device=device,\n+                )\n+                model_name = \"SamGeo\"\n+\n+            self.model_status.setText(f\"Model: {model_name} loaded\")\n+            self.model_status.setStyleSheet(\"color: green;\")\n+            self.log_message(f\"{model_name} model loaded successfully\")\n+\n+        except Exception as e:\n+            self.model_status.setText(\"Model: Failed to load\")\n+            self.model_status.setStyleSheet(\"color: red;\")\n+            self.show_error(f\"Failed to load model: {str(e)}\")\n+\n+        finally:\n+            self.progress_bar.setVisible(False)\n+\n+    def set_image_from_layer(self):\n+        \"\"\"Set the image from the selected QGIS layer.\"\"\"\n+        if self.sam is None:\n+            self.show_error(\"Please load the model first.\")\n+            return\n+\n+        layer_id = self.layer_combo.currentData()\n+        if not layer_id:\n+            self.show_error(\"Please select a raster layer.\")\n+            return\n+\n+        layer = QgsProject.instance().mapLayer(layer_id)\n+        if not layer:\n+            self.show_error(\"Layer not found.\")\n+            return\n+\n+        # Get the layer's file path\n+        source = layer.source()\n+        if not os.path.exists(source):\n+            self.show_error(f\"Layer source file not found: {source}\")\n+            return\n+\n+        try:\n+            self.progress_bar.setVisible(True)\n+            self.progress_bar.setRange(0, 0)\n+            self.image_status.setText(\"Setting image...\")\n+            QCoreApplication.processEvents()\n+\n+            self.sam.set_image(source)\n+            self.current_layer = layer\n+            self.current_image_path = source\n+\n+            self.image_status.setText(f\"Image: {layer.name()}\")\n+            self.image_status.setStyleSheet(\"color: green;\")\n+            self.log_message(f\"Image set from layer: {layer.name()}\")\n+\n+        except Exception as e:\n+            self.image_status.setText(\"Image: Failed to set\")\n+            self.image_status.setStyleSheet(\"color: red;\")\n+            self.show_error(f\"Failed to set image: {str(e)}\")\n+\n+        finally:\n+            self.progress_bar.setVisible(False)\n+\n+    def set_image_from_file(self):\n+        \"\"\"Set the image from the file path.\"\"\"\n+        if self.sam is None:\n+            self.show_error(\"Please load the model first.\")\n+            return\n+\n+        file_path = self.image_path_edit.text()\n+        if not file_path or not os.path.exists(file_path):\n+            self.show_error(\"Please select a valid image file.\")\n+            return\n+\n+        try:\n+            self.progress_bar.setVisible(True)\n+            self.progress_bar.setRange(0, 0)\n+            self.image_status.setText(\"Setting image...\")\n+            QCoreApplication.processEvents()\n+\n+            self.sam.set_image(file_path)\n+            self.current_image_path = file_path\n+            self.current_layer = None\n+\n+            self.image_status.setText(f\"Image: {os.path.basename(file_path)}\")\n+            self.image_status.setStyleSheet(\"color: green;\")\n+            self.log_message(f\"Image set from file: {file_path}\")\n+\n+            # Optionally add the layer to the map\n+            layer = QgsRasterLayer(file_path, os.path.basename(file_path))\n+            if layer.isValid():\n+                QgsProject.instance().addMapLayer(layer)\n+                self.current_layer = layer\n+                self.refresh_layers()\n+\n+        except Exception as e:\n+            self.image_status.setText(\"Image: Failed to set\")\n+            self.image_status.setStyleSheet(\"color: red;\")\n+            self.show_error(f\"Failed to set image: {str(e)}\")\n+\n+        finally:\n+            self.progress_bar.setVisible(False)\n+\n+    def segment_by_text(self):\n+        \"\"\"Segment the image using text prompt.\"\"\"\n+        if self.sam is None:\n+            self.show_error(\"Please load the model first.\")\n+            return\n+\n+        if self.current_image_path is None:\n+            self.show_error(\"Please set an image first.\")\n+            return\n+\n+        prompt = self.text_prompt_edit.text().strip()\n+        if not prompt:\n+            self.show_error(\"Please enter a text prompt.\")\n+            return\n+\n+        try:\n+            self.progress_bar.setVisible(True)\n+            self.progress_bar.setRange(0, 0)\n+            self.text_status_label.setText(\"Processing...\")\n+            self.text_status_label.setStyleSheet(\"color: orange;\")\n+            QCoreApplication.processEvents()\n+\n+            min_size = self.min_size_spin.value()\n+            max_size = (\n+                self.max_size_spin.value() if self.max_size_spin.value() > 0 else None\n+            )\n+\n+            self.sam.generate_masks(prompt, min_size=min_size, max_size=max_size)\n+\n+            num_masks = len(self.sam.masks) if self.sam.masks else 0\n+            self.results_text.setText(\n+                f\"Text Segmentation Results:\\n\"\n+                f\"Prompt: {prompt}\\n\"\n+                f\"Objects found: {num_masks}\\n\"\n+            )\n+\n+            # Update status label\n+            if num_masks > 0:\n+                self.text_status_label.setText(\n+                    f\"Found {num_masks} object(s). Go to Output tab to save.\"\n+                )\n+                self.text_status_label.setStyleSheet(\"color: green;\")\n+            else:\n+                self.text_status_label.setText(\n+                    \"No objects found. Try a different prompt.\"\n+                )\n+                self.text_status_label.setStyleSheet(\"color: orange;\")\n+\n+            self.log_message(f\"Text segmentation complete. Found {num_masks} objects.\")\n+\n+        except Exception as e:\n+            self.text_status_label.setText(\"Segmentation failed!\")\n+            self.text_status_label.setStyleSheet(\"color: red;\")\n+            self.show_error(f\"Segmentation failed: {str(e)}\")\n+\n+        finally:\n+            self.progress_bar.setVisible(False)\n+\n+    def start_point_tool(self, foreground=True):\n+        \"\"\"Start the point prompt tool.\"\"\"\n+        if self.current_layer is None:\n+            self.show_error(\"Please set an image first.\")\n+            self.add_fg_point_btn.setChecked(False)\n+            self.add_bg_point_btn.setChecked(False)\n+            return\n+\n+        # Uncheck the other button\n+        if foreground:\n+            self.add_bg_point_btn.setChecked(False)\n+        else:\n+            self.add_fg_point_btn.setChecked(False)\n+\n+        if self.point_tool is None:\n+            self.point_tool = PointPromptTool(self.canvas, self)\n+\n+        self.point_tool.set_foreground(foreground)\n+        self.previous_tool = self.canvas.mapTool()\n+        self.canvas.setMapTool(self.point_tool)\n+\n+    def add_point(self, point, foreground):\n+        \"\"\"Add a point prompt.\"\"\"\n+        # Convert map coordinates to pixel coordinates\n+        if self.current_layer is not None:\n+            extent = self.current_layer.extent()\n+            width = self.current_layer.width()\n+            height = self.current_layer.height()\n+\n+            # Calculate pixel coordinates\n+            px = (point.x() - extent.xMinimum()) / extent.width() * width\n+            py = (extent.yMaximum() - point.y()) / extent.height() * height\n+\n+            self.point_coords.append([px, py])\n+            self.point_labels.append(1 if foreground else 0)\n+\n+            # Update list widget\n+            label_text = \"FG\" if foreground else \"BG\"\n+            item = QListWidgetItem(f\"{label_text}: ({px:.1f}, {py:.1f})\")\n+            item.setForeground(Qt.green if foreground else Qt.red)\n+            self.points_list.addItem(item)\n+\n+    def clear_points(self):\n+        \"\"\"Clear all point prompts.\"\"\"\n+        self.point_coords = []\n+        self.point_labels = []\n+        self.points_list.clear()\n+\n+        # Clear rubber bands from point tool\n+        if self.point_tool is not None:\n+            self.point_tool.clear_markers()\n+\n+    def start_batch_point_tool(self):\n+        \"\"\"Start the batch point tool for adding multiple points.\"\"\"\n+        if self.current_layer is None:\n+            self.show_error(\"Please set an image first.\")\n+            self.batch_add_point_btn.setChecked(False)\n+            return\n+\n+        if self.batch_point_tool is None:\n+            self.batch_point_tool = PointPromptTool(self.canvas, self, batch_mode=True)\n+\n+        self.batch_point_tool.set_foreground(True)  # All batch points are foreground\n+        self.previous_tool = self.canvas.mapTool()\n+        self.canvas.setMapTool(self.batch_point_tool)\n+\n+    def add_batch_point(self, point):\n+        \"\"\"Add a batch point prompt.\"\"\"\n+        if self.current_layer is not None:\n+            extent = self.current_layer.extent()\n+            width = self.current_layer.width()\n+            height = self.current_layer.height()\n+\n+            # Calculate pixel coordinates\n+            px = (point.x() - extent.xMinimum()) / extent.width() * width\n+            py = (extent.yMaximum() - point.y()) / extent.height() * height\n+\n+            self.batch_point_coords.append([px, py])\n+            self.batch_point_coords_map.append([point.x(), point.y()])\n+\n+            # Update list widget\n+            item = QListWidgetItem(\n+                f\"Point {len(self.batch_point_coords)}: ({px:.1f}, {py:.1f})\"\n+            )\n+            item.setForeground(Qt.green)",
    "comment": "Using Qt.green directly is deprecated in PyQt5/PyQt6. Should use QColor or the GlobalColor enum properly. For cross-compatibility with different Qt versions, it's better to use QColor(Qt.green), or better yet, QColor(\"green\").\n```suggestion\n            item.setForeground(QColor(\"green\"))\n```",
    "line_number": 959,
    "enriched": "File: qgis_plugin/geoai_plugin/dialogs/samgeo.py\nCode: @@ -0,0 +1,1430 @@\n+\"\"\"\n+SamGeo Dock Widget for GeoAI Plugin\n+\n+This dock widget provides an interface for remote sensing image segmentation\n+using the SamGeo library (SAM, SAM2, and SAM3 models).\n+\"\"\"\n+\n+import os\n+\n+from qgis.PyQt.QtCore import Qt, QCoreApplication\n+from qgis.PyQt.QtGui import QColor\n+from qgis.PyQt.QtWidgets import (\n+    QDockWidget,\n+    QWidget,\n+    QVBoxLayout,\n+    QHBoxLayout,\n+    QLabel,\n+    QPushButton,\n+    QLineEdit,\n+    QComboBox,\n+    QSpinBox,\n+    QDoubleSpinBox,\n+    QCheckBox,\n+    QGroupBox,\n+    QFileDialog,\n+    QMessageBox,\n+    QProgressBar,\n+    QTabWidget,\n+    QTextEdit,\n+    QListWidget,\n+    QListWidgetItem,\n+    QScrollArea,\n+)\n+from qgis.core import (\n+    QgsProject,\n+    QgsRasterLayer,\n+    QgsVectorLayer,\n+    QgsWkbTypes,\n+    Qgis,\n+    QgsMessageLog,\n+)\n+\n+from .map_tools import PointPromptTool, BoxPromptTool\n+\n+\n+class SamGeoDockWidget(QDockWidget):\n+    \"\"\"Dock widget for SamGeo segmentation operations.\"\"\"\n+\n+    def __init__(self, iface, parent=None):\n+        \"\"\"Initialize the SamGeo dock widget.\n+\n+        Args:\n+            iface: The QGIS interface instance.\n+            parent: Parent widget.\n+        \"\"\"\n+        super().__init__(\"SamGeo Segmentation\", parent)\n+        self.iface = iface\n+        self.canvas = iface.mapCanvas()\n+        self.setAllowedAreas(Qt.LeftDockWidgetArea | Qt.RightDockWidgetArea)\n+\n+        # SamGeo model instance\n+        self.sam = None\n+        self.current_layer = None\n+        self.current_image_path = None\n+\n+        # Point and box prompts\n+        self.point_coords = []\n+        self.point_labels = []\n+        self.box_coords = None\n+\n+        # Batch point prompts\n+        self.batch_point_coords = []\n+        self.batch_point_coords_map = []  # Map coordinates for display\n+\n+        # Map tools\n+        self.point_tool = None\n+        self.batch_point_tool = None\n+        self.box_tool = None\n+        self.previous_tool = None\n+\n+        self._setup_ui()\n+\n+    def _setup_ui(self):\n+        \"\"\"Set up the user interface.\"\"\"\n+        # Main widget with scroll area\n+        scroll_area = QScrollArea()\n+        scroll_area.setWidgetResizable(True)\n+        scroll_area.setHorizontalScrollBarPolicy(Qt.ScrollBarAlwaysOff)\n+\n+        main_widget = QWidget()\n+        main_layout = QVBoxLayout()\n+        main_widget.setLayout(main_layout)\n+\n+        # Tab widget for different modes\n+        tab_widget = QTabWidget()\n+\n+        # === Model Settings Tab ===\n+        model_tab = self._create_model_tab()\n+        tab_widget.addTab(model_tab, \"Model\")\n+\n+        # === Text Prompts Tab ===\n+        text_tab = self._create_text_tab()\n+        tab_widget.addTab(text_tab, \"Text\")\n+\n+        # === Interactive Tab ===\n+        interactive_tab = self._create_interactive_tab()\n+        tab_widget.addTab(interactive_tab, \"Interactive\")\n+\n+        # === Batch Tab ===\n+        batch_tab = self._create_batch_tab()\n+        tab_widget.addTab(batch_tab, \"Batch\")\n+\n+        # === Output Tab ===\n+        output_tab = self._create_output_tab()\n+        tab_widget.addTab(output_tab, \"Output\")\n+\n+        # Add tab widget to main layout\n+        main_layout.addWidget(tab_widget)\n+\n+        # Progress bar\n+        self.progress_bar = QProgressBar()\n+        self.progress_bar.setVisible(False)\n+        main_layout.addWidget(self.progress_bar)\n+\n+        scroll_area.setWidget(main_widget)\n+        self.setWidget(scroll_area)\n+\n+    def _create_model_tab(self):\n+        \"\"\"Create the model settings tab.\"\"\"\n+        model_tab = QWidget()\n+        model_layout = QVBoxLayout()\n+        model_tab.setLayout(model_layout)\n+\n+        # Backend selection\n+        backend_group = QGroupBox(\"Model Settings\")\n+        backend_layout = QVBoxLayout()\n+\n+        # Model version selection\n+        version_row = QHBoxLayout()\n+        version_row.addWidget(QLabel(\"Model:\"))\n+        self.model_combo = QComboBox()\n+        self.model_combo.addItems([\"SamGeo3 (SAM3)\", \"SamGeo2 (SAM2)\", \"SamGeo (SAM1)\"])\n+        version_row.addWidget(self.model_combo)\n+        backend_layout.addLayout(version_row)\n+\n+        backend_row = QHBoxLayout()\n+        backend_row.addWidget(QLabel(\"Backend:\"))\n+        self.backend_combo = QComboBox()\n+        self.backend_combo.addItems([\"meta\", \"transformers\"])\n+        backend_row.addWidget(self.backend_combo)\n+        backend_layout.addLayout(backend_row)\n+\n+        # Device selection\n+        device_row = QHBoxLayout()\n+        device_row.addWidget(QLabel(\"Device:\"))\n+        self.device_combo = QComboBox()\n+        self.device_combo.addItems([\"auto\", \"cuda\", \"cpu\"])\n+        device_row.addWidget(self.device_combo)\n+        backend_layout.addLayout(device_row)\n+\n+        # Confidence threshold\n+        conf_row = QHBoxLayout()\n+        conf_row.addWidget(QLabel(\"Confidence:\"))\n+        self.conf_spin = QDoubleSpinBox()\n+        self.conf_spin.setRange(0.0, 1.0)\n+        self.conf_spin.setValue(0.5)\n+        self.conf_spin.setSingleStep(0.05)\n+        conf_row.addWidget(self.conf_spin)\n+        backend_layout.addLayout(conf_row)\n+\n+        # Interactive mode checkbox\n+        self.interactive_check = QCheckBox(\n+            \"Enable Interactive Mode (Point/Box Prompts)\"\n+        )\n+        self.interactive_check.setChecked(True)\n+        backend_layout.addWidget(self.interactive_check)\n+\n+        # Load model button\n+        self.load_model_btn = QPushButton(\"Load Model\")\n+        self.load_model_btn.clicked.connect(self.load_model)\n+        backend_layout.addWidget(self.load_model_btn)\n+\n+        # Model status\n+        self.model_status = QLabel(\"Model: Not loaded\")\n+        self.model_status.setStyleSheet(\"color: gray;\")\n+        backend_layout.addWidget(self.model_status)\n+\n+        backend_group.setLayout(backend_layout)\n+        model_layout.addWidget(backend_group)\n+\n+        # Layer selection\n+        layer_group = QGroupBox(\"Input Layer\")\n+        layer_layout = QVBoxLayout()\n+\n+        layer_row = QHBoxLayout()\n+        self.layer_combo = QComboBox()\n+        self.refresh_layers()\n+        layer_row.addWidget(self.layer_combo)\n+\n+        refresh_btn = QPushButton(\"\")\n+        refresh_btn.setMaximumWidth(30)\n+        refresh_btn.clicked.connect(self.refresh_layers)\n+        layer_row.addWidget(refresh_btn)\n+        layer_layout.addLayout(layer_row)\n+\n+        self.set_layer_btn = QPushButton(\"Set Image from Layer\")\n+        self.set_layer_btn.clicked.connect(self.set_image_from_layer)\n+        layer_layout.addWidget(self.set_layer_btn)\n+\n+        # Or load from file\n+        file_row = QHBoxLayout()\n+        self.image_path_edit = QLineEdit()\n+        self.image_path_edit.setPlaceholderText(\"Or select image file...\")\n+        file_row.addWidget(self.image_path_edit)\n+\n+        browse_btn = QPushButton(\"...\")\n+        browse_btn.setMaximumWidth(30)\n+        browse_btn.clicked.connect(self.browse_image)\n+        file_row.addWidget(browse_btn)\n+        layer_layout.addLayout(file_row)\n+\n+        self.set_file_btn = QPushButton(\"Set Image from File\")\n+        self.set_file_btn.clicked.connect(self.set_image_from_file)\n+        layer_layout.addWidget(self.set_file_btn)\n+\n+        self.image_status = QLabel(\"Image: Not set\")\n+        self.image_status.setStyleSheet(\"color: gray;\")\n+        layer_layout.addWidget(self.image_status)\n+\n+        layer_group.setLayout(layer_layout)\n+        model_layout.addWidget(layer_group)\n+\n+        model_layout.addStretch()\n+        return model_tab\n+\n+    def _create_text_tab(self):\n+        \"\"\"Create the text prompts tab.\"\"\"\n+        text_tab = QWidget()\n+        text_layout = QVBoxLayout()\n+        text_tab.setLayout(text_layout)\n+\n+        text_group = QGroupBox(\"Text-Based Segmentation\")\n+        text_group_layout = QVBoxLayout()\n+\n+        text_group_layout.addWidget(QLabel(\"Describe objects to segment:\"))\n+        self.text_prompt_edit = QLineEdit()\n+        self.text_prompt_edit.setPlaceholderText(\"e.g., tree, building, road...\")\n+        text_group_layout.addWidget(self.text_prompt_edit)\n+\n+        # Size filters\n+        size_row = QHBoxLayout()\n+        size_row.addWidget(QLabel(\"Min size:\"))\n+        self.min_size_spin = QSpinBox()\n+        self.min_size_spin.setRange(0, 1000000)\n+        self.min_size_spin.setValue(0)\n+        size_row.addWidget(self.min_size_spin)\n+\n+        size_row.addWidget(QLabel(\"Max size:\"))\n+        self.max_size_spin = QSpinBox()\n+        self.max_size_spin.setRange(0, 10000000)\n+        self.max_size_spin.setValue(0)\n+        self.max_size_spin.setSpecialValueText(\"No limit\")\n+        size_row.addWidget(self.max_size_spin)\n+        text_group_layout.addLayout(size_row)\n+\n+        self.text_segment_btn = QPushButton(\"Segment by Text\")\n+        self.text_segment_btn.clicked.connect(self.segment_by_text)\n+        text_group_layout.addWidget(self.text_segment_btn)\n+\n+        # Text segmentation status\n+        self.text_status_label = QLabel(\"\")\n+        text_group_layout.addWidget(self.text_status_label)\n+\n+        text_group.setLayout(text_group_layout)\n+        text_layout.addWidget(text_group)\n+\n+        text_layout.addStretch()\n+        return text_tab\n+\n+    def _create_interactive_tab(self):\n+        \"\"\"Create the interactive prompts tab.\"\"\"\n+        interactive_tab = QWidget()\n+        interactive_layout = QVBoxLayout()\n+        interactive_tab.setLayout(interactive_layout)\n+\n+        # Point prompts\n+        point_group = QGroupBox(\"Point Prompts\")\n+        point_layout = QVBoxLayout()\n+\n+        point_btn_row = QHBoxLayout()\n+        self.add_fg_point_btn = QPushButton(\"Add Foreground Points\")\n+        self.add_fg_point_btn.setCheckable(True)\n+        self.add_fg_point_btn.clicked.connect(\n+            lambda: self.start_point_tool(foreground=True)\n+        )\n+        point_btn_row.addWidget(self.add_fg_point_btn)\n+\n+        self.add_bg_point_btn = QPushButton(\"Add Background Points\")\n+        self.add_bg_point_btn.setCheckable(True)\n+        self.add_bg_point_btn.clicked.connect(\n+            lambda: self.start_point_tool(foreground=False)\n+        )\n+        point_btn_row.addWidget(self.add_bg_point_btn)\n+        point_layout.addLayout(point_btn_row)\n+\n+        self.points_list = QListWidget()\n+        self.points_list.setMaximumHeight(100)\n+        point_layout.addWidget(self.points_list)\n+\n+        point_action_row = QHBoxLayout()\n+        clear_points_btn = QPushButton(\"Clear Points\")\n+        clear_points_btn.clicked.connect(self.clear_points)\n+        point_action_row.addWidget(clear_points_btn)\n+\n+        self.point_segment_btn = QPushButton(\"Segment by Points\")\n+        self.point_segment_btn.clicked.connect(self.segment_by_points)\n+        point_action_row.addWidget(self.point_segment_btn)\n+        point_layout.addLayout(point_action_row)\n+\n+        # Point segmentation status\n+        self.point_status_label = QLabel(\"\")\n+        point_layout.addWidget(self.point_status_label)\n+\n+        point_group.setLayout(point_layout)\n+        interactive_layout.addWidget(point_group)\n+\n+        # Box prompts\n+        box_group = QGroupBox(\"Box Prompts\")\n+        box_layout = QVBoxLayout()\n+\n+        self.draw_box_btn = QPushButton(\"Draw Box\")\n+        self.draw_box_btn.setCheckable(True)\n+        self.draw_box_btn.clicked.connect(self.start_box_tool)\n+        box_layout.addWidget(self.draw_box_btn)\n+\n+        self.box_label = QLabel(\"Box: Not set\")\n+        box_layout.addWidget(self.box_label)\n+\n+        box_action_row = QHBoxLayout()\n+        clear_box_btn = QPushButton(\"Clear Box\")\n+        clear_box_btn.clicked.connect(self.clear_box)\n+        box_action_row.addWidget(clear_box_btn)\n+\n+        self.box_segment_btn = QPushButton(\"Segment by Box\")\n+        self.box_segment_btn.clicked.connect(self.segment_by_box)\n+        box_action_row.addWidget(self.box_segment_btn)\n+        box_layout.addLayout(box_action_row)\n+\n+        # Box segmentation status\n+        self.box_status_label = QLabel(\"\")\n+        box_layout.addWidget(self.box_status_label)\n+\n+        box_group.setLayout(box_layout)\n+        interactive_layout.addWidget(box_group)\n+\n+        interactive_layout.addStretch()\n+        return interactive_tab\n+\n+    def _create_batch_tab(self):\n+        \"\"\"Create the batch processing tab.\"\"\"\n+        batch_tab = QWidget()\n+        batch_layout = QVBoxLayout()\n+        batch_tab.setLayout(batch_layout)\n+\n+        # Interactive Points for Batch\n+        batch_interactive_group = QGroupBox(\"Create Points Interactively\")\n+        batch_interactive_layout = QVBoxLayout()\n+\n+        batch_point_btn_row = QHBoxLayout()\n+        self.batch_add_point_btn = QPushButton(\"Add Points on Map\")\n+        self.batch_add_point_btn.setCheckable(True)\n+        self.batch_add_point_btn.clicked.connect(self.start_batch_point_tool)\n+        batch_point_btn_row.addWidget(self.batch_add_point_btn)\n+\n+        self.batch_clear_points_btn = QPushButton(\"Clear Points\")\n+        self.batch_clear_points_btn.clicked.connect(self.clear_batch_points)\n+        batch_point_btn_row.addWidget(self.batch_clear_points_btn)\n+        batch_interactive_layout.addLayout(batch_point_btn_row)\n+\n+        # Batch points list\n+        self.batch_points_list = QListWidget()\n+        self.batch_points_list.setMaximumHeight(80)\n+        batch_interactive_layout.addWidget(self.batch_points_list)\n+\n+        self.batch_points_count_label = QLabel(\"Points: 0\")\n+        batch_interactive_layout.addWidget(self.batch_points_count_label)\n+\n+        batch_interactive_group.setLayout(batch_interactive_layout)\n+        batch_layout.addWidget(batch_interactive_group)\n+\n+        # Or Load from File/Layer\n+        batch_file_group = QGroupBox(\"Or Load Points from File/Layer\")\n+        batch_file_layout = QVBoxLayout()\n+\n+        # Vector layer selection\n+        vector_layer_row = QHBoxLayout()\n+        vector_layer_row.addWidget(QLabel(\"Layer:\"))\n+        self.vector_layer_combo = QComboBox()\n+        self.refresh_vector_layers()\n+        vector_layer_row.addWidget(self.vector_layer_combo)\n+\n+        refresh_vector_btn = QPushButton(\"\")\n+        refresh_vector_btn.setMaximumWidth(30)\n+        refresh_vector_btn.clicked.connect(self.refresh_vector_layers)\n+        vector_layer_row.addWidget(refresh_vector_btn)\n+        batch_file_layout.addLayout(vector_layer_row)\n+\n+        # Or load from file\n+        vector_file_row = QHBoxLayout()\n+        self.vector_file_edit = QLineEdit()\n+        self.vector_file_edit.setPlaceholderText(\"Or select vector file...\")\n+        vector_file_row.addWidget(self.vector_file_edit)\n+\n+        browse_vector_btn = QPushButton(\"...\")\n+        browse_vector_btn.setMaximumWidth(30)\n+        browse_vector_btn.clicked.connect(self.browse_vector_file)\n+        vector_file_row.addWidget(browse_vector_btn)\n+        batch_file_layout.addLayout(vector_file_row)\n+\n+        # CRS selection\n+        crs_row = QHBoxLayout()\n+        crs_row.addWidget(QLabel(\"CRS:\"))\n+        self.point_crs_edit = QLineEdit()\n+        self.point_crs_edit.setPlaceholderText(\"e.g., EPSG:4326 (auto-detect if empty)\")\n+        crs_row.addWidget(self.point_crs_edit)\n+        batch_file_layout.addLayout(crs_row)\n+\n+        batch_file_group.setLayout(batch_file_layout)\n+        batch_layout.addWidget(batch_file_group)\n+\n+        # Batch Settings\n+        batch_settings_group = QGroupBox(\"Batch Settings\")\n+        batch_settings_layout = QVBoxLayout()\n+\n+        # Batch size filters\n+        batch_size_row = QHBoxLayout()\n+        batch_size_row.addWidget(QLabel(\"Min size:\"))\n+        self.batch_min_size_spin = QSpinBox()\n+        self.batch_min_size_spin.setRange(0, 1000000)\n+        self.batch_min_size_spin.setValue(0)\n+        batch_size_row.addWidget(self.batch_min_size_spin)\n+\n+        batch_size_row.addWidget(QLabel(\"Max size:\"))\n+        self.batch_max_size_spin = QSpinBox()\n+        self.batch_max_size_spin.setRange(0, 10000000)\n+        self.batch_max_size_spin.setValue(0)\n+        self.batch_max_size_spin.setSpecialValueText(\"No limit\")\n+        batch_size_row.addWidget(self.batch_max_size_spin)\n+        batch_settings_layout.addLayout(batch_size_row)\n+\n+        # Output options for batch\n+        batch_output_row = QHBoxLayout()\n+        self.batch_output_edit = QLineEdit()\n+        self.batch_output_edit.setPlaceholderText(\"Output raster file (optional)...\")\n+        batch_output_row.addWidget(self.batch_output_edit)\n+\n+        browse_batch_output_btn = QPushButton(\"...\")\n+        browse_batch_output_btn.setMaximumWidth(30)\n+        browse_batch_output_btn.clicked.connect(self.browse_batch_output)\n+        batch_output_row.addWidget(browse_batch_output_btn)\n+        batch_settings_layout.addLayout(batch_output_row)\n+\n+        # Batch unique values\n+        self.batch_unique_check = QCheckBox(\"Unique values for each object\")\n+        self.batch_unique_check.setChecked(True)\n+        batch_settings_layout.addWidget(self.batch_unique_check)\n+\n+        batch_settings_group.setLayout(batch_settings_layout)\n+        batch_layout.addWidget(batch_settings_group)\n+\n+        # Segment button\n+        self.batch_segment_btn = QPushButton(\"Run Batch Segmentation\")\n+        self.batch_segment_btn.clicked.connect(self.segment_by_points_batch)\n+        batch_layout.addWidget(self.batch_segment_btn)\n+\n+        # Batch status\n+        self.batch_status_label = QLabel(\"\")\n+        batch_layout.addWidget(self.batch_status_label)\n+\n+        # Info text\n+        batch_info = QLabel(\n+            \"<i>Batch mode processes each point as a separate prompt, \"\n+            \"generating individual masks for each point.</i>\"\n+        )\n+        batch_info.setWordWrap(True)\n+        batch_layout.addWidget(batch_info)\n+\n+        batch_layout.addStretch()\n+        return batch_tab\n+\n+    def _create_output_tab(self):\n+        \"\"\"Create the output settings tab.\"\"\"\n+        output_tab = QWidget()\n+        output_layout = QVBoxLayout()\n+        output_tab.setLayout(output_layout)\n+\n+        output_group = QGroupBox(\"Output Settings\")\n+        output_group_layout = QVBoxLayout()\n+\n+        # Output format\n+        format_row = QHBoxLayout()\n+        format_row.addWidget(QLabel(\"Format:\"))\n+        self.output_format_combo = QComboBox()\n+        self.output_format_combo.addItems(\n+            [\"Raster (GeoTIFF)\", \"Vector (GeoPackage)\", \"Vector (Shapefile)\"]\n+        )\n+        self.output_format_combo.currentIndexChanged.connect(\n+            self._on_output_format_changed\n+        )\n+        format_row.addWidget(self.output_format_combo)\n+        output_group_layout.addLayout(format_row)\n+\n+        # Unique values\n+        self.unique_check = QCheckBox(\"Unique values for each object\")\n+        self.unique_check.setChecked(True)\n+        output_group_layout.addWidget(self.unique_check)\n+\n+        # Add to map\n+        self.add_to_map_check = QCheckBox(\"Add result to map\")\n+        self.add_to_map_check.setChecked(True)\n+        output_group_layout.addWidget(self.add_to_map_check)\n+\n+        # Output path\n+        output_path_row = QHBoxLayout()\n+        self.output_path_edit = QLineEdit()\n+        self.output_path_edit.setPlaceholderText(\n+            \"Output file path (optional, uses temp file if empty)...\"\n+        )\n+        output_path_row.addWidget(self.output_path_edit)\n+\n+        output_browse_btn = QPushButton(\"...\")\n+        output_browse_btn.setMaximumWidth(30)\n+        output_browse_btn.clicked.connect(self.browse_output)\n+        output_path_row.addWidget(output_browse_btn)\n+        output_group_layout.addLayout(output_path_row)\n+\n+        output_group.setLayout(output_group_layout)\n+        output_layout.addWidget(output_group)\n+\n+        # Regularize options (for vector output)\n+        self.regularize_group = QGroupBox(\"Regularize Options (Vector Only)\")\n+        regularize_layout = QVBoxLayout()\n+\n+        # Regularize checkbox\n+        self.regularize_check = QCheckBox(\"Regularize polygons (orthogonalize)\")\n+        self.regularize_check.setChecked(False)\n+        self.regularize_check.stateChanged.connect(self._on_regularize_changed)\n+        regularize_layout.addWidget(self.regularize_check)\n+\n+        # Epsilon (Douglas-Peucker tolerance)\n+        epsilon_row = QHBoxLayout()\n+        epsilon_row.addWidget(QLabel(\"Epsilon:\"))\n+        self.epsilon_spin = QDoubleSpinBox()\n+        self.epsilon_spin.setRange(0.0, 100.0)\n+        self.epsilon_spin.setValue(2.0)\n+        self.epsilon_spin.setSingleStep(0.5)\n+        self.epsilon_spin.setToolTip(\n+            \"Douglas-Peucker simplification tolerance for orthogonalization\"\n+        )\n+        self.epsilon_spin.setEnabled(False)\n+        epsilon_row.addWidget(self.epsilon_spin)\n+        regularize_layout.addLayout(epsilon_row)\n+\n+        # Min Area filter\n+        min_area_row = QHBoxLayout()\n+        min_area_row.addWidget(QLabel(\"Min Area:\"))\n+        self.min_area_spin = QDoubleSpinBox()\n+        self.min_area_spin.setRange(0.0, 100000.0)\n+        self.min_area_spin.setValue(0.0)\n+        self.min_area_spin.setSuffix(\" m\")\n+        self.min_area_spin.setToolTip(\n+            \"Minimum area filter - polygons smaller than this will be removed\"\n+        )\n+        self.min_area_spin.setEnabled(False)\n+        min_area_row.addWidget(self.min_area_spin)\n+        regularize_layout.addLayout(min_area_row)\n+\n+        self.regularize_group.setLayout(regularize_layout)\n+        self.regularize_group.setVisible(False)  # Hidden by default (raster selected)\n+        output_layout.addWidget(self.regularize_group)\n+\n+        # Save button\n+        self.save_btn = QPushButton(\"Save Masks\")\n+        self.save_btn.clicked.connect(self.save_masks)\n+        output_layout.addWidget(self.save_btn)\n+\n+        # Results info\n+        results_group = QGroupBox(\"Results\")\n+        results_layout = QVBoxLayout()\n+\n+        self.results_text = QTextEdit()\n+        self.results_text.setReadOnly(True)\n+        self.results_text.setMaximumHeight(150)\n+        results_layout.addWidget(self.results_text)\n+\n+        results_group.setLayout(results_layout)\n+        output_layout.addWidget(results_group)\n+\n+        output_layout.addStretch()\n+        return output_tab\n+\n+    def _on_output_format_changed(self, index):\n+        \"\"\"Handle output format combo box change.\"\"\"\n+        format_text = self.output_format_combo.currentText()\n+        is_vector = \"Vector\" in format_text\n+        self.regularize_group.setVisible(is_vector)\n+\n+    def _on_regularize_changed(self, state):\n+        \"\"\"Handle regularize checkbox state change.\"\"\"\n+        enabled = state == Qt.Checked\n+        self.epsilon_spin.setEnabled(enabled)\n+        self.min_area_spin.setEnabled(enabled)\n+\n+    def refresh_layers(self):\n+        \"\"\"Refresh the list of raster layers.\"\"\"\n+        self.layer_combo.clear()\n+        layers = QgsProject.instance().mapLayers().values()\n+        for layer in layers:\n+            if isinstance(layer, QgsRasterLayer):\n+                self.layer_combo.addItem(layer.name(), layer.id())\n+\n+    def refresh_vector_layers(self):\n+        \"\"\"Refresh the list of vector layers (for batch point mode).\"\"\"\n+        self.vector_layer_combo.clear()\n+        self.vector_layer_combo.addItem(\"-- Select from file instead --\", None)\n+        layers = QgsProject.instance().mapLayers().values()\n+        for layer in layers:\n+            if isinstance(layer, QgsVectorLayer):\n+                # Only include point layers\n+                if layer.geometryType() == QgsWkbTypes.PointGeometry:\n+                    self.vector_layer_combo.addItem(layer.name(), layer.id())\n+\n+    def browse_vector_file(self):\n+        \"\"\"Browse for a vector file containing points.\"\"\"\n+        file_path, _ = QFileDialog.getOpenFileName(\n+            self,\n+            \"Select Vector File\",\n+            \"\",\n+            \"Vector files (*.geojson *.json *.shp *.gpkg *.kml);;All files (*.*)\",\n+        )\n+        if file_path:\n+            self.vector_file_edit.setText(file_path)\n+\n+    def browse_batch_output(self):\n+        \"\"\"Browse for batch output file location.\"\"\"\n+        file_path, _ = QFileDialog.getSaveFileName(\n+            self, \"Save Batch Output\", \"\", \"GeoTIFF (*.tif)\"\n+        )\n+        if file_path:\n+            self.batch_output_edit.setText(file_path)\n+\n+    def browse_image(self):\n+        \"\"\"Browse for an image file.\"\"\"\n+        file_path, _ = QFileDialog.getOpenFileName(\n+            self,\n+            \"Select Image\",\n+            \"\",\n+            \"Images (*.tif *.tiff *.jpg *.jpeg *.png);;All files (*.*)\",\n+        )\n+        if file_path:\n+            self.image_path_edit.setText(file_path)\n+\n+    def browse_output(self):\n+        \"\"\"Browse for output file location.\"\"\"\n+        format_text = self.output_format_combo.currentText()\n+        if \"GeoPackage\" in format_text:\n+            filter_str = \"GeoPackage (*.gpkg)\"\n+        elif \"Shapefile\" in format_text:\n+            filter_str = \"Shapefile (*.shp)\"\n+        else:\n+            filter_str = \"GeoTIFF (*.tif)\"\n+\n+        file_path, _ = QFileDialog.getSaveFileName(self, \"Save Output\", \"\", filter_str)\n+        if file_path:\n+            self.output_path_edit.setText(file_path)\n+\n+    def load_model(self):\n+        \"\"\"Load the SamGeo model.\"\"\"\n+        try:\n+            self.progress_bar.setVisible(True)\n+            self.progress_bar.setRange(0, 0)  # Indeterminate\n+            self.model_status.setText(\"Loading model...\")\n+            self.model_status.setStyleSheet(\"color: orange;\")\n+            QCoreApplication.processEvents()\n+\n+            model_version = self.model_combo.currentText()\n+            backend = self.backend_combo.currentText()\n+            device = self.device_combo.currentText()\n+            if device == \"auto\":\n+                device = None\n+\n+            confidence = self.conf_spin.value()\n+            enable_interactive = self.interactive_check.isChecked()\n+\n+            # Import and initialize the appropriate model\n+            if \"SamGeo3\" in model_version:\n+                from samgeo import SamGeo3\n+\n+                self.sam = SamGeo3(\n+                    backend=backend,\n+                    device=device,\n+                    confidence_threshold=confidence,\n+                    enable_inst_interactivity=enable_interactive,\n+                )\n+                model_name = \"SamGeo3\"\n+            elif \"SamGeo2\" in model_version:\n+                from samgeo import SamGeo2\n+\n+                self.sam = SamGeo2(\n+                    device=device,\n+                )\n+                model_name = \"SamGeo2\"\n+            else:\n+                from samgeo import SamGeo\n+\n+                self.sam = SamGeo(\n+                    device=device,\n+                )\n+                model_name = \"SamGeo\"\n+\n+            self.model_status.setText(f\"Model: {model_name} loaded\")\n+            self.model_status.setStyleSheet(\"color: green;\")\n+            self.log_message(f\"{model_name} model loaded successfully\")\n+\n+        except Exception as e:\n+            self.model_status.setText(\"Model: Failed to load\")\n+            self.model_status.setStyleSheet(\"color: red;\")\n+            self.show_error(f\"Failed to load model: {str(e)}\")\n+\n+        finally:\n+            self.progress_bar.setVisible(False)\n+\n+    def set_image_from_layer(self):\n+        \"\"\"Set the image from the selected QGIS layer.\"\"\"\n+        if self.sam is None:\n+            self.show_error(\"Please load the model first.\")\n+            return\n+\n+        layer_id = self.layer_combo.currentData()\n+        if not layer_id:\n+            self.show_error(\"Please select a raster layer.\")\n+            return\n+\n+        layer = QgsProject.instance().mapLayer(layer_id)\n+        if not layer:\n+            self.show_error(\"Layer not found.\")\n+            return\n+\n+        # Get the layer's file path\n+        source = layer.source()\n+        if not os.path.exists(source):\n+            self.show_error(f\"Layer source file not found: {source}\")\n+            return\n+\n+        try:\n+            self.progress_bar.setVisible(True)\n+            self.progress_bar.setRange(0, 0)\n+            self.image_status.setText(\"Setting image...\")\n+            QCoreApplication.processEvents()\n+\n+            self.sam.set_image(source)\n+            self.current_layer = layer\n+            self.current_image_path = source\n+\n+            self.image_status.setText(f\"Image: {layer.name()}\")\n+            self.image_status.setStyleSheet(\"color: green;\")\n+            self.log_message(f\"Image set from layer: {layer.name()}\")\n+\n+        except Exception as e:\n+            self.image_status.setText(\"Image: Failed to set\")\n+            self.image_status.setStyleSheet(\"color: red;\")\n+            self.show_error(f\"Failed to set image: {str(e)}\")\n+\n+        finally:\n+            self.progress_bar.setVisible(False)\n+\n+    def set_image_from_file(self):\n+        \"\"\"Set the image from the file path.\"\"\"\n+        if self.sam is None:\n+            self.show_error(\"Please load the model first.\")\n+            return\n+\n+        file_path = self.image_path_edit.text()\n+        if not file_path or not os.path.exists(file_path):\n+            self.show_error(\"Please select a valid image file.\")\n+            return\n+\n+        try:\n+            self.progress_bar.setVisible(True)\n+            self.progress_bar.setRange(0, 0)\n+            self.image_status.setText(\"Setting image...\")\n+            QCoreApplication.processEvents()\n+\n+            self.sam.set_image(file_path)\n+            self.current_image_path = file_path\n+            self.current_layer = None\n+\n+            self.image_status.setText(f\"Image: {os.path.basename(file_path)}\")\n+            self.image_status.setStyleSheet(\"color: green;\")\n+            self.log_message(f\"Image set from file: {file_path}\")\n+\n+            # Optionally add the layer to the map\n+            layer = QgsRasterLayer(file_path, os.path.basename(file_path))\n+            if layer.isValid():\n+                QgsProject.instance().addMapLayer(layer)\n+                self.current_layer = layer\n+                self.refresh_layers()\n+\n+        except Exception as e:\n+            self.image_status.setText(\"Image: Failed to set\")\n+            self.image_status.setStyleSheet(\"color: red;\")\n+            self.show_error(f\"Failed to set image: {str(e)}\")\n+\n+        finally:\n+            self.progress_bar.setVisible(False)\n+\n+    def segment_by_text(self):\n+        \"\"\"Segment the image using text prompt.\"\"\"\n+        if self.sam is None:\n+            self.show_error(\"Please load the model first.\")\n+            return\n+\n+        if self.current_image_path is None:\n+            self.show_error(\"Please set an image first.\")\n+            return\n+\n+        prompt = self.text_prompt_edit.text().strip()\n+        if not prompt:\n+            self.show_error(\"Please enter a text prompt.\")\n+            return\n+\n+        try:\n+            self.progress_bar.setVisible(True)\n+            self.progress_bar.setRange(0, 0)\n+            self.text_status_label.setText(\"Processing...\")\n+            self.text_status_label.setStyleSheet(\"color: orange;\")\n+            QCoreApplication.processEvents()\n+\n+            min_size = self.min_size_spin.value()\n+            max_size = (\n+                self.max_size_spin.value() if self.max_size_spin.value() > 0 else None\n+            )\n+\n+            self.sam.generate_masks(prompt, min_size=min_size, max_size=max_size)\n+\n+            num_masks = len(self.sam.masks) if self.sam.masks else 0\n+            self.results_text.setText(\n+                f\"Text Segmentation Results:\\n\"\n+                f\"Prompt: {prompt}\\n\"\n+                f\"Objects found: {num_masks}\\n\"\n+            )\n+\n+            # Update status label\n+            if num_masks > 0:\n+                self.text_status_label.setText(\n+                    f\"Found {num_masks} object(s). Go to Output tab to save.\"\n+                )\n+                self.text_status_label.setStyleSheet(\"color: green;\")\n+            else:\n+                self.text_status_label.setText(\n+                    \"No objects found. Try a different prompt.\"\n+                )\n+                self.text_status_label.setStyleSheet(\"color: orange;\")\n+\n+            self.log_message(f\"Text segmentation complete. Found {num_masks} objects.\")\n+\n+        except Exception as e:\n+            self.text_status_label.setText(\"Segmentation failed!\")\n+            self.text_status_label.setStyleSheet(\"color: red;\")\n+            self.show_error(f\"Segmentation failed: {str(e)}\")\n+\n+        finally:\n+            self.progress_bar.setVisible(False)\n+\n+    def start_point_tool(self, foreground=True):\n+        \"\"\"Start the point prompt tool.\"\"\"\n+        if self.current_layer is None:\n+            self.show_error(\"Please set an image first.\")\n+            self.add_fg_point_btn.setChecked(False)\n+            self.add_bg_point_btn.setChecked(False)\n+            return\n+\n+        # Uncheck the other button\n+        if foreground:\n+            self.add_bg_point_btn.setChecked(False)\n+        else:\n+            self.add_fg_point_btn.setChecked(False)\n+\n+        if self.point_tool is None:\n+            self.point_tool = PointPromptTool(self.canvas, self)\n+\n+        self.point_tool.set_foreground(foreground)\n+        self.previous_tool = self.canvas.mapTool()\n+        self.canvas.setMapTool(self.point_tool)\n+\n+    def add_point(self, point, foreground):\n+        \"\"\"Add a point prompt.\"\"\"\n+        # Convert map coordinates to pixel coordinates\n+        if self.current_layer is not None:\n+            extent = self.current_layer.extent()\n+            width = self.current_layer.width()\n+            height = self.current_layer.height()\n+\n+            # Calculate pixel coordinates\n+            px = (point.x() - extent.xMinimum()) / extent.width() * width\n+            py = (extent.yMaximum() - point.y()) / extent.height() * height\n+\n+            self.point_coords.append([px, py])\n+            self.point_labels.append(1 if foreground else 0)\n+\n+            # Update list widget\n+            label_text = \"FG\" if foreground else \"BG\"\n+            item = QListWidgetItem(f\"{label_text}: ({px:.1f}, {py:.1f})\")\n+            item.setForeground(Qt.green if foreground else Qt.red)\n+            self.points_list.addItem(item)\n+\n+    def clear_points(self):\n+        \"\"\"Clear all point prompts.\"\"\"\n+        self.point_coords = []\n+        self.point_labels = []\n+        self.points_list.clear()\n+\n+        # Clear rubber bands from point tool\n+        if self.point_tool is not None:\n+            self.point_tool.clear_markers()\n+\n+    def start_batch_point_tool(self):\n+        \"\"\"Start the batch point tool for adding multiple points.\"\"\"\n+        if self.current_layer is None:\n+            self.show_error(\"Please set an image first.\")\n+            self.batch_add_point_btn.setChecked(False)\n+            return\n+\n+        if self.batch_point_tool is None:\n+            self.batch_point_tool = PointPromptTool(self.canvas, self, batch_mode=True)\n+\n+        self.batch_point_tool.set_foreground(True)  # All batch points are foreground\n+        self.previous_tool = self.canvas.mapTool()\n+        self.canvas.setMapTool(self.batch_point_tool)\n+\n+    def add_batch_point(self, point):\n+        \"\"\"Add a batch point prompt.\"\"\"\n+        if self.current_layer is not None:\n+            extent = self.current_layer.extent()\n+            width = self.current_layer.width()\n+            height = self.current_layer.height()\n+\n+            # Calculate pixel coordinates\n+            px = (point.x() - extent.xMinimum()) / extent.width() * width\n+            py = (extent.yMaximum() - point.y()) / extent.height() * height\n+\n+            self.batch_point_coords.append([px, py])\n+            self.batch_point_coords_map.append([point.x(), point.y()])\n+\n+            # Update list widget\n+            item = QListWidgetItem(\n+                f\"Point {len(self.batch_point_coords)}: ({px:.1f}, {py:.1f})\"\n+            )\n+            item.setForeground(Qt.green)\nComment: Using Qt.green directly is deprecated in PyQt5/PyQt6. Should use QColor or the GlobalColor enum properly. For cross-compatibility with different Qt versions, it's better to use QColor(Qt.green), or better yet, QColor(\"green\").\n```suggestion\n            item.setForeground(QColor(\"green\"))\n```",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "qgis_plugin/geoai_plugin/dialogs/samgeo.py",
    "pr_number": 387,
    "repo": "geoai",
    "owner": "opengeos",
    "comment_id": 2608936770,
    "comment_created_at": "2025-12-11T02:57:13Z"
  },
  {
    "code": "@@ -0,0 +1,651 @@\n+\"\"\"\n+Moondream Dock Widget for GeoAI Plugin\n+\n+This dock widget provides an interactive interface for using the Moondream\n+vision-language model for geospatial image analysis in QGIS.\n+\"\"\"\n+\n+import os\n+import tempfile\n+from typing import Optional\n+\n+from qgis.PyQt.QtCore import Qt, QThread, pyqtSignal\n+from qgis.PyQt.QtWidgets import (\n+    QDockWidget,\n+    QWidget,\n+    QVBoxLayout,\n+    QHBoxLayout,\n+    QLabel,\n+    QLineEdit,\n+    QComboBox,\n+    QPushButton,\n+    QTextEdit,\n+    QFileDialog,\n+    QGroupBox,\n+    QFormLayout,\n+    QProgressBar,\n+    QMessageBox,\n+    QCheckBox,\n+    QColorDialog,\n+    QScrollArea,\n+    QSizePolicy,\n+)\n+from qgis.PyQt.QtGui import QColor\n+\n+from qgis.core import (\n+    QgsProject,\n+    QgsVectorLayer,\n+    QgsRasterLayer,\n+)\n+from qgis.gui import QgsMapLayerComboBox\n+from qgis.core import QgsMapLayerProxyModel\n+\n+\n+class MoondreamWorker(QThread):\n+    \"\"\"Worker thread for running Moondream operations.\"\"\"\n+\n+    finished = pyqtSignal(dict)\n+    error = pyqtSignal(str)\n+    progress = pyqtSignal(str)\n+\n+    def __init__(\n+        self,\n+        moondream,\n+        mode: str,\n+        source_path: str,\n+        prompt: str = \"\",\n+        caption_length: str = \"normal\",\n+    ):\n+        super().__init__()\n+        self.moondream = moondream\n+        self.mode = mode\n+        self.source_path = source_path\n+        self.prompt = prompt\n+        self.caption_length = caption_length\n+\n+    def run(self):\n+        \"\"\"Execute the Moondream operation.\"\"\"\n+        try:\n+            if self.mode == \"Caption\":\n+                self.progress.emit(f\"Generating {self.caption_length} caption...\")\n+                result = self.moondream.caption(\n+                    self.source_path,\n+                    length=self.caption_length,\n+                    stream=False,\n+                )\n+                self.finished.emit({\"type\": \"caption\", \"result\": result})\n+\n+            elif self.mode == \"Query\":\n+                self.progress.emit(f\"Processing query: {self.prompt}\")\n+                result = self.moondream.query(\n+                    self.prompt,\n+                    source=self.source_path,\n+                    stream=False,\n+                )\n+                self.finished.emit(\n+                    {\"type\": \"query\", \"result\": result, \"question\": self.prompt}\n+                )\n+\n+            elif self.mode == \"Detect\":\n+                self.progress.emit(f\"Detecting: {self.prompt}\")\n+                result = self.moondream.detect(\n+                    self.source_path,\n+                    self.prompt,\n+                )\n+                self.finished.emit(\n+                    {\"type\": \"detect\", \"result\": result, \"object_type\": self.prompt}\n+                )\n+\n+            elif self.mode == \"Point\":\n+                self.progress.emit(f\"Locating: {self.prompt}\")\n+                result = self.moondream.point(\n+                    self.source_path,\n+                    self.prompt,\n+                )\n+                self.finished.emit(\n+                    {\"type\": \"point\", \"result\": result, \"description\": self.prompt}\n+                )\n+\n+        except Exception as e:\n+            self.error.emit(str(e))\n+\n+\n+class MoondreamDockWidget(QDockWidget):\n+    \"\"\"Dockable widget for Moondream vision-language model interaction.\"\"\"\n+\n+    def __init__(self, iface, parent=None):\n+        \"\"\"Initialize the Moondream dock widget.\n+\n+        Args:\n+            iface: QGIS interface instance.\n+            parent: Parent widget.\n+        \"\"\"\n+        super().__init__(\"GeoAI - Moondream\", parent)\n+        self.iface = iface\n+        self.moondream = None\n+        self.current_image_path = None\n+        self.last_result = None\n+        self.worker = None\n+\n+        self.setAllowedAreas(Qt.LeftDockWidgetArea | Qt.RightDockWidgetArea)\n+        self.setup_ui()\n+        self.connect_signals()\n+\n+    def setup_ui(self):\n+        \"\"\"Set up the user interface.\"\"\"\n+        # Main widget with scroll area\n+        scroll = QScrollArea()\n+        scroll.setWidgetResizable(True)\n+        scroll.setHorizontalScrollBarPolicy(Qt.ScrollBarAlwaysOff)\n+\n+        main_widget = QWidget()\n+        layout = QVBoxLayout()\n+        layout.setContentsMargins(5, 5, 5, 5)\n+        layout.setSpacing(5)\n+\n+        # Consistent height for all input elements (24px)\n+        input_height = 24\n+        combo_style = f\"QComboBox {{ min-height: {input_height}px; max-height: {input_height}px; }}\"\n+        line_style = f\"QLineEdit {{ min-height: {input_height}px; max-height: {input_height}px; }}\"\n+        btn_style = f\"QPushButton {{ min-height: {input_height}px; max-height: {input_height}px; }}\"\n+        spin_style = f\"QSpinBox, QDoubleSpinBox {{ min-height: {input_height}px; max-height: {input_height}px; }}\"\n+\n+        # Model Settings Group\n+        model_group = QGroupBox(\"Model Settings\")\n+        model_layout = QFormLayout()\n+        model_layout.setSpacing(5)\n+\n+        self.model_combo = QComboBox()\n+        self.model_combo.setStyleSheet(combo_style)\n+        self.model_combo.addItems(\n+            [\n+                \"vikhyatk/moondream2\",\n+                \"moondream/moondream3-preview\",\n+            ]\n+        )\n+        model_layout.addRow(\"Model:\", self.model_combo)\n+\n+        self.device_combo = QComboBox()\n+        self.device_combo.setStyleSheet(combo_style)\n+        self.device_combo.addItems([\"Auto\", \"cuda\", \"cpu\", \"mps\"])",
    "comment": "The device selection has an \"Auto\" option that converts to None, but there's no validation or documentation about the implications of using \"Auto\" versus explicit device selection. Consider adding a tooltip or help text explaining what \"Auto\" means (e.g., \"Automatically selects CUDA if available, otherwise CPU\").\n```suggestion\n        self.device_combo.addItems([\"Auto\", \"cuda\", \"cpu\", \"mps\"])\n        self.device_combo.setToolTip(\n            'Device to run the model on. \"Auto\" will select CUDA if available, otherwise CPU.'\n        )\n```",
    "line_number": 170,
    "enriched": "File: qgis_plugin/geoai_plugin/dialogs/moondream.py\nCode: @@ -0,0 +1,651 @@\n+\"\"\"\n+Moondream Dock Widget for GeoAI Plugin\n+\n+This dock widget provides an interactive interface for using the Moondream\n+vision-language model for geospatial image analysis in QGIS.\n+\"\"\"\n+\n+import os\n+import tempfile\n+from typing import Optional\n+\n+from qgis.PyQt.QtCore import Qt, QThread, pyqtSignal\n+from qgis.PyQt.QtWidgets import (\n+    QDockWidget,\n+    QWidget,\n+    QVBoxLayout,\n+    QHBoxLayout,\n+    QLabel,\n+    QLineEdit,\n+    QComboBox,\n+    QPushButton,\n+    QTextEdit,\n+    QFileDialog,\n+    QGroupBox,\n+    QFormLayout,\n+    QProgressBar,\n+    QMessageBox,\n+    QCheckBox,\n+    QColorDialog,\n+    QScrollArea,\n+    QSizePolicy,\n+)\n+from qgis.PyQt.QtGui import QColor\n+\n+from qgis.core import (\n+    QgsProject,\n+    QgsVectorLayer,\n+    QgsRasterLayer,\n+)\n+from qgis.gui import QgsMapLayerComboBox\n+from qgis.core import QgsMapLayerProxyModel\n+\n+\n+class MoondreamWorker(QThread):\n+    \"\"\"Worker thread for running Moondream operations.\"\"\"\n+\n+    finished = pyqtSignal(dict)\n+    error = pyqtSignal(str)\n+    progress = pyqtSignal(str)\n+\n+    def __init__(\n+        self,\n+        moondream,\n+        mode: str,\n+        source_path: str,\n+        prompt: str = \"\",\n+        caption_length: str = \"normal\",\n+    ):\n+        super().__init__()\n+        self.moondream = moondream\n+        self.mode = mode\n+        self.source_path = source_path\n+        self.prompt = prompt\n+        self.caption_length = caption_length\n+\n+    def run(self):\n+        \"\"\"Execute the Moondream operation.\"\"\"\n+        try:\n+            if self.mode == \"Caption\":\n+                self.progress.emit(f\"Generating {self.caption_length} caption...\")\n+                result = self.moondream.caption(\n+                    self.source_path,\n+                    length=self.caption_length,\n+                    stream=False,\n+                )\n+                self.finished.emit({\"type\": \"caption\", \"result\": result})\n+\n+            elif self.mode == \"Query\":\n+                self.progress.emit(f\"Processing query: {self.prompt}\")\n+                result = self.moondream.query(\n+                    self.prompt,\n+                    source=self.source_path,\n+                    stream=False,\n+                )\n+                self.finished.emit(\n+                    {\"type\": \"query\", \"result\": result, \"question\": self.prompt}\n+                )\n+\n+            elif self.mode == \"Detect\":\n+                self.progress.emit(f\"Detecting: {self.prompt}\")\n+                result = self.moondream.detect(\n+                    self.source_path,\n+                    self.prompt,\n+                )\n+                self.finished.emit(\n+                    {\"type\": \"detect\", \"result\": result, \"object_type\": self.prompt}\n+                )\n+\n+            elif self.mode == \"Point\":\n+                self.progress.emit(f\"Locating: {self.prompt}\")\n+                result = self.moondream.point(\n+                    self.source_path,\n+                    self.prompt,\n+                )\n+                self.finished.emit(\n+                    {\"type\": \"point\", \"result\": result, \"description\": self.prompt}\n+                )\n+\n+        except Exception as e:\n+            self.error.emit(str(e))\n+\n+\n+class MoondreamDockWidget(QDockWidget):\n+    \"\"\"Dockable widget for Moondream vision-language model interaction.\"\"\"\n+\n+    def __init__(self, iface, parent=None):\n+        \"\"\"Initialize the Moondream dock widget.\n+\n+        Args:\n+            iface: QGIS interface instance.\n+            parent: Parent widget.\n+        \"\"\"\n+        super().__init__(\"GeoAI - Moondream\", parent)\n+        self.iface = iface\n+        self.moondream = None\n+        self.current_image_path = None\n+        self.last_result = None\n+        self.worker = None\n+\n+        self.setAllowedAreas(Qt.LeftDockWidgetArea | Qt.RightDockWidgetArea)\n+        self.setup_ui()\n+        self.connect_signals()\n+\n+    def setup_ui(self):\n+        \"\"\"Set up the user interface.\"\"\"\n+        # Main widget with scroll area\n+        scroll = QScrollArea()\n+        scroll.setWidgetResizable(True)\n+        scroll.setHorizontalScrollBarPolicy(Qt.ScrollBarAlwaysOff)\n+\n+        main_widget = QWidget()\n+        layout = QVBoxLayout()\n+        layout.setContentsMargins(5, 5, 5, 5)\n+        layout.setSpacing(5)\n+\n+        # Consistent height for all input elements (24px)\n+        input_height = 24\n+        combo_style = f\"QComboBox {{ min-height: {input_height}px; max-height: {input_height}px; }}\"\n+        line_style = f\"QLineEdit {{ min-height: {input_height}px; max-height: {input_height}px; }}\"\n+        btn_style = f\"QPushButton {{ min-height: {input_height}px; max-height: {input_height}px; }}\"\n+        spin_style = f\"QSpinBox, QDoubleSpinBox {{ min-height: {input_height}px; max-height: {input_height}px; }}\"\n+\n+        # Model Settings Group\n+        model_group = QGroupBox(\"Model Settings\")\n+        model_layout = QFormLayout()\n+        model_layout.setSpacing(5)\n+\n+        self.model_combo = QComboBox()\n+        self.model_combo.setStyleSheet(combo_style)\n+        self.model_combo.addItems(\n+            [\n+                \"vikhyatk/moondream2\",\n+                \"moondream/moondream3-preview\",\n+            ]\n+        )\n+        model_layout.addRow(\"Model:\", self.model_combo)\n+\n+        self.device_combo = QComboBox()\n+        self.device_combo.setStyleSheet(combo_style)\n+        self.device_combo.addItems([\"Auto\", \"cuda\", \"cpu\", \"mps\"])\nComment: The device selection has an \"Auto\" option that converts to None, but there's no validation or documentation about the implications of using \"Auto\" versus explicit device selection. Consider adding a tooltip or help text explaining what \"Auto\" means (e.g., \"Automatically selects CUDA if available, otherwise CPU\").\n```suggestion\n        self.device_combo.addItems([\"Auto\", \"cuda\", \"cpu\", \"mps\"])\n        self.device_combo.setToolTip(\n            'Device to run the model on. \"Auto\" will select CUDA if available, otherwise CPU.'\n        )\n```",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "qgis_plugin/geoai_plugin/dialogs/moondream.py",
    "pr_number": 386,
    "repo": "geoai",
    "owner": "opengeos",
    "comment_id": 2608593679,
    "comment_created_at": "2025-12-10T23:47:08Z"
  },
  {
    "code": "@@ -355,9 +355,6 @@ def load_image(\n                 \"transform\": None,\n                 \"bounds\": None,\n             }",
    "comment": "The removed lines include a necessary return statement for the regular image case. After setting `self._source_path` and `self._metadata` for regular images (lines 350-357), the function should return `image, self._metadata`. Without this return statement, the function will return `None` instead of the expected tuple, causing issues when users load non-GeoTIFF images.\n```suggestion\n            }\n            return image, self._metadata\n```",
    "line_number": 357,
    "enriched": "File: geoai/moondream.py\nCode: @@ -355,9 +355,6 @@ def load_image(\n                 \"transform\": None,\n                 \"bounds\": None,\n             }\nComment: The removed lines include a necessary return statement for the regular image case. After setting `self._source_path` and `self._metadata` for regular images (lines 350-357), the function should return `image, self._metadata`. Without this return statement, the function will return `None` instead of the expected tuple, causing issues when users load non-GeoTIFF images.\n```suggestion\n            }\n            return image, self._metadata\n```",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "geoai/moondream.py",
    "pr_number": 384,
    "repo": "geoai",
    "owner": "opengeos",
    "comment_id": 2607844416,
    "comment_created_at": "2025-12-10T18:51:02Z"
  },
  {
    "code": "@@ -0,0 +1,3 @@\n+# moondream module\n+\n+::: geoai.moondream",
    "comment": "The documentation file is named `moondeream.md` (with \"deream\") but should be `moondream.md` to match the reference in mkdocs.yml line 181. This typo will cause a broken documentation link.",
    "line_number": 3,
    "enriched": "File: docs/moondeream.md\nCode: @@ -0,0 +1,3 @@\n+# moondream module\n+\n+::: geoai.moondream\nComment: The documentation file is named `moondeream.md` (with \"deream\") but should be `moondream.md` to match the reference in mkdocs.yml line 181. This typo will cause a broken documentation link.",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "docs/moondeream.md",
    "pr_number": 382,
    "repo": "geoai",
    "owner": "opengeos",
    "comment_id": 2607481318,
    "comment_created_at": "2025-12-10T17:00:17Z"
  },
  {
    "code": "@@ -202,6 +203,40 @@ def create_bedrock_model(\n     )\n \n \n+def create_gemini_model(\n+    model_id: str = \"gemini-2.5-flash\",\n+    api_key: str = None,\n+    client_args: dict = None,\n+    **kwargs: Any,\n+) -> GeminiModel:\n+    \"\"\"Create a Gemini model.\n+\n+    Args:\n+        model_id: Gemini model ID.\n+        api_key: Gemini API key.\n+        client_args: Client arguments for the Gemini model.\n+        **kwargs: Additional keyword arguments for the Gemini model.\n+\n+    Returns:\n+        GeminiModel: A Gemini model.\n+    \"\"\"\n+\n+    if api_key is None and \"api_key\" not in client_args:",
    "comment": "This condition will raise an `AttributeError` when `client_args` is `None`. The check `\"api_key\" not in client_args` is evaluated before `api_key is None`, so when `client_args` is `None` (the default value), the `in` operator will fail.\n\nThis should be:\n```python\nif api_key is None and (client_args is None or \"api_key\" not in client_args):\n```\n\nOr check for `None` first:\n```python\nif api_key is None:\n    if client_args is not None and \"api_key\" in client_args:\n        # api_key is in client_args, proceed\n        pass\n    else:\n        # need to get from environment\n        try:\n            api_key = os.getenv(\"GOOGLE_API_KEY\", None)\n            if api_key is None:\n                raise ValueError(\"GOOGLE_API_KEY is not set\")\n        except Exception:\n            raise ValueError(\"GOOGLE_API_KEY is not set\")\n```\n```suggestion\n    if api_key is None and (client_args is None or \"api_key\" not in client_args):\n```",
    "line_number": 224,
    "enriched": "File: geoai/agents/geo_agents.py\nCode: @@ -202,6 +203,40 @@ def create_bedrock_model(\n     )\n \n \n+def create_gemini_model(\n+    model_id: str = \"gemini-2.5-flash\",\n+    api_key: str = None,\n+    client_args: dict = None,\n+    **kwargs: Any,\n+) -> GeminiModel:\n+    \"\"\"Create a Gemini model.\n+\n+    Args:\n+        model_id: Gemini model ID.\n+        api_key: Gemini API key.\n+        client_args: Client arguments for the Gemini model.\n+        **kwargs: Additional keyword arguments for the Gemini model.\n+\n+    Returns:\n+        GeminiModel: A Gemini model.\n+    \"\"\"\n+\n+    if api_key is None and \"api_key\" not in client_args:\nComment: This condition will raise an `AttributeError` when `client_args` is `None`. The check `\"api_key\" not in client_args` is evaluated before `api_key is None`, so when `client_args` is `None` (the default value), the `in` operator will fail.\n\nThis should be:\n```python\nif api_key is None and (client_args is None or \"api_key\" not in client_args):\n```\n\nOr check for `None` first:\n```python\nif api_key is None:\n    if client_args is not None and \"api_key\" in client_args:\n        # api_key is in client_args, proceed\n        pass\n    else:\n        # need to get from environment\n        try:\n            api_key = os.getenv(\"GOOGLE_API_KEY\", None)\n            if api_key is None:\n                raise ValueError(\"GOOGLE_API_KEY is not set\")\n        except Exception:\n            raise ValueError(\"GOOGLE_API_KEY is not set\")\n```\n```suggestion\n    if api_key is None and (client_args is None or \"api_key\" not in client_args):\n```",
    "subcategory": "validation",
    "category": "refactoring",
    "file_path": "geoai/agents/geo_agents.py",
    "pr_number": 378,
    "repo": "geoai",
    "owner": "opengeos",
    "comment_id": 2595051399,
    "comment_created_at": "2025-12-06T15:48:10Z"
  },
  {
    "code": "@@ -0,0 +1,182 @@\n+\"\"\"\n+Super-resolution utilities using OpenSR latent diffusion models.\n+\n+This module provides functions to perform super-resolution on multispectral\n+GeoTIFF images using the latent diffusion models from the ESA OpenSR project:\n+\n+    GitHub: https://github.com/ESAOpenSR/opensr-model.git\n+\"\"\"\n+\n+import os\n+from typing import Tuple\n+\n+import torch\n+import numpy as np\n+import rasterio\n+import requests\n+from rasterio.transform import Affine\n+from io import StringIO\n+from omegaconf import OmegaConf\n+\n+try:\n+    import opensr_model\n+\n+    OPENSR_MODEL_AVAILABLE = True\n+except ImportError:\n+    OPENSR_MODEL_AVAILABLE = False\n+\n+\n+def super_resolution(\n+    input_lr_path: str,\n+    output_sr_path: str,\n+    output_uncertainty_path: str,\n+    rgb_nir_bands: list[int] = [3, 2, 1, 4],  # Default example: R=3,G=2,B=1,NIR=4\n+    sampling_steps: int = 100,\n+    n_variations: int = 25,\n+    scale: int = 4,  # OpenSR scaling factor, e.g., 10m -> 2.5m\n+) -> Tuple[np.ndarray, np.ndarray]:\n+    \"\"\"\n+    Perform super-resolution on RGB+NIR bands of a multispectral GeoTIFF using OpenSR latent diffusion.\n+\n+    Args:\n+        input_lr_path (str): Path to the input low-resolution GeoTIFF.\n+        output_sr_path (str): Path to save the super-resolution GeoTIFF.\n+        output_uncertainty_path (str): Path to save the uncertainty map GeoTIFF.\n+        rgb_nir_bands (list[int]): List of 4 band indices corresponding to [R, G, B, NIR].\n+        sampling_steps (int): Number of diffusion sampling steps. Default is 100.\n+        n_variations (int): Number of samples to compute uncertainty. Default is 25.\n+        scale (int, optional): Super-resolution scale factor. Default is 4.\n+            This adjusts the affine transform to ensure georeference matches\n+            the original image.\n+\n+    Returns:\n+        Tuple[np.ndarray, np.ndarray]: Tuple containing:\n+            - sr_image: Super-resolution image as a NumPy array (4, H, W)\n+            - uncertainty: Uncertainty map as a NumPy array (H, W)\n+    \"\"\"\n+    if len(rgb_nir_bands) != 4:\n+        raise ValueError(\"rgb_nir_bands must be a list of 4 integers: [R, G, B, NIR]\")\n+\n+    if not OPENSR_MODEL_AVAILABLE:\n+        raise ImportError(\n+            \"The 'opensr-model' package is required for super-resolution. \"\n+            \"Please install it using: pip install opensr-model\\n\"\n+            \"Or install GeoAI with the sr optional dependency: pip install geoai-py[sr]\"\n+        )\n+\n+    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+\n+    # Download configuration YAML from GitHub\n+    config_url = \"https://raw.githubusercontent.com/ESAOpenSR/opensr-model/refs/heads/main/opensr_model/configs/config_10m.yaml\"\n+    print(\"Downloading model configuration from:\", config_url)\n+    response = requests.get(config_url)\n+    config = OmegaConf.load(StringIO(response.text))\n+\n+    # Initialize latent diffusion model and load pretrained weights\n+    model = opensr_model.SRLatentDiffusion(config, device=device)\n+    model.load_pretrained(config.ckpt_version)\n+\n+    # Load only the specified RGB+NIR bands\n+    lr_tensor, profile = load_image_tensor(\n+        image_path=input_lr_path, device=device, bands=rgb_nir_bands\n+    )\n+\n+    # Generate super-resolution tensor\n+    sr_tensor = model.forward(lr_tensor, sampling_steps=sampling_steps)\n+    sr_image = sr_tensor.squeeze(0).cpu().numpy().astype(np.float32)\n+    save_geotiff(sr_image, profile, output_sr_path, scale)\n+    print(\"Saved super-resolution image to:\", output_sr_path)\n+\n+    # Compute uncertainty map\n+    unc_tensor = model.uncertainty_map(lr_tensor, n_variations=n_variations)\n+    uncertainty = unc_tensor.squeeze(0).cpu().numpy().astype(np.float32)\n+    save_geotiff(uncertainty, profile, output_uncertainty_path, scale)\n+    print(\"Saved uncertainty map to:\", output_uncertainty_path)\n+\n+    return sr_image, uncertainty\n+\n+\n+def save_geotiff(\n+    data: np.ndarray, reference_profile: dict, output_path: str, scale: int = 4\n+):\n+    \"\"\"\n+    Save a 2D or 3D NumPy array as a GeoTIFF with super-resolution scaling\n+    and corrected georeference.\n+\n+    Args:\n+        data (np.ndarray): Image array to save. Can be:\n+            - 2D array (H, W) for a single-band image\n+            - 3D array (C, H, W) for multi-band images (e.g., RGB+NIR)\n+        reference_profile (dict): Rasterio metadata from a reference GeoTIFF.\n+            Used to preserve CRS, transform, and other metadata.\n+        output_path (str): Path to save the output GeoTIFF.\n+        scale (int, optional): Super-resolution scale factor. Default is 4.\n+            This adjusts the affine transform to ensure georeference matches\n+            the original image.\n+\n+    Returns:\n+        None\n+            Writes the image to disk at the specified output path.",
    "comment": "The documentation states \"Returns: None\" but then contradicts itself by saying \"Writes the image to disk at the specified output path.\" The function indeed returns None (implicitly), so either remove the \"Returns:\" section entirely or keep just \"Returns: None\".\n```suggestion\n\n    Note:\n        Writes the image to disk at the specified output path.\n```",
    "line_number": 119,
    "enriched": "File: geoai/tools/sr.py\nCode: @@ -0,0 +1,182 @@\n+\"\"\"\n+Super-resolution utilities using OpenSR latent diffusion models.\n+\n+This module provides functions to perform super-resolution on multispectral\n+GeoTIFF images using the latent diffusion models from the ESA OpenSR project:\n+\n+    GitHub: https://github.com/ESAOpenSR/opensr-model.git\n+\"\"\"\n+\n+import os\n+from typing import Tuple\n+\n+import torch\n+import numpy as np\n+import rasterio\n+import requests\n+from rasterio.transform import Affine\n+from io import StringIO\n+from omegaconf import OmegaConf\n+\n+try:\n+    import opensr_model\n+\n+    OPENSR_MODEL_AVAILABLE = True\n+except ImportError:\n+    OPENSR_MODEL_AVAILABLE = False\n+\n+\n+def super_resolution(\n+    input_lr_path: str,\n+    output_sr_path: str,\n+    output_uncertainty_path: str,\n+    rgb_nir_bands: list[int] = [3, 2, 1, 4],  # Default example: R=3,G=2,B=1,NIR=4\n+    sampling_steps: int = 100,\n+    n_variations: int = 25,\n+    scale: int = 4,  # OpenSR scaling factor, e.g., 10m -> 2.5m\n+) -> Tuple[np.ndarray, np.ndarray]:\n+    \"\"\"\n+    Perform super-resolution on RGB+NIR bands of a multispectral GeoTIFF using OpenSR latent diffusion.\n+\n+    Args:\n+        input_lr_path (str): Path to the input low-resolution GeoTIFF.\n+        output_sr_path (str): Path to save the super-resolution GeoTIFF.\n+        output_uncertainty_path (str): Path to save the uncertainty map GeoTIFF.\n+        rgb_nir_bands (list[int]): List of 4 band indices corresponding to [R, G, B, NIR].\n+        sampling_steps (int): Number of diffusion sampling steps. Default is 100.\n+        n_variations (int): Number of samples to compute uncertainty. Default is 25.\n+        scale (int, optional): Super-resolution scale factor. Default is 4.\n+            This adjusts the affine transform to ensure georeference matches\n+            the original image.\n+\n+    Returns:\n+        Tuple[np.ndarray, np.ndarray]: Tuple containing:\n+            - sr_image: Super-resolution image as a NumPy array (4, H, W)\n+            - uncertainty: Uncertainty map as a NumPy array (H, W)\n+    \"\"\"\n+    if len(rgb_nir_bands) != 4:\n+        raise ValueError(\"rgb_nir_bands must be a list of 4 integers: [R, G, B, NIR]\")\n+\n+    if not OPENSR_MODEL_AVAILABLE:\n+        raise ImportError(\n+            \"The 'opensr-model' package is required for super-resolution. \"\n+            \"Please install it using: pip install opensr-model\\n\"\n+            \"Or install GeoAI with the sr optional dependency: pip install geoai-py[sr]\"\n+        )\n+\n+    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+\n+    # Download configuration YAML from GitHub\n+    config_url = \"https://raw.githubusercontent.com/ESAOpenSR/opensr-model/refs/heads/main/opensr_model/configs/config_10m.yaml\"\n+    print(\"Downloading model configuration from:\", config_url)\n+    response = requests.get(config_url)\n+    config = OmegaConf.load(StringIO(response.text))\n+\n+    # Initialize latent diffusion model and load pretrained weights\n+    model = opensr_model.SRLatentDiffusion(config, device=device)\n+    model.load_pretrained(config.ckpt_version)\n+\n+    # Load only the specified RGB+NIR bands\n+    lr_tensor, profile = load_image_tensor(\n+        image_path=input_lr_path, device=device, bands=rgb_nir_bands\n+    )\n+\n+    # Generate super-resolution tensor\n+    sr_tensor = model.forward(lr_tensor, sampling_steps=sampling_steps)\n+    sr_image = sr_tensor.squeeze(0).cpu().numpy().astype(np.float32)\n+    save_geotiff(sr_image, profile, output_sr_path, scale)\n+    print(\"Saved super-resolution image to:\", output_sr_path)\n+\n+    # Compute uncertainty map\n+    unc_tensor = model.uncertainty_map(lr_tensor, n_variations=n_variations)\n+    uncertainty = unc_tensor.squeeze(0).cpu().numpy().astype(np.float32)\n+    save_geotiff(uncertainty, profile, output_uncertainty_path, scale)\n+    print(\"Saved uncertainty map to:\", output_uncertainty_path)\n+\n+    return sr_image, uncertainty\n+\n+\n+def save_geotiff(\n+    data: np.ndarray, reference_profile: dict, output_path: str, scale: int = 4\n+):\n+    \"\"\"\n+    Save a 2D or 3D NumPy array as a GeoTIFF with super-resolution scaling\n+    and corrected georeference.\n+\n+    Args:\n+        data (np.ndarray): Image array to save. Can be:\n+            - 2D array (H, W) for a single-band image\n+            - 3D array (C, H, W) for multi-band images (e.g., RGB+NIR)\n+        reference_profile (dict): Rasterio metadata from a reference GeoTIFF.\n+            Used to preserve CRS, transform, and other metadata.\n+        output_path (str): Path to save the output GeoTIFF.\n+        scale (int, optional): Super-resolution scale factor. Default is 4.\n+            This adjusts the affine transform to ensure georeference matches\n+            the original image.\n+\n+    Returns:\n+        None\n+            Writes the image to disk at the specified output path.\nComment: The documentation states \"Returns: None\" but then contradicts itself by saying \"Writes the image to disk at the specified output path.\" The function indeed returns None (implicitly), so either remove the \"Returns:\" section entirely or keep just \"Returns: None\".\n```suggestion\n\n    Note:\n        Writes the image to disk at the specified output path.\n```",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "geoai/tools/sr.py",
    "pr_number": 368,
    "repo": "geoai",
    "owner": "opengeos",
    "comment_id": 2548179289,
    "comment_created_at": "2025-11-21T00:54:26Z"
  },
  {
    "code": "@@ -110,6 +111,7 @@ nav:\n           - examples/image_tiling.ipynb\n           - examples/create_training_data.ipynb\n           - examples/export_training_data_formats.ipynb\n+          - examples/data_augmentation.ipynb",
    "comment": "The notebook 'examples/data_augmentation.ipynb' is listed twice in the navigation structure - once at line 114 and again at line 162. This creates a duplicate entry in the documentation menu. Remove one of these duplicate entries.",
    "line_number": 114,
    "enriched": "File: mkdocs.yml\nCode: @@ -110,6 +111,7 @@ nav:\n           - examples/image_tiling.ipynb\n           - examples/create_training_data.ipynb\n           - examples/export_training_data_formats.ipynb\n+          - examples/data_augmentation.ipynb\nComment: The notebook 'examples/data_augmentation.ipynb' is listed twice in the navigation structure - once at line 114 and again at line 162. This creates a duplicate entry in the documentation menu. Remove one of these duplicate entries.",
    "subcategory": "visual representation",
    "category": "refactoring",
    "file_path": "mkdocs.yml",
    "pr_number": 362,
    "repo": "geoai",
    "owner": "opengeos",
    "comment_id": 2524868248,
    "comment_created_at": "2025-11-13T20:39:21Z"
  },
  {
    "code": "@@ -14,7 +14,7 @@\n     \"This notebook is designed for workshop presented at the [CANAVS 2025 Conference](https://www.sciencesocieties.org/canvas?q=canvas/) on November 10, 2025.\\n\",",
    "comment": "Typo in \"CANAVS\" - should be \"CANVAS\" to match the conference name.\n```suggestion\n    \"This notebook is designed for workshop presented at the [CANVAS 2025 Conference](https://www.sciencesocieties.org/canvas?q=canvas/) on November 10, 2025.\\n\",\n```",
    "line_number": 14,
    "enriched": "File: docs/workshops/CANVAS_2025.ipynb\nCode: @@ -14,7 +14,7 @@\n     \"This notebook is designed for workshop presented at the [CANAVS 2025 Conference](https://www.sciencesocieties.org/canvas?q=canvas/) on November 10, 2025.\\n\",\nComment: Typo in \"CANAVS\" - should be \"CANVAS\" to match the conference name.\n```suggestion\n    \"This notebook is designed for workshop presented at the [CANVAS 2025 Conference](https://www.sciencesocieties.org/canvas?q=canvas/) on November 10, 2025.\\n\",\n```",
    "subcategory": "support issues",
    "category": "functional",
    "file_path": "docs/workshops/CANVAS_2025.ipynb",
    "pr_number": 360,
    "repo": "geoai",
    "owner": "opengeos",
    "comment_id": 2511319186,
    "comment_created_at": "2025-11-10T17:03:08Z"
  },
  {
    "code": "@@ -1171,14 +1174,16 @@ def view_vector_interactive(\n                 tiles_args = {}\n             if kwargs[\"tiles\"].lower().startswith(\"http\"):\n                 basemap_layer_name = \"Remote Raster\"\n-                kwargs[\"tiles\"] = cog_tile(kwargs[\"tiles\"], **tiles_args)\n-                kwargs[\"attr\"] = \"TiTiler\"\n+                m.add_geotiff(kwargs[\"tiles\"], name=basemap_layer_name, **tiles_args)\n             else:\n                 basemap_layer_name = \"Local Raster\"\n                 client = TileClient(kwargs[\"tiles\"])\n-                raster_layer = get_folium_tile_layer(client, **tiles_args)\n-                kwargs[\"tiles\"] = raster_layer.tiles\n-                kwargs[\"attr\"] = \"localtileserver\"\n+                m.add_tile_layer(\n+                    raster_layer.tiles,\n+                    name=basemap_layer_name,\n+                    attribution=\"localtileserver\",\n+                    **tiles_args,\n+                )",
    "comment": "The variable `raster_layer` is used at line 1182 but is never assigned a value after creating the `TileClient`. The code should call `get_folium_tile_layer(client, **tiles_args)` to create the raster_layer, or use the client object directly to get the tiles URL.",
    "line_number": 1186,
    "enriched": "File: geoai/utils.py\nCode: @@ -1171,14 +1174,16 @@ def view_vector_interactive(\n                 tiles_args = {}\n             if kwargs[\"tiles\"].lower().startswith(\"http\"):\n                 basemap_layer_name = \"Remote Raster\"\n-                kwargs[\"tiles\"] = cog_tile(kwargs[\"tiles\"], **tiles_args)\n-                kwargs[\"attr\"] = \"TiTiler\"\n+                m.add_geotiff(kwargs[\"tiles\"], name=basemap_layer_name, **tiles_args)\n             else:\n                 basemap_layer_name = \"Local Raster\"\n                 client = TileClient(kwargs[\"tiles\"])\n-                raster_layer = get_folium_tile_layer(client, **tiles_args)\n-                kwargs[\"tiles\"] = raster_layer.tiles\n-                kwargs[\"attr\"] = \"localtileserver\"\n+                m.add_tile_layer(\n+                    raster_layer.tiles,\n+                    name=basemap_layer_name,\n+                    attribution=\"localtileserver\",\n+                    **tiles_args,\n+                )\nComment: The variable `raster_layer` is used at line 1182 but is never assigned a value after creating the `TileClient`. The code should call `get_folium_tile_layer(client, **tiles_args)` to create the raster_layer, or use the client object directly to get the tiles URL.",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "geoai/utils.py",
    "pr_number": 359,
    "repo": "geoai",
    "owner": "opengeos",
    "comment_id": 2496488571,
    "comment_created_at": "2025-11-05T23:16:12Z"
  },
  {
    "code": "@@ -123,26 +123,39 @@ def view_raster(\n     if isinstance(source, dict):\n         source = dict_to_image(source)\n \n-    if (\n-        isinstance(source, str)\n-        and source.lower().endswith(\".tif\")\n-        and source.startswith(\"http\")\n-    ):\n-        if indexes is not None:\n-            kwargs[\"bidx\"] = indexes\n-        if colormap is not None:\n-            kwargs[\"colormap_name\"] = colormap\n-        if attribution is None:\n-            attribution = \"TiTiler\"\n-\n-        m.add_cog_layer(\n-            source,\n-            name=layer_name,\n-            opacity=opacity,\n-            attribution=attribution,\n-            zoom_to_layer=zoom_to_layer,\n-            **kwargs,\n-        )\n+    if isinstance(source, str) and source.startswith(\"http\"):",
    "comment": "The removed check for `.tif` file extension (`source.lower().endswith('.tif')`) means the folium branch will now attempt to handle all HTTP URLs, not just GeoTIFF files. This could cause `add_geotiff` to fail on non-GeoTIFF URLs. The original condition ensured only GeoTIFF files were processed through the COG layer logic.\n```suggestion\n    if (\n        isinstance(source, str)\n        and source.startswith(\"http\")\n        and (source.lower().endswith(\".tif\") or source.lower().endswith(\".tiff\"))\n    ):\n```",
    "line_number": 126,
    "enriched": "File: geoai/utils.py\nCode: @@ -123,26 +123,39 @@ def view_raster(\n     if isinstance(source, dict):\n         source = dict_to_image(source)\n \n-    if (\n-        isinstance(source, str)\n-        and source.lower().endswith(\".tif\")\n-        and source.startswith(\"http\")\n-    ):\n-        if indexes is not None:\n-            kwargs[\"bidx\"] = indexes\n-        if colormap is not None:\n-            kwargs[\"colormap_name\"] = colormap\n-        if attribution is None:\n-            attribution = \"TiTiler\"\n-\n-        m.add_cog_layer(\n-            source,\n-            name=layer_name,\n-            opacity=opacity,\n-            attribution=attribution,\n-            zoom_to_layer=zoom_to_layer,\n-            **kwargs,\n-        )\n+    if isinstance(source, str) and source.startswith(\"http\"):\nComment: The removed check for `.tif` file extension (`source.lower().endswith('.tif')`) means the folium branch will now attempt to handle all HTTP URLs, not just GeoTIFF files. This could cause `add_geotiff` to fail on non-GeoTIFF URLs. The original condition ensured only GeoTIFF files were processed through the COG layer logic.\n```suggestion\n    if (\n        isinstance(source, str)\n        and source.startswith(\"http\")\n        and (source.lower().endswith(\".tif\") or source.lower().endswith(\".tiff\"))\n    ):\n```",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "geoai/utils.py",
    "pr_number": 357,
    "repo": "geoai",
    "owner": "opengeos",
    "comment_id": 2496072356,
    "comment_created_at": "2025-11-05T20:38:30Z"
  },
  {
    "code": "@@ -7,14 +7,14 @@\n     \"id\": \"0\"\n    },\n    \"source\": [\n-    \"[![image](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/opengeos/geoai/blob/main/docs/workshops/ASA_2025.ipynb)\\n\",\n+    \"[![image](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/opengeos/geoai/blob/main/docs/workshops/CANVAS_2025.ipynb)\\n\",\n     \"\\n\",\n     \"**Open Source Pipeline to Integrate Drone and Satellite Geospatial Data Products for Agricultural Applications**\\n\",\n     \"\\n\",\n     \"This notebook is designed for workshop presented at the [American Society of Agronomy (ASA)](https://www.agronomy.org) Annual Meeting on November 10, 2025.\\n\",",
    "comment": "The notebook filename and references have been changed to 'CANVAS_2025' but the description still mentions 'American Society of Agronomy (ASA)'. If CANVAS is a different conference or the new name for this workshop, this description should be updated to reflect the correct event name.\n```suggestion\n                \"This notebook is designed for the workshop presented at the [CANVAS 2025 Conference](https://canvas-conference.org) on November 10, 2025.\\n\",\n```",
    "line_number": 14,
    "enriched": "File: docs/workshops/CANVAS_2025.ipynb\nCode: @@ -7,14 +7,14 @@\n     \"id\": \"0\"\n    },\n    \"source\": [\n-    \"[![image](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/opengeos/geoai/blob/main/docs/workshops/ASA_2025.ipynb)\\n\",\n+    \"[![image](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/opengeos/geoai/blob/main/docs/workshops/CANVAS_2025.ipynb)\\n\",\n     \"\\n\",\n     \"**Open Source Pipeline to Integrate Drone and Satellite Geospatial Data Products for Agricultural Applications**\\n\",\n     \"\\n\",\n     \"This notebook is designed for workshop presented at the [American Society of Agronomy (ASA)](https://www.agronomy.org) Annual Meeting on November 10, 2025.\\n\",\nComment: The notebook filename and references have been changed to 'CANVAS_2025' but the description still mentions 'American Society of Agronomy (ASA)'. If CANVAS is a different conference or the new name for this workshop, this description should be updated to reflect the correct event name.\n```suggestion\n                \"This notebook is designed for the workshop presented at the [CANVAS 2025 Conference](https://canvas-conference.org) on November 10, 2025.\\n\",\n```",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/workshops/CANVAS_2025.ipynb",
    "pr_number": 356,
    "repo": "geoai",
    "owner": "opengeos",
    "comment_id": 2495204981,
    "comment_created_at": "2025-11-05T16:09:49Z"
  },
  {
    "code": "@@ -2993,7 +2680,7 @@ def batch_vector_to_raster(\n def export_geotiff_tiles(\n     in_raster,\n     out_folder,\n-    in_class_data=None,\n+    in_class_data,",
    "comment": "Breaking change: The `in_class_data` parameter is now required (removed Optional with default None). This breaks backward compatibility with code that called `export_geotiff_tiles` without mask data to export image-only tiles. Consider keeping it optional with a default of None to maintain backward compatibility, or clearly document this breaking change in release notes.\n```suggestion\n    in_class_data: Optional[Any] = None,\n```",
    "line_number": 2683,
    "enriched": "File: geoai/utils.py\nCode: @@ -2993,7 +2680,7 @@ def batch_vector_to_raster(\n def export_geotiff_tiles(\n     in_raster,\n     out_folder,\n-    in_class_data=None,\n+    in_class_data,\nComment: Breaking change: The `in_class_data` parameter is now required (removed Optional with default None). This breaks backward compatibility with code that called `export_geotiff_tiles` without mask data to export image-only tiles. Consider keeping it optional with a default of None to maintain backward compatibility, or clearly document this breaking change in release notes.\n```suggestion\n    in_class_data: Optional[Any] = None,\n```",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "geoai/utils.py",
    "pr_number": 353,
    "repo": "geoai",
    "owner": "opengeos",
    "comment_id": 2493193740,
    "comment_created_at": "2025-11-05T06:30:53Z"
  },
  {
    "code": "@@ -1451,25 +1685,29 @@\n   {\n    \"cell_type\": \"code\",\n    \"execution_count\": null,\n-   \"id\": \"122\",\n-   \"metadata\": {},\n+   \"id\": \"125\",\n+   \"metadata\": {\n+    \"id\": \"122\"\n+   },\n    \"outputs\": [],\n    \"source\": [\n     \"# Define separate regions for training and testing\\n\",\n     \"# Training: Use June 2022 imagery with tree boundaries\\n\",\n     \"# Testing: Use December 2022 imagery to test model generalization\\n\",\n-    \"train_bbox = [-97.956252, 26.165315, -97.954992, 26.165883]\\n\",\n-    \"test_bbox = [-97.956267, 26.165941, -97.954969, 26.166552]\"\n+    \"bbox = [-97.956252, 26.165315, -97.954992, 26.165883]\"",
    "comment": "The removal of separate train_bbox and test_bbox variables and use of a single bbox for both training and testing data eliminates the ability to test model generalization on a different spatial region, which was the original intent mentioned in the comment on line 1696. Consider keeping separate bounding boxes or updating the comment to reflect the change.",
    "line_number": 1697,
    "enriched": "File: docs/workshops/ASA_2025.ipynb\nCode: @@ -1451,25 +1685,29 @@\n   {\n    \"cell_type\": \"code\",\n    \"execution_count\": null,\n-   \"id\": \"122\",\n-   \"metadata\": {},\n+   \"id\": \"125\",\n+   \"metadata\": {\n+    \"id\": \"122\"\n+   },\n    \"outputs\": [],\n    \"source\": [\n     \"# Define separate regions for training and testing\\n\",\n     \"# Training: Use June 2022 imagery with tree boundaries\\n\",\n     \"# Testing: Use December 2022 imagery to test model generalization\\n\",\n-    \"train_bbox = [-97.956252, 26.165315, -97.954992, 26.165883]\\n\",\n-    \"test_bbox = [-97.956267, 26.165941, -97.954969, 26.166552]\"\n+    \"bbox = [-97.956252, 26.165315, -97.954992, 26.165883]\"\nComment: The removal of separate train_bbox and test_bbox variables and use of a single bbox for both training and testing data eliminates the ability to test model generalization on a different spatial region, which was the original intent mentioned in the comment on line 1696. Consider keeping separate bounding boxes or updating the comment to reflect the change.",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/workshops/ASA_2025.ipynb",
    "pr_number": 352,
    "repo": "geoai",
    "owner": "opengeos",
    "comment_id": 2492904388,
    "comment_created_at": "2025-11-05T04:34:06Z"
  },
  {
    "code": "@@ -0,0 +1,1490 @@\n+{\n+ \"cells\": [\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"0\",\n+   \"metadata\": {\n+    \"id\": \"0\"\n+   },\n+   \"source\": [\n+    \"[![image](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/opengeos/geoai/blob/main/docs/workshops/ASA_2025.ipynb)\\n\",\n+    \"\\n\",\n+    \"**Open Source Pipeline to Integrate Drone and Satellite Geospatial Data Products for Agricultural Applications**\\n\",\n+    \"\\n\",\n+    \"This notebook is designed for workshop presented at the [American Society of Agronomy (ASA)](https://www.agronomy.org) Annual Meeting on November 10, 2025. \\n\",\n+    \"\\n\",\n+    \"-   Registration: <https://scisoc.confex.com/scisoc/2025am/meetingapp.cgi/Session/27796>\\n\",\n+    \"-   Notebook: <https://opengeos.org/workshops/ASA_2025>\\n\",\n+    \"-   Leafmap: <https://leafmap.org>\\n\",\n+    \"-   Samgeo: <https://samgeo.gishub.org>\\n\",\n+    \"-   Data to Science (D2S): <https://ps2.d2s.org>\\n\",\n+    \"-   D2S Python API: <https://py.d2s.org>\\n\",\n+    \"\\n\",\n+    \"## Introduction\\n\",\n+    \"\\n\",\n+    \"Recent advances in drone technology have revolutionized the remote sensing community by providing means to collect fine spatial and high temporal resolutions at affordable costs. As people are gaining access to increasingly larger volumes of drone and satellite geospatial data products, there is a growing need to extract relevant information from the vast amount of freely available geospatial data. However, the lack of specialized software packages tailored for processing such data makes it challenging to develop transdisciplinary research collaboration around them. This workshop aims to bridge the gap between big geospatial data and research scientists by providing training on an open-source online platform for managing big drone data known as Data to Science.\\n\",\n+    \"\\n\",\n+    \"## Agenda\\n\",\n+    \"\\n\",\n+    \"The main topics to be covered in this workshop include:\\n\",\n+    \"\\n\",\n+    \"* Create interactive maps using leafmap\\n\",\n+    \"* Visualize drone imagery from D2S\\n\",\n+    \"* Segment drone imagery using samgeo\\n\",\n+    \"\\n\",\n+    \"## Environment setup\\n\",\n+    \"\\n\",\n+    \"[![image](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/opengeos/geoai/blob/main/docs/workshops/ASA_2025.ipynb)\\n\",\n+    \"\\n\",\n+    \"\\n\",\n+    \"### Change runtime type to GPU\\n\",\n+    \"\\n\",\n+    \"To speed up the processing, you can change the Colab runtime type to GPU. Go to the \\\"Runtime\\\" menu, select \\\"Change runtime type\\\", and choose \\\"T4 GPU\\\" from the \\\"Hardware accelerator\\\" dropdown menu.\\n\",\n+    \"\\n\",\n+    \"![image](https://github.com/user-attachments/assets/e92d2a19-0555-456d-b4be-36680c0af09f)\\n\",\n+    \"\\n\",\n+    \"\\n\",\n+    \"### Install packages\\n\",\n+    \"\\n\",\n+    \"Uncomment the following code to install the required packages.\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"id\": \"1\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"# %pip install -U \\\"leafmap[raster]\\\" segment-geospatial geoai-py\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"2\",\n+   \"metadata\": {\n+    \"id\": \"3\"\n+   },\n+   \"source\": [\n+    \"### Import libraries\\n\",\n+    \"\\n\",\n+    \"Import the necessary libraries for this workshop.\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"id\": \"3\",\n+   \"metadata\": {\n+    \"id\": \"4\"\n+   },\n+   \"outputs\": [],\n+   \"source\": [\n+    \"import leafmap\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"4\",\n+   \"metadata\": {\n+    \"id\": \"9\"\n+   },\n+   \"source\": [\n+    \"## Creating interactive maps\\n\",\n+    \"\\n\",\n+    \"Let's create an interactive map using the `ipyleaflet` plotting backend. The [`leafmap.Map`](https://leafmap.org/leafmap/#leafmap.leafmap.m) class inherits the [`ipyleaflet.Map`](https://ipyleaflet.readthedocs.io/en/latest/map_and_basemaps/map.html) class. Therefore, you can use the same syntax to create an interactive map as you would with `ipyleaflet.Map`.\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"id\": \"5\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"m = leafmap.Map()\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"6\",\n+   \"metadata\": {\n+    \"id\": \"11\"\n+   },\n+   \"source\": [\n+    \"To display it in a Jupyter notebook, simply ask for the object representation:\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"id\": \"7\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"m\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"8\",\n+   \"metadata\": {\n+    \"id\": \"13\"\n+   },\n+   \"source\": [\n+    \"To customize the map, you can specify various keyword arguments, such as `center` ([lat, lon]), `zoom`, `width`, and `height`. The default `width` is `100%`, which takes up the entire cell width of the Jupyter notebook. The `height` argument accepts a number or a string. If a number is provided, it represents the height of the map in pixels. If a string is provided, the string must be in the format of a number followed by `px`, e.g., `600px`.\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"id\": \"9\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"m = leafmap.Map(center=[40, -100], zoom=4, height=\\\"600px\\\")\\n\",\n+    \"m\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"10\",\n+   \"metadata\": {\n+    \"id\": \"17\"\n+   },\n+   \"source\": [\n+    \"### Adding basemaps\\n\",\n+    \"\\n\",\n+    \"There are several ways to add basemaps to a map. You can specify the basemap to use in the `basemap` keyword argument when creating the map. Alternatively, you can add basemap layers to the map using the `add_basemap` method. leafmap has hundreds of built-in basemaps available that can be easily added to the map with only one line of code.\\n\",\n+    \"\\n\",\n+    \"Create a map by specifying the basemap to use as follows. For example, the `Esri.WorldImagery` basemap represents the Esri world imagery basemap.\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"id\": \"11\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"m = leafmap.Map(basemap=\\\"Esri.WorldImagery\\\")\\n\",\n+    \"m\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"12\",\n+   \"metadata\": {\n+    \"id\": \"19\"\n+   },\n+   \"source\": [\n+    \"You can add as many basemaps as you like to the map. For example, the following code adds the `OpenTopoMap` basemap to the map above:\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"id\": \"13\",\n+   \"metadata\": {\n+    \"id\": \"20\",\n+    \"outputId\": \"b1fea60d-6a8c-4c9e-fc13-0e3525c4a917\"\n+   },\n+   \"outputs\": [],\n+   \"source\": [\n+    \"m.add_basemap(\\\"OpenTopoMap\\\")\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"14\",\n+   \"metadata\": {\n+    \"id\": \"21\"\n+   },\n+   \"source\": [\n+    \"You can also add an XYZ tile layer to the map.\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"id\": \"15\",\n+   \"metadata\": {\n+    \"id\": \"22\",\n+    \"outputId\": \"c4f2108b-8a2b-4903-db1a-bcc0690a3651\"\n+   },\n+   \"outputs\": [],\n+   \"source\": [\n+    \"basemap_url = \\\"https://mt1.google.com/vt/lyrs=y&x={x}&y={y}&z={z}\\\"\\n\",\n+    \"m.add_tile_layer(basemap_url, name=\\\"Hybrid\\\", attribution=\\\"Google\\\")\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"16\",\n+   \"metadata\": {\n+    \"id\": \"23\"\n+   },\n+   \"source\": [\n+    \"You can also change basemaps interactively using the basemap GUI.\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"id\": \"17\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"m = leafmap.Map()\\n\",\n+    \"m.add_basemap_gui()\\n\",\n+    \"m\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"18\",\n+   \"metadata\": {\n+    \"id\": \"25\"\n+   },\n+   \"source\": [\n+    \"## Visualizing Drone Imagery from D2S\\n\",\n+    \"\\n\",\n+    \"The Data to Science (D2S) platform (https://ps2.d2s.org) hosts a large collection of drone imagery that can be accessed through the D2S API (https://py.d2s.org). To visualize drone imagery from D2S, you need to [sign up](https://ps2.d2s.org/auth/register) for a free account on the D2S platform and obtain an API key.\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"19\",\n+   \"metadata\": {\n+    \"id\": \"de1a9ee4-d27e-49cb-9dcf-db32aab48ccc\"\n+   },\n+   \"source\": [\n+    \"### Login to D2S\\n\",\n+    \"Login and connect to your D2S workspace in one go using d2spy.\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"id\": \"20\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"import os\\n\",\n+    \"from datetime import date\\n\",\n+    \"from d2spy.workspace import Workspace\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"id\": \"21\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"if os.environ.get(\\\"D2S_EMAIL\\\") is None:\\n\",\n+    \"    os.environ[\\\"D2S_EMAIL\\\"] = \\\"workshop@d2s.org\\\"  # Replace with your email address\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"id\": \"22\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"d2s_url = \\\"https://ps2.d2s.org\\\"\\n\",\n+    \"workspace = Workspace.connect(d2s_url)\\n\",\n+    \"api_key = workspace.api_key\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"id\": \"23\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"os.environ[\\\"D2S_API_KEY\\\"] = api_key\\n\",\n+    \"os.environ[\\\"TITILER_ENDPOINT\\\"] = \\\"https://tt.d2s.org\\\"\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"24\",\n+   \"metadata\": {\n+    \"id\": \"TOx4ytjZKLOC\"\n+   },\n+   \"source\": [\n+    \"### Choose a project to work with\\n\",\n+    \"\\n\",\n+    \"The Workspace `get_projects` method will retrieve a collection of the projects your account can currently access on the D2S instance.\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"id\": \"25\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"# Get list of all your projects\\n\",\n+    \"projects = workspace.get_projects()\\n\",\n+    \"for project in projects:\\n\",\n+    \"    print(project)\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"26\",\n+   \"metadata\": {\n+    \"id\": \"zTQJkyxaKxKn\"\n+   },\n+   \"source\": [\n+    \"The `projects` variable is a `ProjectCollection`. The collection can be filtered by either the project descriptions or titles using the methods `filter_by_title` or `filter_by_name`.\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"id\": \"27\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"# Example of creating new collection of only projects with the keyword \\\"Citrus Orchard\\\" in the title\\n\",\n+    \"filtered_projects = projects.filter_by_title(\\\"Citrus Orchard\\\")\\n\",\n+    \"print(filtered_projects)\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"28\",\n+   \"metadata\": {\n+    \"id\": \"O6fdeffmLIN5\"\n+   },\n+   \"source\": [\n+    \"Now you can choose a specific project to work with. In this case, the filtered projects returned only one project, so we will use that project.\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"id\": \"29\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"project = filtered_projects[0]\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"30\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"### Get the project boundary\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"31\",\n+   \"metadata\": {\n+    \"id\": \"yqo000pqLYn4\"\n+   },\n+   \"source\": [\n+    \"`get_project_boundary` method of the `Project` class will retrieve a GeoJSON object of the project boundary.\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"id\": \"32\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"# Get project boundary as Python dictionary in GeoJSON structure\\n\",\n+    \"project_boundary = project.get_project_boundary()\\n\",\n+    \"project_boundary\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"33\",\n+   \"metadata\": {\n+    \"id\": \"OvFhMBNtLv9X\"\n+   },\n+   \"source\": [\n+    \"### Get project flights\\n\",\n+    \"\\n\",\n+    \"The `Project` `get_flights` method will retrieve a list of flights associated with the project.\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"id\": \"34\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"# Get list of all flights for a project\\n\",\n+    \"flights = project.get_flights()\\n\",\n+    \"# Print first flight object (if one exists)\\n\",\n+    \"for flight in flights:\\n\",\n+    \"    print(flight)\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"35\",\n+   \"metadata\": {\n+    \"id\": \"eT8rrteTMM4z\"\n+   },\n+   \"source\": [\n+    \"### Filter flights by date\\n\",\n+    \"\\n\",\n+    \"The `flights` variable is a `FlightCollection`. The collection can be filtered by the acquisition date using the method `filter_by_date`. This method will return all flights with an acquisition date between the provided start and end dates.\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"id\": \"36\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"# Example of creating new collection of only flights from June 2022\\n\",\n+    \"filtered_flights = flights.filter_by_date(\\n\",\n+    \"    start_date=date(2022, 6, 1), end_date=date(2022, 7, 1)\\n\",\n+    \")\\n\",\n+    \"for flight in filtered_flights:\\n\",\n+    \"    print(flight)\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"37\",\n+   \"metadata\": {\n+    \"id\": \"dOlbqBhRNX4e\"\n+   },\n+   \"source\": [\n+    \"Now, we can choose a flight from the filtered flight. Let's choose the flight on June 9, 2022.\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"id\": \"38\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"flight = filtered_flights[0]\\n\",\n+    \"flight\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"39\",\n+   \"metadata\": {\n+    \"id\": \"ngIYB3tANljF\"\n+   },\n+   \"source\": [\n+    \"### Get data products\\n\",\n+    \"\\n\",\n+    \"The Flight `get_data_products` method will retrieve a list of data products associated with the flight.\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"id\": \"40\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"# Get list of data products from a flight\\n\",\n+    \"data_products = flight.get_data_products()\\n\",\n+    \"\\n\",\n+    \"for data_product in data_products:\\n\",\n+    \"    print(data_product)\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"41\",\n+   \"metadata\": {\n+    \"id\": \"yiKBMsv7N4cy\"\n+   },\n+   \"source\": [\n+    \"The `data_products` variable is a `DataProductCollection`. The collection can be filtered by data type using the method `filter_by_data_type`. This method will return all data products that match the requested data type.\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"id\": \"42\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"# Example of creating new collection of data products with the \\\"ortho\\\" data type\\n\",\n+    \"ortho_data_products = data_products.filter_by_data_type(\\\"ortho\\\")\\n\",\n+    \"print(ortho_data_products)\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"43\",\n+   \"metadata\": {\n+    \"id\": \"ZryOKEuROOnu\"\n+   },\n+   \"source\": [\n+    \"### Visualize ortho imagery\\n\",\n+    \"\\n\",\n+    \"Now we can grab the ortho URL to display it using leafmap.\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"id\": \"44\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"m = leafmap.Map()\\n\",\n+    \"m.add_basemap(\\\"HYBRID\\\", show=False)\\n\",\n+    \"ortho_data = ortho_data_products[0]\\n\",\n+    \"ortho_url_202206 = ortho_data.url\\n\",\n+    \"ortho_url_202206 = leafmap.d2s_tile(ortho_url_202206)\\n\",\n+    \"m.add_cog_layer(ortho_url_202206, name=\\\"Ortho Imagery 202206\\\")\\n\",\n+    \"m\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"45\",\n+   \"metadata\": {\n+    \"id\": \"33\"\n+   },\n+   \"source\": [\n+    \"### Visualize DSM\\n\",\n+    \"\\n\",\n+    \"Similarly, you can visualize the Digital Surface Model (DSM) from D2S using the code below.\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"id\": \"46\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"# Example of creating new collection of data products with the \\\"dsm\\\" data type\\n\",\n+    \"dsm_data_products = data_products.filter_by_data_type(\\\"dsm\\\")\\n\",\n+    \"print(dsm_data_products)\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"id\": \"47\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"dsm_data = dsm_data_products[0]\\n\",\n+    \"dsm_url_202206 = dsm_data.url\\n\",\n+    \"dsm_url_202206 = leafmap.d2s_tile(dsm_url_202206)\\n\",\n+    \"m.add_cog_layer(dsm_url_202206, colormap_name=\\\"terrain\\\", name=\\\"DSM 202206\\\")\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"id\": \"48\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"leafmap.cog_stats(dsm_url_202206)\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"49\",\n+   \"metadata\": {\n+    \"id\": \"35\"\n+   },\n+   \"source\": [\n+    \"Add a colorbar to the map.\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"id\": \"50\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"m.add_colormap(cmap=\\\"terrain\\\", vmin=3, vmax=33, label=\\\"Elevation (m)\\\")\\n\",\n+    \"m\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"51\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"### Visualize CHM\\n\",\n+    \"\\n\",\n+    \"Similarly, you can visualize the Canopy Height Model (CHM) from D2S using the code below.\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"id\": \"52\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"# Example of creating new collection of data products with the \\\"chm\\\" data type\\n\",\n+    \"chm_data_products = data_products.filter_by_data_type(\\\"chm\\\")\\n\",\n+    \"print(chm_data_products)\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"id\": \"53\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"chm_data = chm_data_products[0]\\n\",\n+    \"chm_url_202206 = chm_data.url\\n\",\n+    \"chm_url_202206 = leafmap.d2s_tile(chm_url_202206)\\n\",\n+    \"m.add_cog_layer(chm_url_202206, colormap_name=\\\"jet\\\", name=\\\"CHM 202206\\\")\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"id\": \"54\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"leafmap.cog_stats(chm_url_202206)\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"id\": \"55\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"m.add_colormap(cmap=\\\"jet\\\", vmin=0, vmax=13, label=\\\"Elevation (m)\\\")\\n\",\n+    \"m\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"56\",\n+   \"metadata\": {\n+    \"id\": \"37\"\n+   },\n+   \"source\": [\n+    \"Add the project boundary to the map.\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"id\": \"57\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"project_boundary\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"id\": \"58\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"m.add_geojson(project_boundary, layer_name=\\\"Project Boundary\\\")\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"59\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"Add tree boundaries to the map.\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"id\": \"60\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"map_layers = project.get_map_layers()\\n\",\n+    \"tree_boundaries = map_layers[0]\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"id\": \"61\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"m.add_geojson(tree_boundaries, layer_name=\\\"Tree Boundaries\\\")\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"62\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"### Get another flight\\n\",\n+    \"\\n\",\n+    \"Retrieve the Ortho data product for the December 2022 flight.\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"id\": \"63\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"filtered_flights = flights.filter_by_date(\\n\",\n+    \"    start_date=date(2022, 12, 1), end_date=date(2022, 12, 31)\\n\",\n+    \")\\n\",\n+    \"for flight in filtered_flights:\\n\",\n+    \"    print(flight)\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"id\": \"64\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"flight_202212 = filtered_flights[0]\\n\",\n+    \"data_products = flight_202212.get_data_products()\\n\",\n+    \"ortho_data_products = data_products.filter_by_data_type(\\\"ortho\\\")\\n\",\n+    \"ortho_data = ortho_data_products[0]\\n\",\n+    \"ortho_url_202212 = ortho_data.url\\n\",\n+    \"ortho_url_202212 = leafmap.d2s_tile(ortho_url_202212)\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"65\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"### Compare two ortho images\\n\",\n+    \"\\n\",\n+    \"Create a split map for comparing the 2022 and 2024 ortho images.\"",
    "comment": "The comment states '2022 and 2024' but the code actually compares June 2022 (ortho_url_202206) and December 2022 (ortho_url_202212) images. The year '2024' should be '2022' to accurately reflect what the code does.\n```suggestion\n    \"Create a split map for comparing the June 2022 and December 2022 ortho images.\"\n```",
    "line_number": 788,
    "enriched": "File: docs/workshops/ASA_2025.ipynb\nCode: @@ -0,0 +1,1490 @@\n+{\n+ \"cells\": [\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"0\",\n+   \"metadata\": {\n+    \"id\": \"0\"\n+   },\n+   \"source\": [\n+    \"[![image](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/opengeos/geoai/blob/main/docs/workshops/ASA_2025.ipynb)\\n\",\n+    \"\\n\",\n+    \"**Open Source Pipeline to Integrate Drone and Satellite Geospatial Data Products for Agricultural Applications**\\n\",\n+    \"\\n\",\n+    \"This notebook is designed for workshop presented at the [American Society of Agronomy (ASA)](https://www.agronomy.org) Annual Meeting on November 10, 2025. \\n\",\n+    \"\\n\",\n+    \"-   Registration: <https://scisoc.confex.com/scisoc/2025am/meetingapp.cgi/Session/27796>\\n\",\n+    \"-   Notebook: <https://opengeos.org/workshops/ASA_2025>\\n\",\n+    \"-   Leafmap: <https://leafmap.org>\\n\",\n+    \"-   Samgeo: <https://samgeo.gishub.org>\\n\",\n+    \"-   Data to Science (D2S): <https://ps2.d2s.org>\\n\",\n+    \"-   D2S Python API: <https://py.d2s.org>\\n\",\n+    \"\\n\",\n+    \"## Introduction\\n\",\n+    \"\\n\",\n+    \"Recent advances in drone technology have revolutionized the remote sensing community by providing means to collect fine spatial and high temporal resolutions at affordable costs. As people are gaining access to increasingly larger volumes of drone and satellite geospatial data products, there is a growing need to extract relevant information from the vast amount of freely available geospatial data. However, the lack of specialized software packages tailored for processing such data makes it challenging to develop transdisciplinary research collaboration around them. This workshop aims to bridge the gap between big geospatial data and research scientists by providing training on an open-source online platform for managing big drone data known as Data to Science.\\n\",\n+    \"\\n\",\n+    \"## Agenda\\n\",\n+    \"\\n\",\n+    \"The main topics to be covered in this workshop include:\\n\",\n+    \"\\n\",\n+    \"* Create interactive maps using leafmap\\n\",\n+    \"* Visualize drone imagery from D2S\\n\",\n+    \"* Segment drone imagery using samgeo\\n\",\n+    \"\\n\",\n+    \"## Environment setup\\n\",\n+    \"\\n\",\n+    \"[![image](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/opengeos/geoai/blob/main/docs/workshops/ASA_2025.ipynb)\\n\",\n+    \"\\n\",\n+    \"\\n\",\n+    \"### Change runtime type to GPU\\n\",\n+    \"\\n\",\n+    \"To speed up the processing, you can change the Colab runtime type to GPU. Go to the \\\"Runtime\\\" menu, select \\\"Change runtime type\\\", and choose \\\"T4 GPU\\\" from the \\\"Hardware accelerator\\\" dropdown menu.\\n\",\n+    \"\\n\",\n+    \"![image](https://github.com/user-attachments/assets/e92d2a19-0555-456d-b4be-36680c0af09f)\\n\",\n+    \"\\n\",\n+    \"\\n\",\n+    \"### Install packages\\n\",\n+    \"\\n\",\n+    \"Uncomment the following code to install the required packages.\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"id\": \"1\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"# %pip install -U \\\"leafmap[raster]\\\" segment-geospatial geoai-py\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"2\",\n+   \"metadata\": {\n+    \"id\": \"3\"\n+   },\n+   \"source\": [\n+    \"### Import libraries\\n\",\n+    \"\\n\",\n+    \"Import the necessary libraries for this workshop.\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"id\": \"3\",\n+   \"metadata\": {\n+    \"id\": \"4\"\n+   },\n+   \"outputs\": [],\n+   \"source\": [\n+    \"import leafmap\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"4\",\n+   \"metadata\": {\n+    \"id\": \"9\"\n+   },\n+   \"source\": [\n+    \"## Creating interactive maps\\n\",\n+    \"\\n\",\n+    \"Let's create an interactive map using the `ipyleaflet` plotting backend. The [`leafmap.Map`](https://leafmap.org/leafmap/#leafmap.leafmap.m) class inherits the [`ipyleaflet.Map`](https://ipyleaflet.readthedocs.io/en/latest/map_and_basemaps/map.html) class. Therefore, you can use the same syntax to create an interactive map as you would with `ipyleaflet.Map`.\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"id\": \"5\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"m = leafmap.Map()\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"6\",\n+   \"metadata\": {\n+    \"id\": \"11\"\n+   },\n+   \"source\": [\n+    \"To display it in a Jupyter notebook, simply ask for the object representation:\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"id\": \"7\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"m\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"8\",\n+   \"metadata\": {\n+    \"id\": \"13\"\n+   },\n+   \"source\": [\n+    \"To customize the map, you can specify various keyword arguments, such as `center` ([lat, lon]), `zoom`, `width`, and `height`. The default `width` is `100%`, which takes up the entire cell width of the Jupyter notebook. The `height` argument accepts a number or a string. If a number is provided, it represents the height of the map in pixels. If a string is provided, the string must be in the format of a number followed by `px`, e.g., `600px`.\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"id\": \"9\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"m = leafmap.Map(center=[40, -100], zoom=4, height=\\\"600px\\\")\\n\",\n+    \"m\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"10\",\n+   \"metadata\": {\n+    \"id\": \"17\"\n+   },\n+   \"source\": [\n+    \"### Adding basemaps\\n\",\n+    \"\\n\",\n+    \"There are several ways to add basemaps to a map. You can specify the basemap to use in the `basemap` keyword argument when creating the map. Alternatively, you can add basemap layers to the map using the `add_basemap` method. leafmap has hundreds of built-in basemaps available that can be easily added to the map with only one line of code.\\n\",\n+    \"\\n\",\n+    \"Create a map by specifying the basemap to use as follows. For example, the `Esri.WorldImagery` basemap represents the Esri world imagery basemap.\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"id\": \"11\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"m = leafmap.Map(basemap=\\\"Esri.WorldImagery\\\")\\n\",\n+    \"m\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"12\",\n+   \"metadata\": {\n+    \"id\": \"19\"\n+   },\n+   \"source\": [\n+    \"You can add as many basemaps as you like to the map. For example, the following code adds the `OpenTopoMap` basemap to the map above:\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"id\": \"13\",\n+   \"metadata\": {\n+    \"id\": \"20\",\n+    \"outputId\": \"b1fea60d-6a8c-4c9e-fc13-0e3525c4a917\"\n+   },\n+   \"outputs\": [],\n+   \"source\": [\n+    \"m.add_basemap(\\\"OpenTopoMap\\\")\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"14\",\n+   \"metadata\": {\n+    \"id\": \"21\"\n+   },\n+   \"source\": [\n+    \"You can also add an XYZ tile layer to the map.\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"id\": \"15\",\n+   \"metadata\": {\n+    \"id\": \"22\",\n+    \"outputId\": \"c4f2108b-8a2b-4903-db1a-bcc0690a3651\"\n+   },\n+   \"outputs\": [],\n+   \"source\": [\n+    \"basemap_url = \\\"https://mt1.google.com/vt/lyrs=y&x={x}&y={y}&z={z}\\\"\\n\",\n+    \"m.add_tile_layer(basemap_url, name=\\\"Hybrid\\\", attribution=\\\"Google\\\")\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"16\",\n+   \"metadata\": {\n+    \"id\": \"23\"\n+   },\n+   \"source\": [\n+    \"You can also change basemaps interactively using the basemap GUI.\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"id\": \"17\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"m = leafmap.Map()\\n\",\n+    \"m.add_basemap_gui()\\n\",\n+    \"m\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"18\",\n+   \"metadata\": {\n+    \"id\": \"25\"\n+   },\n+   \"source\": [\n+    \"## Visualizing Drone Imagery from D2S\\n\",\n+    \"\\n\",\n+    \"The Data to Science (D2S) platform (https://ps2.d2s.org) hosts a large collection of drone imagery that can be accessed through the D2S API (https://py.d2s.org). To visualize drone imagery from D2S, you need to [sign up](https://ps2.d2s.org/auth/register) for a free account on the D2S platform and obtain an API key.\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"19\",\n+   \"metadata\": {\n+    \"id\": \"de1a9ee4-d27e-49cb-9dcf-db32aab48ccc\"\n+   },\n+   \"source\": [\n+    \"### Login to D2S\\n\",\n+    \"Login and connect to your D2S workspace in one go using d2spy.\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"id\": \"20\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"import os\\n\",\n+    \"from datetime import date\\n\",\n+    \"from d2spy.workspace import Workspace\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"id\": \"21\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"if os.environ.get(\\\"D2S_EMAIL\\\") is None:\\n\",\n+    \"    os.environ[\\\"D2S_EMAIL\\\"] = \\\"workshop@d2s.org\\\"  # Replace with your email address\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"id\": \"22\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"d2s_url = \\\"https://ps2.d2s.org\\\"\\n\",\n+    \"workspace = Workspace.connect(d2s_url)\\n\",\n+    \"api_key = workspace.api_key\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"id\": \"23\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"os.environ[\\\"D2S_API_KEY\\\"] = api_key\\n\",\n+    \"os.environ[\\\"TITILER_ENDPOINT\\\"] = \\\"https://tt.d2s.org\\\"\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"24\",\n+   \"metadata\": {\n+    \"id\": \"TOx4ytjZKLOC\"\n+   },\n+   \"source\": [\n+    \"### Choose a project to work with\\n\",\n+    \"\\n\",\n+    \"The Workspace `get_projects` method will retrieve a collection of the projects your account can currently access on the D2S instance.\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"id\": \"25\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"# Get list of all your projects\\n\",\n+    \"projects = workspace.get_projects()\\n\",\n+    \"for project in projects:\\n\",\n+    \"    print(project)\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"26\",\n+   \"metadata\": {\n+    \"id\": \"zTQJkyxaKxKn\"\n+   },\n+   \"source\": [\n+    \"The `projects` variable is a `ProjectCollection`. The collection can be filtered by either the project descriptions or titles using the methods `filter_by_title` or `filter_by_name`.\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"id\": \"27\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"# Example of creating new collection of only projects with the keyword \\\"Citrus Orchard\\\" in the title\\n\",\n+    \"filtered_projects = projects.filter_by_title(\\\"Citrus Orchard\\\")\\n\",\n+    \"print(filtered_projects)\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"28\",\n+   \"metadata\": {\n+    \"id\": \"O6fdeffmLIN5\"\n+   },\n+   \"source\": [\n+    \"Now you can choose a specific project to work with. In this case, the filtered projects returned only one project, so we will use that project.\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"id\": \"29\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"project = filtered_projects[0]\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"30\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"### Get the project boundary\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"31\",\n+   \"metadata\": {\n+    \"id\": \"yqo000pqLYn4\"\n+   },\n+   \"source\": [\n+    \"`get_project_boundary` method of the `Project` class will retrieve a GeoJSON object of the project boundary.\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"id\": \"32\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"# Get project boundary as Python dictionary in GeoJSON structure\\n\",\n+    \"project_boundary = project.get_project_boundary()\\n\",\n+    \"project_boundary\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"33\",\n+   \"metadata\": {\n+    \"id\": \"OvFhMBNtLv9X\"\n+   },\n+   \"source\": [\n+    \"### Get project flights\\n\",\n+    \"\\n\",\n+    \"The `Project` `get_flights` method will retrieve a list of flights associated with the project.\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"id\": \"34\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"# Get list of all flights for a project\\n\",\n+    \"flights = project.get_flights()\\n\",\n+    \"# Print first flight object (if one exists)\\n\",\n+    \"for flight in flights:\\n\",\n+    \"    print(flight)\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"35\",\n+   \"metadata\": {\n+    \"id\": \"eT8rrteTMM4z\"\n+   },\n+   \"source\": [\n+    \"### Filter flights by date\\n\",\n+    \"\\n\",\n+    \"The `flights` variable is a `FlightCollection`. The collection can be filtered by the acquisition date using the method `filter_by_date`. This method will return all flights with an acquisition date between the provided start and end dates.\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"id\": \"36\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"# Example of creating new collection of only flights from June 2022\\n\",\n+    \"filtered_flights = flights.filter_by_date(\\n\",\n+    \"    start_date=date(2022, 6, 1), end_date=date(2022, 7, 1)\\n\",\n+    \")\\n\",\n+    \"for flight in filtered_flights:\\n\",\n+    \"    print(flight)\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"37\",\n+   \"metadata\": {\n+    \"id\": \"dOlbqBhRNX4e\"\n+   },\n+   \"source\": [\n+    \"Now, we can choose a flight from the filtered flight. Let's choose the flight on June 9, 2022.\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"id\": \"38\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"flight = filtered_flights[0]\\n\",\n+    \"flight\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"39\",\n+   \"metadata\": {\n+    \"id\": \"ngIYB3tANljF\"\n+   },\n+   \"source\": [\n+    \"### Get data products\\n\",\n+    \"\\n\",\n+    \"The Flight `get_data_products` method will retrieve a list of data products associated with the flight.\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"id\": \"40\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"# Get list of data products from a flight\\n\",\n+    \"data_products = flight.get_data_products()\\n\",\n+    \"\\n\",\n+    \"for data_product in data_products:\\n\",\n+    \"    print(data_product)\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"41\",\n+   \"metadata\": {\n+    \"id\": \"yiKBMsv7N4cy\"\n+   },\n+   \"source\": [\n+    \"The `data_products` variable is a `DataProductCollection`. The collection can be filtered by data type using the method `filter_by_data_type`. This method will return all data products that match the requested data type.\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"id\": \"42\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"# Example of creating new collection of data products with the \\\"ortho\\\" data type\\n\",\n+    \"ortho_data_products = data_products.filter_by_data_type(\\\"ortho\\\")\\n\",\n+    \"print(ortho_data_products)\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"43\",\n+   \"metadata\": {\n+    \"id\": \"ZryOKEuROOnu\"\n+   },\n+   \"source\": [\n+    \"### Visualize ortho imagery\\n\",\n+    \"\\n\",\n+    \"Now we can grab the ortho URL to display it using leafmap.\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"id\": \"44\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"m = leafmap.Map()\\n\",\n+    \"m.add_basemap(\\\"HYBRID\\\", show=False)\\n\",\n+    \"ortho_data = ortho_data_products[0]\\n\",\n+    \"ortho_url_202206 = ortho_data.url\\n\",\n+    \"ortho_url_202206 = leafmap.d2s_tile(ortho_url_202206)\\n\",\n+    \"m.add_cog_layer(ortho_url_202206, name=\\\"Ortho Imagery 202206\\\")\\n\",\n+    \"m\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"45\",\n+   \"metadata\": {\n+    \"id\": \"33\"\n+   },\n+   \"source\": [\n+    \"### Visualize DSM\\n\",\n+    \"\\n\",\n+    \"Similarly, you can visualize the Digital Surface Model (DSM) from D2S using the code below.\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"id\": \"46\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"# Example of creating new collection of data products with the \\\"dsm\\\" data type\\n\",\n+    \"dsm_data_products = data_products.filter_by_data_type(\\\"dsm\\\")\\n\",\n+    \"print(dsm_data_products)\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"id\": \"47\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"dsm_data = dsm_data_products[0]\\n\",\n+    \"dsm_url_202206 = dsm_data.url\\n\",\n+    \"dsm_url_202206 = leafmap.d2s_tile(dsm_url_202206)\\n\",\n+    \"m.add_cog_layer(dsm_url_202206, colormap_name=\\\"terrain\\\", name=\\\"DSM 202206\\\")\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"id\": \"48\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"leafmap.cog_stats(dsm_url_202206)\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"49\",\n+   \"metadata\": {\n+    \"id\": \"35\"\n+   },\n+   \"source\": [\n+    \"Add a colorbar to the map.\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"id\": \"50\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"m.add_colormap(cmap=\\\"terrain\\\", vmin=3, vmax=33, label=\\\"Elevation (m)\\\")\\n\",\n+    \"m\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"51\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"### Visualize CHM\\n\",\n+    \"\\n\",\n+    \"Similarly, you can visualize the Canopy Height Model (CHM) from D2S using the code below.\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"id\": \"52\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"# Example of creating new collection of data products with the \\\"chm\\\" data type\\n\",\n+    \"chm_data_products = data_products.filter_by_data_type(\\\"chm\\\")\\n\",\n+    \"print(chm_data_products)\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"id\": \"53\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"chm_data = chm_data_products[0]\\n\",\n+    \"chm_url_202206 = chm_data.url\\n\",\n+    \"chm_url_202206 = leafmap.d2s_tile(chm_url_202206)\\n\",\n+    \"m.add_cog_layer(chm_url_202206, colormap_name=\\\"jet\\\", name=\\\"CHM 202206\\\")\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"id\": \"54\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"leafmap.cog_stats(chm_url_202206)\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"id\": \"55\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"m.add_colormap(cmap=\\\"jet\\\", vmin=0, vmax=13, label=\\\"Elevation (m)\\\")\\n\",\n+    \"m\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"56\",\n+   \"metadata\": {\n+    \"id\": \"37\"\n+   },\n+   \"source\": [\n+    \"Add the project boundary to the map.\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"id\": \"57\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"project_boundary\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"id\": \"58\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"m.add_geojson(project_boundary, layer_name=\\\"Project Boundary\\\")\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"59\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"Add tree boundaries to the map.\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"id\": \"60\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"map_layers = project.get_map_layers()\\n\",\n+    \"tree_boundaries = map_layers[0]\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"id\": \"61\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"m.add_geojson(tree_boundaries, layer_name=\\\"Tree Boundaries\\\")\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"62\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"### Get another flight\\n\",\n+    \"\\n\",\n+    \"Retrieve the Ortho data product for the December 2022 flight.\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"id\": \"63\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"filtered_flights = flights.filter_by_date(\\n\",\n+    \"    start_date=date(2022, 12, 1), end_date=date(2022, 12, 31)\\n\",\n+    \")\\n\",\n+    \"for flight in filtered_flights:\\n\",\n+    \"    print(flight)\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"id\": \"64\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"flight_202212 = filtered_flights[0]\\n\",\n+    \"data_products = flight_202212.get_data_products()\\n\",\n+    \"ortho_data_products = data_products.filter_by_data_type(\\\"ortho\\\")\\n\",\n+    \"ortho_data = ortho_data_products[0]\\n\",\n+    \"ortho_url_202212 = ortho_data.url\\n\",\n+    \"ortho_url_202212 = leafmap.d2s_tile(ortho_url_202212)\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"id\": \"65\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"### Compare two ortho images\\n\",\n+    \"\\n\",\n+    \"Create a split map for comparing the 2022 and 2024 ortho images.\"\nComment: The comment states '2022 and 2024' but the code actually compares June 2022 (ortho_url_202206) and December 2022 (ortho_url_202212) images. The year '2024' should be '2022' to accurately reflect what the code does.\n```suggestion\n    \"Create a split map for comparing the June 2022 and December 2022 ortho images.\"\n```",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "docs/workshops/ASA_2025.ipynb",
    "pr_number": 349,
    "repo": "geoai",
    "owner": "opengeos",
    "comment_id": 2488611705,
    "comment_created_at": "2025-11-04T04:30:03Z"
  },
  {
    "code": "@@ -771,6 +771,296 @@ def calc_segmentation_metrics(\n     return results\n \n \n+class FocalLoss(torch.nn.Module):\n+    \"\"\"\n+    Focal Loss for addressing class imbalance in segmentation.\n+    \n+    Reference: Lin, T. Y., Goyal, P., Girshick, R., He, K., & Dollr, P. (2017).\n+    Focal loss for dense object detection. ICCV.\n+    \"\"\"\n+    \n+    def __init__(self, alpha=1.0, gamma=2.0, ignore_index=-100, reduction='mean', weight=None):\n+        \"\"\"\n+        Initialize Focal Loss.\n+        \n+        Args:\n+            alpha (float): Weighting factor in range (0,1) to balance positive vs negative examples\n+                or a list of weights for each class. Default: 1.0\n+            gamma (float): Exponent of the modulating factor (1 - p_t)^gamma. Default: 2.0\n+            ignore_index (int or bool): Specifies a target value that is ignored and does not \n+                contribute to the input gradient. If False, no pixels are ignored. Default: -100\n+            reduction (str): Specifies the reduction to apply to the output: \n+                'none' | 'mean' | 'sum'. Default: 'mean'\n+            weight (torch.Tensor, optional): Manual rescaling weight for each class. Default: None\n+        \"\"\"\n+        super(FocalLoss, self).__init__()\n+        self.alpha = alpha\n+        self.gamma = gamma\n+        self.ignore_index = ignore_index\n+        self.reduction = reduction\n+        self.weight = weight  # Class weights tensor\n+        \n+    def forward(self, inputs, targets):\n+        \"\"\"\n+        Compute focal loss.\n+        \n+        Args:\n+            inputs: Tensor of shape [N, C, H, W] where C is number of classes\n+            targets: Tensor of shape [N, H, W] with class indices\n+            \n+        Returns:\n+            torch.Tensor: Computed focal loss\n+        \"\"\"\n+        import torch.nn.functional as F\n+        \n+        # Handle ignore_index parameter - if False, disable ignoring\n+        if self.ignore_index is False:\n+            ignore_idx = -100  # Use a value that won't match any target\n+        else:\n+            ignore_idx = self.ignore_index\n+        \n+        # Apply log_softmax to get log probabilities\n+        log_pt = F.log_softmax(inputs, dim=1)\n+        \n+        # Use NLL loss with ignore_index and class weights to handle ignored pixels properly\n+        ce_loss = F.nll_loss(log_pt, targets, ignore_index=ignore_idx, weight=self.weight, reduction='none')\n+        \n+        # Get probabilities by exponentiating the negative cross entropy\n+        pt = torch.exp(-ce_loss)\n+        \n+        # Apply focal loss formula: FL = -alpha * (1-pt)^gamma * log(pt)\n+        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n+        \n+        # Handle reduction\n+        if self.reduction == 'mean':\n+            # Only average over non-ignored pixels\n+            if ignore_idx != -100:\n+                valid_mask = (targets != ignore_idx)\n+                if valid_mask.sum() > 0:\n+                    return focal_loss[valid_mask].mean()\n+                else:\n+                    return torch.tensor(0.0, device=inputs.device, requires_grad=True)\n+            else:\n+                return focal_loss.mean()\n+        elif self.reduction == 'sum':\n+            if ignore_idx != -100:\n+                valid_mask = (targets != ignore_idx)\n+                return focal_loss[valid_mask].sum()\n+            else:",
    "comment": "**<sub><sub>![P1 Badge](https://img.shields.io/badge/P1-orange?style=flat)</sub></sub>  FocalLoss averages ignored pixels with default ignore_index**\n\nWhen `ignore_index` is left at the PyTorch default of `-100`, the reduction branch treats it as if no pixels are ignored (`if ignore_idx != -100:`) and returns `focal_loss.mean()`/`.sum()` without masking out ignored entries. Although `torch.nn.functional.nll_loss` zeros the ignored positions, averaging over the full tensor divides by the total number of pixels rather than just the valid ones, shrinking the loss and gradients whenever many pixels are ignored. This breaks parity with `CrossEntropyLoss` and can significantly underweight the remaining pixels in common segmentation datasets that use `-100` for unlabeled areas. The masking should depend on whether ignoring is enabled, not on the sentinel value itself.\n\nUseful? React with / .",
    "line_number": 849,
    "enriched": "File: geoai/utils.py\nCode: @@ -771,6 +771,296 @@ def calc_segmentation_metrics(\n     return results\n \n \n+class FocalLoss(torch.nn.Module):\n+    \"\"\"\n+    Focal Loss for addressing class imbalance in segmentation.\n+    \n+    Reference: Lin, T. Y., Goyal, P., Girshick, R., He, K., & Dollr, P. (2017).\n+    Focal loss for dense object detection. ICCV.\n+    \"\"\"\n+    \n+    def __init__(self, alpha=1.0, gamma=2.0, ignore_index=-100, reduction='mean', weight=None):\n+        \"\"\"\n+        Initialize Focal Loss.\n+        \n+        Args:\n+            alpha (float): Weighting factor in range (0,1) to balance positive vs negative examples\n+                or a list of weights for each class. Default: 1.0\n+            gamma (float): Exponent of the modulating factor (1 - p_t)^gamma. Default: 2.0\n+            ignore_index (int or bool): Specifies a target value that is ignored and does not \n+                contribute to the input gradient. If False, no pixels are ignored. Default: -100\n+            reduction (str): Specifies the reduction to apply to the output: \n+                'none' | 'mean' | 'sum'. Default: 'mean'\n+            weight (torch.Tensor, optional): Manual rescaling weight for each class. Default: None\n+        \"\"\"\n+        super(FocalLoss, self).__init__()\n+        self.alpha = alpha\n+        self.gamma = gamma\n+        self.ignore_index = ignore_index\n+        self.reduction = reduction\n+        self.weight = weight  # Class weights tensor\n+        \n+    def forward(self, inputs, targets):\n+        \"\"\"\n+        Compute focal loss.\n+        \n+        Args:\n+            inputs: Tensor of shape [N, C, H, W] where C is number of classes\n+            targets: Tensor of shape [N, H, W] with class indices\n+            \n+        Returns:\n+            torch.Tensor: Computed focal loss\n+        \"\"\"\n+        import torch.nn.functional as F\n+        \n+        # Handle ignore_index parameter - if False, disable ignoring\n+        if self.ignore_index is False:\n+            ignore_idx = -100  # Use a value that won't match any target\n+        else:\n+            ignore_idx = self.ignore_index\n+        \n+        # Apply log_softmax to get log probabilities\n+        log_pt = F.log_softmax(inputs, dim=1)\n+        \n+        # Use NLL loss with ignore_index and class weights to handle ignored pixels properly\n+        ce_loss = F.nll_loss(log_pt, targets, ignore_index=ignore_idx, weight=self.weight, reduction='none')\n+        \n+        # Get probabilities by exponentiating the negative cross entropy\n+        pt = torch.exp(-ce_loss)\n+        \n+        # Apply focal loss formula: FL = -alpha * (1-pt)^gamma * log(pt)\n+        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n+        \n+        # Handle reduction\n+        if self.reduction == 'mean':\n+            # Only average over non-ignored pixels\n+            if ignore_idx != -100:\n+                valid_mask = (targets != ignore_idx)\n+                if valid_mask.sum() > 0:\n+                    return focal_loss[valid_mask].mean()\n+                else:\n+                    return torch.tensor(0.0, device=inputs.device, requires_grad=True)\n+            else:\n+                return focal_loss.mean()\n+        elif self.reduction == 'sum':\n+            if ignore_idx != -100:\n+                valid_mask = (targets != ignore_idx)\n+                return focal_loss[valid_mask].sum()\n+            else:\nComment: **<sub><sub>![P1 Badge](https://img.shields.io/badge/P1-orange?style=flat)</sub></sub>  FocalLoss averages ignored pixels with default ignore_index**\n\nWhen `ignore_index` is left at the PyTorch default of `-100`, the reduction branch treats it as if no pixels are ignored (`if ignore_idx != -100:`) and returns `focal_loss.mean()`/`.sum()` without masking out ignored entries. Although `torch.nn.functional.nll_loss` zeros the ignored positions, averaging over the full tensor divides by the total number of pixels rather than just the valid ones, shrinking the loss and gradients whenever many pixels are ignored. This breaks parity with `CrossEntropyLoss` and can significantly underweight the remaining pixels in common segmentation datasets that use `-100` for unlabeled areas. The masking should depend on whether ignoring is enabled, not on the sentinel value itself.\n\nUseful? React with / .",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "geoai/utils.py",
    "pr_number": 347,
    "repo": "geoai",
    "owner": "opengeos",
    "comment_id": 2479001951,
    "comment_created_at": "2025-10-30T17:49:33Z"
  },
  {
    "code": "@@ -0,0 +1,358 @@\n+\"\"\"\n+MultiClean integration utilities for cleaning segmentation results.\n+\n+This module provides functions to use MultiClean (https://github.com/DPIRD-DMA/MultiClean)\n+for post-processing segmentation masks and classification rasters. MultiClean performs\n+morphological operations to smooth edges, remove noise islands, and fill gaps.\n+\"\"\"\n+\n+import os\n+from typing import Optional, List, Union, Tuple\n+import numpy as np\n+\n+try:\n+    from multiclean import clean_array\n+\n+    MULTICLEAN_AVAILABLE = True\n+except ImportError:\n+    MULTICLEAN_AVAILABLE = False\n+\n+try:\n+    import rasterio\n+\n+    RASTERIO_AVAILABLE = True\n+except ImportError:\n+    RASTERIO_AVAILABLE = False\n+\n+\n+def check_multiclean_available():\n+    \"\"\"\n+    Check if multiclean is installed.\n+\n+    Raises:\n+        ImportError: If multiclean is not installed.\n+    \"\"\"\n+    if not MULTICLEAN_AVAILABLE:\n+        raise ImportError(\n+            \"multiclean is not installed. \"\n+            \"Please install it with: pip install multiclean \"\n+            \"or: pip install geoai-py[extra]\"\n+        )\n+\n+\n+def clean_segmentation_mask(\n+    mask: np.ndarray,\n+    class_values: Optional[Union[int, List[int]]] = None,\n+    smooth_edge_size: int = 2,\n+    min_island_size: int = 100,\n+    connectivity: int = 8,\n+    max_workers: Optional[int] = None,\n+    fill_nan: bool = False,\n+) -> np.ndarray:\n+    \"\"\"\n+    Clean a segmentation mask using MultiClean morphological operations.\n+\n+    This function applies three cleaning operations:\n+    1. Edge smoothing - Uses morphological opening to reduce jagged boundaries\n+    2. Island removal - Eliminates small connected components (noise)\n+    3. Gap filling - Replaces invalid pixels with nearest valid class\n+\n+    Args:\n+        mask (np.ndarray): 2D numpy array containing segmentation classes.\n+            Can be int or float. NaN values are treated as nodata.\n+        class_values (int, list of int, or None): Target class values to process.\n+            If None, auto-detects unique values from the mask. Defaults to None.\n+        smooth_edge_size (int): Kernel width in pixels for edge smoothing.\n+            Set to 0 to disable smoothing. Defaults to 2.\n+        min_island_size (int): Minimum area (in pixels) for connected components.\n+            Components with area strictly less than this are removed. Defaults to 100.\n+        connectivity (int): Connectivity for component detection. Use 4 or 8.\n+            8-connectivity considers diagonal neighbors. Defaults to 8.\n+        max_workers (int, optional): Thread pool size for parallel processing.\n+            If None, uses default threading. Defaults to None.\n+        fill_nan (bool): Whether to fill NaN pixels with nearest valid class.\n+            Defaults to False.\n+\n+    Returns:\n+        np.ndarray: Cleaned 2D segmentation mask with same shape as input.\n+\n+    Raises:\n+        ImportError: If multiclean is not installed.\n+        ValueError: If mask is not 2D or if connectivity is not 4 or 8.\n+\n+    Example:\n+        >>> import numpy as np\n+        >>> from geoai.tools.multiclean import clean_segmentation_mask\n+        >>> mask = np.random.randint(0, 3, (512, 512))\n+        >>> cleaned = clean_segmentation_mask(\n+        ...     mask,\n+        ...     class_values=[0, 1, 2],\n+        ...     smooth_edge_size=2,\n+        ...     min_island_size=50\n+        ... )\n+    \"\"\"\n+    check_multiclean_available()\n+\n+    if mask.ndim != 2:\n+        raise ValueError(f\"Mask must be 2D, got shape {mask.shape}\")\n+\n+    if connectivity not in [4, 8]:\n+        raise ValueError(f\"Connectivity must be 4 or 8, got {connectivity}\")\n+\n+    # Apply MultiClean\n+    cleaned = clean_array(\n+        mask,\n+        class_values=class_values,\n+        smooth_edge_size=smooth_edge_size,\n+        min_island_size=min_island_size,\n+        connectivity=connectivity,\n+        max_workers=max_workers,\n+        fill_nan=fill_nan,\n+    )\n+\n+    return cleaned\n+\n+\n+def clean_raster(\n+    input_path: str,\n+    output_path: str,\n+    class_values: Optional[Union[int, List[int]]] = None,\n+    smooth_edge_size: int = 2,\n+    min_island_size: int = 100,\n+    connectivity: int = 8,\n+    max_workers: Optional[int] = None,\n+    fill_nan: bool = False,\n+    band: int = 1,\n+    nodata: Optional[float] = None,\n+) -> None:\n+    \"\"\"\n+    Clean a classification raster (GeoTIFF) and save the result.\n+\n+    Reads a GeoTIFF file, applies MultiClean morphological operations,\n+    and saves the cleaned result while preserving geospatial metadata\n+    (CRS, transform, nodata value).\n+\n+    Args:\n+        input_path (str): Path to input GeoTIFF file.\n+        output_path (str): Path to save cleaned GeoTIFF file.\n+        class_values (int, list of int, or None): Target class values to process.\n+            If None, auto-detects unique values. Defaults to None.\n+        smooth_edge_size (int): Kernel width in pixels for edge smoothing.\n+            Defaults to 2.\n+        min_island_size (int): Minimum area (in pixels) for components.\n+            Defaults to 100.\n+        connectivity (int): Connectivity for component detection (4 or 8).\n+            Defaults to 8.\n+        max_workers (int, optional): Thread pool size. Defaults to None.\n+        fill_nan (bool): Whether to fill NaN/nodata pixels. Defaults to False.\n+        band (int): Band index to read (1-indexed). Defaults to 1.\n+        nodata (float, optional): Nodata value to use. If None, uses value\n+            from input file. Defaults to None.\n+\n+    Returns:\n+        None: Writes cleaned raster to output_path.\n+\n+    Raises:\n+        ImportError: If multiclean or rasterio is not installed.\n+        FileNotFoundError: If input_path does not exist.\n+\n+    Example:\n+        >>> from geoai.tools.multiclean import clean_raster\n+        >>> clean_raster(\n+        ...     \"segmentation_raw.tif\",\n+        ...     \"segmentation_cleaned.tif\",\n+        ...     class_values=[0, 1, 2],\n+        ...     smooth_edge_size=3,\n+        ...     min_island_size=50\n+        ... )\n+    \"\"\"\n+    check_multiclean_available()\n+\n+    if not RASTERIO_AVAILABLE:\n+        raise ImportError(\n+            \"rasterio is required for raster operations. \"\n+            \"Please install it with: pip install rasterio\"\n+        )\n+\n+    if not os.path.exists(input_path):\n+        raise FileNotFoundError(f\"Input file not found: {input_path}\")\n+\n+    # Read input raster\n+    with rasterio.open(input_path) as src:\n+        # Read the specified band\n+        mask = src.read(band)\n+\n+        # Get metadata\n+        profile = src.profile.copy()\n+\n+        # Handle nodata\n+        if nodata is None:\n+            nodata = src.nodata\n+\n+        # Convert nodata to NaN if specified\n+        if nodata is not None:\n+            mask = mask.astype(np.float32)\n+            mask[mask == nodata] = np.nan\n+\n+    # Clean the mask\n+    cleaned = clean_segmentation_mask(\n+        mask,\n+        class_values=class_values,\n+        smooth_edge_size=smooth_edge_size,\n+        min_island_size=min_island_size,\n+        connectivity=connectivity,\n+        max_workers=max_workers,\n+        fill_nan=fill_nan,\n+    )\n+\n+    # Convert NaN back to nodata if needed\n+    if nodata is not None and fill_nan:\n+        # If fill_nan is True, there should be no NaN values\n+        pass\n+    elif nodata is not None:\n+        # Convert NaN back to nodata value",
    "comment": "The empty `pass` block at lines 210-211 serves no purpose and reduces code clarity. If fill_nan is True and there should be no NaN values, this condition can be removed entirely by restructuring the logic to only check `elif nodata is not None:` and handle the NaN conversion unconditionally when nodata is set.\n```suggestion\n    if nodata is not None:\n        # Convert any remaining NaN values back to nodata value\n```",
    "line_number": 213,
    "enriched": "File: geoai/tools/multiclean.py\nCode: @@ -0,0 +1,358 @@\n+\"\"\"\n+MultiClean integration utilities for cleaning segmentation results.\n+\n+This module provides functions to use MultiClean (https://github.com/DPIRD-DMA/MultiClean)\n+for post-processing segmentation masks and classification rasters. MultiClean performs\n+morphological operations to smooth edges, remove noise islands, and fill gaps.\n+\"\"\"\n+\n+import os\n+from typing import Optional, List, Union, Tuple\n+import numpy as np\n+\n+try:\n+    from multiclean import clean_array\n+\n+    MULTICLEAN_AVAILABLE = True\n+except ImportError:\n+    MULTICLEAN_AVAILABLE = False\n+\n+try:\n+    import rasterio\n+\n+    RASTERIO_AVAILABLE = True\n+except ImportError:\n+    RASTERIO_AVAILABLE = False\n+\n+\n+def check_multiclean_available():\n+    \"\"\"\n+    Check if multiclean is installed.\n+\n+    Raises:\n+        ImportError: If multiclean is not installed.\n+    \"\"\"\n+    if not MULTICLEAN_AVAILABLE:\n+        raise ImportError(\n+            \"multiclean is not installed. \"\n+            \"Please install it with: pip install multiclean \"\n+            \"or: pip install geoai-py[extra]\"\n+        )\n+\n+\n+def clean_segmentation_mask(\n+    mask: np.ndarray,\n+    class_values: Optional[Union[int, List[int]]] = None,\n+    smooth_edge_size: int = 2,\n+    min_island_size: int = 100,\n+    connectivity: int = 8,\n+    max_workers: Optional[int] = None,\n+    fill_nan: bool = False,\n+) -> np.ndarray:\n+    \"\"\"\n+    Clean a segmentation mask using MultiClean morphological operations.\n+\n+    This function applies three cleaning operations:\n+    1. Edge smoothing - Uses morphological opening to reduce jagged boundaries\n+    2. Island removal - Eliminates small connected components (noise)\n+    3. Gap filling - Replaces invalid pixels with nearest valid class\n+\n+    Args:\n+        mask (np.ndarray): 2D numpy array containing segmentation classes.\n+            Can be int or float. NaN values are treated as nodata.\n+        class_values (int, list of int, or None): Target class values to process.\n+            If None, auto-detects unique values from the mask. Defaults to None.\n+        smooth_edge_size (int): Kernel width in pixels for edge smoothing.\n+            Set to 0 to disable smoothing. Defaults to 2.\n+        min_island_size (int): Minimum area (in pixels) for connected components.\n+            Components with area strictly less than this are removed. Defaults to 100.\n+        connectivity (int): Connectivity for component detection. Use 4 or 8.\n+            8-connectivity considers diagonal neighbors. Defaults to 8.\n+        max_workers (int, optional): Thread pool size for parallel processing.\n+            If None, uses default threading. Defaults to None.\n+        fill_nan (bool): Whether to fill NaN pixels with nearest valid class.\n+            Defaults to False.\n+\n+    Returns:\n+        np.ndarray: Cleaned 2D segmentation mask with same shape as input.\n+\n+    Raises:\n+        ImportError: If multiclean is not installed.\n+        ValueError: If mask is not 2D or if connectivity is not 4 or 8.\n+\n+    Example:\n+        >>> import numpy as np\n+        >>> from geoai.tools.multiclean import clean_segmentation_mask\n+        >>> mask = np.random.randint(0, 3, (512, 512))\n+        >>> cleaned = clean_segmentation_mask(\n+        ...     mask,\n+        ...     class_values=[0, 1, 2],\n+        ...     smooth_edge_size=2,\n+        ...     min_island_size=50\n+        ... )\n+    \"\"\"\n+    check_multiclean_available()\n+\n+    if mask.ndim != 2:\n+        raise ValueError(f\"Mask must be 2D, got shape {mask.shape}\")\n+\n+    if connectivity not in [4, 8]:\n+        raise ValueError(f\"Connectivity must be 4 or 8, got {connectivity}\")\n+\n+    # Apply MultiClean\n+    cleaned = clean_array(\n+        mask,\n+        class_values=class_values,\n+        smooth_edge_size=smooth_edge_size,\n+        min_island_size=min_island_size,\n+        connectivity=connectivity,\n+        max_workers=max_workers,\n+        fill_nan=fill_nan,\n+    )\n+\n+    return cleaned\n+\n+\n+def clean_raster(\n+    input_path: str,\n+    output_path: str,\n+    class_values: Optional[Union[int, List[int]]] = None,\n+    smooth_edge_size: int = 2,\n+    min_island_size: int = 100,\n+    connectivity: int = 8,\n+    max_workers: Optional[int] = None,\n+    fill_nan: bool = False,\n+    band: int = 1,\n+    nodata: Optional[float] = None,\n+) -> None:\n+    \"\"\"\n+    Clean a classification raster (GeoTIFF) and save the result.\n+\n+    Reads a GeoTIFF file, applies MultiClean morphological operations,\n+    and saves the cleaned result while preserving geospatial metadata\n+    (CRS, transform, nodata value).\n+\n+    Args:\n+        input_path (str): Path to input GeoTIFF file.\n+        output_path (str): Path to save cleaned GeoTIFF file.\n+        class_values (int, list of int, or None): Target class values to process.\n+            If None, auto-detects unique values. Defaults to None.\n+        smooth_edge_size (int): Kernel width in pixels for edge smoothing.\n+            Defaults to 2.\n+        min_island_size (int): Minimum area (in pixels) for components.\n+            Defaults to 100.\n+        connectivity (int): Connectivity for component detection (4 or 8).\n+            Defaults to 8.\n+        max_workers (int, optional): Thread pool size. Defaults to None.\n+        fill_nan (bool): Whether to fill NaN/nodata pixels. Defaults to False.\n+        band (int): Band index to read (1-indexed). Defaults to 1.\n+        nodata (float, optional): Nodata value to use. If None, uses value\n+            from input file. Defaults to None.\n+\n+    Returns:\n+        None: Writes cleaned raster to output_path.\n+\n+    Raises:\n+        ImportError: If multiclean or rasterio is not installed.\n+        FileNotFoundError: If input_path does not exist.\n+\n+    Example:\n+        >>> from geoai.tools.multiclean import clean_raster\n+        >>> clean_raster(\n+        ...     \"segmentation_raw.tif\",\n+        ...     \"segmentation_cleaned.tif\",\n+        ...     class_values=[0, 1, 2],\n+        ...     smooth_edge_size=3,\n+        ...     min_island_size=50\n+        ... )\n+    \"\"\"\n+    check_multiclean_available()\n+\n+    if not RASTERIO_AVAILABLE:\n+        raise ImportError(\n+            \"rasterio is required for raster operations. \"\n+            \"Please install it with: pip install rasterio\"\n+        )\n+\n+    if not os.path.exists(input_path):\n+        raise FileNotFoundError(f\"Input file not found: {input_path}\")\n+\n+    # Read input raster\n+    with rasterio.open(input_path) as src:\n+        # Read the specified band\n+        mask = src.read(band)\n+\n+        # Get metadata\n+        profile = src.profile.copy()\n+\n+        # Handle nodata\n+        if nodata is None:\n+            nodata = src.nodata\n+\n+        # Convert nodata to NaN if specified\n+        if nodata is not None:\n+            mask = mask.astype(np.float32)\n+            mask[mask == nodata] = np.nan\n+\n+    # Clean the mask\n+    cleaned = clean_segmentation_mask(\n+        mask,\n+        class_values=class_values,\n+        smooth_edge_size=smooth_edge_size,\n+        min_island_size=min_island_size,\n+        connectivity=connectivity,\n+        max_workers=max_workers,\n+        fill_nan=fill_nan,\n+    )\n+\n+    # Convert NaN back to nodata if needed\n+    if nodata is not None and fill_nan:\n+        # If fill_nan is True, there should be no NaN values\n+        pass\n+    elif nodata is not None:\n+        # Convert NaN back to nodata value\nComment: The empty `pass` block at lines 210-211 serves no purpose and reduces code clarity. If fill_nan is True and there should be no NaN values, this condition can be removed entirely by restructuring the logic to only check `elif nodata is not None:` and handle the NaN conversion unconditionally when nodata is set.\n```suggestion\n    if nodata is not None:\n        # Convert any remaining NaN values back to nodata value\n```",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "geoai/tools/multiclean.py",
    "pr_number": 344,
    "repo": "geoai",
    "owner": "opengeos",
    "comment_id": 2465936014,
    "comment_created_at": "2025-10-27T14:37:01Z"
  },
  {
    "code": "@@ -1917,6 +1917,96 @@ def __call__(\n         return image, mask\n \n \n+class SemanticRandomVerticalFlip:\n+    \"\"\"Random vertical flip transform for semantic segmentation.\"\"\"\n+\n+    def __init__(self, prob: float = 0.5) -> None:\n+        self.prob = prob\n+\n+    def __call__(\n+        self, image: torch.Tensor, mask: torch.Tensor\n+    ) -> Tuple[torch.Tensor, torch.Tensor]:\n+        if random.random() < self.prob:\n+            # Flip image and mask along height dimension\n+            image = torch.flip(image, dims=[1])\n+            mask = torch.flip(mask, dims=[0])",
    "comment": "Inconsistent flip dimensions between image and mask. For vertical flip, the image uses dim [1] (height dimension in CHW format) while mask uses dim [0] (also height in HW format), which is correct. However, this should match the horizontal flip pattern seen elsewhere in the codebase. Verify that SemanticRandomHorizontalFlip uses dims=[2] for image and dims=[1] for mask to ensure consistency.",
    "line_number": 1932,
    "enriched": "File: geoai/train.py\nCode: @@ -1917,6 +1917,96 @@ def __call__(\n         return image, mask\n \n \n+class SemanticRandomVerticalFlip:\n+    \"\"\"Random vertical flip transform for semantic segmentation.\"\"\"\n+\n+    def __init__(self, prob: float = 0.5) -> None:\n+        self.prob = prob\n+\n+    def __call__(\n+        self, image: torch.Tensor, mask: torch.Tensor\n+    ) -> Tuple[torch.Tensor, torch.Tensor]:\n+        if random.random() < self.prob:\n+            # Flip image and mask along height dimension\n+            image = torch.flip(image, dims=[1])\n+            mask = torch.flip(mask, dims=[0])\nComment: Inconsistent flip dimensions between image and mask. For vertical flip, the image uses dim [1] (height dimension in CHW format) while mask uses dim [0] (also height in HW format), which is correct. However, this should match the horizontal flip pattern seen elsewhere in the codebase. Verify that SemanticRandomHorizontalFlip uses dims=[2] for image and dims=[1] for mask to ensure consistency.",
    "subcategory": "design discussion",
    "category": "discussion",
    "file_path": "geoai/train.py",
    "pr_number": 343,
    "repo": "geoai",
    "owner": "opengeos",
    "comment_id": 2465753699,
    "comment_created_at": "2025-10-27T13:48:45Z"
  },
  {
    "code": "@@ -55,7 +55,10 @@ def __call__(self, **kwargs):\n                 # Make tool names more user-friendly\n                 friendly_name = tool_name.replace(\"_\", \" \").title()\n                 self.status_widget.value = (\n-                    f\"<span style='color:#0a7'> {friendly_name}...</span>\"\n+                    f\"<link rel='stylesheet' href='https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css'>\"",
    "comment": "The FontAwesome stylesheet link is being injected every time a tool is called, causing redundant HTTP requests and DOM updates. Consider loading the stylesheet once at initialization (e.g., in `__init__` or at the widget creation level) or check if it's already loaded before injecting it.",
    "line_number": 58,
    "enriched": "File: geoai/agents/geo_agents.py\nCode: @@ -55,7 +55,10 @@ def __call__(self, **kwargs):\n                 # Make tool names more user-friendly\n                 friendly_name = tool_name.replace(\"_\", \" \").title()\n                 self.status_widget.value = (\n-                    f\"<span style='color:#0a7'> {friendly_name}...</span>\"\n+                    f\"<link rel='stylesheet' href='https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css'>\"\nComment: The FontAwesome stylesheet link is being injected every time a tool is called, causing redundant HTTP requests and DOM updates. Consider loading the stylesheet once at initialization (e.g., in `__init__` or at the widget creation level) or check if it's already loaded before injecting it.",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "geoai/agents/geo_agents.py",
    "pr_number": 333,
    "repo": "geoai",
    "owner": "opengeos",
    "comment_id": 2443802961,
    "comment_created_at": "2025-10-20T04:44:13Z"
  },
  {
    "code": "@@ -462,7 +498,14 @@ def _on_send(_: Any = None) -> None:\n             _lock(True)\n             self._ui.status.value = \"<span style='color:#0a7'>Running</span>\"\n             try:\n-                out = self.ask(text)  # fresh Agent/model per call, silent\n+                # Create a callback handler that updates the status widget\n+                callback_handler = UICallbackHandler(status_widget=self._ui.status)\n+\n+                # Temporarily set callback_handler for this call\n+                old_callback = self.callback_handler\n+                self.callback_handler = callback_handler",
    "comment": "If an exception occurs before line 518, the original callback handler won't be restored. Move the callback restoration to a finally block or use a context manager to ensure cleanup happens even if `self.ask(text)` raises an exception before reaching the existing finally block.",
    "line_number": 506,
    "enriched": "File: geoai/agents/geo_agents.py\nCode: @@ -462,7 +498,14 @@ def _on_send(_: Any = None) -> None:\n             _lock(True)\n             self._ui.status.value = \"<span style='color:#0a7'>Running</span>\"\n             try:\n-                out = self.ask(text)  # fresh Agent/model per call, silent\n+                # Create a callback handler that updates the status widget\n+                callback_handler = UICallbackHandler(status_widget=self._ui.status)\n+\n+                # Temporarily set callback_handler for this call\n+                old_callback = self.callback_handler\n+                self.callback_handler = callback_handler\nComment: If an exception occurs before line 518, the original callback handler won't be restored. Move the callback restoration to a finally block or use a context manager to ensure cleanup happens even if `self.ask(text)` raises an exception before reaching the existing finally block.",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "geoai/agents/geo_agents.py",
    "pr_number": 332,
    "repo": "geoai",
    "owner": "opengeos",
    "comment_id": 2443673856,
    "comment_created_at": "2025-10-20T02:30:49Z"
  },
  {
    "code": "@@ -0,0 +1,907 @@\n+\"\"\"Tools for searching data catalogs.\"\"\"\n+\n+import io\n+import json\n+from typing import Any, Dict, List, Optional, Union\n+\n+import pandas as pd\n+import requests\n+from strands import tool\n+\n+from .catalog_models import CatalogDatasetInfo, CatalogSearchResult, LocationInfo\n+\n+\n+class CatalogTools:\n+    \"\"\"Collection of tools for searching and interacting with data catalogs.\"\"\"\n+\n+    # Common location cache to avoid repeated geocoding\n+    _LOCATION_CACHE = {\n+        \"san francisco\": {\n+            \"name\": \"San Francisco\",\n+            \"bbox\": [-122.5155, 37.7034, -122.3549, 37.8324],\n+            \"center\": [-122.4194, 37.7749],\n+        },\n+        \"new york\": {\n+            \"name\": \"New York\",\n+            \"bbox\": [-74.0479, 40.6829, -73.9067, 40.8820],\n+            \"center\": [-73.9352, 40.7306],\n+        },\n+        \"new york city\": {\n+            \"name\": \"New York City\",\n+            \"bbox\": [-74.0479, 40.6829, -73.9067, 40.8820],\n+            \"center\": [-73.9352, 40.7306],\n+        },\n+        \"paris\": {\n+            \"name\": \"Paris\",\n+            \"bbox\": [2.2241, 48.8156, 2.4698, 48.9022],\n+            \"center\": [2.3522, 48.8566],\n+        },\n+        \"london\": {\n+            \"name\": \"London\",\n+            \"bbox\": [-0.5103, 51.2868, 0.3340, 51.6919],\n+            \"center\": [-0.1276, 51.5074],\n+        },\n+        \"tokyo\": {\n+            \"name\": \"Tokyo\",\n+            \"bbox\": [139.5694, 35.5232, 139.9182, 35.8173],\n+            \"center\": [139.6917, 35.6895],\n+        },\n+        \"los angeles\": {\n+            \"name\": \"Los Angeles\",\n+            \"bbox\": [-118.6682, 33.7037, -118.1553, 34.3373],\n+            \"center\": [-118.2437, 34.0522],\n+        },\n+        \"chicago\": {\n+            \"name\": \"Chicago\",\n+            \"bbox\": [-87.9401, 41.6445, -87.5241, 42.0230],\n+            \"center\": [-87.6298, 41.8781],\n+        },\n+        \"seattle\": {\n+            \"name\": \"Seattle\",\n+            \"bbox\": [-122.4595, 47.4810, -122.2244, 47.7341],\n+            \"center\": [-122.3321, 47.6062],\n+        },\n+        \"california\": {\n+            \"name\": \"California\",\n+            \"bbox\": [-124.4820, 32.5288, -114.1315, 42.0095],\n+            \"center\": [-119.4179, 36.7783],\n+        },\n+        \"las vegas\": {\n+            \"name\": \"Las Vegas\",\n+            \"bbox\": [-115.3711, 35.9630, -114.9372, 36.2610],\n+            \"center\": [-115.1400, 36.1177],\n+        },\n+    }\n+\n+    def __init__(\n+        self,\n+        catalog_url: Optional[str] = None,\n+        catalog_df: Optional[pd.DataFrame] = None,\n+    ) -> None:\n+        \"\"\"Initialize CatalogTools.\n+\n+        Args:\n+            catalog_url: URL to a catalog file (TSV, CSV, or JSON). If None, must provide catalog_df.\n+            catalog_df: Pre-loaded catalog as a pandas DataFrame. If None, must provide catalog_url.\n+        \"\"\"\n+        self.catalog_url = catalog_url\n+        self._catalog_df = catalog_df\n+        self._cache = {}\n+        # Runtime cache for geocoding results\n+        self._geocode_cache = {}\n+\n+        # Load catalog if URL provided\n+        if catalog_url and catalog_df is None:\n+            self._catalog_df = self._load_catalog(catalog_url)\n+\n+    def _load_catalog(self, url: str) -> pd.DataFrame:\n+        \"\"\"Load catalog from a URL.\n+\n+        Args:\n+            url: URL to catalog file (TSV, CSV, or JSON).\n+\n+        Returns:\n+            DataFrame containing catalog data.\n+        \"\"\"\n+        # Check cache first\n+        if url in self._cache:\n+            return self._cache[url]\n+\n+        try:\n+            # Download the file\n+            response = requests.get(url, timeout=30)\n+            response.raise_for_status()\n+\n+            # Determine file type and parse\n+            if url.endswith(\".tsv\"):\n+                df = pd.read_csv(io.StringIO(response.text), sep=\"\\t\")\n+            elif url.endswith(\".csv\"):\n+                df = pd.read_csv(io.StringIO(response.text))\n+            elif url.endswith(\".json\"):\n+                df = pd.read_json(io.StringIO(response.text))\n+            else:\n+                # Try to auto-detect (default to TSV)\n+                df = pd.read_csv(io.StringIO(response.text), sep=\"\\t\")\n+\n+            # Cache the result\n+            self._cache[url] = df\n+            return df\n+\n+        except Exception as e:\n+            raise ValueError(f\"Failed to load catalog from {url}: {str(e)}\")\n+\n+    def _parse_bbox_string(self, bbox_str: str) -> Optional[List[float]]:\n+        \"\"\"Parse a bbox string to a list of floats.\n+\n+        Args:\n+            bbox_str: Bounding box string in format \"minLon, minLat, maxLon, maxLat\".\n+\n+        Returns:\n+            List of floats [minLon, minLat, maxLon, maxLat] or None if parsing fails.\n+        \"\"\"\n+        try:\n+            if pd.isna(bbox_str) or not bbox_str:\n+                return None\n+            parts = str(bbox_str).split(\",\")\n+            if len(parts) != 4:\n+                return None\n+            bbox = [float(p.strip()) for p in parts]\n+            return bbox\n+        except (ValueError, AttributeError):\n+            return None\n+\n+    def _bbox_intersects(self, bbox1: List[float], bbox2: List[float]) -> bool:\n+        \"\"\"Check if two bounding boxes intersect.\n+\n+        Args:\n+            bbox1: First bbox as [minLon, minLat, maxLon, maxLat].\n+            bbox2: Second bbox as [minLon, minLat, maxLon, maxLat].\n+\n+        Returns:\n+            True if bboxes intersect, False otherwise.\n+        \"\"\"\n+        # Check if boxes do NOT intersect, then negate\n+        # bbox1 is completely to the left, right, below, or above bbox2\n+        return not (\n+            bbox1[2] < bbox2[0]  # bbox1 maxLon < bbox2 minLon (left of)\n+            or bbox1[0] > bbox2[2]  # bbox1 minLon > bbox2 maxLon (right of)\n+            or bbox1[3] < bbox2[1]  # bbox1 maxLat < bbox2 minLat (below)\n+            or bbox1[1] > bbox2[3]  # bbox1 minLat > bbox2 maxLat (above)\n+        )\n+\n+    def _bbox_contains_point(self, bbox: List[float], lon: float, lat: float) -> bool:\n+        \"\"\"Check if a bounding box contains a point.\n+\n+        Args:\n+            bbox: Bounding box as [minLon, minLat, maxLon, maxLat].\n+            lon: Longitude of the point.\n+            lat: Latitude of the point.\n+\n+        Returns:\n+            True if bbox contains the point, False otherwise.\n+        \"\"\"\n+        return bbox[0] <= lon <= bbox[2] and bbox[1] <= lat <= bbox[3]\n+\n+    def _search_dataframe(\n+        self,\n+        df: pd.DataFrame,\n+        keywords: Optional[str] = None,\n+        dataset_type: Optional[str] = None,\n+        provider: Optional[str] = None,\n+        start_date: Optional[str] = None,\n+        end_date: Optional[str] = None,\n+        max_results: int = 10,\n+    ) -> pd.DataFrame:\n+        \"\"\"Search dataframe with filters.\n+\n+        Args:\n+            df: DataFrame to search.\n+            keywords: Keywords to search for (searches in id, title, keywords, description).\n+            dataset_type: Filter by dataset type.\n+            provider: Filter by provider.\n+            start_date: Filter datasets that have data after this date (YYYY-MM-DD).\n+            end_date: Filter datasets that have data before this date (YYYY-MM-DD).\n+            max_results: Maximum number of results to return.\n+\n+        Returns:\n+            Filtered DataFrame.\n+        \"\"\"\n+        result_df = df.copy()\n+\n+        # Apply keyword search\n+        if keywords:\n+            keyword_lower = keywords.lower()\n+            mask = pd.Series([False] * len(result_df), index=result_df.index)\n+\n+            # Search in id\n+            if \"id\" in result_df.columns:\n+                mask |= (\n+                    result_df[\"id\"]\n+                    .astype(str)\n+                    .str.lower()\n+                    .str.contains(keyword_lower, na=False)\n+                )\n+\n+            # Search in title\n+            if \"title\" in result_df.columns:\n+                mask |= (\n+                    result_df[\"title\"]\n+                    .astype(str)\n+                    .str.lower()\n+                    .str.contains(keyword_lower, na=False)\n+                )\n+\n+            # Search in keywords\n+            if \"keywords\" in result_df.columns:\n+                mask |= (\n+                    result_df[\"keywords\"]\n+                    .astype(str)\n+                    .str.lower()\n+                    .str.contains(keyword_lower, na=False)\n+                )\n+\n+            # Search in description\n+            if \"description\" in result_df.columns:\n+                mask |= (\n+                    result_df[\"description\"]\n+                    .astype(str)\n+                    .str.lower()\n+                    .str.contains(keyword_lower, na=False)\n+                )\n+\n+            result_df = result_df[mask]\n+\n+        # Filter by type\n+        if dataset_type and \"type\" in result_df.columns:\n+            result_df = result_df[\n+                result_df[\"type\"]\n+                .astype(str)\n+                .str.lower()\n+                .str.contains(dataset_type.lower(), na=False)\n+            ]\n+\n+        # Filter by provider\n+        if provider and \"provider\" in result_df.columns:\n+            result_df = result_df[\n+                result_df[\"provider\"]\n+                .astype(str)\n+                .str.lower()\n+                .str.contains(provider.lower(), na=False)\n+            ]\n+\n+        # Filter by temporal range\n+        if start_date and \"end_date\" in result_df.columns:\n+            # Keep datasets where end_date >= start_date (dataset has data after start_date)\n+            result_df = result_df[\n+                (result_df[\"end_date\"].notna()) & (result_df[\"end_date\"] >= start_date)\n+            ]\n+\n+        if end_date and \"state_date\" in result_df.columns:\n+            # Keep datasets where state_date <= end_date (dataset has data before end_date)\n+            result_df = result_df[\n+                (result_df[\"state_date\"].notna())\n+                & (result_df[\"state_date\"] <= end_date)",
    "comment": "Corrected spelling of 'state_date' to 'start_date'.\n```suggestion\n        if end_date and \"start_date\" in result_df.columns:\n            # Keep datasets where start_date <= end_date (dataset has data before end_date)\n            result_df = result_df[\n                (result_df[\"start_date\"].notna())\n                & (result_df[\"start_date\"] <= end_date)\n```",
    "line_number": 283,
    "enriched": "File: geoai/agents/catalog_tools.py\nCode: @@ -0,0 +1,907 @@\n+\"\"\"Tools for searching data catalogs.\"\"\"\n+\n+import io\n+import json\n+from typing import Any, Dict, List, Optional, Union\n+\n+import pandas as pd\n+import requests\n+from strands import tool\n+\n+from .catalog_models import CatalogDatasetInfo, CatalogSearchResult, LocationInfo\n+\n+\n+class CatalogTools:\n+    \"\"\"Collection of tools for searching and interacting with data catalogs.\"\"\"\n+\n+    # Common location cache to avoid repeated geocoding\n+    _LOCATION_CACHE = {\n+        \"san francisco\": {\n+            \"name\": \"San Francisco\",\n+            \"bbox\": [-122.5155, 37.7034, -122.3549, 37.8324],\n+            \"center\": [-122.4194, 37.7749],\n+        },\n+        \"new york\": {\n+            \"name\": \"New York\",\n+            \"bbox\": [-74.0479, 40.6829, -73.9067, 40.8820],\n+            \"center\": [-73.9352, 40.7306],\n+        },\n+        \"new york city\": {\n+            \"name\": \"New York City\",\n+            \"bbox\": [-74.0479, 40.6829, -73.9067, 40.8820],\n+            \"center\": [-73.9352, 40.7306],\n+        },\n+        \"paris\": {\n+            \"name\": \"Paris\",\n+            \"bbox\": [2.2241, 48.8156, 2.4698, 48.9022],\n+            \"center\": [2.3522, 48.8566],\n+        },\n+        \"london\": {\n+            \"name\": \"London\",\n+            \"bbox\": [-0.5103, 51.2868, 0.3340, 51.6919],\n+            \"center\": [-0.1276, 51.5074],\n+        },\n+        \"tokyo\": {\n+            \"name\": \"Tokyo\",\n+            \"bbox\": [139.5694, 35.5232, 139.9182, 35.8173],\n+            \"center\": [139.6917, 35.6895],\n+        },\n+        \"los angeles\": {\n+            \"name\": \"Los Angeles\",\n+            \"bbox\": [-118.6682, 33.7037, -118.1553, 34.3373],\n+            \"center\": [-118.2437, 34.0522],\n+        },\n+        \"chicago\": {\n+            \"name\": \"Chicago\",\n+            \"bbox\": [-87.9401, 41.6445, -87.5241, 42.0230],\n+            \"center\": [-87.6298, 41.8781],\n+        },\n+        \"seattle\": {\n+            \"name\": \"Seattle\",\n+            \"bbox\": [-122.4595, 47.4810, -122.2244, 47.7341],\n+            \"center\": [-122.3321, 47.6062],\n+        },\n+        \"california\": {\n+            \"name\": \"California\",\n+            \"bbox\": [-124.4820, 32.5288, -114.1315, 42.0095],\n+            \"center\": [-119.4179, 36.7783],\n+        },\n+        \"las vegas\": {\n+            \"name\": \"Las Vegas\",\n+            \"bbox\": [-115.3711, 35.9630, -114.9372, 36.2610],\n+            \"center\": [-115.1400, 36.1177],\n+        },\n+    }\n+\n+    def __init__(\n+        self,\n+        catalog_url: Optional[str] = None,\n+        catalog_df: Optional[pd.DataFrame] = None,\n+    ) -> None:\n+        \"\"\"Initialize CatalogTools.\n+\n+        Args:\n+            catalog_url: URL to a catalog file (TSV, CSV, or JSON). If None, must provide catalog_df.\n+            catalog_df: Pre-loaded catalog as a pandas DataFrame. If None, must provide catalog_url.\n+        \"\"\"\n+        self.catalog_url = catalog_url\n+        self._catalog_df = catalog_df\n+        self._cache = {}\n+        # Runtime cache for geocoding results\n+        self._geocode_cache = {}\n+\n+        # Load catalog if URL provided\n+        if catalog_url and catalog_df is None:\n+            self._catalog_df = self._load_catalog(catalog_url)\n+\n+    def _load_catalog(self, url: str) -> pd.DataFrame:\n+        \"\"\"Load catalog from a URL.\n+\n+        Args:\n+            url: URL to catalog file (TSV, CSV, or JSON).\n+\n+        Returns:\n+            DataFrame containing catalog data.\n+        \"\"\"\n+        # Check cache first\n+        if url in self._cache:\n+            return self._cache[url]\n+\n+        try:\n+            # Download the file\n+            response = requests.get(url, timeout=30)\n+            response.raise_for_status()\n+\n+            # Determine file type and parse\n+            if url.endswith(\".tsv\"):\n+                df = pd.read_csv(io.StringIO(response.text), sep=\"\\t\")\n+            elif url.endswith(\".csv\"):\n+                df = pd.read_csv(io.StringIO(response.text))\n+            elif url.endswith(\".json\"):\n+                df = pd.read_json(io.StringIO(response.text))\n+            else:\n+                # Try to auto-detect (default to TSV)\n+                df = pd.read_csv(io.StringIO(response.text), sep=\"\\t\")\n+\n+            # Cache the result\n+            self._cache[url] = df\n+            return df\n+\n+        except Exception as e:\n+            raise ValueError(f\"Failed to load catalog from {url}: {str(e)}\")\n+\n+    def _parse_bbox_string(self, bbox_str: str) -> Optional[List[float]]:\n+        \"\"\"Parse a bbox string to a list of floats.\n+\n+        Args:\n+            bbox_str: Bounding box string in format \"minLon, minLat, maxLon, maxLat\".\n+\n+        Returns:\n+            List of floats [minLon, minLat, maxLon, maxLat] or None if parsing fails.\n+        \"\"\"\n+        try:\n+            if pd.isna(bbox_str) or not bbox_str:\n+                return None\n+            parts = str(bbox_str).split(\",\")\n+            if len(parts) != 4:\n+                return None\n+            bbox = [float(p.strip()) for p in parts]\n+            return bbox\n+        except (ValueError, AttributeError):\n+            return None\n+\n+    def _bbox_intersects(self, bbox1: List[float], bbox2: List[float]) -> bool:\n+        \"\"\"Check if two bounding boxes intersect.\n+\n+        Args:\n+            bbox1: First bbox as [minLon, minLat, maxLon, maxLat].\n+            bbox2: Second bbox as [minLon, minLat, maxLon, maxLat].\n+\n+        Returns:\n+            True if bboxes intersect, False otherwise.\n+        \"\"\"\n+        # Check if boxes do NOT intersect, then negate\n+        # bbox1 is completely to the left, right, below, or above bbox2\n+        return not (\n+            bbox1[2] < bbox2[0]  # bbox1 maxLon < bbox2 minLon (left of)\n+            or bbox1[0] > bbox2[2]  # bbox1 minLon > bbox2 maxLon (right of)\n+            or bbox1[3] < bbox2[1]  # bbox1 maxLat < bbox2 minLat (below)\n+            or bbox1[1] > bbox2[3]  # bbox1 minLat > bbox2 maxLat (above)\n+        )\n+\n+    def _bbox_contains_point(self, bbox: List[float], lon: float, lat: float) -> bool:\n+        \"\"\"Check if a bounding box contains a point.\n+\n+        Args:\n+            bbox: Bounding box as [minLon, minLat, maxLon, maxLat].\n+            lon: Longitude of the point.\n+            lat: Latitude of the point.\n+\n+        Returns:\n+            True if bbox contains the point, False otherwise.\n+        \"\"\"\n+        return bbox[0] <= lon <= bbox[2] and bbox[1] <= lat <= bbox[3]\n+\n+    def _search_dataframe(\n+        self,\n+        df: pd.DataFrame,\n+        keywords: Optional[str] = None,\n+        dataset_type: Optional[str] = None,\n+        provider: Optional[str] = None,\n+        start_date: Optional[str] = None,\n+        end_date: Optional[str] = None,\n+        max_results: int = 10,\n+    ) -> pd.DataFrame:\n+        \"\"\"Search dataframe with filters.\n+\n+        Args:\n+            df: DataFrame to search.\n+            keywords: Keywords to search for (searches in id, title, keywords, description).\n+            dataset_type: Filter by dataset type.\n+            provider: Filter by provider.\n+            start_date: Filter datasets that have data after this date (YYYY-MM-DD).\n+            end_date: Filter datasets that have data before this date (YYYY-MM-DD).\n+            max_results: Maximum number of results to return.\n+\n+        Returns:\n+            Filtered DataFrame.\n+        \"\"\"\n+        result_df = df.copy()\n+\n+        # Apply keyword search\n+        if keywords:\n+            keyword_lower = keywords.lower()\n+            mask = pd.Series([False] * len(result_df), index=result_df.index)\n+\n+            # Search in id\n+            if \"id\" in result_df.columns:\n+                mask |= (\n+                    result_df[\"id\"]\n+                    .astype(str)\n+                    .str.lower()\n+                    .str.contains(keyword_lower, na=False)\n+                )\n+\n+            # Search in title\n+            if \"title\" in result_df.columns:\n+                mask |= (\n+                    result_df[\"title\"]\n+                    .astype(str)\n+                    .str.lower()\n+                    .str.contains(keyword_lower, na=False)\n+                )\n+\n+            # Search in keywords\n+            if \"keywords\" in result_df.columns:\n+                mask |= (\n+                    result_df[\"keywords\"]\n+                    .astype(str)\n+                    .str.lower()\n+                    .str.contains(keyword_lower, na=False)\n+                )\n+\n+            # Search in description\n+            if \"description\" in result_df.columns:\n+                mask |= (\n+                    result_df[\"description\"]\n+                    .astype(str)\n+                    .str.lower()\n+                    .str.contains(keyword_lower, na=False)\n+                )\n+\n+            result_df = result_df[mask]\n+\n+        # Filter by type\n+        if dataset_type and \"type\" in result_df.columns:\n+            result_df = result_df[\n+                result_df[\"type\"]\n+                .astype(str)\n+                .str.lower()\n+                .str.contains(dataset_type.lower(), na=False)\n+            ]\n+\n+        # Filter by provider\n+        if provider and \"provider\" in result_df.columns:\n+            result_df = result_df[\n+                result_df[\"provider\"]\n+                .astype(str)\n+                .str.lower()\n+                .str.contains(provider.lower(), na=False)\n+            ]\n+\n+        # Filter by temporal range\n+        if start_date and \"end_date\" in result_df.columns:\n+            # Keep datasets where end_date >= start_date (dataset has data after start_date)\n+            result_df = result_df[\n+                (result_df[\"end_date\"].notna()) & (result_df[\"end_date\"] >= start_date)\n+            ]\n+\n+        if end_date and \"state_date\" in result_df.columns:\n+            # Keep datasets where state_date <= end_date (dataset has data before end_date)\n+            result_df = result_df[\n+                (result_df[\"state_date\"].notna())\n+                & (result_df[\"state_date\"] <= end_date)\nComment: Corrected spelling of 'state_date' to 'start_date'.\n```suggestion\n        if end_date and \"start_date\" in result_df.columns:\n            # Keep datasets where start_date <= end_date (dataset has data before end_date)\n            result_df = result_df[\n                (result_df[\"start_date\"].notna())\n                & (result_df[\"start_date\"] <= end_date)\n```",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "geoai/agents/catalog_tools.py",
    "pr_number": 328,
    "repo": "geoai",
    "owner": "opengeos",
    "comment_id": 2442455331,
    "comment_created_at": "2025-10-18T14:59:05Z"
  },
  {
    "code": "@@ -0,0 +1,321 @@\n+\"\"\"Tools for STAC catalog search and interaction.\"\"\"\n+\n+import json\n+from typing import Any, Dict, List, Optional\n+\n+from strands import tool\n+\n+from ..download import pc_collection_list, pc_stac_search\n+from .stac_models import (\n+    LocationInfo,\n+    STACAssetInfo,\n+    STACCollectionInfo,\n+    STACItemInfo,\n+    STACSearchResult,\n+)\n+\n+\n+class STACTools:\n+    \"\"\"Collection of tools for searching and interacting with STAC catalogs.\"\"\"\n+\n+    def __init__(\n+        self,\n+        endpoint: str = \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n+    ) -> None:\n+        \"\"\"Initialize STAC tools.\n+\n+        Args:\n+            endpoint: STAC API endpoint URL. Defaults to Microsoft Planetary Computer.\n+        \"\"\"\n+        self.endpoint = endpoint\n+\n+    @tool(\n+        description=\"List and search available STAC collections from Planetary Computer\"\n+    )\n+    def list_collections(\n+        self,\n+        filter_keyword: Optional[str] = None,\n+        detailed: bool = False,\n+    ) -> str:\n+        \"\"\"List available STAC collections from Planetary Computer.\n+\n+        Args:\n+            filter_keyword: Optional keyword to filter collections (searches in id, title, description).\n+            detailed: If True, return detailed information including temporal extent, license, etc.\n+\n+        Returns:\n+            JSON string containing list of collections with their metadata.\n+        \"\"\"\n+        try:\n+            # Get collections using existing function\n+            df = pc_collection_list(\n+                endpoint=self.endpoint,\n+                detailed=detailed,\n+                filter_by=None,\n+                sort_by=\"id\",\n+            )\n+\n+            # Apply keyword filtering if specified\n+            if filter_keyword:\n+                mask = df[\"id\"].str.contains(filter_keyword, case=False, na=False) | df[\n+                    \"title\"\n+                ].str.contains(filter_keyword, case=False, na=False)\n+                if \"description\" in df.columns:\n+                    mask |= df[\"description\"].str.contains(\n+                        filter_keyword, case=False, na=False\n+                    )\n+                df = df[mask]\n+\n+            # Convert to list of dictionaries\n+            collections = df.to_dict(\"records\")\n+\n+            # Convert to structured models\n+            collection_models = []\n+            for col in collections:\n+                collection_models.append(\n+                    STACCollectionInfo(\n+                        id=col.get(\"id\", \"\"),\n+                        title=col.get(\"title\", \"\"),\n+                        description=col.get(\"description\"),\n+                        license=col.get(\"license\"),\n+                        temporal_extent=col.get(\"temporal_extent\"),\n+                        spatial_extent=col.get(\"bbox\"),\n+                        providers=col.get(\"providers\"),\n+                        keywords=col.get(\"keywords\"),\n+                    )\n+                )\n+\n+            result = {\n+                \"count\": len(collection_models),\n+                \"filter_keyword\": filter_keyword,\n+                \"collections\": [c.model_dump() for c in collection_models],\n+            }\n+\n+            return json.dumps(result, indent=2)\n+\n+        except Exception as e:\n+            return json.dumps({\"error\": str(e)})\n+\n+    @tool(\n+        description=\"Search for STAC items in a specific collection with optional filters\"\n+    )\n+    def search_items(\n+        self,\n+        collection: str,\n+        bbox: Optional[List[float]] = None,\n+        time_range: Optional[str] = None,\n+        query: Optional[Dict[str, Any]] = None,\n+        limit: Optional[int] = 10,\n+        max_items: Optional[int] = None,\n+    ) -> str:\n+        \"\"\"Search for STAC items in the Planetary Computer catalog.\n+\n+        Args:\n+            collection: Collection ID to search (e.g., \"sentinel-2-l2a\", \"naip\", \"landsat-c2-l2\").\n+            bbox: Bounding box as [west, south, east, north] in WGS84 coordinates.\n+                Example: [-122.5, 37.7, -122.3, 37.8] for San Francisco area.\n+            time_range: Time range as \"start/end\" string in ISO format.\n+                Example: \"2024-09-01/2024-09-30\" or \"2024-09-01/2024-09-01\" for single day.\n+            query: Query parameters for filtering.\n+                Example: {\"eo:cloud_cover\": {\"lt\": 10}} for cloud cover less than 10%.\n+            limit: Number of items to return per page.\n+                Example: 10 for 10 items per page.\n+            max_items: Maximum number of items to return (default: 10).\n+\n+        Returns:\n+            JSON string containing search results with item details including IDs, URLs, and metadata.\n+        \"\"\"\n+        try:\n+            # Search using existing function\n+            items = pc_stac_search(\n+                collection=collection,\n+                bbox=bbox,\n+                time_range=time_range,\n+                query=query,\n+                limit=limit,\n+                max_items=max_items,\n+                endpoint=self.endpoint,\n+            )\n+\n+            # Convert to structured models\n+            item_models = []\n+            for item in items:\n+                # Extract assets\n+                assets = []\n+                for key, asset in item.assets.items():\n+                    assets.append(\n+                        STACAssetInfo(\n+                            key=key,\n+                            title=asset.title,\n+                        )\n+                    )\n+\n+                item_models.append(\n+                    STACItemInfo(\n+                        id=item.id,\n+                        collection=item.collection_id,\n+                        datetime=str(item.datetime) if item.datetime else None,\n+                        bbox=list(item.bbox) if item.bbox else None,\n+                        assets=assets,\n+                        # properties=item.properties,\n+                    )\n+                )\n+\n+            # Create search result\n+            result = STACSearchResult(\n+                query=f\"Collection: {collection}\",\n+                collection=collection,\n+                item_count=len(item_models),\n+                items=item_models,\n+                bbox=bbox,\n+                time_range=time_range,\n+            )\n+\n+            return json.dumps(result.model_dump(), indent=2)\n+\n+        except Exception as e:\n+            return json.dumps({\"error\": str(e)})\n+\n+    @tool(description=\"Get detailed information about a specific STAC item\")\n+    def get_item_info(\n+        self,\n+        item_id: str,\n+        collection: str,\n+    ) -> str:\n+        \"\"\"Get detailed information about a specific STAC item.\n+\n+        Args:\n+            item_id: The STAC item ID to retrieve.\n+            collection: The collection ID containing the item.\n+\n+        Returns:\n+            JSON string with detailed item information including all assets and metadata.\n+        \"\"\"\n+        try:\n+            # Search for the specific item\n+            items = pc_stac_search(\n+                collection=collection,\n+                bbox=None,\n+                time_range=None,\n+                query={\"id\": {\"eq\": item_id}},\n+                limit=1,\n+                max_items=1,\n+                endpoint=self.endpoint,\n+            )\n+",
    "comment": "Filtering by item ID via query={'id': {'eq': ...}} is not part of the STAC Search spec and typically won't work on Planetary Computer (the correct way is using the top-level 'ids' parameter). This can produce empty results for valid IDs. Fix by either (a) querying with pystac_client directly using ids=[item_id], or (b) updating pc_stac_search to accept an ids parameter and passing it through to Client.search(ids=...).",
    "line_number": 205,
    "enriched": "File: geoai/agents/stac_tools.py\nCode: @@ -0,0 +1,321 @@\n+\"\"\"Tools for STAC catalog search and interaction.\"\"\"\n+\n+import json\n+from typing import Any, Dict, List, Optional\n+\n+from strands import tool\n+\n+from ..download import pc_collection_list, pc_stac_search\n+from .stac_models import (\n+    LocationInfo,\n+    STACAssetInfo,\n+    STACCollectionInfo,\n+    STACItemInfo,\n+    STACSearchResult,\n+)\n+\n+\n+class STACTools:\n+    \"\"\"Collection of tools for searching and interacting with STAC catalogs.\"\"\"\n+\n+    def __init__(\n+        self,\n+        endpoint: str = \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n+    ) -> None:\n+        \"\"\"Initialize STAC tools.\n+\n+        Args:\n+            endpoint: STAC API endpoint URL. Defaults to Microsoft Planetary Computer.\n+        \"\"\"\n+        self.endpoint = endpoint\n+\n+    @tool(\n+        description=\"List and search available STAC collections from Planetary Computer\"\n+    )\n+    def list_collections(\n+        self,\n+        filter_keyword: Optional[str] = None,\n+        detailed: bool = False,\n+    ) -> str:\n+        \"\"\"List available STAC collections from Planetary Computer.\n+\n+        Args:\n+            filter_keyword: Optional keyword to filter collections (searches in id, title, description).\n+            detailed: If True, return detailed information including temporal extent, license, etc.\n+\n+        Returns:\n+            JSON string containing list of collections with their metadata.\n+        \"\"\"\n+        try:\n+            # Get collections using existing function\n+            df = pc_collection_list(\n+                endpoint=self.endpoint,\n+                detailed=detailed,\n+                filter_by=None,\n+                sort_by=\"id\",\n+            )\n+\n+            # Apply keyword filtering if specified\n+            if filter_keyword:\n+                mask = df[\"id\"].str.contains(filter_keyword, case=False, na=False) | df[\n+                    \"title\"\n+                ].str.contains(filter_keyword, case=False, na=False)\n+                if \"description\" in df.columns:\n+                    mask |= df[\"description\"].str.contains(\n+                        filter_keyword, case=False, na=False\n+                    )\n+                df = df[mask]\n+\n+            # Convert to list of dictionaries\n+            collections = df.to_dict(\"records\")\n+\n+            # Convert to structured models\n+            collection_models = []\n+            for col in collections:\n+                collection_models.append(\n+                    STACCollectionInfo(\n+                        id=col.get(\"id\", \"\"),\n+                        title=col.get(\"title\", \"\"),\n+                        description=col.get(\"description\"),\n+                        license=col.get(\"license\"),\n+                        temporal_extent=col.get(\"temporal_extent\"),\n+                        spatial_extent=col.get(\"bbox\"),\n+                        providers=col.get(\"providers\"),\n+                        keywords=col.get(\"keywords\"),\n+                    )\n+                )\n+\n+            result = {\n+                \"count\": len(collection_models),\n+                \"filter_keyword\": filter_keyword,\n+                \"collections\": [c.model_dump() for c in collection_models],\n+            }\n+\n+            return json.dumps(result, indent=2)\n+\n+        except Exception as e:\n+            return json.dumps({\"error\": str(e)})\n+\n+    @tool(\n+        description=\"Search for STAC items in a specific collection with optional filters\"\n+    )\n+    def search_items(\n+        self,\n+        collection: str,\n+        bbox: Optional[List[float]] = None,\n+        time_range: Optional[str] = None,\n+        query: Optional[Dict[str, Any]] = None,\n+        limit: Optional[int] = 10,\n+        max_items: Optional[int] = None,\n+    ) -> str:\n+        \"\"\"Search for STAC items in the Planetary Computer catalog.\n+\n+        Args:\n+            collection: Collection ID to search (e.g., \"sentinel-2-l2a\", \"naip\", \"landsat-c2-l2\").\n+            bbox: Bounding box as [west, south, east, north] in WGS84 coordinates.\n+                Example: [-122.5, 37.7, -122.3, 37.8] for San Francisco area.\n+            time_range: Time range as \"start/end\" string in ISO format.\n+                Example: \"2024-09-01/2024-09-30\" or \"2024-09-01/2024-09-01\" for single day.\n+            query: Query parameters for filtering.\n+                Example: {\"eo:cloud_cover\": {\"lt\": 10}} for cloud cover less than 10%.\n+            limit: Number of items to return per page.\n+                Example: 10 for 10 items per page.\n+            max_items: Maximum number of items to return (default: 10).\n+\n+        Returns:\n+            JSON string containing search results with item details including IDs, URLs, and metadata.\n+        \"\"\"\n+        try:\n+            # Search using existing function\n+            items = pc_stac_search(\n+                collection=collection,\n+                bbox=bbox,\n+                time_range=time_range,\n+                query=query,\n+                limit=limit,\n+                max_items=max_items,\n+                endpoint=self.endpoint,\n+            )\n+\n+            # Convert to structured models\n+            item_models = []\n+            for item in items:\n+                # Extract assets\n+                assets = []\n+                for key, asset in item.assets.items():\n+                    assets.append(\n+                        STACAssetInfo(\n+                            key=key,\n+                            title=asset.title,\n+                        )\n+                    )\n+\n+                item_models.append(\n+                    STACItemInfo(\n+                        id=item.id,\n+                        collection=item.collection_id,\n+                        datetime=str(item.datetime) if item.datetime else None,\n+                        bbox=list(item.bbox) if item.bbox else None,\n+                        assets=assets,\n+                        # properties=item.properties,\n+                    )\n+                )\n+\n+            # Create search result\n+            result = STACSearchResult(\n+                query=f\"Collection: {collection}\",\n+                collection=collection,\n+                item_count=len(item_models),\n+                items=item_models,\n+                bbox=bbox,\n+                time_range=time_range,\n+            )\n+\n+            return json.dumps(result.model_dump(), indent=2)\n+\n+        except Exception as e:\n+            return json.dumps({\"error\": str(e)})\n+\n+    @tool(description=\"Get detailed information about a specific STAC item\")\n+    def get_item_info(\n+        self,\n+        item_id: str,\n+        collection: str,\n+    ) -> str:\n+        \"\"\"Get detailed information about a specific STAC item.\n+\n+        Args:\n+            item_id: The STAC item ID to retrieve.\n+            collection: The collection ID containing the item.\n+\n+        Returns:\n+            JSON string with detailed item information including all assets and metadata.\n+        \"\"\"\n+        try:\n+            # Search for the specific item\n+            items = pc_stac_search(\n+                collection=collection,\n+                bbox=None,\n+                time_range=None,\n+                query={\"id\": {\"eq\": item_id}},\n+                limit=1,\n+                max_items=1,\n+                endpoint=self.endpoint,\n+            )\n+\nComment: Filtering by item ID via query={'id': {'eq': ...}} is not part of the STAC Search spec and typically won't work on Planetary Computer (the correct way is using the top-level 'ids' parameter). This can produce empty results for valid IDs. Fix by either (a) querying with pystac_client directly using ids=[item_id], or (b) updating pc_stac_search to accept an ids parameter and passing it through to Client.search(ids=...).",
    "subcategory": "logical",
    "category": "functional",
    "file_path": "geoai/agents/stac_tools.py",
    "pr_number": 327,
    "repo": "geoai",
    "owner": "opengeos",
    "comment_id": 2440443639,
    "comment_created_at": "2025-10-17T15:48:30Z"
  },
  {
    "code": "@@ -3176,6 +3179,34 @@ def semantic_inference_on_geotiff(\n             if not quiet:\n                 print(f\"Saved probability map to {probability_path}\")\n \n+            # Save individual class probabilities if requested\n+            if save_class_probabilities:\n+                # Prepare single-band metadata\n+                single_band_meta = meta.copy()\n+                single_band_meta.update({\"count\": 1, \"dtype\": \"float32\"})\n+\n+                # Get base filename and extension\n+                prob_base = os.path.splitext(probability_path)[0]\n+                prob_ext = os.path.splitext(probability_path)[1]\n+\n+                for class_idx in range(num_classes):\n+                    # Create filename for this class\n+                    class_prob_path = f\"{prob_base}_class_{class_idx}{prob_ext}\"\n+\n+                    # Normalize probabilities\n+                    prob_band = np.zeros((height, width), dtype=np.float32)\n+                    prob_band[valid_pixels] = (\n+                        prob_accumulator[class_idx, valid_pixels]\n+                        / count_accumulator[valid_pixels]\n+                    )",
    "comment": "This probability normalization logic is duplicated from the multi-band probability saving code above (lines 3172-3177). Consider extracting this into a helper function to reduce code duplication.",
    "line_number": 3201,
    "enriched": "File: geoai/train.py\nCode: @@ -3176,6 +3179,34 @@ def semantic_inference_on_geotiff(\n             if not quiet:\n                 print(f\"Saved probability map to {probability_path}\")\n \n+            # Save individual class probabilities if requested\n+            if save_class_probabilities:\n+                # Prepare single-band metadata\n+                single_band_meta = meta.copy()\n+                single_band_meta.update({\"count\": 1, \"dtype\": \"float32\"})\n+\n+                # Get base filename and extension\n+                prob_base = os.path.splitext(probability_path)[0]\n+                prob_ext = os.path.splitext(probability_path)[1]\n+\n+                for class_idx in range(num_classes):\n+                    # Create filename for this class\n+                    class_prob_path = f\"{prob_base}_class_{class_idx}{prob_ext}\"\n+\n+                    # Normalize probabilities\n+                    prob_band = np.zeros((height, width), dtype=np.float32)\n+                    prob_band[valid_pixels] = (\n+                        prob_accumulator[class_idx, valid_pixels]\n+                        / count_accumulator[valid_pixels]\n+                    )\nComment: This probability normalization logic is duplicated from the multi-band probability saving code above (lines 3172-3177). Consider extracting this into a helper function to reduce code duplication.",
    "subcategory": "code organization",
    "category": "refactoring",
    "file_path": "geoai/train.py",
    "pr_number": 326,
    "repo": "geoai",
    "owner": "opengeos",
    "comment_id": 2437543312,
    "comment_created_at": "2025-10-16T21:46:48Z"
  },
  {
    "code": "@@ -8765,18 +8765,42 @@ def write_colormap(\n \n def plot_performance_metrics(\n     history_path: str,\n-    figsize: Tuple[int, int] = (15, 5),\n+    figsize: Optional[Tuple[int, int]] = None,\n     verbose: bool = True,\n     save_path: Optional[str] = None,\n+    csv_path: Optional[str] = None,\n     kwargs: Optional[Dict] = None,\n-) -> None:\n-    \"\"\"Plot performance metrics from a history object.\n+) -> pd.DataFrame:\n+    \"\"\"Plot performance metrics from a training history object and return as DataFrame.\n+\n+    This function loads training history, plots available metrics (loss, IoU, F1,\n+    precision, recall), optionally exports to CSV, and returns all metrics as a\n+    pandas DataFrame for further analysis.\n \n     Args:\n-        history_path: The history object to plot.\n-        figsize: The figure size.\n-        verbose: Whether to print the best and final metrics.\n+        history_path (str): Path to the saved training history (.pth file).\n+        figsize (Optional[Tuple[int, int]]): Figure size in inches. If None,\n+            automatically determined based on number of metrics.\n+        verbose (bool): Whether to print best and final metric values. Defaults to True.\n+        save_path (Optional[str]): Path to save the plot image. If None, plot is not saved.\n+        csv_path (Optional[str]): Path to export metrics as CSV. If None, CSV is not exported.\n+        kwargs (Optional[Dict]): Additional keyword arguments for plt.savefig().\n+\n+    Returns:\n+        pd.DataFrame: DataFrame containing all metrics with columns for epoch and each metric.\n+            Columns include: 'epoch', 'train_loss', 'val_loss', 'val_iou', 'val_f1',\n+            'val_precision', 'val_recall' (depending on availability in history).\n+\n+    Example:\n+        >>> df = plot_performance_metrics(\n+        ...     'training_history.pth',\n+        ...     save_path='metrics_plot.png',\n+        ...     csv_path='metrics.csv'\n+        ... )\n+        >>> print(df.head())\n     \"\"\"\n+    import pandas as pd\n+",
    "comment": "The pandas import should be placed at the module level (top of the file) rather than inside the function. This follows Python conventions and avoids repeated imports if the function is called multiple times.\n```suggestion\n\n```",
    "line_number": 8803,
    "enriched": "File: geoai/utils.py\nCode: @@ -8765,18 +8765,42 @@ def write_colormap(\n \n def plot_performance_metrics(\n     history_path: str,\n-    figsize: Tuple[int, int] = (15, 5),\n+    figsize: Optional[Tuple[int, int]] = None,\n     verbose: bool = True,\n     save_path: Optional[str] = None,\n+    csv_path: Optional[str] = None,\n     kwargs: Optional[Dict] = None,\n-) -> None:\n-    \"\"\"Plot performance metrics from a history object.\n+) -> pd.DataFrame:\n+    \"\"\"Plot performance metrics from a training history object and return as DataFrame.\n+\n+    This function loads training history, plots available metrics (loss, IoU, F1,\n+    precision, recall), optionally exports to CSV, and returns all metrics as a\n+    pandas DataFrame for further analysis.\n \n     Args:\n-        history_path: The history object to plot.\n-        figsize: The figure size.\n-        verbose: Whether to print the best and final metrics.\n+        history_path (str): Path to the saved training history (.pth file).\n+        figsize (Optional[Tuple[int, int]]): Figure size in inches. If None,\n+            automatically determined based on number of metrics.\n+        verbose (bool): Whether to print best and final metric values. Defaults to True.\n+        save_path (Optional[str]): Path to save the plot image. If None, plot is not saved.\n+        csv_path (Optional[str]): Path to export metrics as CSV. If None, CSV is not exported.\n+        kwargs (Optional[Dict]): Additional keyword arguments for plt.savefig().\n+\n+    Returns:\n+        pd.DataFrame: DataFrame containing all metrics with columns for epoch and each metric.\n+            Columns include: 'epoch', 'train_loss', 'val_loss', 'val_iou', 'val_f1',\n+            'val_precision', 'val_recall' (depending on availability in history).\n+\n+    Example:\n+        >>> df = plot_performance_metrics(\n+        ...     'training_history.pth',\n+        ...     save_path='metrics_plot.png',\n+        ...     csv_path='metrics.csv'\n+        ... )\n+        >>> print(df.head())\n     \"\"\"\n+    import pandas as pd\n+\nComment: The pandas import should be placed at the module level (top of the file) rather than inside the function. This follows Python conventions and avoids repeated imports if the function is called multiple times.\n```suggestion\n\n```",
    "subcategory": "interface",
    "category": "functional",
    "file_path": "geoai/utils.py",
    "pr_number": 325,
    "repo": "geoai",
    "owner": "opengeos",
    "comment_id": 2437511376,
    "comment_created_at": "2025-10-16T21:29:29Z"
  },
  {
    "code": "@@ -8785,12 +8785,17 @@ def plot_performance_metrics(\n     train_loss_key = \"train_losses\" if \"train_losses\" in history else \"train_loss\"\n     val_loss_key = \"val_losses\" if \"val_losses\" in history else \"val_loss\"\n     val_iou_key = \"val_ious\" if \"val_ious\" in history else \"val_iou\"\n-    val_dice_key = \"val_dices\" if \"val_dices\" in history else \"val_dice\"\n+    # Support both new (f1) and old (dice) key formats for backward compatibility\n+    val_f1_key = (",
    "comment": "This nested ternary operator is complex and hard to read. Consider breaking it into multiple lines or using a helper function to determine the appropriate key.\n```suggestion\n    if \"val_f1s\" in history:\n        val_f1_key = \"val_f1s\"\n    elif \"val_dices\" in history:\n        val_f1_key = \"val_dices\"\n    else:\n        val_f1_key = \"val_dice\"\n```",
    "line_number": 8789,
    "enriched": "File: geoai/utils.py\nCode: @@ -8785,12 +8785,17 @@ def plot_performance_metrics(\n     train_loss_key = \"train_losses\" if \"train_losses\" in history else \"train_loss\"\n     val_loss_key = \"val_losses\" if \"val_losses\" in history else \"val_loss\"\n     val_iou_key = \"val_ious\" if \"val_ious\" in history else \"val_iou\"\n-    val_dice_key = \"val_dices\" if \"val_dices\" in history else \"val_dice\"\n+    # Support both new (f1) and old (dice) key formats for backward compatibility\n+    val_f1_key = (\nComment: This nested ternary operator is complex and hard to read. Consider breaking it into multiple lines or using a helper function to determine the appropriate key.\n```suggestion\n    if \"val_f1s\" in history:\n        val_f1_key = \"val_f1s\"\n    elif \"val_dices\" in history:\n        val_f1_key = \"val_dices\"\n    else:\n        val_f1_key = \"val_dice\"\n```",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "geoai/utils.py",
    "pr_number": 318,
    "repo": "geoai",
    "owner": "opengeos",
    "comment_id": 2422372408,
    "comment_created_at": "2025-10-11T02:07:48Z"
  },
  {
    "code": "@@ -383,6 +383,392 @@ def calc_stats(dataset, divide_by: float = 1.0) -> Tuple[np.ndarray, np.ndarray]\n     return accum_mean / len(files), accum_std / len(files)\n \n \n+def calc_iou(\n+    ground_truth: Union[str, np.ndarray, torch.Tensor],\n+    prediction: Union[str, np.ndarray, torch.Tensor],\n+    num_classes: Optional[int] = None,\n+    ignore_index: Optional[int] = None,\n+    smooth: float = 1e-6,\n+    band: int = 1,\n+) -> Union[float, np.ndarray]:\n+    \"\"\"\n+    Calculate Intersection over Union (IoU) between ground truth and prediction masks.\n+\n+    This function computes the IoU metric for segmentation tasks. It supports both\n+    binary and multi-class segmentation, and can handle numpy arrays, PyTorch tensors,\n+    or file paths to raster files.\n+\n+    Args:\n+        ground_truth (Union[str, np.ndarray, torch.Tensor]): Ground truth segmentation mask.\n+            Can be a file path (str) to a raster file, numpy array, or PyTorch tensor.\n+            For binary segmentation: shape (H, W) with values {0, 1}.\n+            For multi-class segmentation: shape (H, W) with class indices.\n+        prediction (Union[str, np.ndarray, torch.Tensor]): Predicted segmentation mask.\n+            Can be a file path (str) to a raster file, numpy array, or PyTorch tensor.\n+            Should have the same shape and format as ground_truth.\n+        num_classes (Optional[int], optional): Number of classes for multi-class segmentation.\n+            If None, assumes binary segmentation. Defaults to None.\n+        ignore_index (Optional[int], optional): Class index to ignore in computation.\n+            Useful for ignoring background or unlabeled pixels. Defaults to None.\n+        smooth (float, optional): Smoothing factor to avoid division by zero.\n+            Defaults to 1e-6.\n+        band (int, optional): Band index to read from raster file (1-based indexing).\n+            Only used when input is a file path. Defaults to 1.\n+\n+    Returns:\n+        Union[float, np.ndarray]: For binary segmentation, returns a single float IoU score.\n+            For multi-class segmentation, returns an array of IoU scores for each class.\n+\n+    Examples:\n+        >>> # Binary segmentation with arrays\n+        >>> gt = np.array([[0, 0, 1, 1], [0, 1, 1, 1]])\n+        >>> pred = np.array([[0, 0, 1, 1], [0, 0, 1, 1]])\n+        >>> iou = calc_iou(gt, pred)\n+        >>> print(f\"IoU: {iou:.4f}\")\n+        IoU: 0.8333\n+\n+        >>> # Multi-class segmentation\n+        >>> gt = np.array([[0, 0, 1, 1], [0, 2, 2, 1]])\n+        >>> pred = np.array([[0, 0, 1, 1], [0, 0, 2, 2]])\n+        >>> iou = calc_iou(gt, pred, num_classes=3)\n+        >>> print(f\"IoU per class: {iou}\")\n+        IoU per class: [0.8333 0.5000 0.5000]\n+\n+        >>> # Using PyTorch tensors\n+        >>> gt_tensor = torch.tensor([[0, 0, 1, 1], [0, 1, 1, 1]])\n+        >>> pred_tensor = torch.tensor([[0, 0, 1, 1], [0, 0, 1, 1]])\n+        >>> iou = calc_iou(gt_tensor, pred_tensor)\n+        >>> print(f\"IoU: {iou:.4f}\")\n+        IoU: 0.8333\n+\n+        >>> # Using raster file paths\n+        >>> iou = calc_iou(\"ground_truth.tif\", \"prediction.tif\", num_classes=3)\n+        >>> print(f\"Mean IoU: {np.nanmean(iou):.4f}\")\n+        Mean IoU: 0.7500\n+    \"\"\"\n+    # Load from file if string path is provided\n+    if isinstance(ground_truth, str):\n+        with rasterio.open(ground_truth) as src:\n+            ground_truth = src.read(band)\n+    if isinstance(prediction, str):\n+        with rasterio.open(prediction) as src:\n+            prediction = src.read(band)\n+\n+    # Convert to numpy if torch tensor\n+    if isinstance(ground_truth, torch.Tensor):\n+        ground_truth = ground_truth.cpu().numpy()\n+    if isinstance(prediction, torch.Tensor):\n+        prediction = prediction.cpu().numpy()\n+\n+    # Ensure inputs have the same shape\n+    if ground_truth.shape != prediction.shape:\n+        raise ValueError(\n+            f\"Shape mismatch: ground_truth {ground_truth.shape} vs prediction {prediction.shape}\"\n+        )\n+\n+    # Binary segmentation\n+    if num_classes is None:\n+        ground_truth = ground_truth.astype(bool)\n+        prediction = prediction.astype(bool)\n+\n+        intersection = np.logical_and(ground_truth, prediction).sum()\n+        union = np.logical_or(ground_truth, prediction).sum()\n+\n+        if union == 0:\n+            return 1.0 if intersection == 0 else 0.0\n+\n+        iou = (intersection + smooth) / (union + smooth)\n+        return float(iou)\n+\n+    # Multi-class segmentation\n+    else:\n+        iou_per_class = []\n+\n+        for class_idx in range(num_classes):\n+            # Skip ignored class\n+            if ignore_index is not None and class_idx == ignore_index:\n+                continue\n+\n+            # Create binary masks for current class\n+            gt_class = (ground_truth == class_idx).astype(bool)\n+            pred_class = (prediction == class_idx).astype(bool)\n+\n+            intersection = np.logical_and(gt_class, pred_class).sum()\n+            union = np.logical_or(gt_class, pred_class).sum()\n+\n+            if union == 0:\n+                # If class is not present in both gt and pred\n+                iou_per_class.append(np.nan)\n+            else:\n+                iou_per_class.append((intersection + smooth) / (union + smooth))\n+\n+        return np.array(iou_per_class)\n+\n+\n+def calc_f1_score(\n+    ground_truth: Union[str, np.ndarray, torch.Tensor],\n+    prediction: Union[str, np.ndarray, torch.Tensor],\n+    num_classes: Optional[int] = None,\n+    ignore_index: Optional[int] = None,\n+    smooth: float = 1e-6,\n+    band: int = 1,\n+) -> Union[float, np.ndarray]:\n+    \"\"\"\n+    Calculate F1 score between ground truth and prediction masks.\n+\n+    The F1 score is the harmonic mean of precision and recall, computed as:\n+    F1 = 2 * (precision * recall) / (precision + recall)\n+    where precision = TP / (TP + FP) and recall = TP / (TP + FN).\n+\n+    This function supports both binary and multi-class segmentation, and can handle\n+    numpy arrays, PyTorch tensors, or file paths to raster files.\n+\n+    Args:\n+        ground_truth (Union[str, np.ndarray, torch.Tensor]): Ground truth segmentation mask.\n+            Can be a file path (str) to a raster file, numpy array, or PyTorch tensor.\n+            For binary segmentation: shape (H, W) with values {0, 1}.\n+            For multi-class segmentation: shape (H, W) with class indices.\n+        prediction (Union[str, np.ndarray, torch.Tensor]): Predicted segmentation mask.\n+            Can be a file path (str) to a raster file, numpy array, or PyTorch tensor.\n+            Should have the same shape and format as ground_truth.\n+        num_classes (Optional[int], optional): Number of classes for multi-class segmentation.\n+            If None, assumes binary segmentation. Defaults to None.\n+        ignore_index (Optional[int], optional): Class index to ignore in computation.\n+            Useful for ignoring background or unlabeled pixels. Defaults to None.\n+        smooth (float, optional): Smoothing factor to avoid division by zero.\n+            Defaults to 1e-6.\n+        band (int, optional): Band index to read from raster file (1-based indexing).\n+            Only used when input is a file path. Defaults to 1.\n+\n+    Returns:\n+        Union[float, np.ndarray]: For binary segmentation, returns a single float F1 score.\n+            For multi-class segmentation, returns an array of F1 scores for each class.\n+\n+    Examples:\n+        >>> # Binary segmentation with arrays\n+        >>> gt = np.array([[0, 0, 1, 1], [0, 1, 1, 1]])\n+        >>> pred = np.array([[0, 0, 1, 1], [0, 0, 1, 1]])\n+        >>> f1 = calc_f1_score(gt, pred)\n+        >>> print(f\"F1 Score: {f1:.4f}\")\n+        F1 Score: 0.8571\n+\n+        >>> # Multi-class segmentation\n+        >>> gt = np.array([[0, 0, 1, 1], [0, 2, 2, 1]])\n+        >>> pred = np.array([[0, 0, 1, 1], [0, 0, 2, 2]])\n+        >>> f1 = calc_f1_score(gt, pred, num_classes=3)\n+        >>> print(f\"F1 Score per class: {f1}\")\n+        F1 Score per class: [0.8571 0.6667 0.6667]\n+\n+        >>> # Using PyTorch tensors\n+        >>> gt_tensor = torch.tensor([[0, 0, 1, 1], [0, 1, 1, 1]])\n+        >>> pred_tensor = torch.tensor([[0, 0, 1, 1], [0, 0, 1, 1]])\n+        >>> f1 = calc_f1_score(gt_tensor, pred_tensor)\n+        >>> print(f\"F1 Score: {f1:.4f}\")\n+        F1 Score: 0.8571\n+\n+        >>> # Using raster file paths\n+        >>> f1 = calc_f1_score(\"ground_truth.tif\", \"prediction.tif\", num_classes=3)\n+        >>> print(f\"Mean F1: {np.nanmean(f1):.4f}\")\n+        Mean F1: 0.7302\n+    \"\"\"\n+    # Load from file if string path is provided\n+    if isinstance(ground_truth, str):\n+        with rasterio.open(ground_truth) as src:\n+            ground_truth = src.read(band)\n+    if isinstance(prediction, str):\n+        with rasterio.open(prediction) as src:\n+            prediction = src.read(band)\n+\n+    # Convert to numpy if torch tensor\n+    if isinstance(ground_truth, torch.Tensor):\n+        ground_truth = ground_truth.cpu().numpy()\n+    if isinstance(prediction, torch.Tensor):\n+        prediction = prediction.cpu().numpy()\n+\n+    # Ensure inputs have the same shape\n+    if ground_truth.shape != prediction.shape:\n+        raise ValueError(\n+            f\"Shape mismatch: ground_truth {ground_truth.shape} vs prediction {prediction.shape}\"\n+        )\n+\n+    # Binary segmentation\n+    if num_classes is None:\n+        ground_truth = ground_truth.astype(bool)\n+        prediction = prediction.astype(bool)\n+\n+        # Calculate True Positives, False Positives, False Negatives\n+        tp = np.logical_and(ground_truth, prediction).sum()\n+        fp = np.logical_and(~ground_truth, prediction).sum()\n+        fn = np.logical_and(ground_truth, ~prediction).sum()\n+\n+        # Calculate precision and recall\n+        precision = (tp + smooth) / (tp + fp + smooth)\n+        recall = (tp + smooth) / (tp + fn + smooth)\n+\n+        # Calculate F1 score\n+        f1 = 2 * (precision * recall) / (precision + recall + smooth)\n+        return float(f1)\n+\n+    # Multi-class segmentation\n+    else:\n+        f1_per_class = []\n+\n+        for class_idx in range(num_classes):\n+            # Skip ignored class\n+            if ignore_index is not None and class_idx == ignore_index:",
    "comment": "The logic for handling ignored classes is inconsistent between functions. In `calc_iou`, ignored classes result in missing array elements, while in `calc_f1_score`, they are skipped with `continue`. This creates arrays of different sizes and makes the results incompatible. Consider using a consistent approach, such as appending `np.nan` for ignored classes in both functions.\n```suggestion\n            # Mark ignored class with np.nan\n            if ignore_index is not None and class_idx == ignore_index:\n                f1_per_class.append(np.nan)\n```",
    "line_number": 618,
    "enriched": "File: geoai/utils.py\nCode: @@ -383,6 +383,392 @@ def calc_stats(dataset, divide_by: float = 1.0) -> Tuple[np.ndarray, np.ndarray]\n     return accum_mean / len(files), accum_std / len(files)\n \n \n+def calc_iou(\n+    ground_truth: Union[str, np.ndarray, torch.Tensor],\n+    prediction: Union[str, np.ndarray, torch.Tensor],\n+    num_classes: Optional[int] = None,\n+    ignore_index: Optional[int] = None,\n+    smooth: float = 1e-6,\n+    band: int = 1,\n+) -> Union[float, np.ndarray]:\n+    \"\"\"\n+    Calculate Intersection over Union (IoU) between ground truth and prediction masks.\n+\n+    This function computes the IoU metric for segmentation tasks. It supports both\n+    binary and multi-class segmentation, and can handle numpy arrays, PyTorch tensors,\n+    or file paths to raster files.\n+\n+    Args:\n+        ground_truth (Union[str, np.ndarray, torch.Tensor]): Ground truth segmentation mask.\n+            Can be a file path (str) to a raster file, numpy array, or PyTorch tensor.\n+            For binary segmentation: shape (H, W) with values {0, 1}.\n+            For multi-class segmentation: shape (H, W) with class indices.\n+        prediction (Union[str, np.ndarray, torch.Tensor]): Predicted segmentation mask.\n+            Can be a file path (str) to a raster file, numpy array, or PyTorch tensor.\n+            Should have the same shape and format as ground_truth.\n+        num_classes (Optional[int], optional): Number of classes for multi-class segmentation.\n+            If None, assumes binary segmentation. Defaults to None.\n+        ignore_index (Optional[int], optional): Class index to ignore in computation.\n+            Useful for ignoring background or unlabeled pixels. Defaults to None.\n+        smooth (float, optional): Smoothing factor to avoid division by zero.\n+            Defaults to 1e-6.\n+        band (int, optional): Band index to read from raster file (1-based indexing).\n+            Only used when input is a file path. Defaults to 1.\n+\n+    Returns:\n+        Union[float, np.ndarray]: For binary segmentation, returns a single float IoU score.\n+            For multi-class segmentation, returns an array of IoU scores for each class.\n+\n+    Examples:\n+        >>> # Binary segmentation with arrays\n+        >>> gt = np.array([[0, 0, 1, 1], [0, 1, 1, 1]])\n+        >>> pred = np.array([[0, 0, 1, 1], [0, 0, 1, 1]])\n+        >>> iou = calc_iou(gt, pred)\n+        >>> print(f\"IoU: {iou:.4f}\")\n+        IoU: 0.8333\n+\n+        >>> # Multi-class segmentation\n+        >>> gt = np.array([[0, 0, 1, 1], [0, 2, 2, 1]])\n+        >>> pred = np.array([[0, 0, 1, 1], [0, 0, 2, 2]])\n+        >>> iou = calc_iou(gt, pred, num_classes=3)\n+        >>> print(f\"IoU per class: {iou}\")\n+        IoU per class: [0.8333 0.5000 0.5000]\n+\n+        >>> # Using PyTorch tensors\n+        >>> gt_tensor = torch.tensor([[0, 0, 1, 1], [0, 1, 1, 1]])\n+        >>> pred_tensor = torch.tensor([[0, 0, 1, 1], [0, 0, 1, 1]])\n+        >>> iou = calc_iou(gt_tensor, pred_tensor)\n+        >>> print(f\"IoU: {iou:.4f}\")\n+        IoU: 0.8333\n+\n+        >>> # Using raster file paths\n+        >>> iou = calc_iou(\"ground_truth.tif\", \"prediction.tif\", num_classes=3)\n+        >>> print(f\"Mean IoU: {np.nanmean(iou):.4f}\")\n+        Mean IoU: 0.7500\n+    \"\"\"\n+    # Load from file if string path is provided\n+    if isinstance(ground_truth, str):\n+        with rasterio.open(ground_truth) as src:\n+            ground_truth = src.read(band)\n+    if isinstance(prediction, str):\n+        with rasterio.open(prediction) as src:\n+            prediction = src.read(band)\n+\n+    # Convert to numpy if torch tensor\n+    if isinstance(ground_truth, torch.Tensor):\n+        ground_truth = ground_truth.cpu().numpy()\n+    if isinstance(prediction, torch.Tensor):\n+        prediction = prediction.cpu().numpy()\n+\n+    # Ensure inputs have the same shape\n+    if ground_truth.shape != prediction.shape:\n+        raise ValueError(\n+            f\"Shape mismatch: ground_truth {ground_truth.shape} vs prediction {prediction.shape}\"\n+        )\n+\n+    # Binary segmentation\n+    if num_classes is None:\n+        ground_truth = ground_truth.astype(bool)\n+        prediction = prediction.astype(bool)\n+\n+        intersection = np.logical_and(ground_truth, prediction).sum()\n+        union = np.logical_or(ground_truth, prediction).sum()\n+\n+        if union == 0:\n+            return 1.0 if intersection == 0 else 0.0\n+\n+        iou = (intersection + smooth) / (union + smooth)\n+        return float(iou)\n+\n+    # Multi-class segmentation\n+    else:\n+        iou_per_class = []\n+\n+        for class_idx in range(num_classes):\n+            # Skip ignored class\n+            if ignore_index is not None and class_idx == ignore_index:\n+                continue\n+\n+            # Create binary masks for current class\n+            gt_class = (ground_truth == class_idx).astype(bool)\n+            pred_class = (prediction == class_idx).astype(bool)\n+\n+            intersection = np.logical_and(gt_class, pred_class).sum()\n+            union = np.logical_or(gt_class, pred_class).sum()\n+\n+            if union == 0:\n+                # If class is not present in both gt and pred\n+                iou_per_class.append(np.nan)\n+            else:\n+                iou_per_class.append((intersection + smooth) / (union + smooth))\n+\n+        return np.array(iou_per_class)\n+\n+\n+def calc_f1_score(\n+    ground_truth: Union[str, np.ndarray, torch.Tensor],\n+    prediction: Union[str, np.ndarray, torch.Tensor],\n+    num_classes: Optional[int] = None,\n+    ignore_index: Optional[int] = None,\n+    smooth: float = 1e-6,\n+    band: int = 1,\n+) -> Union[float, np.ndarray]:\n+    \"\"\"\n+    Calculate F1 score between ground truth and prediction masks.\n+\n+    The F1 score is the harmonic mean of precision and recall, computed as:\n+    F1 = 2 * (precision * recall) / (precision + recall)\n+    where precision = TP / (TP + FP) and recall = TP / (TP + FN).\n+\n+    This function supports both binary and multi-class segmentation, and can handle\n+    numpy arrays, PyTorch tensors, or file paths to raster files.\n+\n+    Args:\n+        ground_truth (Union[str, np.ndarray, torch.Tensor]): Ground truth segmentation mask.\n+            Can be a file path (str) to a raster file, numpy array, or PyTorch tensor.\n+            For binary segmentation: shape (H, W) with values {0, 1}.\n+            For multi-class segmentation: shape (H, W) with class indices.\n+        prediction (Union[str, np.ndarray, torch.Tensor]): Predicted segmentation mask.\n+            Can be a file path (str) to a raster file, numpy array, or PyTorch tensor.\n+            Should have the same shape and format as ground_truth.\n+        num_classes (Optional[int], optional): Number of classes for multi-class segmentation.\n+            If None, assumes binary segmentation. Defaults to None.\n+        ignore_index (Optional[int], optional): Class index to ignore in computation.\n+            Useful for ignoring background or unlabeled pixels. Defaults to None.\n+        smooth (float, optional): Smoothing factor to avoid division by zero.\n+            Defaults to 1e-6.\n+        band (int, optional): Band index to read from raster file (1-based indexing).\n+            Only used when input is a file path. Defaults to 1.\n+\n+    Returns:\n+        Union[float, np.ndarray]: For binary segmentation, returns a single float F1 score.\n+            For multi-class segmentation, returns an array of F1 scores for each class.\n+\n+    Examples:\n+        >>> # Binary segmentation with arrays\n+        >>> gt = np.array([[0, 0, 1, 1], [0, 1, 1, 1]])\n+        >>> pred = np.array([[0, 0, 1, 1], [0, 0, 1, 1]])\n+        >>> f1 = calc_f1_score(gt, pred)\n+        >>> print(f\"F1 Score: {f1:.4f}\")\n+        F1 Score: 0.8571\n+\n+        >>> # Multi-class segmentation\n+        >>> gt = np.array([[0, 0, 1, 1], [0, 2, 2, 1]])\n+        >>> pred = np.array([[0, 0, 1, 1], [0, 0, 2, 2]])\n+        >>> f1 = calc_f1_score(gt, pred, num_classes=3)\n+        >>> print(f\"F1 Score per class: {f1}\")\n+        F1 Score per class: [0.8571 0.6667 0.6667]\n+\n+        >>> # Using PyTorch tensors\n+        >>> gt_tensor = torch.tensor([[0, 0, 1, 1], [0, 1, 1, 1]])\n+        >>> pred_tensor = torch.tensor([[0, 0, 1, 1], [0, 0, 1, 1]])\n+        >>> f1 = calc_f1_score(gt_tensor, pred_tensor)\n+        >>> print(f\"F1 Score: {f1:.4f}\")\n+        F1 Score: 0.8571\n+\n+        >>> # Using raster file paths\n+        >>> f1 = calc_f1_score(\"ground_truth.tif\", \"prediction.tif\", num_classes=3)\n+        >>> print(f\"Mean F1: {np.nanmean(f1):.4f}\")\n+        Mean F1: 0.7302\n+    \"\"\"\n+    # Load from file if string path is provided\n+    if isinstance(ground_truth, str):\n+        with rasterio.open(ground_truth) as src:\n+            ground_truth = src.read(band)\n+    if isinstance(prediction, str):\n+        with rasterio.open(prediction) as src:\n+            prediction = src.read(band)\n+\n+    # Convert to numpy if torch tensor\n+    if isinstance(ground_truth, torch.Tensor):\n+        ground_truth = ground_truth.cpu().numpy()\n+    if isinstance(prediction, torch.Tensor):\n+        prediction = prediction.cpu().numpy()\n+\n+    # Ensure inputs have the same shape\n+    if ground_truth.shape != prediction.shape:\n+        raise ValueError(\n+            f\"Shape mismatch: ground_truth {ground_truth.shape} vs prediction {prediction.shape}\"\n+        )\n+\n+    # Binary segmentation\n+    if num_classes is None:\n+        ground_truth = ground_truth.astype(bool)\n+        prediction = prediction.astype(bool)\n+\n+        # Calculate True Positives, False Positives, False Negatives\n+        tp = np.logical_and(ground_truth, prediction).sum()\n+        fp = np.logical_and(~ground_truth, prediction).sum()\n+        fn = np.logical_and(ground_truth, ~prediction).sum()\n+\n+        # Calculate precision and recall\n+        precision = (tp + smooth) / (tp + fp + smooth)\n+        recall = (tp + smooth) / (tp + fn + smooth)\n+\n+        # Calculate F1 score\n+        f1 = 2 * (precision * recall) / (precision + recall + smooth)\n+        return float(f1)\n+\n+    # Multi-class segmentation\n+    else:\n+        f1_per_class = []\n+\n+        for class_idx in range(num_classes):\n+            # Skip ignored class\n+            if ignore_index is not None and class_idx == ignore_index:\nComment: The logic for handling ignored classes is inconsistent between functions. In `calc_iou`, ignored classes result in missing array elements, while in `calc_f1_score`, they are skipped with `continue`. This creates arrays of different sizes and makes the results incompatible. Consider using a consistent approach, such as appending `np.nan` for ignored classes in both functions.\n```suggestion\n            # Mark ignored class with np.nan\n            if ignore_index is not None and class_idx == ignore_index:\n                f1_per_class.append(np.nan)\n```",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "geoai/utils.py",
    "pr_number": 317,
    "repo": "geoai",
    "owner": "opengeos",
    "comment_id": 2422352471,
    "comment_created_at": "2025-10-11T01:13:34Z"
  },
  {
    "code": "@@ -0,0 +1,489 @@\n+\"\"\"Super-resolution module for enhancing geospatial imagery resolution using deep learning.\"\"\"\n+\n+import logging\n+import os\n+from pathlib import Path\n+from typing import Any, Dict, List, Optional, Tuple, Union\n+\n+import numpy as np\n+import rasterio\n+import torch\n+import torch.nn as nn\n+import torch.nn.functional as F\n+from rasterio.transform import from_bounds\n+from torch.utils.data import DataLoader, Dataset\n+from tqdm import tqdm\n+\n+# Configure logging\n+logging.basicConfig(level=logging.INFO)\n+logger = logging.getLogger(__name__)\n+\n+\n+class ResidualBlock(nn.Module):\n+    \"\"\"Residual block for ESRGAN generator.\"\"\"\n+\n+    def __init__(self, channels: int):\n+        super(ResidualBlock, self).__init__()\n+        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n+        self.bn1 = nn.BatchNorm2d(channels)\n+        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n+        self.bn2 = nn.BatchNorm2d(channels)\n+\n+    def forward(self, x: torch.Tensor) -> torch.Tensor:\n+        residual = x\n+        out = F.relu(self.bn1(self.conv1(x)))\n+        out = self.bn2(self.conv2(out))\n+        out += residual\n+        return out\n+\n+\n+class ESRGANGenerator(nn.Module):\n+    \"\"\"ESRGAN-inspired generator network for super-resolution.\"\"\"\n+\n+    def __init__(self, upscale_factor: int = 4, num_channels: int = 3):\n+        super(ESRGANGenerator, self).__init__()\n+        self.upscale_factor = upscale_factor\n+\n+        # Initial convolution\n+        self.conv1 = nn.Conv2d(num_channels, 64, kernel_size=9, padding=4)\n+\n+        # Residual blocks\n+        self.residual_blocks = nn.Sequential(*[ResidualBlock(64) for _ in range(16)])\n+\n+        # Second convolution\n+        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n+        self.bn2 = nn.BatchNorm2d(64)\n+\n+        # Upsampling layers\n+        self.upsample = nn.Sequential()\n+        for _ in range(int(np.log2(upscale_factor))):\n+            self.upsample.append(nn.Conv2d(64, 256, kernel_size=3, padding=1))\n+            self.upsample.append(nn.PixelShuffle(2))\n+            self.upsample.append(nn.ReLU(inplace=True))\n+\n+        # Final convolution\n+        self.conv3 = nn.Conv2d(64, num_channels, kernel_size=9, padding=4)\n+\n+    def forward(self, x: torch.Tensor) -> torch.Tensor:\n+        out1 = F.relu(self.conv1(x))\n+        out = self.residual_blocks(out1)\n+        out2 = self.bn2(self.conv2(out))\n+        out = out1 + out2  # Skip connection\n+        out = self.upsample(out)\n+        out = self.conv3(out)\n+        return out\n+\n+\n+class SRCNN(nn.Module):\n+    \"\"\"SRCNN (Super-Resolution CNN) implementation.\"\"\"\n+\n+    def __init__(self, upscale_factor: int = 2, num_channels: int = 3):\n+        super(SRCNN, self).__init__()\n+        self.upscale_factor = upscale_factor\n+\n+        self.layers = nn.Sequential(\n+            nn.Conv2d(num_channels, 64, kernel_size=9, padding=4),\n+            nn.ReLU(inplace=True),\n+            nn.Conv2d(64, 32, kernel_size=1),\n+            nn.ReLU(inplace=True),\n+            nn.Conv2d(32, num_channels, kernel_size=5, padding=2),\n+        )\n+\n+    def forward(self, x: torch.Tensor) -> torch.Tensor:\n+        # Bicubic upsampling first\n+        x = F.interpolate(\n+            x, scale_factor=self.upscale_factor, mode=\"bicubic\", align_corners=False\n+        )\n+        # Then refine with CNN\n+        x = self.layers(x)\n+        return x\n+\n+\n+class GeospatialSRDataset(Dataset):\n+    \"\"\"Dataset for geospatial super-resolution training.\"\"\"\n+\n+    def __init__(self, image_dir: str, upscale_factor: int = 4, patch_size: int = 64):\n+        self.image_dir = Path(image_dir)\n+        self.upscale_factor = upscale_factor\n+        self.patch_size = patch_size\n+        self.image_files = list(self.image_dir.glob(\"*.tif\")) + list(\n+            self.image_dir.glob(\"*.tiff\")\n+        )\n+\n+    def __len__(self) -> int:\n+        return len(self.image_files)\n+\n+    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n+        with rasterio.open(self.image_files[idx]) as src:\n+            # Read RGB bands\n+            if src.count >= 3:\n+                image = src.read([1, 2, 3])\n+            else:\n+                image = src.read(1)\n+                image = np.stack([image, image, image])  # Convert to 3-channel\n+\n+            # Normalize to [0, 1]\n+            image = image.astype(np.float32) / 255.0\n+\n+            # Create low-resolution version\n+            hr_image = torch.from_numpy(image)\n+            lr_image = F.interpolate(\n+                hr_image.unsqueeze(0),\n+                scale_factor=1 / self.upscale_factor,\n+                mode=\"bicubic\",\n+                align_corners=False,\n+            ).squeeze(0)\n+\n+            # Random crop to patch size\n+            if hr_image.shape[-1] > self.patch_size:\n+                i = np.random.randint(0, hr_image.shape[-2] - self.patch_size)\n+                j = np.random.randint(0, hr_image.shape[-1] - self.patch_size)\n+                hr_image = hr_image[:, i : i + self.patch_size, j : j + self.patch_size]\n+                lr_image = lr_image[:, i : i + self.patch_size, j : j + self.patch_size]\n+\n+            return lr_image, hr_image\n+\n+\n+class SuperResolutionModel:\n+    \"\"\"Super-resolution model for geospatial imagery enhancement.\"\"\"\n+\n+    def __init__(\n+        self,\n+        model_type: str = \"esrgan\",\n+        upscale_factor: int = 4,\n+        device: Optional[str] = None,\n+        num_channels: int = 3,\n+    ):\n+        \"\"\"\n+        Initialize super-resolution model.\n+\n+        Args:\n+            model_type: Type of model ('esrgan', 'srcnn', 'edsr')",
    "comment": "The docstring mentions 'edsr' as a supported model type, but the implementation only supports 'esrgan' and 'srcnn'. Either remove 'edsr' from the documentation or add support for it.\n```suggestion\n            model_type: Type of model ('esrgan', 'srcnn')\n```",
    "line_number": 161,
    "enriched": "File: geoai/super_resolution.py\nCode: @@ -0,0 +1,489 @@\n+\"\"\"Super-resolution module for enhancing geospatial imagery resolution using deep learning.\"\"\"\n+\n+import logging\n+import os\n+from pathlib import Path\n+from typing import Any, Dict, List, Optional, Tuple, Union\n+\n+import numpy as np\n+import rasterio\n+import torch\n+import torch.nn as nn\n+import torch.nn.functional as F\n+from rasterio.transform import from_bounds\n+from torch.utils.data import DataLoader, Dataset\n+from tqdm import tqdm\n+\n+# Configure logging\n+logging.basicConfig(level=logging.INFO)\n+logger = logging.getLogger(__name__)\n+\n+\n+class ResidualBlock(nn.Module):\n+    \"\"\"Residual block for ESRGAN generator.\"\"\"\n+\n+    def __init__(self, channels: int):\n+        super(ResidualBlock, self).__init__()\n+        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n+        self.bn1 = nn.BatchNorm2d(channels)\n+        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n+        self.bn2 = nn.BatchNorm2d(channels)\n+\n+    def forward(self, x: torch.Tensor) -> torch.Tensor:\n+        residual = x\n+        out = F.relu(self.bn1(self.conv1(x)))\n+        out = self.bn2(self.conv2(out))\n+        out += residual\n+        return out\n+\n+\n+class ESRGANGenerator(nn.Module):\n+    \"\"\"ESRGAN-inspired generator network for super-resolution.\"\"\"\n+\n+    def __init__(self, upscale_factor: int = 4, num_channels: int = 3):\n+        super(ESRGANGenerator, self).__init__()\n+        self.upscale_factor = upscale_factor\n+\n+        # Initial convolution\n+        self.conv1 = nn.Conv2d(num_channels, 64, kernel_size=9, padding=4)\n+\n+        # Residual blocks\n+        self.residual_blocks = nn.Sequential(*[ResidualBlock(64) for _ in range(16)])\n+\n+        # Second convolution\n+        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n+        self.bn2 = nn.BatchNorm2d(64)\n+\n+        # Upsampling layers\n+        self.upsample = nn.Sequential()\n+        for _ in range(int(np.log2(upscale_factor))):\n+            self.upsample.append(nn.Conv2d(64, 256, kernel_size=3, padding=1))\n+            self.upsample.append(nn.PixelShuffle(2))\n+            self.upsample.append(nn.ReLU(inplace=True))\n+\n+        # Final convolution\n+        self.conv3 = nn.Conv2d(64, num_channels, kernel_size=9, padding=4)\n+\n+    def forward(self, x: torch.Tensor) -> torch.Tensor:\n+        out1 = F.relu(self.conv1(x))\n+        out = self.residual_blocks(out1)\n+        out2 = self.bn2(self.conv2(out))\n+        out = out1 + out2  # Skip connection\n+        out = self.upsample(out)\n+        out = self.conv3(out)\n+        return out\n+\n+\n+class SRCNN(nn.Module):\n+    \"\"\"SRCNN (Super-Resolution CNN) implementation.\"\"\"\n+\n+    def __init__(self, upscale_factor: int = 2, num_channels: int = 3):\n+        super(SRCNN, self).__init__()\n+        self.upscale_factor = upscale_factor\n+\n+        self.layers = nn.Sequential(\n+            nn.Conv2d(num_channels, 64, kernel_size=9, padding=4),\n+            nn.ReLU(inplace=True),\n+            nn.Conv2d(64, 32, kernel_size=1),\n+            nn.ReLU(inplace=True),\n+            nn.Conv2d(32, num_channels, kernel_size=5, padding=2),\n+        )\n+\n+    def forward(self, x: torch.Tensor) -> torch.Tensor:\n+        # Bicubic upsampling first\n+        x = F.interpolate(\n+            x, scale_factor=self.upscale_factor, mode=\"bicubic\", align_corners=False\n+        )\n+        # Then refine with CNN\n+        x = self.layers(x)\n+        return x\n+\n+\n+class GeospatialSRDataset(Dataset):\n+    \"\"\"Dataset for geospatial super-resolution training.\"\"\"\n+\n+    def __init__(self, image_dir: str, upscale_factor: int = 4, patch_size: int = 64):\n+        self.image_dir = Path(image_dir)\n+        self.upscale_factor = upscale_factor\n+        self.patch_size = patch_size\n+        self.image_files = list(self.image_dir.glob(\"*.tif\")) + list(\n+            self.image_dir.glob(\"*.tiff\")\n+        )\n+\n+    def __len__(self) -> int:\n+        return len(self.image_files)\n+\n+    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n+        with rasterio.open(self.image_files[idx]) as src:\n+            # Read RGB bands\n+            if src.count >= 3:\n+                image = src.read([1, 2, 3])\n+            else:\n+                image = src.read(1)\n+                image = np.stack([image, image, image])  # Convert to 3-channel\n+\n+            # Normalize to [0, 1]\n+            image = image.astype(np.float32) / 255.0\n+\n+            # Create low-resolution version\n+            hr_image = torch.from_numpy(image)\n+            lr_image = F.interpolate(\n+                hr_image.unsqueeze(0),\n+                scale_factor=1 / self.upscale_factor,\n+                mode=\"bicubic\",\n+                align_corners=False,\n+            ).squeeze(0)\n+\n+            # Random crop to patch size\n+            if hr_image.shape[-1] > self.patch_size:\n+                i = np.random.randint(0, hr_image.shape[-2] - self.patch_size)\n+                j = np.random.randint(0, hr_image.shape[-1] - self.patch_size)\n+                hr_image = hr_image[:, i : i + self.patch_size, j : j + self.patch_size]\n+                lr_image = lr_image[:, i : i + self.patch_size, j : j + self.patch_size]\n+\n+            return lr_image, hr_image\n+\n+\n+class SuperResolutionModel:\n+    \"\"\"Super-resolution model for geospatial imagery enhancement.\"\"\"\n+\n+    def __init__(\n+        self,\n+        model_type: str = \"esrgan\",\n+        upscale_factor: int = 4,\n+        device: Optional[str] = None,\n+        num_channels: int = 3,\n+    ):\n+        \"\"\"\n+        Initialize super-resolution model.\n+\n+        Args:\n+            model_type: Type of model ('esrgan', 'srcnn', 'edsr')\nComment: The docstring mentions 'edsr' as a supported model type, but the implementation only supports 'esrgan' and 'srcnn'. Either remove 'edsr' from the documentation or add support for it.\n```suggestion\n            model_type: Type of model ('esrgan', 'srcnn')\n```",
    "subcategory": "documentation",
    "category": "documentation",
    "file_path": "geoai/super_resolution.py",
    "pr_number": 315,
    "repo": "geoai",
    "owner": "opengeos",
    "comment_id": 2422180075,
    "comment_created_at": "2025-10-10T22:23:05Z"
  },
  {
    "code": "@@ -3323,33 +3335,38 @@ def export_geotiff_tiles_batch(\n     skip_empty_tiles=False,\n     image_extensions=None,\n     mask_extensions=None,\n-    match_by_name=True,\n+    match_by_name=False,",
    "comment": "Changing the default value of `match_by_name` from `True` to `False` is a breaking change that could affect existing users who rely on the current default behavior. Consider keeping the original default or documenting this change prominently.\n```suggestion\n    match_by_name=True,\n```",
    "line_number": 3338,
    "enriched": "File: geoai/utils.py\nCode: @@ -3323,33 +3335,38 @@ def export_geotiff_tiles_batch(\n     skip_empty_tiles=False,\n     image_extensions=None,\n     mask_extensions=None,\n-    match_by_name=True,\n+    match_by_name=False,\nComment: Changing the default value of `match_by_name` from `True` to `False` is a breaking change that could affect existing users who rely on the current default behavior. Consider keeping the original default or documenting this change prominently.\n```suggestion\n    match_by_name=True,\n```",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "geoai/utils.py",
    "pr_number": 313,
    "repo": "geoai",
    "owner": "opengeos",
    "comment_id": 2416922999,
    "comment_created_at": "2025-10-09T14:04:57Z"
  },
  {
    "code": "@@ -3600,6 +3616,8 @@ def export_geotiff_tiles_batch(\n                 quiet=quiet,\n                 mask_gdf=mask_gdf,  # Pass pre-loaded GeoDataFrame if using single mask\n                 use_single_mask_file=use_single_mask_file,\n+                metadata_format=metadata_format,\n+                ann_dir=ann_dir if metadata_format in [\"PASCAL_VOC\", \"COCO\"] else None,",
    "comment": "The `ann_dir` variable is referenced but may not be defined when `metadata_format` is 'YOLO'. This will cause a NameError since `ann_dir` is only created when `metadata_format` is in ['PASCAL_VOC', 'COCO'].\n```suggestion\n                ann_dir=ann_dir if \"ann_dir\" in locals() and metadata_format in [\"PASCAL_VOC\", \"COCO\"] else None,\n```",
    "line_number": 3620,
    "enriched": "File: geoai/utils.py\nCode: @@ -3600,6 +3616,8 @@ def export_geotiff_tiles_batch(\n                 quiet=quiet,\n                 mask_gdf=mask_gdf,  # Pass pre-loaded GeoDataFrame if using single mask\n                 use_single_mask_file=use_single_mask_file,\n+                metadata_format=metadata_format,\n+                ann_dir=ann_dir if metadata_format in [\"PASCAL_VOC\", \"COCO\"] else None,\nComment: The `ann_dir` variable is referenced but may not be defined when `metadata_format` is 'YOLO'. This will cause a NameError since `ann_dir` is only created when `metadata_format` is in ['PASCAL_VOC', 'COCO'].\n```suggestion\n                ann_dir=ann_dir if \"ann_dir\" in locals() and metadata_format in [\"PASCAL_VOC\", \"COCO\"] else None,\n```",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "geoai/utils.py",
    "pr_number": 307,
    "repo": "geoai",
    "owner": "opengeos",
    "comment_id": 2404665942,
    "comment_created_at": "2025-10-05T21:00:27Z"
  },
  {
    "code": "@@ -3115,8 +3115,9 @@ def export_geotiff_tiles(\n \n def export_geotiff_tiles_batch(\n     images_folder,\n-    masks_folder,\n-    output_folder,\n+    masks_folder=None,\n+    masks_file=None,\n+    output_folder=None,",
    "comment": "Making `masks_folder` and `output_folder` optional parameters could break existing code that relies on these being required. Consider adding deprecation warnings for the old signature to ensure backward compatibility.",
    "line_number": 3120,
    "enriched": "File: geoai/utils.py\nCode: @@ -3115,8 +3115,9 @@ def export_geotiff_tiles(\n \n def export_geotiff_tiles_batch(\n     images_folder,\n-    masks_folder,\n-    output_folder,\n+    masks_folder=None,\n+    masks_file=None,\n+    output_folder=None,\nComment: Making `masks_folder` and `output_folder` optional parameters could break existing code that relies on these being required. Consider adding deprecation warnings for the old signature to ensure backward compatibility.",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "geoai/utils.py",
    "pr_number": 299,
    "repo": "geoai",
    "owner": "opengeos",
    "comment_id": 2403729989,
    "comment_created_at": "2025-10-04T04:13:47Z"
  },
  {
    "code": "@@ -3646,3 +3654,241 @@ def instance_segmentation_batch(\n             continue\n \n     print(f\"Batch processing completed. Results saved to {output_dir}\")\n+\n+\n+def lightly_train_model(\n+    data_dir: str,\n+    output_dir: str,\n+    model: str = \"torchvision/resnet50\",\n+    method: str = \"dinov2_distillation\",\n+    epochs: int = 100,\n+    batch_size: int = 64,\n+    learning_rate: float = 1e-4,\n+    **kwargs: Any,\n+) -> str:\n+    \"\"\"\n+    Train a model using Lightly Train for self-supervised pretraining.\n+\n+    Args:\n+        data_dir (str): Directory containing unlabeled images for training.\n+        output_dir (str): Directory to save training outputs and model checkpoints.\n+        model (str): Model architecture to train. Supports models from torchvision,\n+            timm, ultralytics, etc. Default is \"torchvision/resnet50\".\n+        method (str): Self-supervised learning method. Options include:\n+            \"dinov2_distillation\" (recommended), \"dinov2\", \"dino\", \"simclr\".\n+            Default is \"dinov2_distillation\".\n+        epochs (int): Number of training epochs. Default is 100.\n+        batch_size (int): Batch size for training. Default is 64.\n+        learning_rate (float): Learning rate for training. Default is 1e-4.\n+        **kwargs: Additional arguments passed to lightly_train.train().\n+\n+    Returns:\n+        str: Path to the exported model file.\n+\n+    Raises:\n+        ImportError: If lightly-train is not installed.\n+        ValueError: If data_dir does not exist or is empty.\n+\n+    Example:\n+        >>> model_path = lightly_train_model(\n+        ...     data_dir=\"path/to/unlabeled/images\",\n+        ...     output_dir=\"path/to/output\",\n+        ...     model=\"torchvision/resnet50\",\n+        ...     epochs=50\n+        ... )\n+        >>> print(f\"Pretrained model saved to: {model_path}\")\n+    \"\"\"\n+    if not LIGHTLY_TRAIN_AVAILABLE:\n+        raise ImportError(\n+            \"lightly-train is not installed. Please install it with: \"\n+            \"pip install lightly-train\"\n+        )\n+\n+    if not os.path.exists(data_dir):\n+        raise ValueError(f\"Data directory does not exist: {data_dir}\")\n+\n+    # Check if data directory contains images\n+    image_extensions = [\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.tif\", \"*.tiff\", \"*.bmp\"]\n+    image_files = []\n+    for ext in image_extensions:\n+        image_files.extend(glob.glob(os.path.join(data_dir, \"**\", ext), recursive=True))\n+\n+    if not image_files:\n+        raise ValueError(f\"No image files found in {data_dir}\")\n+\n+    print(f\"Found {len(image_files)} images in {data_dir}\")\n+    print(f\"Starting self-supervised pretraining with {method} method...\")\n+\n+    # Create output directory\n+    os.makedirs(output_dir, exist_ok=True)\n+\n+    # Train the model using Lightly Train\n+    lightly_train.train(\n+        out=output_dir,\n+        data=data_dir,\n+        model=model,\n+        method=method,\n+        epochs=epochs,\n+        batch_size=batch_size,\n+        learning_rate=learning_rate,\n+        **kwargs,\n+    )\n+\n+    # Return path to the exported model\n+    exported_model_path = os.path.join(\n+        output_dir, \"exported_models\", \"exported_last.pt\"\n+    )\n+\n+    if os.path.exists(exported_model_path):\n+        print(\n+            f\"Model training completed. Exported model saved to: {exported_model_path}\"\n+        )\n+        return exported_model_path\n+    else:\n+        # Check for alternative export paths\n+        possible_paths = [\n+            os.path.join(output_dir, \"exported_models\", \"exported_best.pt\"),\n+            os.path.join(output_dir, \"checkpoints\", \"last.ckpt\"),\n+        ]\n+\n+        for path in possible_paths:\n+            if os.path.exists(path):\n+                print(f\"Model training completed. Exported model saved to: {path}\")\n+                return path\n+\n+        print(f\"Model training completed. Output saved to: {output_dir}\")\n+        return output_dir\n+\n+\n+def load_lightly_pretrained_model(\n+    model_path: str,\n+    model_architecture: str = \"torchvision/resnet50\",\n+) -> torch.nn.Module:\n+    \"\"\"\n+    Load a pretrained model from Lightly Train.\n+\n+    Args:\n+        model_path (str): Path to the pretrained model file (.pt format).\n+        model_architecture (str): Architecture of the model to load.\n+            Default is \"torchvision/resnet50\".\n+\n+    Returns:\n+        torch.nn.Module: Loaded pretrained model ready for fine-tuning.\n+\n+    Raises:\n+        FileNotFoundError: If model_path does not exist.\n+        ImportError: If required libraries are not available.\n+\n+    Example:\n+        >>> model = load_lightly_pretrained_model(\n+        ...     model_path=\"path/to/pretrained_model.pt\",\n+        ...     model_architecture=\"torchvision/resnet50\"\n+        ... )\n+        >>> # Fine-tune the model with your existing training pipeline\n+    \"\"\"\n+    if not os.path.exists(model_path):\n+        raise FileNotFoundError(f\"Model file not found: {model_path}\")\n+\n+    print(f\"Loading pretrained model from: {model_path}\")\n+\n+    # Load the model based on architecture\n+    if model_architecture.startswith(\"torchvision/\"):\n+        model_name = model_architecture.replace(\"torchvision/\", \"\")\n+\n+        # Import the model from torchvision\n+        if hasattr(torchvision.models, model_name):\n+            model = getattr(torchvision.models, model_name)()\n+        else:\n+            raise ValueError(f\"Unknown torchvision model: {model_name}\")\n+\n+    elif model_architecture.startswith(\"timm/\"):\n+        try:\n+            import timm\n+\n+            model_name = model_architecture.replace(\"timm/\", \"\")\n+            model = timm.create_model(model_name)\n+        except ImportError:\n+            raise ImportError(\n+                \"timm is required for TIMM models. Install with: pip install timm\"\n+            )\n+\n+    else:\n+        # For other architectures, try to import from torchvision as default\n+        try:\n+            model = getattr(torchvision.models, model_architecture)()\n+        except AttributeError:\n+            raise ValueError(f\"Unsupported model architecture: {model_architecture}\")\n+\n+    # Load the pretrained weights\n+    state_dict = torch.load(model_path, map_location=\"cpu\", weights_only=True)",
    "comment": "The weights_only=True parameter may not be compatible with older PyTorch versions. Consider adding a try-catch block to fall back to torch.load without this parameter for backward compatibility.\n```suggestion\n    try:\n        state_dict = torch.load(model_path, map_location=\"cpu\", weights_only=True)\n    except TypeError:\n        # For backward compatibility with older PyTorch versions\n        state_dict = torch.load(model_path, map_location=\"cpu\")\n```",
    "line_number": 3823,
    "enriched": "File: geoai/train.py\nCode: @@ -3646,3 +3654,241 @@ def instance_segmentation_batch(\n             continue\n \n     print(f\"Batch processing completed. Results saved to {output_dir}\")\n+\n+\n+def lightly_train_model(\n+    data_dir: str,\n+    output_dir: str,\n+    model: str = \"torchvision/resnet50\",\n+    method: str = \"dinov2_distillation\",\n+    epochs: int = 100,\n+    batch_size: int = 64,\n+    learning_rate: float = 1e-4,\n+    **kwargs: Any,\n+) -> str:\n+    \"\"\"\n+    Train a model using Lightly Train for self-supervised pretraining.\n+\n+    Args:\n+        data_dir (str): Directory containing unlabeled images for training.\n+        output_dir (str): Directory to save training outputs and model checkpoints.\n+        model (str): Model architecture to train. Supports models from torchvision,\n+            timm, ultralytics, etc. Default is \"torchvision/resnet50\".\n+        method (str): Self-supervised learning method. Options include:\n+            \"dinov2_distillation\" (recommended), \"dinov2\", \"dino\", \"simclr\".\n+            Default is \"dinov2_distillation\".\n+        epochs (int): Number of training epochs. Default is 100.\n+        batch_size (int): Batch size for training. Default is 64.\n+        learning_rate (float): Learning rate for training. Default is 1e-4.\n+        **kwargs: Additional arguments passed to lightly_train.train().\n+\n+    Returns:\n+        str: Path to the exported model file.\n+\n+    Raises:\n+        ImportError: If lightly-train is not installed.\n+        ValueError: If data_dir does not exist or is empty.\n+\n+    Example:\n+        >>> model_path = lightly_train_model(\n+        ...     data_dir=\"path/to/unlabeled/images\",\n+        ...     output_dir=\"path/to/output\",\n+        ...     model=\"torchvision/resnet50\",\n+        ...     epochs=50\n+        ... )\n+        >>> print(f\"Pretrained model saved to: {model_path}\")\n+    \"\"\"\n+    if not LIGHTLY_TRAIN_AVAILABLE:\n+        raise ImportError(\n+            \"lightly-train is not installed. Please install it with: \"\n+            \"pip install lightly-train\"\n+        )\n+\n+    if not os.path.exists(data_dir):\n+        raise ValueError(f\"Data directory does not exist: {data_dir}\")\n+\n+    # Check if data directory contains images\n+    image_extensions = [\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.tif\", \"*.tiff\", \"*.bmp\"]\n+    image_files = []\n+    for ext in image_extensions:\n+        image_files.extend(glob.glob(os.path.join(data_dir, \"**\", ext), recursive=True))\n+\n+    if not image_files:\n+        raise ValueError(f\"No image files found in {data_dir}\")\n+\n+    print(f\"Found {len(image_files)} images in {data_dir}\")\n+    print(f\"Starting self-supervised pretraining with {method} method...\")\n+\n+    # Create output directory\n+    os.makedirs(output_dir, exist_ok=True)\n+\n+    # Train the model using Lightly Train\n+    lightly_train.train(\n+        out=output_dir,\n+        data=data_dir,\n+        model=model,\n+        method=method,\n+        epochs=epochs,\n+        batch_size=batch_size,\n+        learning_rate=learning_rate,\n+        **kwargs,\n+    )\n+\n+    # Return path to the exported model\n+    exported_model_path = os.path.join(\n+        output_dir, \"exported_models\", \"exported_last.pt\"\n+    )\n+\n+    if os.path.exists(exported_model_path):\n+        print(\n+            f\"Model training completed. Exported model saved to: {exported_model_path}\"\n+        )\n+        return exported_model_path\n+    else:\n+        # Check for alternative export paths\n+        possible_paths = [\n+            os.path.join(output_dir, \"exported_models\", \"exported_best.pt\"),\n+            os.path.join(output_dir, \"checkpoints\", \"last.ckpt\"),\n+        ]\n+\n+        for path in possible_paths:\n+            if os.path.exists(path):\n+                print(f\"Model training completed. Exported model saved to: {path}\")\n+                return path\n+\n+        print(f\"Model training completed. Output saved to: {output_dir}\")\n+        return output_dir\n+\n+\n+def load_lightly_pretrained_model(\n+    model_path: str,\n+    model_architecture: str = \"torchvision/resnet50\",\n+) -> torch.nn.Module:\n+    \"\"\"\n+    Load a pretrained model from Lightly Train.\n+\n+    Args:\n+        model_path (str): Path to the pretrained model file (.pt format).\n+        model_architecture (str): Architecture of the model to load.\n+            Default is \"torchvision/resnet50\".\n+\n+    Returns:\n+        torch.nn.Module: Loaded pretrained model ready for fine-tuning.\n+\n+    Raises:\n+        FileNotFoundError: If model_path does not exist.\n+        ImportError: If required libraries are not available.\n+\n+    Example:\n+        >>> model = load_lightly_pretrained_model(\n+        ...     model_path=\"path/to/pretrained_model.pt\",\n+        ...     model_architecture=\"torchvision/resnet50\"\n+        ... )\n+        >>> # Fine-tune the model with your existing training pipeline\n+    \"\"\"\n+    if not os.path.exists(model_path):\n+        raise FileNotFoundError(f\"Model file not found: {model_path}\")\n+\n+    print(f\"Loading pretrained model from: {model_path}\")\n+\n+    # Load the model based on architecture\n+    if model_architecture.startswith(\"torchvision/\"):\n+        model_name = model_architecture.replace(\"torchvision/\", \"\")\n+\n+        # Import the model from torchvision\n+        if hasattr(torchvision.models, model_name):\n+            model = getattr(torchvision.models, model_name)()\n+        else:\n+            raise ValueError(f\"Unknown torchvision model: {model_name}\")\n+\n+    elif model_architecture.startswith(\"timm/\"):\n+        try:\n+            import timm\n+\n+            model_name = model_architecture.replace(\"timm/\", \"\")\n+            model = timm.create_model(model_name)\n+        except ImportError:\n+            raise ImportError(\n+                \"timm is required for TIMM models. Install with: pip install timm\"\n+            )\n+\n+    else:\n+        # For other architectures, try to import from torchvision as default\n+        try:\n+            model = getattr(torchvision.models, model_architecture)()\n+        except AttributeError:\n+            raise ValueError(f\"Unsupported model architecture: {model_architecture}\")\n+\n+    # Load the pretrained weights\n+    state_dict = torch.load(model_path, map_location=\"cpu\", weights_only=True)\nComment: The weights_only=True parameter may not be compatible with older PyTorch versions. Consider adding a try-catch block to fall back to torch.load without this parameter for backward compatibility.\n```suggestion\n    try:\n        state_dict = torch.load(model_path, map_location=\"cpu\", weights_only=True)\n    except TypeError:\n        # For backward compatibility with older PyTorch versions\n        state_dict = torch.load(model_path, map_location=\"cpu\")\n```",
    "subcategory": "solution approach",
    "category": "functional",
    "file_path": "geoai/train.py",
    "pr_number": 291,
    "repo": "geoai",
    "owner": "opengeos",
    "comment_id": 2395977283,
    "comment_created_at": "2025-10-01T21:41:37Z"
  }
]